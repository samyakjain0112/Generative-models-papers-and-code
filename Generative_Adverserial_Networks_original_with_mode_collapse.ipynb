{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Adverserial Networks_original_with_mode_collapse",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+GvMjzgXC3D0GehAyPPC3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d816a54c3de4036ae1556777826155b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_81c20c067d25486eb936bb9aef0e4259",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_500c222ba3c6465f9b104b004cd3a79e",
              "IPY_MODEL_f5059d89091b48e9804f3a6a9e382b56"
            ]
          }
        },
        "81c20c067d25486eb936bb9aef0e4259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "500c222ba3c6465f9b104b004cd3a79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_741b1f7f0140433e9f57389beebaf4a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c43c304985b047f19c8a011ee47fb7d3"
          }
        },
        "f5059d89091b48e9804f3a6a9e382b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1582f04cb1e848058c66832d5fa32b53",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:01&lt;00:00, 6481487.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cfbbb73a5ea4b979b795eeb10877848"
          }
        },
        "741b1f7f0140433e9f57389beebaf4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c43c304985b047f19c8a011ee47fb7d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1582f04cb1e848058c66832d5fa32b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cfbbb73a5ea4b979b795eeb10877848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6e0178edb144d3eb21b4213bebaaf10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4bdb25a2173946ef98c79b88833fa817",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f13c96a8a6f7412388f93126547424ec",
              "IPY_MODEL_2715632fd32741a0ae1ccc8a0b884287"
            ]
          }
        },
        "4bdb25a2173946ef98c79b88833fa817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f13c96a8a6f7412388f93126547424ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4cc43f421b84b4c82a679693fd7e643",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0da471b755914e73aa45435acc8cb94d"
          }
        },
        "2715632fd32741a0ae1ccc8a0b884287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_300fa7698ed34b369d0d8ab2305be2c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 220323.94it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c11d208e17f417fa234a5ba1aca92b8"
          }
        },
        "c4cc43f421b84b4c82a679693fd7e643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0da471b755914e73aa45435acc8cb94d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "300fa7698ed34b369d0d8ab2305be2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c11d208e17f417fa234a5ba1aca92b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ef95db252cb47bcafe5855fc6fdc4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf92ce91f4014517ad2223ffbf21bb54",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e315826d5bd485594e713a8c8ee15de",
              "IPY_MODEL_1961883bbf7348c2996503f8f7a8d5a4"
            ]
          }
        },
        "bf92ce91f4014517ad2223ffbf21bb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e315826d5bd485594e713a8c8ee15de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8396852414eb46f3a773853449ab4729",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd11c3ff4879465badf4f3c03244d8fe"
          }
        },
        "1961883bbf7348c2996503f8f7a8d5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b09a895496e0491ba5bc5028d3dfb85e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:00&lt;00:00, 2236740.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f566f126297484f9a707952037d7c11"
          }
        },
        "8396852414eb46f3a773853449ab4729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd11c3ff4879465badf4f3c03244d8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b09a895496e0491ba5bc5028d3dfb85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f566f126297484f9a707952037d7c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f49b93ab0f124c50b586703c1647cfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_868563cfeef54f32a947ee060327c1b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2bdb651e5ec848e586a5c353ae5b0e8e",
              "IPY_MODEL_0dbc20e1718947328f74a27552291887"
            ]
          }
        },
        "868563cfeef54f32a947ee060327c1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bdb651e5ec848e586a5c353ae5b0e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07cdb6adda7f41b785d2f472b8998b29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_127723de72e144e08d7d1cfbf8de73fc"
          }
        },
        "0dbc20e1718947328f74a27552291887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa8690966b98408491528c6d59132cc0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 29970.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6da27b5aec2c421781eeaa61587eebe7"
          }
        },
        "07cdb6adda7f41b785d2f472b8998b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "127723de72e144e08d7d1cfbf8de73fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa8690966b98408491528c6d59132cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6da27b5aec2c421781eeaa61587eebe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyakjain0112/Generative-models-papers-and-code/blob/master/Generative_Adverserial_Networks_original_with_mode_collapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryF6KNvfIwsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all requirements\n",
        "import torch as torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu as Relu\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pi-2xlo2im",
        "colab_type": "code",
        "outputId": "13296bdb-37b8-4936-fa12-6f0d4081d461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8d816a54c3de4036ae1556777826155b",
            "81c20c067d25486eb936bb9aef0e4259",
            "500c222ba3c6465f9b104b004cd3a79e",
            "f5059d89091b48e9804f3a6a9e382b56",
            "741b1f7f0140433e9f57389beebaf4a7",
            "c43c304985b047f19c8a011ee47fb7d3",
            "1582f04cb1e848058c66832d5fa32b53",
            "7cfbbb73a5ea4b979b795eeb10877848",
            "e6e0178edb144d3eb21b4213bebaaf10",
            "4bdb25a2173946ef98c79b88833fa817",
            "f13c96a8a6f7412388f93126547424ec",
            "2715632fd32741a0ae1ccc8a0b884287",
            "c4cc43f421b84b4c82a679693fd7e643",
            "0da471b755914e73aa45435acc8cb94d",
            "300fa7698ed34b369d0d8ab2305be2c4",
            "6c11d208e17f417fa234a5ba1aca92b8",
            "9ef95db252cb47bcafe5855fc6fdc4dd",
            "bf92ce91f4014517ad2223ffbf21bb54",
            "4e315826d5bd485594e713a8c8ee15de",
            "1961883bbf7348c2996503f8f7a8d5a4",
            "8396852414eb46f3a773853449ab4729",
            "dd11c3ff4879465badf4f3c03244d8fe",
            "b09a895496e0491ba5bc5028d3dfb85e",
            "7f566f126297484f9a707952037d7c11",
            "f49b93ab0f124c50b586703c1647cfcf",
            "868563cfeef54f32a947ee060327c1b8",
            "2bdb651e5ec848e586a5c353ae5b0e8e",
            "0dbc20e1718947328f74a27552291887",
            "07cdb6adda7f41b785d2f472b8998b29",
            "127723de72e144e08d7d1cfbf8de73fc",
            "aa8690966b98408491528c6d59132cc0",
            "6da27b5aec2c421781eeaa61587eebe7"
          ]
        }
      },
      "source": [
        "#using the inbuilt Dataset class where all data is loaded into the cpu memory and using getitem we can get the data by just passing the corresponding index4\n",
        "\n",
        "#VARIABLE=DATASET\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d816a54c3de4036ae1556777826155b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6e0178edb144d3eb21b4213bebaaf10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ef95db252cb47bcafe5855fc6fdc4dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f49b93ab0f124c50b586703c1647cfcf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuEHh_4vpfh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading all the data using Dataloaader inbuilt class it loads all the data again into the cpu memory without making a copy because it uses iter for it and even if suffle examples is true then also no cpy is \n",
        "#required it just reshuffles the order of the iteraror in the list. \n",
        "#NOTE: only when we will load the data for training in the training class then the data will be needed to be loaded in the gpu memory\n",
        "batch_size=32\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO6OQlJluQy3",
        "colab_type": "code",
        "outputId": "50a90c73-ca36-4c90-efcb-ef0203856049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_generator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=32):\n",
        "    super(model_generator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.mean=100\n",
        "    self.std=100\n",
        "    self.g_input_dim=100\n",
        "    self.g_output_dim=self.filter_size*self.filter_size\n",
        "    \n",
        "    self.fc1 = nn.Linear(self.g_input_dim, 128)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, self.g_output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,x_input,latent_space=False,latent=None,marker=False,plot=False,batcher=1):\n",
        "    #taking x_input as input for coding simplicity but not using it in any way\n",
        "    if latent_space==True or marker==True or plot==True:\n",
        "      self.x=(torch.randn([batcher,self.g_input_dim]) ).cuda()\n",
        "    else:\n",
        "      self.x=(torch.randn(self.batch_size,self.g_input_dim)).cuda()\n",
        "\n",
        "    x = F.leaky_relu(self.fc1(self.x), 0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    return torch.tanh(self.fc4(x))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atPKIhw3CY_M",
        "colab_type": "code",
        "outputId": "ce6cf41c-d580-43c8-f64a-44120b28d1a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_discriminator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=32):\n",
        "    super(model_discriminator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.d_input_dim=self.filter_size*self.filter_size\n",
        "\n",
        "    self.fc1 = nn.Linear(self.d_input_dim, 1024)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "\n",
        "  def forward(self,x,latent_space=False,latent=None,marker=False,plot=False):\n",
        "      x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "      if plot==True:\n",
        "        return x\n",
        "      if latent_space==True:\n",
        "        return x\n",
        "      if marker==True:\n",
        "        x=latent\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      return torch.sigmoid(self.fc4(x))\n",
        "\"\"\"\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x=self.layer3(x)\n",
        "    x=Relu(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.layer4(x)\n",
        "    x=Relu(x)\n",
        "    x=self.layer5(x)\n",
        "    x=torch.sigmoid(x)\n",
        "    #print(x.size())\n",
        "    #x=x.cpu().detach().numpy()\n",
        "    #x=np.array(np.amax(x,axis=1))\n",
        "    \n",
        "    \n",
        "    #print(x.size())\n",
        "\n",
        "    return x\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    if plot==True:\\n      return x\\n    if latent_space==True:\\n      return x\\n    if marker==True:\\n      x=latent\\n    x=self.layer3(x)\\n    x=Relu(x)\\n    x=torch.flatten(x,1)\\n    x=self.layer4(x)\\n    x=Relu(x)\\n    x=self.layer5(x)\\n    x=torch.sigmoid(x)\\n    #print(x.size())\\n    #x=x.cpu().detach().numpy()\\n    #x=np.array(np.amax(x,axis=1))\\n    \\n    \\n    #print(x.size())\\n\\n    return x\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zeJguY2gvG",
        "colab_type": "code",
        "outputId": "6e74df75-a46d-48c3-8f00-4bddbd7cc4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "class model_generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(model_generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class model_discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(model_discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))\n",
        "        \n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nclass model_generator(nn.Module):\\n    def __init__(self, g_input_dim, g_output_dim):\\n        super(model_generator, self).__init__()       \\n        self.fc1 = nn.Linear(g_input_dim, 256)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\\n\\n    \\n    # forward method\\n    def forward(self, x): \\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        return torch.tanh(self.fc4(x))\\n    \\nclass model_discriminator(nn.Module):\\n    def __init__(self, d_input_dim):\\n        super(model_discriminator, self).__init__()\\n        self.fc1 = nn.Linear(d_input_dim, 1024)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, 1)\\n    \\n    # forward method\\n    def forward(self, x):\\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        return torch.sigmoid(self.fc4(x))\\n        \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lMPEo6izHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#latent space visualization\n",
        "class modify_space(object):\n",
        "  def __init__(self,importer,test_gen,label=1,label2=2,num_latents=10):\n",
        "    self.batch_size=1\n",
        "    self.label=label\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.label2=label2\n",
        "    self.test_gen=test_gen\n",
        "    self.num_latents=num_latents\n",
        "    self.net_generator=importer.net_generator\n",
        "    self.net_discriminator=importer.net_discriminator\n",
        "    self.generate()\n",
        "    self.latent()\n",
        "\n",
        "\n",
        "  def generate(self):\n",
        "    w=0\n",
        "    e=0\n",
        "    print(1)\n",
        "    for (image,label) in self.test_gen:\n",
        "      if w==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label:\n",
        "          self.image=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "\n",
        "          self.label=label[i].cuda()\n",
        "          w=1\n",
        "          break\n",
        "    for (image,label) in self.test_gen:\n",
        "      if e==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label2:\n",
        "          self.image2=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "          self.label2=label[i].cuda()\n",
        "          e=1\n",
        "          break\n",
        "\n",
        "  def latent(self):\n",
        "    latent1=self.net_generator(self.image,latent_space=True)\n",
        "    latent2=self.net_generator(self.image2,latent_space=True)\n",
        "    for count in range(self.num_latents+1):\n",
        "      input_latent=(latent1*(count/(self.num_latents))+latent2*(1-count/self.num_latents))\n",
        "      output=self.net_generator(self.image2,latent=input_latent,marker=True)\n",
        "      output=output.view(1, 1,self.filter_size ,self.filter_size ) \n",
        "      save_image(output,'latent_layer1_'+str(count)+'.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4mZAqyQgp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualizing the latent space points in two dimenions using principal component analysis\n",
        "a=0\n",
        "class plotter(object):\n",
        "  def __init__(self,render):\n",
        "    self.batch_size=1\n",
        "    self.test_gen=test_gen\n",
        "    self.net_generator=render.net_generator\n",
        "    self.net_discriminator=render.net_discriminator\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.latent=[]\n",
        "    self.latent_size=256\n",
        "    self.label_array=[]\n",
        "    self.visualize()\n",
        "   \n",
        "  def visualize(self):\n",
        "    for (image,label) in self.test_gen:\n",
        "      for i in range(len(label)):\n",
        "        latent=self.net_generator(np.reshape(image[i],[self.batch_size,self.num_layers*self.filter_size*self.filter_size]).cuda(),plot=True).cpu().detach().numpy()\n",
        "        latent_x=np.reshape(latent,(self.latent_size))\n",
        "        self.latent.append(latent_x)\n",
        "        self.label_array.append(label[i].item())\n",
        "        #feat_cols = ['feature'+str(i) for i in range(latent_x.shape[1])]\n",
        "        #latent=pd.DataFrame(latent_x,columns=feat_cols)\n",
        "        #print(latent.shape)\n",
        "        #print(latent)\n",
        "\n",
        "    pca_latent = PCA(n_components=2)\n",
        "  \n",
        "    twodim_pca = pca_latent.fit_transform(np.array(self.latent))\n",
        "    colormap = np.array(['r', 'g', 'b','pink','orange','lightblue','black','brown','yellow','white'])\n",
        "    for i in range(10):\n",
        "      temp1=[]\n",
        "      temp2=[]\n",
        "      \n",
        "      for j in range(len(self.label_array)):\n",
        "\n",
        "        if self.label_array[j]==i:\n",
        "          temp1.append(twodim_pca[j][0])\n",
        "          temp2.append(twodim_pca[j][1]) \n",
        "      plt.scatter(temp1,temp2,c=colormap[i])\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN5MeSz2w6n3",
        "colab_type": "code",
        "outputId": "e2b14557-79d0-474d-8b7c-1aafc66f3856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#MAIN DO NOT TOUCH IT\n",
        "net_generator=model_generator().cuda()\n",
        "net_discriminator=model_discriminator().cuda()\n",
        "lr=0.0001\n",
        "opt_generator=torch.optim.Adam(net_generator.parameters(),lr)\n",
        "opt_discriminator=torch.optim.Adam(net_discriminator.parameters(),lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8JeXOyqY-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If we are building a class without using any functionality of any other class then object can be written it means that the class will return an object type dataset\n",
        "#However if we write like data.Datasets of Sampler then it means that it will have functionality of the superclass data.Datasets or Sampler however ultimately these super classes will also return an object.\n",
        "#so here in a way we are returning an object of the super class via the sub class.\n",
        "#BUILDING A SIMPLE AUTOENCODER WHICH JUST AIMS AT RECONSTRUCTION WITHOUT ANY STOCHASTICITY\n",
        "\n",
        "\n",
        "class training(object):\n",
        "\n",
        "  def __init__(self,epochs=20,train_g=train_gen,test_g=test_gen,generate_latent=False,plot=False,opt_generator=opt_generator,opt_discriminator=opt_discriminator):\n",
        "    self.epochs=epochs\n",
        "    self.lr=lr\n",
        "    self.train_gen=train_g\n",
        "    self.batch_size=32\n",
        "    self.test_gen=test_g\n",
        "    self.device='cuda'\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.net_generator=net_generator\n",
        "    self.net_discriminator=net_discriminator\n",
        "    self.loss=torch.nn.BCELoss()\n",
        "    self.opt_discriminator=opt_discriminator\n",
        "    self.opt_generator=opt_generator\n",
        "    self.generation_iters=1\n",
        "    self.d_iters=2\n",
        "    self.train()\n",
        "    \n",
        "    if generate_latent==True:\n",
        "      modify_space(self,test_gen)\n",
        "    if plot==True:\n",
        "      plotter(self)\n",
        "    self.test()\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    for epoch_no in range(self.epochs):\n",
        "      counter=0\n",
        "      for (image,label) in self.train_gen:\n",
        "        img=image.to(self.device)\n",
        "        lab_real=torch.from_numpy(np.ones([self.batch_size,1],np.float32)).to(self.device)\n",
        "        lab_fake=torch.from_numpy(np.zeros([self.batch_size,1],np.float32)).to(self.device)\n",
        "\n",
        "        #train_discriminator\n",
        "        #self.opt_generator.zero_grad()\n",
        "\n",
        "        self.opt_discriminator.zero_grad()\n",
        "        \"\"\"random_choice=np.random.choice([0,1])\n",
        "        if random_choice==0:\n",
        "          input_label=lab_fake\n",
        "          input_img=output_generator[:,:,:27,:27]\n",
        "          print(\"fake_image\",input_img.size())\n",
        "        else:\n",
        "          input_label=lab_real\n",
        "          input_img=img[:,:,:27,:27]\n",
        "          print(\"real_image\",input_img.size())\n",
        "          \"\"\"\n",
        "        loss_d=0\n",
        "        for i in range(self.d_iters):  \n",
        "          output_generator=self.net_generator(img).to(self.device)\n",
        "          logits_fake = self.net_discriminator(output_generator).to(self.device)\n",
        "          logits_real = self.net_discriminator(img.view(self.batch_size,self.num_layers*self.filter_size*self.filter_size)).to(self.device)\n",
        "          loss_discriminator_real = self.loss(logits_real,lab_real)\n",
        "          loss_discriminator_fake = self.loss(logits_fake,lab_fake)\n",
        "          loss_discriminator=loss_discriminator_real+loss_discriminator_fake\n",
        "          loss_discriminator.backward(retain_graph=True)\n",
        "          self.opt_discriminator.step()\n",
        "          loss_d+=loss_discriminator\n",
        "        #discriminator one iteration complete\n",
        "\n",
        "        #train_generator\n",
        "        tot_loss=0\n",
        "        for i in range(self.generation_iters):\n",
        "          img=0\n",
        "          self.opt_generator.zero_grad()\n",
        "          #self.opt_discriminator.zero_grad()\n",
        "          output_generator=self.net_generator(img).to(self.device)\n",
        "          #print(\"fake_image\",output_generator.size())\n",
        "          logit_disc = self.net_discriminator(output_generator).to(self.device)\n",
        "          loss_generator = self.loss(logit_disc,lab_real)\n",
        "          tot_loss+=loss_generator\n",
        "          loss_generator.backward()\n",
        "          self.opt_generator.step()\n",
        "        #One generator iteration completed\n",
        "        counter+=1\n",
        "        print(\"ITERATION_NO.:\",counter ,\"LOSS_Generator:\",tot_loss.item()/self.generation_iters ,\"LOSS_Discriminator:\",loss_d.item()/self.d_iters)\n",
        "      print(\"EPOCH OVER:\",epoch_no)\n",
        "   \n",
        "  def test(self):\n",
        "    count=0\n",
        "    for (image,label) in self.train_gen:\n",
        "      img=image[0].to(self.device)\n",
        "      lab=label.to(self.device)\n",
        "      outputs = self.net_generator(img).view(32,1,28,28)\n",
        "      count+=1\n",
        "      print(\"Image_no\",count)\n",
        "      if count%100==0:\n",
        "        save_image(outputs,'testing'+str(count)+'.png')\n",
        "       \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUJezNR8OFw",
        "colab_type": "code",
        "outputId": "1d918f1b-87c8-4cbe-f13e-73e9dc3c3e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True,generate_latent=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ITERATION_NO.: 631 LOSS_Generator: 4.708786964416504 LOSS_Discriminator: 0.10940658301115036\n",
            "ITERATION_NO.: 632 LOSS_Generator: 5.426333427429199 LOSS_Discriminator: 0.043165385723114014\n",
            "ITERATION_NO.: 633 LOSS_Generator: 5.109246253967285 LOSS_Discriminator: 0.2963157594203949\n",
            "ITERATION_NO.: 634 LOSS_Generator: 5.527102470397949 LOSS_Discriminator: 0.04022864252328873\n",
            "ITERATION_NO.: 635 LOSS_Generator: 5.348470687866211 LOSS_Discriminator: 0.16961106657981873\n",
            "ITERATION_NO.: 636 LOSS_Generator: 4.0196733474731445 LOSS_Discriminator: 0.2523405849933624\n",
            "ITERATION_NO.: 637 LOSS_Generator: 3.916039228439331 LOSS_Discriminator: 0.15637457370758057\n",
            "ITERATION_NO.: 638 LOSS_Generator: 3.662560224533081 LOSS_Discriminator: 0.09080859273672104\n",
            "ITERATION_NO.: 639 LOSS_Generator: 4.0719170570373535 LOSS_Discriminator: 0.15401794016361237\n",
            "ITERATION_NO.: 640 LOSS_Generator: 4.394209861755371 LOSS_Discriminator: 0.12675467133522034\n",
            "ITERATION_NO.: 641 LOSS_Generator: 4.83981990814209 LOSS_Discriminator: 0.03215078264474869\n",
            "ITERATION_NO.: 642 LOSS_Generator: 5.477919578552246 LOSS_Discriminator: 0.11316978931427002\n",
            "ITERATION_NO.: 643 LOSS_Generator: 5.651481628417969 LOSS_Discriminator: 0.1944482922554016\n",
            "ITERATION_NO.: 644 LOSS_Generator: 5.392829895019531 LOSS_Discriminator: 0.145729660987854\n",
            "ITERATION_NO.: 645 LOSS_Generator: 4.7546234130859375 LOSS_Discriminator: 0.07090497016906738\n",
            "ITERATION_NO.: 646 LOSS_Generator: 4.49546480178833 LOSS_Discriminator: 0.06489870697259903\n",
            "ITERATION_NO.: 647 LOSS_Generator: 3.9659957885742188 LOSS_Discriminator: 0.056308649480342865\n",
            "ITERATION_NO.: 648 LOSS_Generator: 4.831625461578369 LOSS_Discriminator: 0.11907193064689636\n",
            "ITERATION_NO.: 649 LOSS_Generator: 3.6826562881469727 LOSS_Discriminator: 0.1451590359210968\n",
            "ITERATION_NO.: 650 LOSS_Generator: 4.015451431274414 LOSS_Discriminator: 0.08596185594797134\n",
            "ITERATION_NO.: 651 LOSS_Generator: 4.246769905090332 LOSS_Discriminator: 0.27648985385894775\n",
            "ITERATION_NO.: 652 LOSS_Generator: 4.150023937225342 LOSS_Discriminator: 0.0786157101392746\n",
            "ITERATION_NO.: 653 LOSS_Generator: 4.871770858764648 LOSS_Discriminator: 0.15567786991596222\n",
            "ITERATION_NO.: 654 LOSS_Generator: 4.94350004196167 LOSS_Discriminator: 0.10010586678981781\n",
            "ITERATION_NO.: 655 LOSS_Generator: 5.287398815155029 LOSS_Discriminator: 0.04026716202497482\n",
            "ITERATION_NO.: 656 LOSS_Generator: 5.890146255493164 LOSS_Discriminator: 0.07724136114120483\n",
            "ITERATION_NO.: 657 LOSS_Generator: 5.584320068359375 LOSS_Discriminator: 0.3044338822364807\n",
            "ITERATION_NO.: 658 LOSS_Generator: 5.437435626983643 LOSS_Discriminator: 0.26077914237976074\n",
            "ITERATION_NO.: 659 LOSS_Generator: 4.256744384765625 LOSS_Discriminator: 0.24693578481674194\n",
            "ITERATION_NO.: 660 LOSS_Generator: 3.3970775604248047 LOSS_Discriminator: 0.14198511838912964\n",
            "ITERATION_NO.: 661 LOSS_Generator: 3.360391616821289 LOSS_Discriminator: 0.20537345111370087\n",
            "ITERATION_NO.: 662 LOSS_Generator: 4.159963607788086 LOSS_Discriminator: 0.1666443943977356\n",
            "ITERATION_NO.: 663 LOSS_Generator: 5.043731689453125 LOSS_Discriminator: 0.03834422305226326\n",
            "ITERATION_NO.: 664 LOSS_Generator: 5.6741766929626465 LOSS_Discriminator: 0.17679019272327423\n",
            "ITERATION_NO.: 665 LOSS_Generator: 5.394478797912598 LOSS_Discriminator: 0.020782504230737686\n",
            "ITERATION_NO.: 666 LOSS_Generator: 6.122095108032227 LOSS_Discriminator: 0.08720079064369202\n",
            "ITERATION_NO.: 667 LOSS_Generator: 6.715349197387695 LOSS_Discriminator: 0.07266721874475479\n",
            "ITERATION_NO.: 668 LOSS_Generator: 7.089809894561768 LOSS_Discriminator: 0.06504767388105392\n",
            "ITERATION_NO.: 669 LOSS_Generator: 6.369744300842285 LOSS_Discriminator: 0.18465910851955414\n",
            "ITERATION_NO.: 670 LOSS_Generator: 6.463318824768066 LOSS_Discriminator: 0.003671578597277403\n",
            "ITERATION_NO.: 671 LOSS_Generator: 5.09913969039917 LOSS_Discriminator: 0.439292848110199\n",
            "ITERATION_NO.: 672 LOSS_Generator: 5.142685890197754 LOSS_Discriminator: 0.05421882122755051\n",
            "ITERATION_NO.: 673 LOSS_Generator: 3.9015259742736816 LOSS_Discriminator: 0.15617969632148743\n",
            "ITERATION_NO.: 674 LOSS_Generator: 3.6987457275390625 LOSS_Discriminator: 0.24146533012390137\n",
            "ITERATION_NO.: 675 LOSS_Generator: 3.119997501373291 LOSS_Discriminator: 0.1035497784614563\n",
            "ITERATION_NO.: 676 LOSS_Generator: 4.424317836761475 LOSS_Discriminator: 0.3174114227294922\n",
            "ITERATION_NO.: 677 LOSS_Generator: 5.012112617492676 LOSS_Discriminator: 0.037519652396440506\n",
            "ITERATION_NO.: 678 LOSS_Generator: 5.508020877838135 LOSS_Discriminator: 0.18036897480487823\n",
            "ITERATION_NO.: 679 LOSS_Generator: 5.761679649353027 LOSS_Discriminator: 0.06543996930122375\n",
            "ITERATION_NO.: 680 LOSS_Generator: 6.391348838806152 LOSS_Discriminator: 0.04792115092277527\n",
            "ITERATION_NO.: 681 LOSS_Generator: 6.022463798522949 LOSS_Discriminator: 0.44581475853919983\n",
            "ITERATION_NO.: 682 LOSS_Generator: 5.4690399169921875 LOSS_Discriminator: 0.13003700971603394\n",
            "ITERATION_NO.: 683 LOSS_Generator: 4.819662094116211 LOSS_Discriminator: 0.13838036358356476\n",
            "ITERATION_NO.: 684 LOSS_Generator: 3.5042271614074707 LOSS_Discriminator: 0.12618252635002136\n",
            "ITERATION_NO.: 685 LOSS_Generator: 3.3310256004333496 LOSS_Discriminator: 0.16418662667274475\n",
            "ITERATION_NO.: 686 LOSS_Generator: 3.307420253753662 LOSS_Discriminator: 0.21409794688224792\n",
            "ITERATION_NO.: 687 LOSS_Generator: 3.5191216468811035 LOSS_Discriminator: 0.22331349551677704\n",
            "ITERATION_NO.: 688 LOSS_Generator: 4.356170654296875 LOSS_Discriminator: 0.07006983458995819\n",
            "ITERATION_NO.: 689 LOSS_Generator: 4.91062593460083 LOSS_Discriminator: 0.04897440969944\n",
            "ITERATION_NO.: 690 LOSS_Generator: 5.693557262420654 LOSS_Discriminator: 0.09436430037021637\n",
            "ITERATION_NO.: 691 LOSS_Generator: 5.633603096008301 LOSS_Discriminator: 0.027482159435749054\n",
            "ITERATION_NO.: 692 LOSS_Generator: 6.427986145019531 LOSS_Discriminator: 0.19870972633361816\n",
            "ITERATION_NO.: 693 LOSS_Generator: 5.746648788452148 LOSS_Discriminator: 0.21387705206871033\n",
            "ITERATION_NO.: 694 LOSS_Generator: 5.129493713378906 LOSS_Discriminator: 0.062476739287376404\n",
            "ITERATION_NO.: 695 LOSS_Generator: 4.110935211181641 LOSS_Discriminator: 0.08549469709396362\n",
            "ITERATION_NO.: 696 LOSS_Generator: 4.42252254486084 LOSS_Discriminator: 0.09116008877754211\n",
            "ITERATION_NO.: 697 LOSS_Generator: 4.047277450561523 LOSS_Discriminator: 0.06069771945476532\n",
            "ITERATION_NO.: 698 LOSS_Generator: 3.667682647705078 LOSS_Discriminator: 0.13309702277183533\n",
            "ITERATION_NO.: 699 LOSS_Generator: 4.517521858215332 LOSS_Discriminator: 0.08478735387325287\n",
            "ITERATION_NO.: 700 LOSS_Generator: 4.8520588874816895 LOSS_Discriminator: 0.14102396368980408\n",
            "ITERATION_NO.: 701 LOSS_Generator: 4.751679420471191 LOSS_Discriminator: 0.043145328760147095\n",
            "ITERATION_NO.: 702 LOSS_Generator: 4.9910888671875 LOSS_Discriminator: 0.024263553321361542\n",
            "ITERATION_NO.: 703 LOSS_Generator: 5.3426289558410645 LOSS_Discriminator: 0.23113703727722168\n",
            "ITERATION_NO.: 704 LOSS_Generator: 5.19073486328125 LOSS_Discriminator: 0.14391177892684937\n",
            "ITERATION_NO.: 705 LOSS_Generator: 5.133998870849609 LOSS_Discriminator: 0.11674438416957855\n",
            "ITERATION_NO.: 706 LOSS_Generator: 4.433629989624023 LOSS_Discriminator: 0.16453120112419128\n",
            "ITERATION_NO.: 707 LOSS_Generator: 3.9180245399475098 LOSS_Discriminator: 0.2220584601163864\n",
            "ITERATION_NO.: 708 LOSS_Generator: 4.193930625915527 LOSS_Discriminator: 0.11951538920402527\n",
            "ITERATION_NO.: 709 LOSS_Generator: 3.702834129333496 LOSS_Discriminator: 0.114705890417099\n",
            "ITERATION_NO.: 710 LOSS_Generator: 4.004770278930664 LOSS_Discriminator: 0.10117977857589722\n",
            "ITERATION_NO.: 711 LOSS_Generator: 4.456632614135742 LOSS_Discriminator: 0.10499247908592224\n",
            "ITERATION_NO.: 712 LOSS_Generator: 4.641367435455322 LOSS_Discriminator: 0.07061238586902618\n",
            "ITERATION_NO.: 713 LOSS_Generator: 5.481253147125244 LOSS_Discriminator: 0.194149911403656\n",
            "ITERATION_NO.: 714 LOSS_Generator: 4.877937316894531 LOSS_Discriminator: 0.21515846252441406\n",
            "ITERATION_NO.: 715 LOSS_Generator: 4.586205959320068 LOSS_Discriminator: 0.1135065108537674\n",
            "ITERATION_NO.: 716 LOSS_Generator: 3.6904234886169434 LOSS_Discriminator: 0.12260524183511734\n",
            "ITERATION_NO.: 717 LOSS_Generator: 3.940624713897705 LOSS_Discriminator: 0.12659434974193573\n",
            "ITERATION_NO.: 718 LOSS_Generator: 4.065835952758789 LOSS_Discriminator: 0.08693325519561768\n",
            "ITERATION_NO.: 719 LOSS_Generator: 4.586472511291504 LOSS_Discriminator: 0.12238557636737823\n",
            "ITERATION_NO.: 720 LOSS_Generator: 5.156528949737549 LOSS_Discriminator: 0.06232094764709473\n",
            "ITERATION_NO.: 721 LOSS_Generator: 6.226631164550781 LOSS_Discriminator: 0.04223056882619858\n",
            "ITERATION_NO.: 722 LOSS_Generator: 5.878600120544434 LOSS_Discriminator: 0.29470139741897583\n",
            "ITERATION_NO.: 723 LOSS_Generator: 5.6220784187316895 LOSS_Discriminator: 0.06915886700153351\n",
            "ITERATION_NO.: 724 LOSS_Generator: 4.8636250495910645 LOSS_Discriminator: 0.15258274972438812\n",
            "ITERATION_NO.: 725 LOSS_Generator: 4.1376495361328125 LOSS_Discriminator: 0.17481626570224762\n",
            "ITERATION_NO.: 726 LOSS_Generator: 3.4701216220855713 LOSS_Discriminator: 0.23205003142356873\n",
            "ITERATION_NO.: 727 LOSS_Generator: 4.301952362060547 LOSS_Discriminator: 0.19297468662261963\n",
            "ITERATION_NO.: 728 LOSS_Generator: 4.779644012451172 LOSS_Discriminator: 0.2606757879257202\n",
            "ITERATION_NO.: 729 LOSS_Generator: 4.904177665710449 LOSS_Discriminator: 0.10055162012577057\n",
            "ITERATION_NO.: 730 LOSS_Generator: 6.084506034851074 LOSS_Discriminator: 0.00872091855853796\n",
            "ITERATION_NO.: 731 LOSS_Generator: 7.103092670440674 LOSS_Discriminator: 0.0061482274904847145\n",
            "ITERATION_NO.: 732 LOSS_Generator: 7.096193790435791 LOSS_Discriminator: 0.2694152593612671\n",
            "ITERATION_NO.: 733 LOSS_Generator: 6.027157306671143 LOSS_Discriminator: 0.1686815619468689\n",
            "ITERATION_NO.: 734 LOSS_Generator: 6.04449462890625 LOSS_Discriminator: 0.15356680750846863\n",
            "ITERATION_NO.: 735 LOSS_Generator: 5.239382743835449 LOSS_Discriminator: 0.0708978995680809\n",
            "ITERATION_NO.: 736 LOSS_Generator: 3.6658382415771484 LOSS_Discriminator: 0.1391114890575409\n",
            "ITERATION_NO.: 737 LOSS_Generator: 3.935213565826416 LOSS_Discriminator: 0.11456453055143356\n",
            "ITERATION_NO.: 738 LOSS_Generator: 4.20240592956543 LOSS_Discriminator: 0.10108613967895508\n",
            "ITERATION_NO.: 739 LOSS_Generator: 4.367847442626953 LOSS_Discriminator: 0.07865789532661438\n",
            "ITERATION_NO.: 740 LOSS_Generator: 5.341484069824219 LOSS_Discriminator: 0.06242797151207924\n",
            "ITERATION_NO.: 741 LOSS_Generator: 5.865354537963867 LOSS_Discriminator: 0.15607382357120514\n",
            "ITERATION_NO.: 742 LOSS_Generator: 5.654092311859131 LOSS_Discriminator: 0.07171617448329926\n",
            "ITERATION_NO.: 743 LOSS_Generator: 5.842371463775635 LOSS_Discriminator: 0.03249109536409378\n",
            "ITERATION_NO.: 744 LOSS_Generator: 5.977433204650879 LOSS_Discriminator: 0.09888137131929398\n",
            "ITERATION_NO.: 745 LOSS_Generator: 5.707648277282715 LOSS_Discriminator: 0.0608825720846653\n",
            "ITERATION_NO.: 746 LOSS_Generator: 4.975187301635742 LOSS_Discriminator: 0.09144748747348785\n",
            "ITERATION_NO.: 747 LOSS_Generator: 4.749527454376221 LOSS_Discriminator: 0.03304672613739967\n",
            "ITERATION_NO.: 748 LOSS_Generator: 4.288036346435547 LOSS_Discriminator: 0.11934476345777512\n",
            "ITERATION_NO.: 749 LOSS_Generator: 4.206518173217773 LOSS_Discriminator: 0.08773398399353027\n",
            "ITERATION_NO.: 750 LOSS_Generator: 4.875851154327393 LOSS_Discriminator: 0.05671362206339836\n",
            "ITERATION_NO.: 751 LOSS_Generator: 5.465391159057617 LOSS_Discriminator: 0.09271662682294846\n",
            "ITERATION_NO.: 752 LOSS_Generator: 5.870544910430908 LOSS_Discriminator: 0.05705874413251877\n",
            "ITERATION_NO.: 753 LOSS_Generator: 5.597607612609863 LOSS_Discriminator: 0.10004574060440063\n",
            "ITERATION_NO.: 754 LOSS_Generator: 5.568397521972656 LOSS_Discriminator: 0.03222672641277313\n",
            "ITERATION_NO.: 755 LOSS_Generator: 5.322027683258057 LOSS_Discriminator: 0.2059338390827179\n",
            "ITERATION_NO.: 756 LOSS_Generator: 5.251910209655762 LOSS_Discriminator: 0.12094807624816895\n",
            "ITERATION_NO.: 757 LOSS_Generator: 4.750536918640137 LOSS_Discriminator: 0.14573884010314941\n",
            "ITERATION_NO.: 758 LOSS_Generator: 3.9426019191741943 LOSS_Discriminator: 0.11513657867908478\n",
            "ITERATION_NO.: 759 LOSS_Generator: 4.0205278396606445 LOSS_Discriminator: 0.13626539707183838\n",
            "ITERATION_NO.: 760 LOSS_Generator: 4.1374406814575195 LOSS_Discriminator: 0.053262852132320404\n",
            "ITERATION_NO.: 761 LOSS_Generator: 5.3446760177612305 LOSS_Discriminator: 0.2053447961807251\n",
            "ITERATION_NO.: 762 LOSS_Generator: 5.327893257141113 LOSS_Discriminator: 0.13949477672576904\n",
            "ITERATION_NO.: 763 LOSS_Generator: 4.955192565917969 LOSS_Discriminator: 0.2208944857120514\n",
            "ITERATION_NO.: 764 LOSS_Generator: 5.3224358558654785 LOSS_Discriminator: 0.04015593230724335\n",
            "ITERATION_NO.: 765 LOSS_Generator: 4.597382068634033 LOSS_Discriminator: 0.15832111239433289\n",
            "ITERATION_NO.: 766 LOSS_Generator: 4.52272891998291 LOSS_Discriminator: 0.10748283565044403\n",
            "ITERATION_NO.: 767 LOSS_Generator: 4.373147964477539 LOSS_Discriminator: 0.06742189824581146\n",
            "ITERATION_NO.: 768 LOSS_Generator: 5.013148307800293 LOSS_Discriminator: 0.11515890061855316\n",
            "ITERATION_NO.: 769 LOSS_Generator: 4.732161998748779 LOSS_Discriminator: 0.08046473562717438\n",
            "ITERATION_NO.: 770 LOSS_Generator: 5.235048294067383 LOSS_Discriminator: 0.11799800395965576\n",
            "ITERATION_NO.: 771 LOSS_Generator: 5.155328750610352 LOSS_Discriminator: 0.09814555197954178\n",
            "ITERATION_NO.: 772 LOSS_Generator: 5.545933723449707 LOSS_Discriminator: 0.03220903128385544\n",
            "ITERATION_NO.: 773 LOSS_Generator: 4.666744232177734 LOSS_Discriminator: 0.06899209320545197\n",
            "ITERATION_NO.: 774 LOSS_Generator: 4.583002090454102 LOSS_Discriminator: 0.03061160445213318\n",
            "ITERATION_NO.: 775 LOSS_Generator: 5.179050445556641 LOSS_Discriminator: 0.08191613852977753\n",
            "ITERATION_NO.: 776 LOSS_Generator: 4.541365623474121 LOSS_Discriminator: 0.17450293898582458\n",
            "ITERATION_NO.: 777 LOSS_Generator: 4.791937828063965 LOSS_Discriminator: 0.18006396293640137\n",
            "ITERATION_NO.: 778 LOSS_Generator: 4.005440711975098 LOSS_Discriminator: 0.2123463898897171\n",
            "ITERATION_NO.: 779 LOSS_Generator: 4.244472503662109 LOSS_Discriminator: 0.15616129338741302\n",
            "ITERATION_NO.: 780 LOSS_Generator: 4.836129188537598 LOSS_Discriminator: 0.14527615904808044\n",
            "ITERATION_NO.: 781 LOSS_Generator: 4.92158842086792 LOSS_Discriminator: 0.14019864797592163\n",
            "ITERATION_NO.: 782 LOSS_Generator: 5.814770698547363 LOSS_Discriminator: 0.11398664861917496\n",
            "ITERATION_NO.: 783 LOSS_Generator: 5.787078857421875 LOSS_Discriminator: 0.04020722210407257\n",
            "ITERATION_NO.: 784 LOSS_Generator: 5.546548843383789 LOSS_Discriminator: 0.004902224522083998\n",
            "ITERATION_NO.: 785 LOSS_Generator: 5.97109842300415 LOSS_Discriminator: 0.1320847123861313\n",
            "ITERATION_NO.: 786 LOSS_Generator: 6.554804801940918 LOSS_Discriminator: 0.27649176120758057\n",
            "ITERATION_NO.: 787 LOSS_Generator: 5.916703224182129 LOSS_Discriminator: 0.08803664892911911\n",
            "ITERATION_NO.: 788 LOSS_Generator: 5.492612838745117 LOSS_Discriminator: 0.14539799094200134\n",
            "ITERATION_NO.: 789 LOSS_Generator: 4.675772666931152 LOSS_Discriminator: 0.1366116851568222\n",
            "ITERATION_NO.: 790 LOSS_Generator: 3.6259522438049316 LOSS_Discriminator: 0.23004022240638733\n",
            "ITERATION_NO.: 791 LOSS_Generator: 3.875415325164795 LOSS_Discriminator: 0.14514684677124023\n",
            "ITERATION_NO.: 792 LOSS_Generator: 4.049359321594238 LOSS_Discriminator: 0.32940369844436646\n",
            "ITERATION_NO.: 793 LOSS_Generator: 5.367927551269531 LOSS_Discriminator: 0.1594482660293579\n",
            "ITERATION_NO.: 794 LOSS_Generator: 6.168299674987793 LOSS_Discriminator: 0.13929404318332672\n",
            "ITERATION_NO.: 795 LOSS_Generator: 6.3138041496276855 LOSS_Discriminator: 0.15761111676692963\n",
            "ITERATION_NO.: 796 LOSS_Generator: 6.734170436859131 LOSS_Discriminator: 0.18401241302490234\n",
            "ITERATION_NO.: 797 LOSS_Generator: 6.0392327308654785 LOSS_Discriminator: 0.15960067510604858\n",
            "ITERATION_NO.: 798 LOSS_Generator: 5.348781585693359 LOSS_Discriminator: 0.19541430473327637\n",
            "ITERATION_NO.: 799 LOSS_Generator: 5.385560035705566 LOSS_Discriminator: 0.01790493167936802\n",
            "ITERATION_NO.: 800 LOSS_Generator: 4.205691337585449 LOSS_Discriminator: 0.24002957344055176\n",
            "ITERATION_NO.: 801 LOSS_Generator: 4.611418724060059 LOSS_Discriminator: 0.1626931130886078\n",
            "ITERATION_NO.: 802 LOSS_Generator: 4.598412036895752 LOSS_Discriminator: 0.1815033257007599\n",
            "ITERATION_NO.: 803 LOSS_Generator: 5.189311981201172 LOSS_Discriminator: 0.05901457369327545\n",
            "ITERATION_NO.: 804 LOSS_Generator: 5.808404922485352 LOSS_Discriminator: 0.22135034203529358\n",
            "ITERATION_NO.: 805 LOSS_Generator: 5.242656707763672 LOSS_Discriminator: 0.03918742761015892\n",
            "ITERATION_NO.: 806 LOSS_Generator: 4.989077568054199 LOSS_Discriminator: 0.0842704027891159\n",
            "ITERATION_NO.: 807 LOSS_Generator: 4.728569507598877 LOSS_Discriminator: 0.14693254232406616\n",
            "ITERATION_NO.: 808 LOSS_Generator: 4.836434841156006 LOSS_Discriminator: 0.08097274601459503\n",
            "ITERATION_NO.: 809 LOSS_Generator: 4.454278945922852 LOSS_Discriminator: 0.1418396532535553\n",
            "ITERATION_NO.: 810 LOSS_Generator: 4.9107255935668945 LOSS_Discriminator: 0.10966858267784119\n",
            "ITERATION_NO.: 811 LOSS_Generator: 5.068630218505859 LOSS_Discriminator: 0.1411081701517105\n",
            "ITERATION_NO.: 812 LOSS_Generator: 5.621743202209473 LOSS_Discriminator: 0.15701472759246826\n",
            "ITERATION_NO.: 813 LOSS_Generator: 5.638773441314697 LOSS_Discriminator: 0.07558681070804596\n",
            "ITERATION_NO.: 814 LOSS_Generator: 5.742621421813965 LOSS_Discriminator: 0.0217742957174778\n",
            "ITERATION_NO.: 815 LOSS_Generator: 5.191502094268799 LOSS_Discriminator: 0.17106381058692932\n",
            "ITERATION_NO.: 816 LOSS_Generator: 5.545657157897949 LOSS_Discriminator: 0.18515200912952423\n",
            "ITERATION_NO.: 817 LOSS_Generator: 4.972465515136719 LOSS_Discriminator: 0.09734416753053665\n",
            "ITERATION_NO.: 818 LOSS_Generator: 4.613412857055664 LOSS_Discriminator: 0.20305992662906647\n",
            "ITERATION_NO.: 819 LOSS_Generator: 3.989992618560791 LOSS_Discriminator: 0.14436770975589752\n",
            "ITERATION_NO.: 820 LOSS_Generator: 4.340064525604248 LOSS_Discriminator: 0.1545005738735199\n",
            "ITERATION_NO.: 821 LOSS_Generator: 4.488846778869629 LOSS_Discriminator: 0.09187625348567963\n",
            "ITERATION_NO.: 822 LOSS_Generator: 5.263430595397949 LOSS_Discriminator: 0.16831433773040771\n",
            "ITERATION_NO.: 823 LOSS_Generator: 5.398096561431885 LOSS_Discriminator: 0.21194526553153992\n",
            "ITERATION_NO.: 824 LOSS_Generator: 5.419576644897461 LOSS_Discriminator: 0.18109342455863953\n",
            "ITERATION_NO.: 825 LOSS_Generator: 5.145588397979736 LOSS_Discriminator: 0.1394067108631134\n",
            "ITERATION_NO.: 826 LOSS_Generator: 4.117549896240234 LOSS_Discriminator: 0.1026042029261589\n",
            "ITERATION_NO.: 827 LOSS_Generator: 4.004697799682617 LOSS_Discriminator: 0.17063042521476746\n",
            "ITERATION_NO.: 828 LOSS_Generator: 3.833109140396118 LOSS_Discriminator: 0.10785475373268127\n",
            "ITERATION_NO.: 829 LOSS_Generator: 4.937430381774902 LOSS_Discriminator: 0.07039142400026321\n",
            "ITERATION_NO.: 830 LOSS_Generator: 5.343755722045898 LOSS_Discriminator: 0.1725997030735016\n",
            "ITERATION_NO.: 831 LOSS_Generator: 6.107615947723389 LOSS_Discriminator: 0.18311908841133118\n",
            "ITERATION_NO.: 832 LOSS_Generator: 6.496479511260986 LOSS_Discriminator: 0.13646796345710754\n",
            "ITERATION_NO.: 833 LOSS_Generator: 6.191240310668945 LOSS_Discriminator: 0.01467851735651493\n",
            "ITERATION_NO.: 834 LOSS_Generator: 5.761507034301758 LOSS_Discriminator: 0.16634640097618103\n",
            "ITERATION_NO.: 835 LOSS_Generator: 5.295755386352539 LOSS_Discriminator: 0.04411637783050537\n",
            "ITERATION_NO.: 836 LOSS_Generator: 4.885621070861816 LOSS_Discriminator: 0.0186808742582798\n",
            "ITERATION_NO.: 837 LOSS_Generator: 4.673714637756348 LOSS_Discriminator: 0.05271323770284653\n",
            "ITERATION_NO.: 838 LOSS_Generator: 4.964687347412109 LOSS_Discriminator: 0.10094862431287766\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.830076694488525 LOSS_Discriminator: 0.05923726409673691\n",
            "ITERATION_NO.: 840 LOSS_Generator: 4.349485397338867 LOSS_Discriminator: 0.1214538961648941\n",
            "ITERATION_NO.: 841 LOSS_Generator: 5.070951461791992 LOSS_Discriminator: 0.04053938388824463\n",
            "ITERATION_NO.: 842 LOSS_Generator: 5.00468635559082 LOSS_Discriminator: 0.18143969774246216\n",
            "ITERATION_NO.: 843 LOSS_Generator: 4.891353607177734 LOSS_Discriminator: 0.018361346796154976\n",
            "ITERATION_NO.: 844 LOSS_Generator: 5.112079620361328 LOSS_Discriminator: 0.12729573249816895\n",
            "ITERATION_NO.: 845 LOSS_Generator: 5.249079704284668 LOSS_Discriminator: 0.03663303330540657\n",
            "ITERATION_NO.: 846 LOSS_Generator: 5.242708206176758 LOSS_Discriminator: 0.08656633645296097\n",
            "ITERATION_NO.: 847 LOSS_Generator: 5.57130241394043 LOSS_Discriminator: 0.019701968878507614\n",
            "ITERATION_NO.: 848 LOSS_Generator: 5.624294757843018 LOSS_Discriminator: 0.09915757179260254\n",
            "ITERATION_NO.: 849 LOSS_Generator: 5.486728191375732 LOSS_Discriminator: 0.34849533438682556\n",
            "ITERATION_NO.: 850 LOSS_Generator: 4.419896125793457 LOSS_Discriminator: 0.07934460043907166\n",
            "ITERATION_NO.: 851 LOSS_Generator: 4.228078842163086 LOSS_Discriminator: 0.07651612162590027\n",
            "ITERATION_NO.: 852 LOSS_Generator: 4.0427937507629395 LOSS_Discriminator: 0.13501901924610138\n",
            "ITERATION_NO.: 853 LOSS_Generator: 4.425653457641602 LOSS_Discriminator: 0.23555988073349\n",
            "ITERATION_NO.: 854 LOSS_Generator: 4.182436466217041 LOSS_Discriminator: 0.10034694522619247\n",
            "ITERATION_NO.: 855 LOSS_Generator: 4.526633262634277 LOSS_Discriminator: 0.24864816665649414\n",
            "ITERATION_NO.: 856 LOSS_Generator: 4.425569534301758 LOSS_Discriminator: 0.20411846041679382\n",
            "ITERATION_NO.: 857 LOSS_Generator: 4.517220973968506 LOSS_Discriminator: 0.11903800070285797\n",
            "ITERATION_NO.: 858 LOSS_Generator: 5.360788345336914 LOSS_Discriminator: 0.14084365963935852\n",
            "ITERATION_NO.: 859 LOSS_Generator: 5.418673038482666 LOSS_Discriminator: 0.14017228782176971\n",
            "ITERATION_NO.: 860 LOSS_Generator: 6.005696773529053 LOSS_Discriminator: 0.1311514973640442\n",
            "ITERATION_NO.: 861 LOSS_Generator: 4.941321849822998 LOSS_Discriminator: 0.09518542140722275\n",
            "ITERATION_NO.: 862 LOSS_Generator: 5.102476119995117 LOSS_Discriminator: 0.5077233910560608\n",
            "ITERATION_NO.: 863 LOSS_Generator: 4.557947158813477 LOSS_Discriminator: 0.023540103808045387\n",
            "ITERATION_NO.: 864 LOSS_Generator: 4.801918029785156 LOSS_Discriminator: 0.16647598147392273\n",
            "ITERATION_NO.: 865 LOSS_Generator: 4.779068470001221 LOSS_Discriminator: 0.25184452533721924\n",
            "ITERATION_NO.: 866 LOSS_Generator: 5.443641662597656 LOSS_Discriminator: 0.07518143951892853\n",
            "ITERATION_NO.: 867 LOSS_Generator: 5.6504034996032715 LOSS_Discriminator: 0.19722670316696167\n",
            "ITERATION_NO.: 868 LOSS_Generator: 5.601803779602051 LOSS_Discriminator: 0.24850916862487793\n",
            "ITERATION_NO.: 869 LOSS_Generator: 5.018732070922852 LOSS_Discriminator: 0.1262393295764923\n",
            "ITERATION_NO.: 870 LOSS_Generator: 4.675718307495117 LOSS_Discriminator: 0.2612072229385376\n",
            "ITERATION_NO.: 871 LOSS_Generator: 4.332062244415283 LOSS_Discriminator: 0.20479610562324524\n",
            "ITERATION_NO.: 872 LOSS_Generator: 4.245408058166504 LOSS_Discriminator: 0.25395792722702026\n",
            "ITERATION_NO.: 873 LOSS_Generator: 4.105499744415283 LOSS_Discriminator: 0.0912661999464035\n",
            "ITERATION_NO.: 874 LOSS_Generator: 4.874666213989258 LOSS_Discriminator: 0.21416723728179932\n",
            "ITERATION_NO.: 875 LOSS_Generator: 5.350667953491211 LOSS_Discriminator: 0.12758180499076843\n",
            "ITERATION_NO.: 876 LOSS_Generator: 5.670248031616211 LOSS_Discriminator: 0.02370934933423996\n",
            "ITERATION_NO.: 877 LOSS_Generator: 5.68623161315918 LOSS_Discriminator: 0.3788495659828186\n",
            "ITERATION_NO.: 878 LOSS_Generator: 6.23058557510376 LOSS_Discriminator: 0.21201173961162567\n",
            "ITERATION_NO.: 879 LOSS_Generator: 6.799843788146973 LOSS_Discriminator: 0.05707991123199463\n",
            "ITERATION_NO.: 880 LOSS_Generator: 5.782815456390381 LOSS_Discriminator: 0.40546709299087524\n",
            "ITERATION_NO.: 881 LOSS_Generator: 4.889486312866211 LOSS_Discriminator: 0.3014412522315979\n",
            "ITERATION_NO.: 882 LOSS_Generator: 4.000704288482666 LOSS_Discriminator: 0.14082548022270203\n",
            "ITERATION_NO.: 883 LOSS_Generator: 3.6257898807525635 LOSS_Discriminator: 0.1486271768808365\n",
            "ITERATION_NO.: 884 LOSS_Generator: 3.508681297302246 LOSS_Discriminator: 0.18164779245853424\n",
            "ITERATION_NO.: 885 LOSS_Generator: 4.4684062004089355 LOSS_Discriminator: 0.21834509074687958\n",
            "ITERATION_NO.: 886 LOSS_Generator: 3.7986912727355957 LOSS_Discriminator: 0.17751172184944153\n",
            "ITERATION_NO.: 887 LOSS_Generator: 5.152827262878418 LOSS_Discriminator: 0.15770500898361206\n",
            "ITERATION_NO.: 888 LOSS_Generator: 4.734127998352051 LOSS_Discriminator: 0.2121964991092682\n",
            "ITERATION_NO.: 889 LOSS_Generator: 5.395016670227051 LOSS_Discriminator: 0.06103554368019104\n",
            "ITERATION_NO.: 890 LOSS_Generator: 5.724278450012207 LOSS_Discriminator: 0.03655451536178589\n",
            "ITERATION_NO.: 891 LOSS_Generator: 4.838814735412598 LOSS_Discriminator: 0.33695393800735474\n",
            "ITERATION_NO.: 892 LOSS_Generator: 4.242572784423828 LOSS_Discriminator: 0.17241132259368896\n",
            "ITERATION_NO.: 893 LOSS_Generator: 4.213771820068359 LOSS_Discriminator: 0.06245899945497513\n",
            "ITERATION_NO.: 894 LOSS_Generator: 4.023899078369141 LOSS_Discriminator: 0.11952043324708939\n",
            "ITERATION_NO.: 895 LOSS_Generator: 4.115645408630371 LOSS_Discriminator: 0.09674004465341568\n",
            "ITERATION_NO.: 896 LOSS_Generator: 5.0846710205078125 LOSS_Discriminator: 0.11298508942127228\n",
            "ITERATION_NO.: 897 LOSS_Generator: 5.219127655029297 LOSS_Discriminator: 0.1381022185087204\n",
            "ITERATION_NO.: 898 LOSS_Generator: 5.414492130279541 LOSS_Discriminator: 0.2523227334022522\n",
            "ITERATION_NO.: 899 LOSS_Generator: 4.968808174133301 LOSS_Discriminator: 0.2967498004436493\n",
            "ITERATION_NO.: 900 LOSS_Generator: 4.729586601257324 LOSS_Discriminator: 0.04422473907470703\n",
            "ITERATION_NO.: 901 LOSS_Generator: 4.612593650817871 LOSS_Discriminator: 0.2811174988746643\n",
            "ITERATION_NO.: 902 LOSS_Generator: 4.219101905822754 LOSS_Discriminator: 0.18850016593933105\n",
            "ITERATION_NO.: 903 LOSS_Generator: 3.9681572914123535 LOSS_Discriminator: 0.18161360919475555\n",
            "ITERATION_NO.: 904 LOSS_Generator: 4.120460510253906 LOSS_Discriminator: 0.1557956337928772\n",
            "ITERATION_NO.: 905 LOSS_Generator: 4.535168647766113 LOSS_Discriminator: 0.11410604417324066\n",
            "ITERATION_NO.: 906 LOSS_Generator: 4.312506675720215 LOSS_Discriminator: 0.14378772675991058\n",
            "ITERATION_NO.: 907 LOSS_Generator: 4.4808197021484375 LOSS_Discriminator: 0.11910748481750488\n",
            "ITERATION_NO.: 908 LOSS_Generator: 4.783272743225098 LOSS_Discriminator: 0.08808202296495438\n",
            "ITERATION_NO.: 909 LOSS_Generator: 4.973111629486084 LOSS_Discriminator: 0.07755335420370102\n",
            "ITERATION_NO.: 910 LOSS_Generator: 5.253750801086426 LOSS_Discriminator: 0.127017542719841\n",
            "ITERATION_NO.: 911 LOSS_Generator: 5.325181007385254 LOSS_Discriminator: 0.09119637310504913\n",
            "ITERATION_NO.: 912 LOSS_Generator: 4.925097942352295 LOSS_Discriminator: 0.3302459716796875\n",
            "ITERATION_NO.: 913 LOSS_Generator: 4.138615608215332 LOSS_Discriminator: 0.15664127469062805\n",
            "ITERATION_NO.: 914 LOSS_Generator: 4.187104225158691 LOSS_Discriminator: 0.12422258406877518\n",
            "ITERATION_NO.: 915 LOSS_Generator: 4.034914016723633 LOSS_Discriminator: 0.10102032124996185\n",
            "ITERATION_NO.: 916 LOSS_Generator: 3.2896454334259033 LOSS_Discriminator: 0.1304202377796173\n",
            "ITERATION_NO.: 917 LOSS_Generator: 3.939638376235962 LOSS_Discriminator: 0.12863190472126007\n",
            "ITERATION_NO.: 918 LOSS_Generator: 4.174325466156006 LOSS_Discriminator: 0.09894224256277084\n",
            "ITERATION_NO.: 919 LOSS_Generator: 4.869841575622559 LOSS_Discriminator: 0.11400090903043747\n",
            "ITERATION_NO.: 920 LOSS_Generator: 4.594514846801758 LOSS_Discriminator: 0.052835848182439804\n",
            "ITERATION_NO.: 921 LOSS_Generator: 4.840671539306641 LOSS_Discriminator: 0.10345590859651566\n",
            "ITERATION_NO.: 922 LOSS_Generator: 4.59830904006958 LOSS_Discriminator: 0.16841217875480652\n",
            "ITERATION_NO.: 923 LOSS_Generator: 4.510223388671875 LOSS_Discriminator: 0.1641731858253479\n",
            "ITERATION_NO.: 924 LOSS_Generator: 4.439064979553223 LOSS_Discriminator: 0.06780561059713364\n",
            "ITERATION_NO.: 925 LOSS_Generator: 4.647500514984131 LOSS_Discriminator: 0.06254574656486511\n",
            "ITERATION_NO.: 926 LOSS_Generator: 4.914992809295654 LOSS_Discriminator: 0.0913241058588028\n",
            "ITERATION_NO.: 927 LOSS_Generator: 4.262351036071777 LOSS_Discriminator: 0.07264640927314758\n",
            "ITERATION_NO.: 928 LOSS_Generator: 5.006315231323242 LOSS_Discriminator: 0.037176039069890976\n",
            "ITERATION_NO.: 929 LOSS_Generator: 4.757997035980225 LOSS_Discriminator: 0.1015971451997757\n",
            "ITERATION_NO.: 930 LOSS_Generator: 6.044605255126953 LOSS_Discriminator: 0.01905876025557518\n",
            "ITERATION_NO.: 931 LOSS_Generator: 5.390573978424072 LOSS_Discriminator: 0.10791153460741043\n",
            "ITERATION_NO.: 932 LOSS_Generator: 6.163022518157959 LOSS_Discriminator: 0.18294182419776917\n",
            "ITERATION_NO.: 933 LOSS_Generator: 4.5636420249938965 LOSS_Discriminator: 0.13903668522834778\n",
            "ITERATION_NO.: 934 LOSS_Generator: 4.66500997543335 LOSS_Discriminator: 0.08591412007808685\n",
            "ITERATION_NO.: 935 LOSS_Generator: 3.0107228755950928 LOSS_Discriminator: 0.25651025772094727\n",
            "ITERATION_NO.: 936 LOSS_Generator: 3.6388168334960938 LOSS_Discriminator: 0.1383739411830902\n",
            "ITERATION_NO.: 937 LOSS_Generator: 3.8409628868103027 LOSS_Discriminator: 0.26111680269241333\n",
            "ITERATION_NO.: 938 LOSS_Generator: 4.727375030517578 LOSS_Discriminator: 0.31391823291778564\n",
            "ITERATION_NO.: 939 LOSS_Generator: 4.9364423751831055 LOSS_Discriminator: 0.08950242400169373\n",
            "ITERATION_NO.: 940 LOSS_Generator: 5.634642601013184 LOSS_Discriminator: 0.07657270133495331\n",
            "ITERATION_NO.: 941 LOSS_Generator: 5.950366973876953 LOSS_Discriminator: 0.1912187784910202\n",
            "ITERATION_NO.: 942 LOSS_Generator: 5.125567436218262 LOSS_Discriminator: 0.11184689402580261\n",
            "ITERATION_NO.: 943 LOSS_Generator: 5.44157600402832 LOSS_Discriminator: 0.028679603710770607\n",
            "ITERATION_NO.: 944 LOSS_Generator: 5.082955360412598 LOSS_Discriminator: 0.02777298353612423\n",
            "ITERATION_NO.: 945 LOSS_Generator: 5.951905250549316 LOSS_Discriminator: 0.04877915233373642\n",
            "ITERATION_NO.: 946 LOSS_Generator: 5.702169418334961 LOSS_Discriminator: 0.038030751049518585\n",
            "ITERATION_NO.: 947 LOSS_Generator: 6.2000346183776855 LOSS_Discriminator: 0.04989499971270561\n",
            "ITERATION_NO.: 948 LOSS_Generator: 6.614116668701172 LOSS_Discriminator: 0.0650099590420723\n",
            "ITERATION_NO.: 949 LOSS_Generator: 6.514208793640137 LOSS_Discriminator: 0.06729056686162949\n",
            "ITERATION_NO.: 950 LOSS_Generator: 6.036563873291016 LOSS_Discriminator: 0.13624954223632812\n",
            "ITERATION_NO.: 951 LOSS_Generator: 5.824616432189941 LOSS_Discriminator: 0.024816524237394333\n",
            "ITERATION_NO.: 952 LOSS_Generator: 5.59351921081543 LOSS_Discriminator: 0.17838025093078613\n",
            "ITERATION_NO.: 953 LOSS_Generator: 5.258868217468262 LOSS_Discriminator: 0.07325686514377594\n",
            "ITERATION_NO.: 954 LOSS_Generator: 4.627187728881836 LOSS_Discriminator: 0.041667480021715164\n",
            "ITERATION_NO.: 955 LOSS_Generator: 4.429388523101807 LOSS_Discriminator: 0.06036936119198799\n",
            "ITERATION_NO.: 956 LOSS_Generator: 4.425708770751953 LOSS_Discriminator: 0.07020112872123718\n",
            "ITERATION_NO.: 957 LOSS_Generator: 4.924206256866455 LOSS_Discriminator: 0.05276103317737579\n",
            "ITERATION_NO.: 958 LOSS_Generator: 5.293220520019531 LOSS_Discriminator: 0.18182167410850525\n",
            "ITERATION_NO.: 959 LOSS_Generator: 4.224880218505859 LOSS_Discriminator: 0.0456877276301384\n",
            "ITERATION_NO.: 960 LOSS_Generator: 4.509521484375 LOSS_Discriminator: 0.2522396147251129\n",
            "ITERATION_NO.: 961 LOSS_Generator: 4.01715087890625 LOSS_Discriminator: 0.1629367619752884\n",
            "ITERATION_NO.: 962 LOSS_Generator: 4.0774993896484375 LOSS_Discriminator: 0.2616198658943176\n",
            "ITERATION_NO.: 963 LOSS_Generator: 3.875840902328491 LOSS_Discriminator: 0.1375785917043686\n",
            "ITERATION_NO.: 964 LOSS_Generator: 4.436784744262695 LOSS_Discriminator: 0.09002231061458588\n",
            "ITERATION_NO.: 965 LOSS_Generator: 4.976038455963135 LOSS_Discriminator: 0.11548636853694916\n",
            "ITERATION_NO.: 966 LOSS_Generator: 6.415009498596191 LOSS_Discriminator: 0.029113000258803368\n",
            "ITERATION_NO.: 967 LOSS_Generator: 6.44701623916626 LOSS_Discriminator: 0.23986920714378357\n",
            "ITERATION_NO.: 968 LOSS_Generator: 6.421182632446289 LOSS_Discriminator: 0.08905036747455597\n",
            "ITERATION_NO.: 969 LOSS_Generator: 6.092911720275879 LOSS_Discriminator: 0.3165416121482849\n",
            "ITERATION_NO.: 970 LOSS_Generator: 4.150129318237305 LOSS_Discriminator: 0.13567090034484863\n",
            "ITERATION_NO.: 971 LOSS_Generator: 4.425118446350098 LOSS_Discriminator: 0.08390089869499207\n",
            "ITERATION_NO.: 972 LOSS_Generator: 4.712490558624268 LOSS_Discriminator: 0.09047940373420715\n",
            "ITERATION_NO.: 973 LOSS_Generator: 5.2440385818481445 LOSS_Discriminator: 0.08484219014644623\n",
            "ITERATION_NO.: 974 LOSS_Generator: 6.051664352416992 LOSS_Discriminator: 0.04838152602314949\n",
            "ITERATION_NO.: 975 LOSS_Generator: 6.078336715698242 LOSS_Discriminator: 0.03730875998735428\n",
            "ITERATION_NO.: 976 LOSS_Generator: 6.655803680419922 LOSS_Discriminator: 0.006851851940155029\n",
            "ITERATION_NO.: 977 LOSS_Generator: 7.368206977844238 LOSS_Discriminator: 0.07983829826116562\n",
            "ITERATION_NO.: 978 LOSS_Generator: 6.296642303466797 LOSS_Discriminator: 0.17299970984458923\n",
            "ITERATION_NO.: 979 LOSS_Generator: 5.669363021850586 LOSS_Discriminator: 0.13549956679344177\n",
            "ITERATION_NO.: 980 LOSS_Generator: 4.08418083190918 LOSS_Discriminator: 0.08701516687870026\n",
            "ITERATION_NO.: 981 LOSS_Generator: 3.91860294342041 LOSS_Discriminator: 0.0768384113907814\n",
            "ITERATION_NO.: 982 LOSS_Generator: 3.9164228439331055 LOSS_Discriminator: 0.12077167630195618\n",
            "ITERATION_NO.: 983 LOSS_Generator: 4.3448333740234375 LOSS_Discriminator: 0.17195576429367065\n",
            "ITERATION_NO.: 984 LOSS_Generator: 4.752182960510254 LOSS_Discriminator: 0.13760718703269958\n",
            "ITERATION_NO.: 985 LOSS_Generator: 5.553704261779785 LOSS_Discriminator: 0.08117885887622833\n",
            "ITERATION_NO.: 986 LOSS_Generator: 5.871712684631348 LOSS_Discriminator: 0.2999139428138733\n",
            "ITERATION_NO.: 987 LOSS_Generator: 6.024351119995117 LOSS_Discriminator: 0.12210722267627716\n",
            "ITERATION_NO.: 988 LOSS_Generator: 6.076858997344971 LOSS_Discriminator: 0.11121456325054169\n",
            "ITERATION_NO.: 989 LOSS_Generator: 5.976471900939941 LOSS_Discriminator: 0.15168572962284088\n",
            "ITERATION_NO.: 990 LOSS_Generator: 5.574584007263184 LOSS_Discriminator: 0.12864091992378235\n",
            "ITERATION_NO.: 991 LOSS_Generator: 5.757031440734863 LOSS_Discriminator: 0.08431022614240646\n",
            "ITERATION_NO.: 992 LOSS_Generator: 5.425220489501953 LOSS_Discriminator: 0.06730487942695618\n",
            "ITERATION_NO.: 993 LOSS_Generator: 4.85323429107666 LOSS_Discriminator: 0.1917531043291092\n",
            "ITERATION_NO.: 994 LOSS_Generator: 5.180655479431152 LOSS_Discriminator: 0.11517386138439178\n",
            "ITERATION_NO.: 995 LOSS_Generator: 5.0309343338012695 LOSS_Discriminator: 0.061094190925359726\n",
            "ITERATION_NO.: 996 LOSS_Generator: 5.7583160400390625 LOSS_Discriminator: 0.09794223308563232\n",
            "ITERATION_NO.: 997 LOSS_Generator: 5.449748516082764 LOSS_Discriminator: 0.06272871792316437\n",
            "ITERATION_NO.: 998 LOSS_Generator: 5.542352199554443 LOSS_Discriminator: 0.1087626963853836\n",
            "ITERATION_NO.: 999 LOSS_Generator: 5.150677680969238 LOSS_Discriminator: 0.04085063189268112\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 5.406991004943848 LOSS_Discriminator: 0.02860673889517784\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 4.6501946449279785 LOSS_Discriminator: 0.029745616018772125\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 5.715847492218018 LOSS_Discriminator: 0.061492446810007095\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 5.411809921264648 LOSS_Discriminator: 0.16346979141235352\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 5.463069915771484 LOSS_Discriminator: 0.1517813503742218\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 5.487155437469482 LOSS_Discriminator: 0.07644075900316238\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 4.822118759155273 LOSS_Discriminator: 0.04606331139802933\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 5.860346794128418 LOSS_Discriminator: 0.0223192498087883\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 6.012399196624756 LOSS_Discriminator: 0.12370859086513519\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 5.5959367752075195 LOSS_Discriminator: 0.022543814033269882\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 5.532374382019043 LOSS_Discriminator: 0.29665476083755493\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 4.6753411293029785 LOSS_Discriminator: 0.07642751932144165\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 5.409975051879883 LOSS_Discriminator: 0.025892294943332672\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 5.0213141441345215 LOSS_Discriminator: 0.11368700116872787\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 5.210395336151123 LOSS_Discriminator: 0.1680920571088791\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 4.605632781982422 LOSS_Discriminator: 0.016323605552315712\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 5.095293998718262 LOSS_Discriminator: 0.06675785779953003\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 4.8120317459106445 LOSS_Discriminator: 0.11529438197612762\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 4.994418144226074 LOSS_Discriminator: 0.05687837302684784\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 5.548907279968262 LOSS_Discriminator: 0.15054863691329956\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 5.117715835571289 LOSS_Discriminator: 0.2618873417377472\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 5.461673736572266 LOSS_Discriminator: 0.13258114457130432\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 5.541670799255371 LOSS_Discriminator: 0.06515231728553772\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 5.4864115715026855 LOSS_Discriminator: 0.13387811183929443\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 6.104576110839844 LOSS_Discriminator: 0.206508606672287\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 4.607710361480713 LOSS_Discriminator: 0.1396670937538147\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 6.173579216003418 LOSS_Discriminator: 0.18242493271827698\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 5.263250827789307 LOSS_Discriminator: 0.07075461000204086\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 5.339982032775879 LOSS_Discriminator: 0.17732030153274536\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 5.804840087890625 LOSS_Discriminator: 0.22605741024017334\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 4.729771614074707 LOSS_Discriminator: 0.15088297426700592\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 4.775649547576904 LOSS_Discriminator: 0.17492282390594482\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 4.7651166915893555 LOSS_Discriminator: 0.15333139896392822\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 4.891203880310059 LOSS_Discriminator: 0.14575514197349548\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 3.8833298683166504 LOSS_Discriminator: 0.22568649053573608\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 4.290205001831055 LOSS_Discriminator: 0.055379197001457214\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 5.282087802886963 LOSS_Discriminator: 0.13550731539726257\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 5.127118110656738 LOSS_Discriminator: 0.12159984558820724\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 5.7157301902771 LOSS_Discriminator: 0.06862518191337585\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 5.1064252853393555 LOSS_Discriminator: 0.14759796857833862\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 5.6239118576049805 LOSS_Discriminator: 0.4669179320335388\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 3.9451074600219727 LOSS_Discriminator: 0.24009814858436584\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 3.0195541381835938 LOSS_Discriminator: 0.16892819106578827\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 3.997981548309326 LOSS_Discriminator: 0.1862185299396515\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 5.1874189376831055 LOSS_Discriminator: 0.09570100903511047\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 6.1137237548828125 LOSS_Discriminator: 0.039628103375434875\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 6.925561904907227 LOSS_Discriminator: 0.13069936633110046\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 6.626766204833984 LOSS_Discriminator: 0.6021952629089355\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 6.351471900939941 LOSS_Discriminator: 0.3560015559196472\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 5.3818817138671875 LOSS_Discriminator: 0.042906198650598526\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 4.96535587310791 LOSS_Discriminator: 0.05965711548924446\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 4.624566078186035 LOSS_Discriminator: 0.15774545073509216\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 4.158515930175781 LOSS_Discriminator: 0.24445787072181702\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 4.097589492797852 LOSS_Discriminator: 0.11118132621049881\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 4.236085891723633 LOSS_Discriminator: 0.19953441619873047\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 5.23148250579834 LOSS_Discriminator: 0.13549581170082092\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 5.7978973388671875 LOSS_Discriminator: 0.14200925827026367\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 6.367423057556152 LOSS_Discriminator: 0.17423641681671143\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 6.434388160705566 LOSS_Discriminator: 0.18503469228744507\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 6.093025207519531 LOSS_Discriminator: 0.29876449704170227\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 5.532437324523926 LOSS_Discriminator: 0.0655425637960434\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 5.305127143859863 LOSS_Discriminator: 0.023608293384313583\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 4.572811603546143 LOSS_Discriminator: 0.04509137198328972\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 5.060569763183594 LOSS_Discriminator: 0.22329369187355042\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 4.509397506713867 LOSS_Discriminator: 0.057571813464164734\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 4.122337341308594 LOSS_Discriminator: 0.07847842574119568\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 4.427066802978516 LOSS_Discriminator: 0.07793918251991272\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 5.284807205200195 LOSS_Discriminator: 0.33246052265167236\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 4.123424053192139 LOSS_Discriminator: 0.08900809288024902\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 5.418300151824951 LOSS_Discriminator: 0.02878335490822792\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.798637390136719 LOSS_Discriminator: 0.08483146131038666\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 4.9278059005737305 LOSS_Discriminator: 0.11276710033416748\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 5.232738018035889 LOSS_Discriminator: 0.0446980744600296\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 5.04464054107666 LOSS_Discriminator: 0.20089048147201538\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 4.951703071594238 LOSS_Discriminator: 0.13632605969905853\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 4.633008003234863 LOSS_Discriminator: 0.14134788513183594\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 4.26639461517334 LOSS_Discriminator: 0.12959937751293182\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 4.1405181884765625 LOSS_Discriminator: 0.31572192907333374\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 3.867711067199707 LOSS_Discriminator: 0.1076182872056961\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 4.139528751373291 LOSS_Discriminator: 0.13140541315078735\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 4.954470157623291 LOSS_Discriminator: 0.08833153545856476\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 4.955395221710205 LOSS_Discriminator: 0.09412439912557602\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 5.422895431518555 LOSS_Discriminator: 0.1076783761382103\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 5.141761779785156 LOSS_Discriminator: 0.16246187686920166\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 5.219812870025635 LOSS_Discriminator: 0.05061539262533188\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 5.349761009216309 LOSS_Discriminator: 0.05820548161864281\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 5.112123012542725 LOSS_Discriminator: 0.03171512484550476\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 5.237250328063965 LOSS_Discriminator: 0.027417849749326706\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 4.9111433029174805 LOSS_Discriminator: 0.29246991872787476\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 5.037754058837891 LOSS_Discriminator: 0.20684362947940826\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 4.350745677947998 LOSS_Discriminator: 0.06441312283277512\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 3.9836549758911133 LOSS_Discriminator: 0.1267358958721161\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 4.231151580810547 LOSS_Discriminator: 0.25925469398498535\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 3.197144031524658 LOSS_Discriminator: 0.0921625941991806\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 3.6070079803466797 LOSS_Discriminator: 0.13972173631191254\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 4.26377010345459 LOSS_Discriminator: 0.11448442935943604\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 5.124912261962891 LOSS_Discriminator: 0.10420934110879898\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 5.591249465942383 LOSS_Discriminator: 0.09344048798084259\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 6.033229827880859 LOSS_Discriminator: 0.013550100848078728\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 6.119537353515625 LOSS_Discriminator: 0.0618022084236145\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 6.566537380218506 LOSS_Discriminator: 0.04742744565010071\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 6.496356964111328 LOSS_Discriminator: 0.1226847916841507\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 6.186601161956787 LOSS_Discriminator: 0.15278324484825134\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 6.065213680267334 LOSS_Discriminator: 0.08614182472229004\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 5.038521766662598 LOSS_Discriminator: 0.03498425334692001\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 4.495355606079102 LOSS_Discriminator: 0.10036373138427734\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 4.36387825012207 LOSS_Discriminator: 0.020056162029504776\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 4.224236488342285 LOSS_Discriminator: 0.2214297652244568\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 4.034649848937988 LOSS_Discriminator: 0.11148081719875336\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 4.224976062774658 LOSS_Discriminator: 0.2223215103149414\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 4.050171375274658 LOSS_Discriminator: 0.08391497284173965\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 4.2388997077941895 LOSS_Discriminator: 0.1624792516231537\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 4.552543640136719 LOSS_Discriminator: 0.14352867007255554\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 5.108790397644043 LOSS_Discriminator: 0.02925969660282135\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 5.980906009674072 LOSS_Discriminator: 0.10932343453168869\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 6.162415027618408 LOSS_Discriminator: 0.2788771986961365\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 5.389151096343994 LOSS_Discriminator: 0.07584536820650101\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.658194541931152 LOSS_Discriminator: 0.018871160224080086\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 5.172271251678467 LOSS_Discriminator: 0.04322849214076996\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 5.043688774108887 LOSS_Discriminator: 0.1035056859254837\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 5.154936790466309 LOSS_Discriminator: 0.039967674762010574\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 4.656538963317871 LOSS_Discriminator: 0.19240343570709229\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 3.748612403869629 LOSS_Discriminator: 0.14807452261447906\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 3.654060125350952 LOSS_Discriminator: 0.13714197278022766\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 3.5095362663269043 LOSS_Discriminator: 0.08781296014785767\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 4.319298267364502 LOSS_Discriminator: 0.1671832948923111\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 4.989921569824219 LOSS_Discriminator: 0.17301419377326965\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 4.50961446762085 LOSS_Discriminator: 0.09580319374799728\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 5.835330009460449 LOSS_Discriminator: 0.07843494415283203\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 5.517304420471191 LOSS_Discriminator: 0.18149499595165253\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 5.423496723175049 LOSS_Discriminator: 0.028904277831315994\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 5.698390007019043 LOSS_Discriminator: 0.07208271324634552\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 4.835670471191406 LOSS_Discriminator: 0.11799992620944977\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 5.212972164154053 LOSS_Discriminator: 0.13055545091629028\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 5.328568935394287 LOSS_Discriminator: 0.1557040512561798\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 5.351971626281738 LOSS_Discriminator: 0.04162248596549034\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 4.958162784576416 LOSS_Discriminator: 0.017868714407086372\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 5.426454544067383 LOSS_Discriminator: 0.08709932118654251\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 4.86436128616333 LOSS_Discriminator: 0.023948747664690018\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 4.9727253913879395 LOSS_Discriminator: 0.027924150228500366\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 5.13149881362915 LOSS_Discriminator: 0.09552222490310669\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 5.525693416595459 LOSS_Discriminator: 0.09351934492588043\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 5.197278022766113 LOSS_Discriminator: 0.09388417750597\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 5.206307888031006 LOSS_Discriminator: 0.09541720151901245\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 4.618609428405762 LOSS_Discriminator: 0.055379144847393036\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 4.8206377029418945 LOSS_Discriminator: 0.04106052964925766\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 5.357766151428223 LOSS_Discriminator: 0.07899318635463715\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 5.608107566833496 LOSS_Discriminator: 0.1340366005897522\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 4.9216718673706055 LOSS_Discriminator: 0.1147703155875206\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 5.260400772094727 LOSS_Discriminator: 0.043980538845062256\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 5.165599822998047 LOSS_Discriminator: 0.2321702539920807\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 4.41906213760376 LOSS_Discriminator: 0.20055978000164032\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 4.899413108825684 LOSS_Discriminator: 0.0727229118347168\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 4.190374374389648 LOSS_Discriminator: 0.09148238599300385\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 4.736244201660156 LOSS_Discriminator: 0.06968660652637482\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 4.965927600860596 LOSS_Discriminator: 0.1243143081665039\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 5.440298557281494 LOSS_Discriminator: 0.028691306710243225\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 5.964576721191406 LOSS_Discriminator: 0.07840176671743393\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 5.6889142990112305 LOSS_Discriminator: 0.194620281457901\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 5.594084739685059 LOSS_Discriminator: 0.17888611555099487\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 5.170998573303223 LOSS_Discriminator: 0.043331537395715714\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 5.437026023864746 LOSS_Discriminator: 0.022830571979284286\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 5.755218982696533 LOSS_Discriminator: 0.12487107515335083\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 5.763172626495361 LOSS_Discriminator: 0.224531888961792\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 4.821535110473633 LOSS_Discriminator: 0.24886906147003174\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 4.900629043579102 LOSS_Discriminator: 0.15400440990924835\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 4.986634254455566 LOSS_Discriminator: 0.24411091208457947\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 5.3361616134643555 LOSS_Discriminator: 0.14938923716545105\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 4.884552955627441 LOSS_Discriminator: 0.26551294326782227\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 5.2488555908203125 LOSS_Discriminator: 0.1517145335674286\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 5.140314102172852 LOSS_Discriminator: 0.1653607189655304\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 4.90770149230957 LOSS_Discriminator: 0.23824530839920044\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 5.027846336364746 LOSS_Discriminator: 0.10251878201961517\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 4.3525166511535645 LOSS_Discriminator: 0.20625944435596466\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 4.791330814361572 LOSS_Discriminator: 0.13033244013786316\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 4.844374656677246 LOSS_Discriminator: 0.18919160962104797\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 5.090123176574707 LOSS_Discriminator: 0.2064560055732727\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 5.491785049438477 LOSS_Discriminator: 0.05686599016189575\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 5.943788528442383 LOSS_Discriminator: 0.1879100203514099\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 5.152772426605225 LOSS_Discriminator: 0.1446525752544403\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 4.389621734619141 LOSS_Discriminator: 0.10448798537254333\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 4.226091384887695 LOSS_Discriminator: 0.17301146686077118\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 4.173542499542236 LOSS_Discriminator: 0.0836273729801178\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 3.922875165939331 LOSS_Discriminator: 0.149976909160614\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 5.153369426727295 LOSS_Discriminator: 0.05900399014353752\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 5.303802013397217 LOSS_Discriminator: 0.22919289767742157\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 5.214607238769531 LOSS_Discriminator: 0.07206806540489197\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 4.846640586853027 LOSS_Discriminator: 0.47159743309020996\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 4.031428337097168 LOSS_Discriminator: 0.3427965044975281\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 4.668525218963623 LOSS_Discriminator: 0.14501634240150452\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 5.033985137939453 LOSS_Discriminator: 0.08441192656755447\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 5.550661563873291 LOSS_Discriminator: 0.10742390900850296\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 5.616786003112793 LOSS_Discriminator: 0.11793260276317596\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 5.920840263366699 LOSS_Discriminator: 0.5155366659164429\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 5.487112998962402 LOSS_Discriminator: 0.028729863464832306\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 4.620615005493164 LOSS_Discriminator: 0.13332754373550415\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 5.019331455230713 LOSS_Discriminator: 0.14584627747535706\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 4.367127418518066 LOSS_Discriminator: 0.060173675417900085\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 4.703198432922363 LOSS_Discriminator: 0.09621870517730713\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 4.4152421951293945 LOSS_Discriminator: 0.078031525015831\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 4.974099159240723 LOSS_Discriminator: 0.10739775747060776\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 5.674248218536377 LOSS_Discriminator: 0.03631078079342842\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 5.602847099304199 LOSS_Discriminator: 0.0240232702344656\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 6.406641960144043 LOSS_Discriminator: 0.2821308970451355\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 5.628274917602539 LOSS_Discriminator: 0.1328994631767273\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 4.783009052276611 LOSS_Discriminator: 0.12113992869853973\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 4.050960063934326 LOSS_Discriminator: 0.20289158821105957\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 4.401224136352539 LOSS_Discriminator: 0.09555321931838989\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 4.268627166748047 LOSS_Discriminator: 0.1849987506866455\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 4.081913471221924 LOSS_Discriminator: 0.14658108353614807\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 5.134058952331543 LOSS_Discriminator: 0.06817902624607086\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 6.195809364318848 LOSS_Discriminator: 0.11478548496961594\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 6.4388861656188965 LOSS_Discriminator: 0.20592984557151794\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 5.895654678344727 LOSS_Discriminator: 0.30812782049179077\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 5.056699752807617 LOSS_Discriminator: 0.36517927050590515\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 4.083181381225586 LOSS_Discriminator: 0.15659570693969727\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 3.085367202758789 LOSS_Discriminator: 0.1556684672832489\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 3.231875419616699 LOSS_Discriminator: 0.2265082150697708\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 3.8467400074005127 LOSS_Discriminator: 0.1549433022737503\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 4.775832176208496 LOSS_Discriminator: 0.1602497398853302\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 4.9799909591674805 LOSS_Discriminator: 0.16175176203250885\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 4.976864337921143 LOSS_Discriminator: 0.1258012056350708\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 5.578545570373535 LOSS_Discriminator: 0.09527909755706787\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 5.990765571594238 LOSS_Discriminator: 0.10454626381397247\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 4.935697555541992 LOSS_Discriminator: 0.08545009791851044\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 4.816728591918945 LOSS_Discriminator: 0.13207603991031647\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 4.246535301208496 LOSS_Discriminator: 0.10469643026590347\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 3.639427661895752 LOSS_Discriminator: 0.15357810258865356\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 4.118502140045166 LOSS_Discriminator: 0.2671108543872833\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 4.195063591003418 LOSS_Discriminator: 0.14890849590301514\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 5.419944763183594 LOSS_Discriminator: 0.05810052156448364\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 5.859951019287109 LOSS_Discriminator: 0.1410055160522461\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 5.796261787414551 LOSS_Discriminator: 0.1401098072528839\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 5.933772087097168 LOSS_Discriminator: 0.13932934403419495\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 4.920851707458496 LOSS_Discriminator: 0.26555126905441284\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 5.27120304107666 LOSS_Discriminator: 0.09593885391950607\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 4.513801574707031 LOSS_Discriminator: 0.07990914583206177\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 4.547564506530762 LOSS_Discriminator: 0.0891122967004776\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 4.5497822761535645 LOSS_Discriminator: 0.15613365173339844\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 5.598656177520752 LOSS_Discriminator: 0.15644262731075287\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 6.188354969024658 LOSS_Discriminator: 0.04755537211894989\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 5.663355350494385 LOSS_Discriminator: 0.0076562389731407166\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 5.900989532470703 LOSS_Discriminator: 0.015373725444078445\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 6.096292018890381 LOSS_Discriminator: 0.024003051221370697\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 7.006617546081543 LOSS_Discriminator: 0.042378317564725876\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 6.577781677246094 LOSS_Discriminator: 0.06090264022350311\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 6.847437858581543 LOSS_Discriminator: 0.1492220163345337\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 6.543059825897217 LOSS_Discriminator: 0.09278266131877899\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 6.268285751342773 LOSS_Discriminator: 0.10368482768535614\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 5.36372184753418 LOSS_Discriminator: 0.30302858352661133\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 4.330990791320801 LOSS_Discriminator: 0.2472991645336151\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 4.058957099914551 LOSS_Discriminator: 0.03289869800209999\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 3.836627244949341 LOSS_Discriminator: 0.11496320366859436\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 4.0214619636535645 LOSS_Discriminator: 0.1279362142086029\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 5.517172336578369 LOSS_Discriminator: 0.07239815592765808\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 5.248588562011719 LOSS_Discriminator: 0.3740696907043457\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 5.232977390289307 LOSS_Discriminator: 0.111843541264534\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 5.261767387390137 LOSS_Discriminator: 0.11488845944404602\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 5.07497501373291 LOSS_Discriminator: 0.18135033547878265\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.052852630615234 LOSS_Discriminator: 0.06831441819667816\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 4.640999794006348 LOSS_Discriminator: 0.12317200005054474\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 5.0086212158203125 LOSS_Discriminator: 0.16334854066371918\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 4.587771415710449 LOSS_Discriminator: 0.08449488878250122\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 4.758659839630127 LOSS_Discriminator: 0.16825944185256958\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 4.328686714172363 LOSS_Discriminator: 0.22099873423576355\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 4.095098495483398 LOSS_Discriminator: 0.09367907047271729\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 3.9862165451049805 LOSS_Discriminator: 0.09783343970775604\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 4.0056657791137695 LOSS_Discriminator: 0.07843838632106781\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 4.34147834777832 LOSS_Discriminator: 0.08871234208345413\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 4.113238334655762 LOSS_Discriminator: 0.06387703120708466\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 5.04153299331665 LOSS_Discriminator: 0.19176983833312988\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 5.278050899505615 LOSS_Discriminator: 0.04123131185770035\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 5.583366394042969 LOSS_Discriminator: 0.15015026926994324\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 5.97941780090332 LOSS_Discriminator: 0.06386014819145203\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 5.735434532165527 LOSS_Discriminator: 0.023694422096014023\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 5.463474273681641 LOSS_Discriminator: 0.06954937428236008\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.8633880615234375 LOSS_Discriminator: 0.09182054549455643\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 5.486542701721191 LOSS_Discriminator: 0.12158791720867157\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 5.531007766723633 LOSS_Discriminator: 0.20118209719657898\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 4.553021430969238 LOSS_Discriminator: 0.18446817994117737\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 4.405996322631836 LOSS_Discriminator: 0.048329539597034454\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 3.965674877166748 LOSS_Discriminator: 0.11173879355192184\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 4.4511213302612305 LOSS_Discriminator: 0.1023215726017952\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 4.949208736419678 LOSS_Discriminator: 0.1159946471452713\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 5.323657035827637 LOSS_Discriminator: 0.021782711148262024\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 4.833645820617676 LOSS_Discriminator: 0.06189418211579323\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 5.233307838439941 LOSS_Discriminator: 0.16145822405815125\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 5.268635272979736 LOSS_Discriminator: 0.03194234520196915\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 5.612972736358643 LOSS_Discriminator: 0.2313046157360077\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 5.122082710266113 LOSS_Discriminator: 0.09262189269065857\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 4.621659278869629 LOSS_Discriminator: 0.1998947709798813\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 3.7815542221069336 LOSS_Discriminator: 0.16333720088005066\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 3.8902745246887207 LOSS_Discriminator: 0.147748664021492\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 4.08121395111084 LOSS_Discriminator: 0.05689598619937897\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 4.387245178222656 LOSS_Discriminator: 0.2501755952835083\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 5.555720806121826 LOSS_Discriminator: 0.13378192484378815\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 5.566499710083008 LOSS_Discriminator: 0.04044898599386215\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 6.094974517822266 LOSS_Discriminator: 0.09222746640443802\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 6.047327518463135 LOSS_Discriminator: 0.03400593250989914\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 5.788949966430664 LOSS_Discriminator: 0.015275086276233196\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 5.968415260314941 LOSS_Discriminator: 0.11279113590717316\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 5.819972515106201 LOSS_Discriminator: 0.046071574091911316\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 5.350644111633301 LOSS_Discriminator: 0.26050305366516113\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 4.452949523925781 LOSS_Discriminator: 0.17132045328617096\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 4.535113334655762 LOSS_Discriminator: 0.13231563568115234\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 3.719329833984375 LOSS_Discriminator: 0.1694064885377884\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 4.257923126220703 LOSS_Discriminator: 0.06661449372768402\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 5.353401184082031 LOSS_Discriminator: 0.07993270456790924\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 5.581464767456055 LOSS_Discriminator: 0.1914224922657013\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 5.699808120727539 LOSS_Discriminator: 0.13862133026123047\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 5.7826151847839355 LOSS_Discriminator: 0.03212945908308029\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 5.814479827880859 LOSS_Discriminator: 0.20453153550624847\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 4.33686637878418 LOSS_Discriminator: 0.1957031637430191\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 4.603264808654785 LOSS_Discriminator: 0.17309626936912537\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 4.460629940032959 LOSS_Discriminator: 0.08049637824296951\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 3.933445453643799 LOSS_Discriminator: 0.27681708335876465\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 4.989031791687012 LOSS_Discriminator: 0.12906765937805176\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 4.583578109741211 LOSS_Discriminator: 0.08651304244995117\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 5.441949367523193 LOSS_Discriminator: 0.1279873549938202\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 5.023313999176025 LOSS_Discriminator: 0.19670787453651428\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 4.725424289703369 LOSS_Discriminator: 0.3534902334213257\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 4.630015850067139 LOSS_Discriminator: 0.15472370386123657\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 4.215276718139648 LOSS_Discriminator: 0.13551057875156403\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 4.2697553634643555 LOSS_Discriminator: 0.1742112636566162\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 4.153364181518555 LOSS_Discriminator: 0.16462108492851257\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 4.603338718414307 LOSS_Discriminator: 0.060938671231269836\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 4.71226167678833 LOSS_Discriminator: 0.15417887270450592\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 4.363185882568359 LOSS_Discriminator: 0.3353261649608612\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 4.060431003570557 LOSS_Discriminator: 0.14389079809188843\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 4.4439167976379395 LOSS_Discriminator: 0.13799992203712463\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 5.097382068634033 LOSS_Discriminator: 0.09067051112651825\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 5.769255638122559 LOSS_Discriminator: 0.14343373477458954\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 6.125791072845459 LOSS_Discriminator: 0.12987999618053436\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 6.5226359367370605 LOSS_Discriminator: 0.0399099625647068\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 5.166481018066406 LOSS_Discriminator: 0.19347360730171204\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 4.767350196838379 LOSS_Discriminator: 0.13302825391292572\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 4.809918403625488 LOSS_Discriminator: 0.05523771047592163\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 4.485967636108398 LOSS_Discriminator: 0.18559476733207703\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 4.8931427001953125 LOSS_Discriminator: 0.2057228982448578\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 4.798871994018555 LOSS_Discriminator: 0.153803288936615\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 4.957346439361572 LOSS_Discriminator: 0.12007395923137665\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 4.7134857177734375 LOSS_Discriminator: 0.03026721067726612\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 5.347428321838379 LOSS_Discriminator: 0.07125869393348694\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 5.196150779724121 LOSS_Discriminator: 0.04925038665533066\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 5.269831657409668 LOSS_Discriminator: 0.2100427895784378\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 4.827013969421387 LOSS_Discriminator: 0.025382647290825844\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 5.490337371826172 LOSS_Discriminator: 0.3182094693183899\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 5.747156620025635 LOSS_Discriminator: 0.10496726632118225\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 6.364309310913086 LOSS_Discriminator: 0.06258586794137955\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 4.897633075714111 LOSS_Discriminator: 0.05941789224743843\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 5.064285755157471 LOSS_Discriminator: 0.39813607931137085\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 4.9503173828125 LOSS_Discriminator: 0.054155539721250534\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 4.514534950256348 LOSS_Discriminator: 0.10535837709903717\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 4.337090492248535 LOSS_Discriminator: 0.10504354536533356\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 5.091395378112793 LOSS_Discriminator: 0.1194780170917511\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 5.019071102142334 LOSS_Discriminator: 0.2259277105331421\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.191718101501465 LOSS_Discriminator: 0.09530776739120483\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 3.762634754180908 LOSS_Discriminator: 0.22614221274852753\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 3.6533143520355225 LOSS_Discriminator: 0.09604732692241669\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 4.433797836303711 LOSS_Discriminator: 0.07855148613452911\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 5.595772743225098 LOSS_Discriminator: 0.027754180133342743\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 5.676208972930908 LOSS_Discriminator: 0.11663874983787537\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 5.888221740722656 LOSS_Discriminator: 0.10607921332120895\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 5.9289350509643555 LOSS_Discriminator: 0.007415476255118847\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 6.489106178283691 LOSS_Discriminator: 0.09424400329589844\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 6.801927089691162 LOSS_Discriminator: 0.19433371722698212\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 5.608181953430176 LOSS_Discriminator: 0.18415848910808563\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 5.177692890167236 LOSS_Discriminator: 0.37494516372680664\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 3.767820358276367 LOSS_Discriminator: 0.04335717111825943\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 4.005638122558594 LOSS_Discriminator: 0.09134572744369507\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 4.044852256774902 LOSS_Discriminator: 0.14501096308231354\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 4.346594333648682 LOSS_Discriminator: 0.16780728101730347\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 4.443099021911621 LOSS_Discriminator: 0.07290597259998322\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 4.377305030822754 LOSS_Discriminator: 0.12503331899642944\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 5.069380283355713 LOSS_Discriminator: 0.09586337208747864\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 4.852629661560059 LOSS_Discriminator: 0.07162803411483765\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 4.731228828430176 LOSS_Discriminator: 0.3739025592803955\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 4.493646144866943 LOSS_Discriminator: 0.1359845995903015\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 4.148821830749512 LOSS_Discriminator: 0.09090791642665863\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 4.136127471923828 LOSS_Discriminator: 0.18877536058425903\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 4.367242336273193 LOSS_Discriminator: 0.32326263189315796\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 4.129420280456543 LOSS_Discriminator: 0.13284724950790405\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 4.5056352615356445 LOSS_Discriminator: 0.10934756696224213\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 4.8438401222229 LOSS_Discriminator: 0.16948330402374268\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 5.801689147949219 LOSS_Discriminator: 0.3155624270439148\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 6.235291481018066 LOSS_Discriminator: 0.03023902326822281\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 5.853023052215576 LOSS_Discriminator: 0.05708204209804535\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 5.246999740600586 LOSS_Discriminator: 0.18576499819755554\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 5.586653709411621 LOSS_Discriminator: 0.12331921607255936\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 4.838428497314453 LOSS_Discriminator: 0.1248425543308258\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 3.9771475791931152 LOSS_Discriminator: 0.031990110874176025\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 4.5174970626831055 LOSS_Discriminator: 0.25284451246261597\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 3.8813834190368652 LOSS_Discriminator: 0.2742491364479065\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 4.65611457824707 LOSS_Discriminator: 0.14059162139892578\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 5.159653186798096 LOSS_Discriminator: 0.08199809491634369\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 5.5742387771606445 LOSS_Discriminator: 0.04021729901432991\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 5.384131908416748 LOSS_Discriminator: 0.09180064499378204\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 5.622337341308594 LOSS_Discriminator: 0.12132947146892548\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 5.610696315765381 LOSS_Discriminator: 0.20833250880241394\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 5.405542373657227 LOSS_Discriminator: 0.023247171193361282\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 5.349259376525879 LOSS_Discriminator: 0.14518210291862488\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 4.79240608215332 LOSS_Discriminator: 0.07459239661693573\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 4.651889801025391 LOSS_Discriminator: 0.16962051391601562\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 4.2251129150390625 LOSS_Discriminator: 0.23042258620262146\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 4.841097354888916 LOSS_Discriminator: 0.10585103929042816\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 4.595637321472168 LOSS_Discriminator: 0.10829358547925949\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 4.1248321533203125 LOSS_Discriminator: 0.1506137251853943\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 4.230400085449219 LOSS_Discriminator: 0.10329482704401016\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 4.101451873779297 LOSS_Discriminator: 0.1832558810710907\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 3.723921537399292 LOSS_Discriminator: 0.10766298323869705\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 4.477834701538086 LOSS_Discriminator: 0.16265332698822021\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 4.451114177703857 LOSS_Discriminator: 0.04181106388568878\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 3.4071969985961914 LOSS_Discriminator: 0.08848986774682999\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 4.133701324462891 LOSS_Discriminator: 0.16924333572387695\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 4.206809043884277 LOSS_Discriminator: 0.14074091613292694\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 4.020462989807129 LOSS_Discriminator: 0.2105541080236435\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 4.652461051940918 LOSS_Discriminator: 0.07732617855072021\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 4.2711920738220215 LOSS_Discriminator: 0.12105058133602142\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 4.8450446128845215 LOSS_Discriminator: 0.02772512659430504\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 5.0669050216674805 LOSS_Discriminator: 0.2077108770608902\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 4.697701930999756 LOSS_Discriminator: 0.3240891993045807\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 4.622692108154297 LOSS_Discriminator: 0.1364736706018448\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 4.051580429077148 LOSS_Discriminator: 0.12797203660011292\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 3.7207202911376953 LOSS_Discriminator: 0.07601769268512726\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 5.19071102142334 LOSS_Discriminator: 0.05051961541175842\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 5.03688907623291 LOSS_Discriminator: 0.10049762576818466\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 5.504745960235596 LOSS_Discriminator: 0.12233111262321472\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 6.096229553222656 LOSS_Discriminator: 0.05363951250910759\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 6.090061664581299 LOSS_Discriminator: 0.15992282330989838\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 5.545305252075195 LOSS_Discriminator: 0.04545534774661064\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 5.224884986877441 LOSS_Discriminator: 0.11546237021684647\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 5.326887130737305 LOSS_Discriminator: 0.06006868556141853\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 4.8094329833984375 LOSS_Discriminator: 0.02725837752223015\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 4.6621012687683105 LOSS_Discriminator: 0.06485025584697723\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 4.595635890960693 LOSS_Discriminator: 0.30505120754241943\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 4.284007549285889 LOSS_Discriminator: 0.2760963439941406\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 4.5732574462890625 LOSS_Discriminator: 0.053858958184719086\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 4.560542106628418 LOSS_Discriminator: 0.07421806454658508\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 4.662724018096924 LOSS_Discriminator: 0.2392961084842682\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 4.711832046508789 LOSS_Discriminator: 0.09073508530855179\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 4.717137336730957 LOSS_Discriminator: 0.15127691626548767\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 5.0573577880859375 LOSS_Discriminator: 0.03614227473735809\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 4.8600311279296875 LOSS_Discriminator: 0.15131133794784546\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 4.648181915283203 LOSS_Discriminator: 0.18185743689537048\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 5.520349502563477 LOSS_Discriminator: 0.05575835332274437\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 5.296500205993652 LOSS_Discriminator: 0.03620041534304619\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 5.055373191833496 LOSS_Discriminator: 0.07210563123226166\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 4.757495880126953 LOSS_Discriminator: 0.11560376733541489\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 5.163407325744629 LOSS_Discriminator: 0.01798449456691742\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 5.853198051452637 LOSS_Discriminator: 0.1164393350481987\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 4.758708953857422 LOSS_Discriminator: 0.2662968635559082\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 4.987873077392578 LOSS_Discriminator: 0.020722275599837303\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 4.473872661590576 LOSS_Discriminator: 0.0882004052400589\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 4.281639099121094 LOSS_Discriminator: 0.12147986888885498\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 4.569141387939453 LOSS_Discriminator: 0.10451547801494598\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 4.782730579376221 LOSS_Discriminator: 0.08944795280694962\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 5.103786468505859 LOSS_Discriminator: 0.08348146826028824\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 4.878048896789551 LOSS_Discriminator: 0.03911043331027031\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 5.094542503356934 LOSS_Discriminator: 0.1972462236881256\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 5.045507907867432 LOSS_Discriminator: 0.13376609981060028\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 4.365996360778809 LOSS_Discriminator: 0.2098771035671234\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 4.311938285827637 LOSS_Discriminator: 0.10141267627477646\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 3.8790721893310547 LOSS_Discriminator: 0.13933634757995605\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 4.451329231262207 LOSS_Discriminator: 0.13832783699035645\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 5.447589874267578 LOSS_Discriminator: 0.27614182233810425\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 5.6872148513793945 LOSS_Discriminator: 0.04595715180039406\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 5.8504438400268555 LOSS_Discriminator: 0.11776134371757507\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 5.750592231750488 LOSS_Discriminator: 0.030189480632543564\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 6.100017547607422 LOSS_Discriminator: 0.01729499362409115\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 5.994715690612793 LOSS_Discriminator: 0.11064855009317398\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 5.750039577484131 LOSS_Discriminator: 0.12836822867393494\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 5.649240016937256 LOSS_Discriminator: 0.10130703449249268\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 5.2213454246521 LOSS_Discriminator: 0.060844287276268005\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 4.113317012786865 LOSS_Discriminator: 0.05686155706644058\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 4.513060092926025 LOSS_Discriminator: 0.12822325527668\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 4.450477600097656 LOSS_Discriminator: 0.09309880435466766\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 4.3277788162231445 LOSS_Discriminator: 0.05649032071232796\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 4.337677955627441 LOSS_Discriminator: 0.2875784933567047\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 4.1826653480529785 LOSS_Discriminator: 0.12909284234046936\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 4.178221702575684 LOSS_Discriminator: 0.20654362440109253\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 5.092797756195068 LOSS_Discriminator: 0.057183172553777695\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 5.221123695373535 LOSS_Discriminator: 0.16147580742835999\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 6.094026565551758 LOSS_Discriminator: 0.2184952199459076\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 5.026309013366699 LOSS_Discriminator: 0.1552773118019104\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 4.965266704559326 LOSS_Discriminator: 0.2591909170150757\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 4.147329807281494 LOSS_Discriminator: 0.1580861210823059\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 4.076483726501465 LOSS_Discriminator: 0.16825905442237854\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 5.094999313354492 LOSS_Discriminator: 0.09232623875141144\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 6.072539329528809 LOSS_Discriminator: 0.033000871539115906\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 6.590876579284668 LOSS_Discriminator: 0.17723070085048676\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 6.434623718261719 LOSS_Discriminator: 0.4149775505065918\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 5.752810955047607 LOSS_Discriminator: 0.14442384243011475\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 5.057679176330566 LOSS_Discriminator: 0.07318638265132904\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 4.566754341125488 LOSS_Discriminator: 0.1335618793964386\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 4.19355583190918 LOSS_Discriminator: 0.13682182133197784\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 4.239470481872559 LOSS_Discriminator: 0.13427899777889252\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 4.467639923095703 LOSS_Discriminator: 0.04380933195352554\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 4.525136947631836 LOSS_Discriminator: 0.21778333187103271\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 4.361812591552734 LOSS_Discriminator: 0.1083626002073288\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 4.592422962188721 LOSS_Discriminator: 0.03760247677564621\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 5.182620048522949 LOSS_Discriminator: 0.04790870100259781\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 5.435197830200195 LOSS_Discriminator: 0.25331544876098633\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 5.824657917022705 LOSS_Discriminator: 0.09331323206424713\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 4.850083351135254 LOSS_Discriminator: 0.044919028878211975\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 5.152617454528809 LOSS_Discriminator: 0.08446633070707321\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 5.0144429206848145 LOSS_Discriminator: 0.09851478040218353\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 5.024331569671631 LOSS_Discriminator: 0.1721130907535553\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 4.764368534088135 LOSS_Discriminator: 0.15243881940841675\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 5.103573322296143 LOSS_Discriminator: 0.13969185948371887\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 5.209769248962402 LOSS_Discriminator: 0.21014361083507538\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 4.965871810913086 LOSS_Discriminator: 0.24291175603866577\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 4.379502773284912 LOSS_Discriminator: 0.3321850299835205\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 4.414708137512207 LOSS_Discriminator: 0.10229025781154633\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 3.9113450050354004 LOSS_Discriminator: 0.24661532044410706\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 4.966527938842773 LOSS_Discriminator: 0.16718947887420654\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 5.535483360290527 LOSS_Discriminator: 0.22663633525371552\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 6.5141282081604 LOSS_Discriminator: 0.17209464311599731\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 6.602754592895508 LOSS_Discriminator: 0.08715388178825378\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 7.086370944976807 LOSS_Discriminator: 0.007669440470635891\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 5.533840656280518 LOSS_Discriminator: 0.6187554597854614\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 5.031396389007568 LOSS_Discriminator: 0.21357107162475586\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 3.749823570251465 LOSS_Discriminator: 0.20092733204364777\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 3.168488025665283 LOSS_Discriminator: 0.36594676971435547\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 3.595348834991455 LOSS_Discriminator: 0.198542058467865\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 4.743932723999023 LOSS_Discriminator: 0.1391950100660324\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 4.826303958892822 LOSS_Discriminator: 0.09600204229354858\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 5.883388042449951 LOSS_Discriminator: 0.030032280832529068\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 4.973733425140381 LOSS_Discriminator: 0.13345734775066376\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 5.22998046875 LOSS_Discriminator: 0.024318519979715347\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 5.568668842315674 LOSS_Discriminator: 0.13629508018493652\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 5.541393280029297 LOSS_Discriminator: 0.1058027595281601\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 4.5594282150268555 LOSS_Discriminator: 0.06753178685903549\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 4.894055366516113 LOSS_Discriminator: 0.1579645723104477\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 4.5273332595825195 LOSS_Discriminator: 0.1676068902015686\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 4.519773483276367 LOSS_Discriminator: 0.1446225494146347\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 3.9762766361236572 LOSS_Discriminator: 0.10673689842224121\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 4.715291976928711 LOSS_Discriminator: 0.07232210040092468\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 4.320996284484863 LOSS_Discriminator: 0.16009517014026642\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 4.520342826843262 LOSS_Discriminator: 0.1253432035446167\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 4.7721638679504395 LOSS_Discriminator: 0.025778159499168396\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 4.565104007720947 LOSS_Discriminator: 0.10729281604290009\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 4.3858866691589355 LOSS_Discriminator: 0.12438441812992096\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 4.756296634674072 LOSS_Discriminator: 0.15589211881160736\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 5.006879806518555 LOSS_Discriminator: 0.023941639810800552\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 4.822638511657715 LOSS_Discriminator: 0.05893445014953613\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 4.794144630432129 LOSS_Discriminator: 0.05785180628299713\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 4.992758750915527 LOSS_Discriminator: 0.038755808025598526\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 4.779780387878418 LOSS_Discriminator: 0.1251930296421051\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 4.310362815856934 LOSS_Discriminator: 0.18485653400421143\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 4.930299282073975 LOSS_Discriminator: 0.07311016321182251\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 4.43524169921875 LOSS_Discriminator: 0.12740150094032288\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 4.649807929992676 LOSS_Discriminator: 0.1952681839466095\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 4.087471008300781 LOSS_Discriminator: 0.2019820660352707\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 3.7030303478240967 LOSS_Discriminator: 0.2042037695646286\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 4.283166408538818 LOSS_Discriminator: 0.05739971622824669\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 4.410299777984619 LOSS_Discriminator: 0.16903552412986755\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 4.78770637512207 LOSS_Discriminator: 0.10543835163116455\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 4.448814392089844 LOSS_Discriminator: 0.31946295499801636\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 4.724941253662109 LOSS_Discriminator: 0.22444231808185577\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 4.725356578826904 LOSS_Discriminator: 0.14180420339107513\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 4.469115257263184 LOSS_Discriminator: 0.08886118233203888\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 4.692069053649902 LOSS_Discriminator: 0.13521993160247803\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 4.270035743713379 LOSS_Discriminator: 0.24714089930057526\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 3.1302876472473145 LOSS_Discriminator: 0.073893703520298\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 4.149785041809082 LOSS_Discriminator: 0.1531386375427246\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 4.626748561859131 LOSS_Discriminator: 0.0916731134057045\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 5.1222405433654785 LOSS_Discriminator: 0.036925558000802994\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.879276275634766 LOSS_Discriminator: 0.07611243426799774\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 5.884927749633789 LOSS_Discriminator: 0.06971651315689087\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 6.574398517608643 LOSS_Discriminator: 0.13414813578128815\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 5.966717720031738 LOSS_Discriminator: 0.17429183423519135\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 5.334904670715332 LOSS_Discriminator: 0.09951356053352356\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 4.407332420349121 LOSS_Discriminator: 0.11245658993721008\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 4.480987548828125 LOSS_Discriminator: 0.11190430819988251\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 3.8354310989379883 LOSS_Discriminator: 0.08541496843099594\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 4.507465362548828 LOSS_Discriminator: 0.1368008255958557\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 4.599392890930176 LOSS_Discriminator: 0.05809808894991875\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 5.171076774597168 LOSS_Discriminator: 0.024881992489099503\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 5.149184226989746 LOSS_Discriminator: 0.07679389417171478\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 5.382065773010254 LOSS_Discriminator: 0.0676477700471878\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 5.7451605796813965 LOSS_Discriminator: 0.22663909196853638\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 5.258893013000488 LOSS_Discriminator: 0.024820495396852493\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 5.692721366882324 LOSS_Discriminator: 0.04155879467725754\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 4.687743663787842 LOSS_Discriminator: 0.34941208362579346\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 4.007192611694336 LOSS_Discriminator: 0.13175876438617706\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.194108963012695 LOSS_Discriminator: 0.1595916897058487\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 4.502033233642578 LOSS_Discriminator: 0.12491381913423538\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 5.093552112579346 LOSS_Discriminator: 0.08226512372493744\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 5.121419429779053 LOSS_Discriminator: 0.034637756645679474\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 5.946812152862549 LOSS_Discriminator: 0.04699409753084183\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 6.170319557189941 LOSS_Discriminator: 0.268246591091156\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 5.869760990142822 LOSS_Discriminator: 0.14173713326454163\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 5.2567548751831055 LOSS_Discriminator: 0.057176049798727036\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 4.835775852203369 LOSS_Discriminator: 0.08683650940656662\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 5.276674270629883 LOSS_Discriminator: 0.13744458556175232\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 4.894011497497559 LOSS_Discriminator: 0.03656512498855591\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 5.398970603942871 LOSS_Discriminator: 0.10090584307909012\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 4.398542404174805 LOSS_Discriminator: 0.0706307590007782\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 5.8937458992004395 LOSS_Discriminator: 0.04375743493437767\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 6.7299323081970215 LOSS_Discriminator: 0.21260978281497955\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 6.320971965789795 LOSS_Discriminator: 0.14665716886520386\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 5.955002784729004 LOSS_Discriminator: 0.04281241446733475\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 5.390523433685303 LOSS_Discriminator: 0.21645411849021912\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 5.921741485595703 LOSS_Discriminator: 0.06987407803535461\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 6.130238056182861 LOSS_Discriminator: 0.04788868501782417\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 4.973203182220459 LOSS_Discriminator: 0.06742912530899048\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 4.8662109375 LOSS_Discriminator: 0.28748202323913574\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 4.390618324279785 LOSS_Discriminator: 0.05191703885793686\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 5.171133995056152 LOSS_Discriminator: 0.2556573152542114\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 3.7883694171905518 LOSS_Discriminator: 0.15282899141311646\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 4.799551010131836 LOSS_Discriminator: 0.14479154348373413\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 4.978898525238037 LOSS_Discriminator: 0.0927148088812828\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 5.681631088256836 LOSS_Discriminator: 0.1859939694404602\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 5.780646324157715 LOSS_Discriminator: 0.1096205934882164\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 5.966093063354492 LOSS_Discriminator: 0.19177360832691193\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 5.4052228927612305 LOSS_Discriminator: 0.2452448308467865\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 4.717880725860596 LOSS_Discriminator: 0.027244050055742264\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 4.890249252319336 LOSS_Discriminator: 0.15733060240745544\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 4.749237060546875 LOSS_Discriminator: 0.26990389823913574\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 4.661754608154297 LOSS_Discriminator: 0.056716956198215485\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 4.071797847747803 LOSS_Discriminator: 0.059610046446323395\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 4.8051252365112305 LOSS_Discriminator: 0.07677207887172699\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 5.060283660888672 LOSS_Discriminator: 0.03994249552488327\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 5.152998924255371 LOSS_Discriminator: 0.041933637112379074\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 5.52359676361084 LOSS_Discriminator: 0.015213986858725548\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 5.2444963455200195 LOSS_Discriminator: 0.2642752528190613\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 4.961812496185303 LOSS_Discriminator: 0.07252083718776703\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 4.783732891082764 LOSS_Discriminator: 0.05685676634311676\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 4.405330181121826 LOSS_Discriminator: 0.2616693675518036\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 4.769752025604248 LOSS_Discriminator: 0.07799862325191498\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 5.027712821960449 LOSS_Discriminator: 0.16848337650299072\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 5.218552589416504 LOSS_Discriminator: 0.16716860234737396\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 5.534793376922607 LOSS_Discriminator: 0.1788458228111267\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 5.022195816040039 LOSS_Discriminator: 0.5686162710189819\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 4.190069675445557 LOSS_Discriminator: 0.22081950306892395\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 3.405190944671631 LOSS_Discriminator: 0.03105713240802288\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 3.9591760635375977 LOSS_Discriminator: 0.0989476889371872\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 4.798947811126709 LOSS_Discriminator: 0.10761523991823196\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 5.223388671875 LOSS_Discriminator: 0.13457109034061432\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 5.023146629333496 LOSS_Discriminator: 0.14596544206142426\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 6.319507598876953 LOSS_Discriminator: 0.17498311400413513\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 5.637754440307617 LOSS_Discriminator: 0.15848544239997864\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 5.928345680236816 LOSS_Discriminator: 0.04902908205986023\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 5.825703144073486 LOSS_Discriminator: 0.09591665863990784\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 5.019756317138672 LOSS_Discriminator: 0.23578296601772308\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 4.426368236541748 LOSS_Discriminator: 0.04150421917438507\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 3.6231303215026855 LOSS_Discriminator: 0.1356232464313507\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 4.234100341796875 LOSS_Discriminator: 0.09207398444414139\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 3.6297576427459717 LOSS_Discriminator: 0.26465341448783875\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 4.017679691314697 LOSS_Discriminator: 0.13150262832641602\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 4.740712642669678 LOSS_Discriminator: 0.10197266936302185\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 4.426587104797363 LOSS_Discriminator: 0.11247771233320236\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 4.265539646148682 LOSS_Discriminator: 0.0945824533700943\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 4.879035949707031 LOSS_Discriminator: 0.14728814363479614\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 5.373589515686035 LOSS_Discriminator: 0.10044334828853607\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 5.838682174682617 LOSS_Discriminator: 0.25697940587997437\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 5.287042617797852 LOSS_Discriminator: 0.030160266906023026\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 4.718820571899414 LOSS_Discriminator: 0.017457395792007446\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 4.646397590637207 LOSS_Discriminator: 0.17263734340667725\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 5.195676803588867 LOSS_Discriminator: 0.13794544339179993\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 3.789905309677124 LOSS_Discriminator: 0.09299883991479874\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 4.2768402099609375 LOSS_Discriminator: 0.033556826412677765\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 4.692988395690918 LOSS_Discriminator: 0.0249427929520607\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 4.958087921142578 LOSS_Discriminator: 0.04248933121562004\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 5.8684234619140625 LOSS_Discriminator: 0.0501345619559288\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 5.205272674560547 LOSS_Discriminator: 0.06012937054038048\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 5.487709045410156 LOSS_Discriminator: 0.12879037857055664\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 5.27213716506958 LOSS_Discriminator: 0.10793284326791763\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 5.356838703155518 LOSS_Discriminator: 0.1361379772424698\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 4.374847412109375 LOSS_Discriminator: 0.16566608846187592\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 3.775890588760376 LOSS_Discriminator: 0.24673037230968475\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 3.4694061279296875 LOSS_Discriminator: 0.18988387286663055\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 3.8523612022399902 LOSS_Discriminator: 0.12808877229690552\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 3.7102091312408447 LOSS_Discriminator: 0.19437849521636963\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 4.878603458404541 LOSS_Discriminator: 0.25878486037254333\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 4.33627462387085 LOSS_Discriminator: 0.27730292081832886\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 5.050140380859375 LOSS_Discriminator: 0.04001760110259056\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 4.483336448669434 LOSS_Discriminator: 0.29088902473449707\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 4.491574287414551 LOSS_Discriminator: 0.06156007945537567\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 3.8730075359344482 LOSS_Discriminator: 0.06019818037748337\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 4.499671936035156 LOSS_Discriminator: 0.1603706330060959\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 5.075424671173096 LOSS_Discriminator: 0.037789490073919296\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 5.434637069702148 LOSS_Discriminator: 0.12492328882217407\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 6.5051655769348145 LOSS_Discriminator: 0.21741950511932373\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 6.499692916870117 LOSS_Discriminator: 0.05681799724698067\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 6.069517612457275 LOSS_Discriminator: 0.06543448567390442\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 6.43742036819458 LOSS_Discriminator: 0.19381386041641235\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 4.985159873962402 LOSS_Discriminator: 0.09478753060102463\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 4.932134628295898 LOSS_Discriminator: 0.15177619457244873\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 5.080035209655762 LOSS_Discriminator: 0.19625641405582428\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 4.6772871017456055 LOSS_Discriminator: 0.06473951041698456\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 4.323521614074707 LOSS_Discriminator: 0.044572025537490845\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 4.293646812438965 LOSS_Discriminator: 0.08323674649000168\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 4.637420177459717 LOSS_Discriminator: 0.054696179926395416\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 5.2685089111328125 LOSS_Discriminator: 0.22876158356666565\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 4.795951843261719 LOSS_Discriminator: 0.3093181848526001\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 3.827224016189575 LOSS_Discriminator: 0.19417281448841095\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 2.9909420013427734 LOSS_Discriminator: 0.09670446813106537\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 3.59291934967041 LOSS_Discriminator: 0.2893613278865814\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 4.349056243896484 LOSS_Discriminator: 0.12951229512691498\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 4.955309867858887 LOSS_Discriminator: 0.06552882492542267\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 5.489764213562012 LOSS_Discriminator: 0.03796794265508652\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 6.580669403076172 LOSS_Discriminator: 0.05496193468570709\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 6.201964855194092 LOSS_Discriminator: 0.21025821566581726\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 7.201635837554932 LOSS_Discriminator: 0.1465756893157959\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 6.590627670288086 LOSS_Discriminator: 0.18390107154846191\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 6.5408525466918945 LOSS_Discriminator: 0.007375784683972597\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 5.7630157470703125 LOSS_Discriminator: 0.09969747066497803\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 5.766241073608398 LOSS_Discriminator: 0.1815066933631897\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 4.981665134429932 LOSS_Discriminator: 0.12459452450275421\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 4.4269700050354 LOSS_Discriminator: 0.16713403165340424\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 3.5964457988739014 LOSS_Discriminator: 0.13951115310192108\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 3.8802168369293213 LOSS_Discriminator: 0.08781546354293823\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 4.489072799682617 LOSS_Discriminator: 0.18119274079799652\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 5.069501876831055 LOSS_Discriminator: 0.0744372010231018\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 4.821052551269531 LOSS_Discriminator: 0.09637708961963654\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 5.08310079574585 LOSS_Discriminator: 0.0682399570941925\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 5.678164958953857 LOSS_Discriminator: 0.2582220137119293\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 5.189948558807373 LOSS_Discriminator: 0.09287045150995255\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 4.380958080291748 LOSS_Discriminator: 0.14296546578407288\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 4.693115711212158 LOSS_Discriminator: 0.06294097751379013\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 5.050565719604492 LOSS_Discriminator: 0.04612632840871811\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 5.500479221343994 LOSS_Discriminator: 0.11478564143180847\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 5.379327297210693 LOSS_Discriminator: 0.044939808547496796\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.79585599899292 LOSS_Discriminator: 0.15029987692832947\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 5.117868423461914 LOSS_Discriminator: 0.04254826903343201\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 5.431889533996582 LOSS_Discriminator: 0.09600801020860672\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 5.12947416305542 LOSS_Discriminator: 0.05091379955410957\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 4.77388334274292 LOSS_Discriminator: 0.04988174885511398\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 5.558113098144531 LOSS_Discriminator: 0.04058494791388512\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 5.05482292175293 LOSS_Discriminator: 0.054964885115623474\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 5.544097900390625 LOSS_Discriminator: 0.12208440899848938\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 5.08601713180542 LOSS_Discriminator: 0.209114670753479\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 4.843420028686523 LOSS_Discriminator: 0.06238797307014465\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 5.2383575439453125 LOSS_Discriminator: 0.03357069939374924\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 4.860464096069336 LOSS_Discriminator: 0.04039948433637619\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 5.040816307067871 LOSS_Discriminator: 0.06135372072458267\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 5.625621795654297 LOSS_Discriminator: 0.037450581789016724\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 6.4132537841796875 LOSS_Discriminator: 0.21173708140850067\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 5.897343635559082 LOSS_Discriminator: 0.018946871161460876\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 6.097719192504883 LOSS_Discriminator: 0.2840748131275177\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 4.936892986297607 LOSS_Discriminator: 0.02591339312493801\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 5.334747791290283 LOSS_Discriminator: 0.08244960755109787\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 4.97394323348999 LOSS_Discriminator: 0.10973204672336578\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 4.10130500793457 LOSS_Discriminator: 0.3205515742301941\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 3.822599411010742 LOSS_Discriminator: 0.20728939771652222\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 3.0129995346069336 LOSS_Discriminator: 0.18984094262123108\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 4.6806440353393555 LOSS_Discriminator: 0.28438007831573486\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 5.258200645446777 LOSS_Discriminator: 0.08300004154443741\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 6.058913230895996 LOSS_Discriminator: 0.0753978043794632\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 6.893203258514404 LOSS_Discriminator: 0.2864019274711609\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 6.774026870727539 LOSS_Discriminator: 0.006173109170049429\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 6.622776508331299 LOSS_Discriminator: 0.2625643312931061\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 6.038764953613281 LOSS_Discriminator: 0.4350138306617737\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 6.384818077087402 LOSS_Discriminator: 0.10136693716049194\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 5.92804479598999 LOSS_Discriminator: 0.25852346420288086\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 4.77017879486084 LOSS_Discriminator: 0.20432329177856445\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 3.7575597763061523 LOSS_Discriminator: 0.2751874327659607\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 3.6314640045166016 LOSS_Discriminator: 0.3436034321784973\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 4.094137668609619 LOSS_Discriminator: 0.20912876725196838\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 4.350566387176514 LOSS_Discriminator: 0.18450753390789032\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 5.519593238830566 LOSS_Discriminator: 0.17125403881072998\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 5.45536470413208 LOSS_Discriminator: 0.43579745292663574\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 5.658265113830566 LOSS_Discriminator: 0.24610312283039093\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 4.987083911895752 LOSS_Discriminator: 0.2438281774520874\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 4.446218967437744 LOSS_Discriminator: 0.15485110878944397\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 4.569291591644287 LOSS_Discriminator: 0.08017776906490326\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 3.9115617275238037 LOSS_Discriminator: 0.23423542082309723\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 3.9041550159454346 LOSS_Discriminator: 0.15836101770401\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 3.961190938949585 LOSS_Discriminator: 0.17719712853431702\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 4.453298091888428 LOSS_Discriminator: 0.2050151824951172\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 4.22667121887207 LOSS_Discriminator: 0.08646053075790405\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 4.870972156524658 LOSS_Discriminator: 0.0744132399559021\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 4.5939435958862305 LOSS_Discriminator: 0.14023789763450623\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 4.888944625854492 LOSS_Discriminator: 0.23102690279483795\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 4.6646342277526855 LOSS_Discriminator: 0.32092657685279846\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 4.504613876342773 LOSS_Discriminator: 0.0672709047794342\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 4.354004859924316 LOSS_Discriminator: 0.09442876279354095\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 4.631516933441162 LOSS_Discriminator: 0.0892384797334671\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 4.6341986656188965 LOSS_Discriminator: 0.21117398142814636\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 4.613668441772461 LOSS_Discriminator: 0.10936768352985382\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 4.6429548263549805 LOSS_Discriminator: 0.16857895255088806\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 4.869438648223877 LOSS_Discriminator: 0.04257972165942192\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 4.9650774002075195 LOSS_Discriminator: 0.1243610605597496\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 4.472545146942139 LOSS_Discriminator: 0.04495776817202568\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 4.844612121582031 LOSS_Discriminator: 0.11110632121562958\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 4.092453956604004 LOSS_Discriminator: 0.10971859097480774\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 4.220550537109375 LOSS_Discriminator: 0.14155003428459167\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 4.709102630615234 LOSS_Discriminator: 0.09099023789167404\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 4.886568069458008 LOSS_Discriminator: 0.18467506766319275\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 4.745368003845215 LOSS_Discriminator: 0.035257283598184586\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 5.208003997802734 LOSS_Discriminator: 0.05630539730191231\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 5.457300662994385 LOSS_Discriminator: 0.12104345858097076\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 5.515710830688477 LOSS_Discriminator: 0.13557139039039612\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 5.202285289764404 LOSS_Discriminator: 0.09888287633657455\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 4.802914619445801 LOSS_Discriminator: 0.10225112736225128\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 4.9338531494140625 LOSS_Discriminator: 0.0878814086318016\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 5.207571506500244 LOSS_Discriminator: 0.05398797243833542\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 4.4153337478637695 LOSS_Discriminator: 0.09399227052927017\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 4.996250152587891 LOSS_Discriminator: 0.14455768465995789\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 5.208219528198242 LOSS_Discriminator: 0.05305393040180206\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 4.890259742736816 LOSS_Discriminator: 0.1443590223789215\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 4.6594390869140625 LOSS_Discriminator: 0.23802636563777924\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 4.255353927612305 LOSS_Discriminator: 0.09470362961292267\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 4.18760871887207 LOSS_Discriminator: 0.18897444009780884\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 3.3939740657806396 LOSS_Discriminator: 0.14927633106708527\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 4.341099739074707 LOSS_Discriminator: 0.18467441201210022\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 4.209866523742676 LOSS_Discriminator: 0.24001292884349823\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 5.067790508270264 LOSS_Discriminator: 0.15689301490783691\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 5.003684043884277 LOSS_Discriminator: 0.17109808325767517\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 5.521481513977051 LOSS_Discriminator: 0.16245780885219574\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 4.986512184143066 LOSS_Discriminator: 0.24815836548805237\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 4.511328220367432 LOSS_Discriminator: 0.27291274070739746\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 3.884385347366333 LOSS_Discriminator: 0.17550073564052582\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 2.9057445526123047 LOSS_Discriminator: 0.1664368212223053\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 3.5087780952453613 LOSS_Discriminator: 0.1806355118751526\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 3.7748003005981445 LOSS_Discriminator: 0.12337428331375122\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 4.672104835510254 LOSS_Discriminator: 0.08583366870880127\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 4.954597473144531 LOSS_Discriminator: 0.16865825653076172\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 5.55580472946167 LOSS_Discriminator: 0.1074913963675499\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 5.496586799621582 LOSS_Discriminator: 0.08894948661327362\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 5.234587669372559 LOSS_Discriminator: 0.200710266828537\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 5.030799865722656 LOSS_Discriminator: 0.2512722909450531\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 4.781008720397949 LOSS_Discriminator: 0.20126904547214508\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 4.194278717041016 LOSS_Discriminator: 0.1976586878299713\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 3.9369449615478516 LOSS_Discriminator: 0.14129650592803955\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 3.2858195304870605 LOSS_Discriminator: 0.12037472426891327\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 4.191730499267578 LOSS_Discriminator: 0.1080264151096344\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 4.05537223815918 LOSS_Discriminator: 0.10798805952072144\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 5.165468215942383 LOSS_Discriminator: 0.05773835629224777\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 5.9861650466918945 LOSS_Discriminator: 0.0894521027803421\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 6.353409767150879 LOSS_Discriminator: 0.38635584712028503\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 5.332403182983398 LOSS_Discriminator: 0.1724540740251541\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 4.917470932006836 LOSS_Discriminator: 0.08826866745948792\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 4.2414140701293945 LOSS_Discriminator: 0.12410694360733032\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 3.6658034324645996 LOSS_Discriminator: 0.138640359044075\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 3.235551357269287 LOSS_Discriminator: 0.1276756376028061\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 3.7728447914123535 LOSS_Discriminator: 0.0629202276468277\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 4.057348251342773 LOSS_Discriminator: 0.16382697224617004\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 4.747749328613281 LOSS_Discriminator: 0.14359000325202942\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 4.712742805480957 LOSS_Discriminator: 0.08401593565940857\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 5.392610549926758 LOSS_Discriminator: 0.1125309020280838\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 5.680140018463135 LOSS_Discriminator: 0.04872673377394676\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 5.7372846603393555 LOSS_Discriminator: 0.2637556791305542\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 5.346505165100098 LOSS_Discriminator: 0.2354331761598587\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 4.458967208862305 LOSS_Discriminator: 0.27331891655921936\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 4.108880043029785 LOSS_Discriminator: 0.06313654035329819\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 4.016542911529541 LOSS_Discriminator: 0.1320783942937851\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 4.44821834564209 LOSS_Discriminator: 0.11859329044818878\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 4.861061096191406 LOSS_Discriminator: 0.10775905847549438\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 4.68721342086792 LOSS_Discriminator: 0.16578322649002075\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 5.68898868560791 LOSS_Discriminator: 0.05471223592758179\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 5.085526466369629 LOSS_Discriminator: 0.20801499485969543\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 5.155608654022217 LOSS_Discriminator: 0.2648290693759918\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 5.27815055847168 LOSS_Discriminator: 0.10425591468811035\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 5.389125823974609 LOSS_Discriminator: 0.1781342327594757\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 4.208636283874512 LOSS_Discriminator: 0.1561361700296402\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 3.7422449588775635 LOSS_Discriminator: 0.11636019498109818\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 3.7143449783325195 LOSS_Discriminator: 0.09419306367635727\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 4.642079830169678 LOSS_Discriminator: 0.1258988082408905\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 5.514893531799316 LOSS_Discriminator: 0.14448675513267517\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 5.234160423278809 LOSS_Discriminator: 0.1160963922739029\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 5.899412155151367 LOSS_Discriminator: 0.04448314011096954\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 5.856175422668457 LOSS_Discriminator: 0.015539295971393585\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 5.657617568969727 LOSS_Discriminator: 0.2339669167995453\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 4.626067638397217 LOSS_Discriminator: 0.17181575298309326\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 3.6992788314819336 LOSS_Discriminator: 0.21781989932060242\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 3.3748745918273926 LOSS_Discriminator: 0.08574548363685608\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 3.482595443725586 LOSS_Discriminator: 0.22749632596969604\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 4.190300941467285 LOSS_Discriminator: 0.12051298469305038\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 5.957272529602051 LOSS_Discriminator: 0.026486016809940338\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 5.907484531402588 LOSS_Discriminator: 0.04257550835609436\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 6.387456893920898 LOSS_Discriminator: 0.4658236503601074\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 5.567635536193848 LOSS_Discriminator: 0.03967921808362007\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 5.72449254989624 LOSS_Discriminator: 0.1467200517654419\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 5.615943908691406 LOSS_Discriminator: 0.015067829750478268\n",
            "EPOCH OVER: 17\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.475392818450928 LOSS_Discriminator: 0.10306206345558167\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.1337571144104 LOSS_Discriminator: 0.02575816959142685\n",
            "ITERATION_NO.: 3 LOSS_Generator: 4.502022743225098 LOSS_Discriminator: 0.07085075974464417\n",
            "ITERATION_NO.: 4 LOSS_Generator: 4.442655563354492 LOSS_Discriminator: 0.22896620631217957\n",
            "ITERATION_NO.: 5 LOSS_Generator: 4.141959190368652 LOSS_Discriminator: 0.19863617420196533\n",
            "ITERATION_NO.: 6 LOSS_Generator: 4.235015392303467 LOSS_Discriminator: 0.18962612748146057\n",
            "ITERATION_NO.: 7 LOSS_Generator: 4.619768142700195 LOSS_Discriminator: 0.2865524888038635\n",
            "ITERATION_NO.: 8 LOSS_Generator: 4.562487602233887 LOSS_Discriminator: 0.05871742591261864\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.393388748168945 LOSS_Discriminator: 0.05359438434243202\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.37373685836792 LOSS_Discriminator: 0.026533160358667374\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.863161563873291 LOSS_Discriminator: 0.2611533999443054\n",
            "ITERATION_NO.: 12 LOSS_Generator: 6.195635795593262 LOSS_Discriminator: 0.12020844966173172\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.898610591888428 LOSS_Discriminator: 0.0644150897860527\n",
            "ITERATION_NO.: 14 LOSS_Generator: 4.5698466300964355 LOSS_Discriminator: 0.37656688690185547\n",
            "ITERATION_NO.: 15 LOSS_Generator: 4.719236373901367 LOSS_Discriminator: 0.09339825809001923\n",
            "ITERATION_NO.: 16 LOSS_Generator: 3.862924098968506 LOSS_Discriminator: 0.11497262120246887\n",
            "ITERATION_NO.: 17 LOSS_Generator: 3.3100485801696777 LOSS_Discriminator: 0.24247612059116364\n",
            "ITERATION_NO.: 18 LOSS_Generator: 3.5476861000061035 LOSS_Discriminator: 0.14668363332748413\n",
            "ITERATION_NO.: 19 LOSS_Generator: 3.6778838634490967 LOSS_Discriminator: 0.13363242149353027\n",
            "ITERATION_NO.: 20 LOSS_Generator: 4.31338357925415 LOSS_Discriminator: 0.11875952780246735\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.461335182189941 LOSS_Discriminator: 0.0945364385843277\n",
            "ITERATION_NO.: 22 LOSS_Generator: 6.26082181930542 LOSS_Discriminator: 0.15445296466350555\n",
            "ITERATION_NO.: 23 LOSS_Generator: 7.0414910316467285 LOSS_Discriminator: 0.07526589930057526\n",
            "ITERATION_NO.: 24 LOSS_Generator: 7.012483596801758 LOSS_Discriminator: 0.1911153793334961\n",
            "ITERATION_NO.: 25 LOSS_Generator: 6.363872528076172 LOSS_Discriminator: 0.133254274725914\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.775177478790283 LOSS_Discriminator: 0.4755091071128845\n",
            "ITERATION_NO.: 27 LOSS_Generator: 4.5435590744018555 LOSS_Discriminator: 0.12402394413948059\n",
            "ITERATION_NO.: 28 LOSS_Generator: 4.648106575012207 LOSS_Discriminator: 0.1588486284017563\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.0777812004089355 LOSS_Discriminator: 0.1039038896560669\n",
            "ITERATION_NO.: 30 LOSS_Generator: 4.155423641204834 LOSS_Discriminator: 0.3760068416595459\n",
            "ITERATION_NO.: 31 LOSS_Generator: 3.590366840362549 LOSS_Discriminator: 0.07859542220830917\n",
            "ITERATION_NO.: 32 LOSS_Generator: 4.551722526550293 LOSS_Discriminator: 0.1430286169052124\n",
            "ITERATION_NO.: 33 LOSS_Generator: 4.82124137878418 LOSS_Discriminator: 0.27641329169273376\n",
            "ITERATION_NO.: 34 LOSS_Generator: 4.5892720222473145 LOSS_Discriminator: 0.18979032337665558\n",
            "ITERATION_NO.: 35 LOSS_Generator: 4.043966770172119 LOSS_Discriminator: 0.17774468660354614\n",
            "ITERATION_NO.: 36 LOSS_Generator: 4.580896854400635 LOSS_Discriminator: 0.16331136226654053\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.002956390380859 LOSS_Discriminator: 0.03234473615884781\n",
            "ITERATION_NO.: 38 LOSS_Generator: 4.579973220825195 LOSS_Discriminator: 0.07761520147323608\n",
            "ITERATION_NO.: 39 LOSS_Generator: 4.6347246170043945 LOSS_Discriminator: 0.1126086637377739\n",
            "ITERATION_NO.: 40 LOSS_Generator: 4.5227460861206055 LOSS_Discriminator: 0.12375032901763916\n",
            "ITERATION_NO.: 41 LOSS_Generator: 3.769120931625366 LOSS_Discriminator: 0.14740288257598877\n",
            "ITERATION_NO.: 42 LOSS_Generator: 4.155903339385986 LOSS_Discriminator: 0.13105542957782745\n",
            "ITERATION_NO.: 43 LOSS_Generator: 4.105548858642578 LOSS_Discriminator: 0.11793020367622375\n",
            "ITERATION_NO.: 44 LOSS_Generator: 4.004651069641113 LOSS_Discriminator: 0.10097964107990265\n",
            "ITERATION_NO.: 45 LOSS_Generator: 4.3220415115356445 LOSS_Discriminator: 0.15825806558132172\n",
            "ITERATION_NO.: 46 LOSS_Generator: 4.4506072998046875 LOSS_Discriminator: 0.24500882625579834\n",
            "ITERATION_NO.: 47 LOSS_Generator: 4.792049407958984 LOSS_Discriminator: 0.18099115788936615\n",
            "ITERATION_NO.: 48 LOSS_Generator: 4.814418792724609 LOSS_Discriminator: 0.03023538365960121\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.2054314613342285 LOSS_Discriminator: 0.04018057882785797\n",
            "ITERATION_NO.: 50 LOSS_Generator: 4.56009578704834 LOSS_Discriminator: 0.4498087465763092\n",
            "ITERATION_NO.: 51 LOSS_Generator: 4.3267035484313965 LOSS_Discriminator: 0.20903706550598145\n",
            "ITERATION_NO.: 52 LOSS_Generator: 4.245110034942627 LOSS_Discriminator: 0.1764017790555954\n",
            "ITERATION_NO.: 53 LOSS_Generator: 3.9147510528564453 LOSS_Discriminator: 0.14372164011001587\n",
            "ITERATION_NO.: 54 LOSS_Generator: 4.064311981201172 LOSS_Discriminator: 0.1448918730020523\n",
            "ITERATION_NO.: 55 LOSS_Generator: 4.2959489822387695 LOSS_Discriminator: 0.10675809532403946\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.334866523742676 LOSS_Discriminator: 0.04732298105955124\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.1450982093811035 LOSS_Discriminator: 0.11068575084209442\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.900273323059082 LOSS_Discriminator: 0.26968449354171753\n",
            "ITERATION_NO.: 59 LOSS_Generator: 4.995068073272705 LOSS_Discriminator: 0.2026047259569168\n",
            "ITERATION_NO.: 60 LOSS_Generator: 3.963684320449829 LOSS_Discriminator: 0.13718755543231964\n",
            "ITERATION_NO.: 61 LOSS_Generator: 4.412671089172363 LOSS_Discriminator: 0.05226504057645798\n",
            "ITERATION_NO.: 62 LOSS_Generator: 4.5357666015625 LOSS_Discriminator: 0.12869879603385925\n",
            "ITERATION_NO.: 63 LOSS_Generator: 4.187723636627197 LOSS_Discriminator: 0.11490035057067871\n",
            "ITERATION_NO.: 64 LOSS_Generator: 4.254781723022461 LOSS_Discriminator: 0.0647956058382988\n",
            "ITERATION_NO.: 65 LOSS_Generator: 4.162328720092773 LOSS_Discriminator: 0.08358165621757507\n",
            "ITERATION_NO.: 66 LOSS_Generator: 4.325971603393555 LOSS_Discriminator: 0.10051321983337402\n",
            "ITERATION_NO.: 67 LOSS_Generator: 4.3624773025512695 LOSS_Discriminator: 0.14585089683532715\n",
            "ITERATION_NO.: 68 LOSS_Generator: 4.359231948852539 LOSS_Discriminator: 0.14823269844055176\n",
            "ITERATION_NO.: 69 LOSS_Generator: 4.489092826843262 LOSS_Discriminator: 0.05706879869103432\n",
            "ITERATION_NO.: 70 LOSS_Generator: 4.9089179039001465 LOSS_Discriminator: 0.040276043117046356\n",
            "ITERATION_NO.: 71 LOSS_Generator: 4.614914894104004 LOSS_Discriminator: 0.1256079226732254\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.0576066970825195 LOSS_Discriminator: 0.04538135230541229\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.176965713500977 LOSS_Discriminator: 0.018168434500694275\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.391934394836426 LOSS_Discriminator: 0.03897351771593094\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.022693634033203 LOSS_Discriminator: 0.20851236581802368\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.306464672088623 LOSS_Discriminator: 0.043523188680410385\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.131930351257324 LOSS_Discriminator: 0.051733437925577164\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.348940849304199 LOSS_Discriminator: 0.021755479276180267\n",
            "ITERATION_NO.: 79 LOSS_Generator: 4.9608025550842285 LOSS_Discriminator: 0.26702284812927246\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.008930206298828 LOSS_Discriminator: 0.07695204019546509\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.04197883605957 LOSS_Discriminator: 0.12439374625682831\n",
            "ITERATION_NO.: 82 LOSS_Generator: 4.876528263092041 LOSS_Discriminator: 0.12456510961055756\n",
            "ITERATION_NO.: 83 LOSS_Generator: 4.2936906814575195 LOSS_Discriminator: 0.13285128772258759\n",
            "ITERATION_NO.: 84 LOSS_Generator: 3.561408519744873 LOSS_Discriminator: 0.26727938652038574\n",
            "ITERATION_NO.: 85 LOSS_Generator: 4.182586193084717 LOSS_Discriminator: 0.10009661316871643\n",
            "ITERATION_NO.: 86 LOSS_Generator: 4.285393714904785 LOSS_Discriminator: 0.10627254098653793\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.410892486572266 LOSS_Discriminator: 0.14427074790000916\n",
            "ITERATION_NO.: 88 LOSS_Generator: 6.167223930358887 LOSS_Discriminator: 0.023012269288301468\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.42906379699707 LOSS_Discriminator: 0.23391851782798767\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.589360237121582 LOSS_Discriminator: 0.12343621999025345\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.833303928375244 LOSS_Discriminator: 0.17565728724002838\n",
            "ITERATION_NO.: 92 LOSS_Generator: 4.911372184753418 LOSS_Discriminator: 0.10609528422355652\n",
            "ITERATION_NO.: 93 LOSS_Generator: 3.7639410495758057 LOSS_Discriminator: 0.17146587371826172\n",
            "ITERATION_NO.: 94 LOSS_Generator: 3.8584117889404297 LOSS_Discriminator: 0.09131893515586853\n",
            "ITERATION_NO.: 95 LOSS_Generator: 3.914919853210449 LOSS_Discriminator: 0.0850101187825203\n",
            "ITERATION_NO.: 96 LOSS_Generator: 4.072628498077393 LOSS_Discriminator: 0.05906970798969269\n",
            "ITERATION_NO.: 97 LOSS_Generator: 4.502541542053223 LOSS_Discriminator: 0.05711037293076515\n",
            "ITERATION_NO.: 98 LOSS_Generator: 4.418234348297119 LOSS_Discriminator: 0.1668991893529892\n",
            "ITERATION_NO.: 99 LOSS_Generator: 4.857975006103516 LOSS_Discriminator: 0.13052639365196228\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.081889629364014 LOSS_Discriminator: 0.03922782838344574\n",
            "ITERATION_NO.: 101 LOSS_Generator: 4.858856201171875 LOSS_Discriminator: 0.2122972011566162\n",
            "ITERATION_NO.: 102 LOSS_Generator: 4.8707356452941895 LOSS_Discriminator: 0.346169650554657\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.292644500732422 LOSS_Discriminator: 0.0868968740105629\n",
            "ITERATION_NO.: 104 LOSS_Generator: 4.2691874504089355 LOSS_Discriminator: 0.1943139135837555\n",
            "ITERATION_NO.: 105 LOSS_Generator: 4.963367462158203 LOSS_Discriminator: 0.1900440752506256\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.08234977722168 LOSS_Discriminator: 0.09861809760332108\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.086645126342773 LOSS_Discriminator: 0.08241736888885498\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.315479755401611 LOSS_Discriminator: 0.10245504975318909\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.292896747589111 LOSS_Discriminator: 0.12502385675907135\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.664365291595459 LOSS_Discriminator: 0.05856959521770477\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.291014194488525 LOSS_Discriminator: 0.09246958792209625\n",
            "ITERATION_NO.: 112 LOSS_Generator: 4.90502405166626 LOSS_Discriminator: 0.2218373566865921\n",
            "ITERATION_NO.: 113 LOSS_Generator: 4.923390865325928 LOSS_Discriminator: 0.04891392961144447\n",
            "ITERATION_NO.: 114 LOSS_Generator: 4.2984232902526855 LOSS_Discriminator: 0.17436927556991577\n",
            "ITERATION_NO.: 115 LOSS_Generator: 4.6347503662109375 LOSS_Discriminator: 0.07432577013969421\n",
            "ITERATION_NO.: 116 LOSS_Generator: 4.534049987792969 LOSS_Discriminator: 0.16089075803756714\n",
            "ITERATION_NO.: 117 LOSS_Generator: 4.807776927947998 LOSS_Discriminator: 0.07153181731700897\n",
            "ITERATION_NO.: 118 LOSS_Generator: 4.526421546936035 LOSS_Discriminator: 0.13485375046730042\n",
            "ITERATION_NO.: 119 LOSS_Generator: 4.21352481842041 LOSS_Discriminator: 0.053879816085100174\n",
            "ITERATION_NO.: 120 LOSS_Generator: 4.831060409545898 LOSS_Discriminator: 0.20702794194221497\n",
            "ITERATION_NO.: 121 LOSS_Generator: 4.017022609710693 LOSS_Discriminator: 0.04280119389295578\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.0042643547058105 LOSS_Discriminator: 0.11922737956047058\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.175153732299805 LOSS_Discriminator: 0.11481168121099472\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.82637882232666 LOSS_Discriminator: 0.045331813395023346\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.165370941162109 LOSS_Discriminator: 0.18546445667743683\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.715018272399902 LOSS_Discriminator: 0.05954454094171524\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.042900085449219 LOSS_Discriminator: 0.25485315918922424\n",
            "ITERATION_NO.: 128 LOSS_Generator: 4.881781578063965 LOSS_Discriminator: 0.1286964863538742\n",
            "ITERATION_NO.: 129 LOSS_Generator: 4.334479331970215 LOSS_Discriminator: 0.05940476059913635\n",
            "ITERATION_NO.: 130 LOSS_Generator: 4.4975481033325195 LOSS_Discriminator: 0.07451914250850677\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.693946838378906 LOSS_Discriminator: 0.13474316895008087\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.428048133850098 LOSS_Discriminator: 0.060884565114974976\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.8505754470825195 LOSS_Discriminator: 0.209065780043602\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.2981276512146 LOSS_Discriminator: 0.4617665410041809\n",
            "ITERATION_NO.: 135 LOSS_Generator: 4.394577980041504 LOSS_Discriminator: 0.24389107525348663\n",
            "ITERATION_NO.: 136 LOSS_Generator: 3.556182622909546 LOSS_Discriminator: 0.1717468500137329\n",
            "ITERATION_NO.: 137 LOSS_Generator: 3.4279234409332275 LOSS_Discriminator: 0.21370498836040497\n",
            "ITERATION_NO.: 138 LOSS_Generator: 4.127839088439941 LOSS_Discriminator: 0.1070149764418602\n",
            "ITERATION_NO.: 139 LOSS_Generator: 4.983104705810547 LOSS_Discriminator: 0.07925746589899063\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.366258144378662 LOSS_Discriminator: 0.10628287494182587\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.536408424377441 LOSS_Discriminator: 0.013592934235930443\n",
            "ITERATION_NO.: 142 LOSS_Generator: 7.074833869934082 LOSS_Discriminator: 0.1326199620962143\n",
            "ITERATION_NO.: 143 LOSS_Generator: 6.737247467041016 LOSS_Discriminator: 0.07430592179298401\n",
            "ITERATION_NO.: 144 LOSS_Generator: 6.913891792297363 LOSS_Discriminator: 0.2270941436290741\n",
            "ITERATION_NO.: 145 LOSS_Generator: 6.5183820724487305 LOSS_Discriminator: 0.09137789160013199\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.636630058288574 LOSS_Discriminator: 0.17717282474040985\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.096991062164307 LOSS_Discriminator: 0.103412926197052\n",
            "ITERATION_NO.: 148 LOSS_Generator: 4.091980934143066 LOSS_Discriminator: 0.19993533194065094\n",
            "ITERATION_NO.: 149 LOSS_Generator: 3.3299572467803955 LOSS_Discriminator: 0.10477887094020844\n",
            "ITERATION_NO.: 150 LOSS_Generator: 3.725975751876831 LOSS_Discriminator: 0.19487541913986206\n",
            "ITERATION_NO.: 151 LOSS_Generator: 4.76002836227417 LOSS_Discriminator: 0.05260433629155159\n",
            "ITERATION_NO.: 152 LOSS_Generator: 4.589957237243652 LOSS_Discriminator: 0.2744140625\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.1482439041137695 LOSS_Discriminator: 0.023555414751172066\n",
            "ITERATION_NO.: 154 LOSS_Generator: 4.599577903747559 LOSS_Discriminator: 0.39194995164871216\n",
            "ITERATION_NO.: 155 LOSS_Generator: 4.870561599731445 LOSS_Discriminator: 0.24614210426807404\n",
            "ITERATION_NO.: 156 LOSS_Generator: 4.753731727600098 LOSS_Discriminator: 0.30302613973617554\n",
            "ITERATION_NO.: 157 LOSS_Generator: 3.6488184928894043 LOSS_Discriminator: 0.14278937876224518\n",
            "ITERATION_NO.: 158 LOSS_Generator: 3.6157169342041016 LOSS_Discriminator: 0.12154361605644226\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.044251441955566 LOSS_Discriminator: 0.2170986831188202\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.463726997375488 LOSS_Discriminator: 0.06110875681042671\n",
            "ITERATION_NO.: 161 LOSS_Generator: 6.28470516204834 LOSS_Discriminator: 0.05265596881508827\n",
            "ITERATION_NO.: 162 LOSS_Generator: 6.286405563354492 LOSS_Discriminator: 0.06676800549030304\n",
            "ITERATION_NO.: 163 LOSS_Generator: 6.508975028991699 LOSS_Discriminator: 0.23400509357452393\n",
            "ITERATION_NO.: 164 LOSS_Generator: 6.531119346618652 LOSS_Discriminator: 0.2472042590379715\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.904931545257568 LOSS_Discriminator: 0.207017719745636\n",
            "ITERATION_NO.: 166 LOSS_Generator: 4.836048126220703 LOSS_Discriminator: 0.47146639227867126\n",
            "ITERATION_NO.: 167 LOSS_Generator: 3.9691600799560547 LOSS_Discriminator: 0.16667069494724274\n",
            "ITERATION_NO.: 168 LOSS_Generator: 4.06631326675415 LOSS_Discriminator: 0.17821121215820312\n",
            "ITERATION_NO.: 169 LOSS_Generator: 4.27730655670166 LOSS_Discriminator: 0.13822874426841736\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.034601211547852 LOSS_Discriminator: 0.13094362616539001\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.303921699523926 LOSS_Discriminator: 0.02180810645222664\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.596217155456543 LOSS_Discriminator: 0.08450998365879059\n",
            "ITERATION_NO.: 173 LOSS_Generator: 6.118427753448486 LOSS_Discriminator: 0.10794146358966827\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.322268486022949 LOSS_Discriminator: 0.08128072321414948\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.324784755706787 LOSS_Discriminator: 0.08547332882881165\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.487113952636719 LOSS_Discriminator: 0.017659274861216545\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.067591667175293 LOSS_Discriminator: 0.04186016321182251\n",
            "ITERATION_NO.: 178 LOSS_Generator: 4.688473224639893 LOSS_Discriminator: 0.10215538740158081\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.35080623626709 LOSS_Discriminator: 0.0703863725066185\n",
            "ITERATION_NO.: 180 LOSS_Generator: 4.614070892333984 LOSS_Discriminator: 0.08853527903556824\n",
            "ITERATION_NO.: 181 LOSS_Generator: 4.6344146728515625 LOSS_Discriminator: 0.20608201622962952\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.074495792388916 LOSS_Discriminator: 0.1565537452697754\n",
            "ITERATION_NO.: 183 LOSS_Generator: 4.466346740722656 LOSS_Discriminator: 0.1436917632818222\n",
            "ITERATION_NO.: 184 LOSS_Generator: 4.983583450317383 LOSS_Discriminator: 0.0756262019276619\n",
            "ITERATION_NO.: 185 LOSS_Generator: 4.940640449523926 LOSS_Discriminator: 0.22987398505210876\n",
            "ITERATION_NO.: 186 LOSS_Generator: 4.250054359436035 LOSS_Discriminator: 0.19544708728790283\n",
            "ITERATION_NO.: 187 LOSS_Generator: 4.606325149536133 LOSS_Discriminator: 0.19891174137592316\n",
            "ITERATION_NO.: 188 LOSS_Generator: 4.5083231925964355 LOSS_Discriminator: 0.19987203180789948\n",
            "ITERATION_NO.: 189 LOSS_Generator: 4.453456878662109 LOSS_Discriminator: 0.1388624757528305\n",
            "ITERATION_NO.: 190 LOSS_Generator: 4.345752716064453 LOSS_Discriminator: 0.12790162861347198\n",
            "ITERATION_NO.: 191 LOSS_Generator: 4.538967609405518 LOSS_Discriminator: 0.0764596164226532\n",
            "ITERATION_NO.: 192 LOSS_Generator: 5.060262680053711 LOSS_Discriminator: 0.15330375730991364\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.722932815551758 LOSS_Discriminator: 0.10575991868972778\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.15859317779541 LOSS_Discriminator: 0.17680954933166504\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.168133735656738 LOSS_Discriminator: 0.07519399374723434\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.598217487335205 LOSS_Discriminator: 0.0922500342130661\n",
            "ITERATION_NO.: 197 LOSS_Generator: 4.50889253616333 LOSS_Discriminator: 0.030272366479039192\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.124719619750977 LOSS_Discriminator: 0.12197499722242355\n",
            "ITERATION_NO.: 199 LOSS_Generator: 4.865378379821777 LOSS_Discriminator: 0.07722118496894836\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.20720911026001 LOSS_Discriminator: 0.060397300869226456\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.204641342163086 LOSS_Discriminator: 0.1285441666841507\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.915524482727051 LOSS_Discriminator: 0.025323454290628433\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.299158096313477 LOSS_Discriminator: 0.13097167015075684\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.07150936126709 LOSS_Discriminator: 0.0681227296590805\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.251725196838379 LOSS_Discriminator: 0.05340711027383804\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.306893348693848 LOSS_Discriminator: 0.048031389713287354\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.425283432006836 LOSS_Discriminator: 0.03624650835990906\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.774367332458496 LOSS_Discriminator: 0.10615071654319763\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.328786849975586 LOSS_Discriminator: 0.09944772720336914\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.246377944946289 LOSS_Discriminator: 0.28358209133148193\n",
            "ITERATION_NO.: 211 LOSS_Generator: 4.105396270751953 LOSS_Discriminator: 0.09818253666162491\n",
            "ITERATION_NO.: 212 LOSS_Generator: 3.4130845069885254 LOSS_Discriminator: 0.16358298063278198\n",
            "ITERATION_NO.: 213 LOSS_Generator: 3.3263416290283203 LOSS_Discriminator: 0.2658027410507202\n",
            "ITERATION_NO.: 214 LOSS_Generator: 4.174225330352783 LOSS_Discriminator: 0.19777856767177582\n",
            "ITERATION_NO.: 215 LOSS_Generator: 4.2510199546813965 LOSS_Discriminator: 0.29248282313346863\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.021612644195557 LOSS_Discriminator: 0.23620593547821045\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.529780387878418 LOSS_Discriminator: 0.05844654142856598\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.737826824188232 LOSS_Discriminator: 0.06329309940338135\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.066701889038086 LOSS_Discriminator: 0.12175361067056656\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.641012668609619 LOSS_Discriminator: 0.19334331154823303\n",
            "ITERATION_NO.: 221 LOSS_Generator: 6.027611255645752 LOSS_Discriminator: 0.10136060416698456\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.658536911010742 LOSS_Discriminator: 0.0515386201441288\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.659751892089844 LOSS_Discriminator: 0.2036626785993576\n",
            "ITERATION_NO.: 224 LOSS_Generator: 4.572329521179199 LOSS_Discriminator: 0.175860196352005\n",
            "ITERATION_NO.: 225 LOSS_Generator: 3.7722744941711426 LOSS_Discriminator: 0.07325893640518188\n",
            "ITERATION_NO.: 226 LOSS_Generator: 3.7934794425964355 LOSS_Discriminator: 0.10030664503574371\n",
            "ITERATION_NO.: 227 LOSS_Generator: 4.269989967346191 LOSS_Discriminator: 0.28718629479408264\n",
            "ITERATION_NO.: 228 LOSS_Generator: 4.208960056304932 LOSS_Discriminator: 0.11696342378854752\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.098135471343994 LOSS_Discriminator: 0.13295653462409973\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.301745891571045 LOSS_Discriminator: 0.19713550806045532\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.500438690185547 LOSS_Discriminator: 0.09489095211029053\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.153624057769775 LOSS_Discriminator: 0.2469692975282669\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.017838001251221 LOSS_Discriminator: 0.12986992299556732\n",
            "ITERATION_NO.: 234 LOSS_Generator: 4.568596839904785 LOSS_Discriminator: 0.20065081119537354\n",
            "ITERATION_NO.: 235 LOSS_Generator: 4.867056846618652 LOSS_Discriminator: 0.21084412932395935\n",
            "ITERATION_NO.: 236 LOSS_Generator: 4.149845600128174 LOSS_Discriminator: 0.16175679862499237\n",
            "ITERATION_NO.: 237 LOSS_Generator: 3.1408023834228516 LOSS_Discriminator: 0.09573493897914886\n",
            "ITERATION_NO.: 238 LOSS_Generator: 4.376705646514893 LOSS_Discriminator: 0.3409639000892639\n",
            "ITERATION_NO.: 239 LOSS_Generator: 4.721898078918457 LOSS_Discriminator: 0.08234044909477234\n",
            "ITERATION_NO.: 240 LOSS_Generator: 4.769796371459961 LOSS_Discriminator: 0.19729745388031006\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.398410797119141 LOSS_Discriminator: 0.09006423503160477\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.931901931762695 LOSS_Discriminator: 0.08070977032184601\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.437026023864746 LOSS_Discriminator: 0.005656976252794266\n",
            "ITERATION_NO.: 244 LOSS_Generator: 6.855410099029541 LOSS_Discriminator: 0.15028071403503418\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.481503963470459 LOSS_Discriminator: 0.22894783318042755\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.510925769805908 LOSS_Discriminator: 0.08087923377752304\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.565346717834473 LOSS_Discriminator: 0.018016047775745392\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.155194282531738 LOSS_Discriminator: 0.12716126441955566\n",
            "ITERATION_NO.: 249 LOSS_Generator: 4.775280952453613 LOSS_Discriminator: 0.1730721890926361\n",
            "ITERATION_NO.: 250 LOSS_Generator: 4.143703937530518 LOSS_Discriminator: 0.16559405624866486\n",
            "ITERATION_NO.: 251 LOSS_Generator: 4.0517191886901855 LOSS_Discriminator: 0.15531125664710999\n",
            "ITERATION_NO.: 252 LOSS_Generator: 4.187350273132324 LOSS_Discriminator: 0.1291528195142746\n",
            "ITERATION_NO.: 253 LOSS_Generator: 4.144326210021973 LOSS_Discriminator: 0.14978909492492676\n",
            "ITERATION_NO.: 254 LOSS_Generator: 4.312171936035156 LOSS_Discriminator: 0.4383432865142822\n",
            "ITERATION_NO.: 255 LOSS_Generator: 4.301088333129883 LOSS_Discriminator: 0.3897385001182556\n",
            "ITERATION_NO.: 256 LOSS_Generator: 4.1553850173950195 LOSS_Discriminator: 0.11501697450876236\n",
            "ITERATION_NO.: 257 LOSS_Generator: 4.32433557510376 LOSS_Discriminator: 0.15307502448558807\n",
            "ITERATION_NO.: 258 LOSS_Generator: 5.084283351898193 LOSS_Discriminator: 0.05116130784153938\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.045857906341553 LOSS_Discriminator: 0.08087107539176941\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.192052841186523 LOSS_Discriminator: 0.0358256921172142\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.933258533477783 LOSS_Discriminator: 0.10078631341457367\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.549455642700195 LOSS_Discriminator: 0.04494883120059967\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.621465682983398 LOSS_Discriminator: 0.18245306611061096\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.04340934753418 LOSS_Discriminator: 0.13620543479919434\n",
            "ITERATION_NO.: 265 LOSS_Generator: 4.318376541137695 LOSS_Discriminator: 0.13020503520965576\n",
            "ITERATION_NO.: 266 LOSS_Generator: 4.275873184204102 LOSS_Discriminator: 0.08822541683912277\n",
            "ITERATION_NO.: 267 LOSS_Generator: 4.120501518249512 LOSS_Discriminator: 0.13177889585494995\n",
            "ITERATION_NO.: 268 LOSS_Generator: 3.6680874824523926 LOSS_Discriminator: 0.12953075766563416\n",
            "ITERATION_NO.: 269 LOSS_Generator: 3.6592276096343994 LOSS_Discriminator: 0.05508582666516304\n",
            "ITERATION_NO.: 270 LOSS_Generator: 3.9886269569396973 LOSS_Discriminator: 0.15737350285053253\n",
            "ITERATION_NO.: 271 LOSS_Generator: 4.501394748687744 LOSS_Discriminator: 0.11058403551578522\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.318322658538818 LOSS_Discriminator: 0.036226335912942886\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.423182964324951 LOSS_Discriminator: 0.0482039712369442\n",
            "ITERATION_NO.: 274 LOSS_Generator: 6.1198248863220215 LOSS_Discriminator: 0.03516007214784622\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.197957992553711 LOSS_Discriminator: 0.0974409282207489\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.537171363830566 LOSS_Discriminator: 0.1357036679983139\n",
            "ITERATION_NO.: 277 LOSS_Generator: 4.315668106079102 LOSS_Discriminator: 0.19412624835968018\n",
            "ITERATION_NO.: 278 LOSS_Generator: 4.628382682800293 LOSS_Discriminator: 0.14216285943984985\n",
            "ITERATION_NO.: 279 LOSS_Generator: 3.416642189025879 LOSS_Discriminator: 0.16203519701957703\n",
            "ITERATION_NO.: 280 LOSS_Generator: 4.2452521324157715 LOSS_Discriminator: 0.1289629191160202\n",
            "ITERATION_NO.: 281 LOSS_Generator: 4.686191558837891 LOSS_Discriminator: 0.043938905000686646\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.331963539123535 LOSS_Discriminator: 0.07611790299415588\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.876099586486816 LOSS_Discriminator: 0.05892297625541687\n",
            "ITERATION_NO.: 284 LOSS_Generator: 6.679141998291016 LOSS_Discriminator: 0.12099162489175797\n",
            "ITERATION_NO.: 285 LOSS_Generator: 6.287243843078613 LOSS_Discriminator: 0.1682996302843094\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.729358196258545 LOSS_Discriminator: 0.03492191806435585\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.541281223297119 LOSS_Discriminator: 0.1784999817609787\n",
            "ITERATION_NO.: 288 LOSS_Generator: 4.686919689178467 LOSS_Discriminator: 0.0566360205411911\n",
            "ITERATION_NO.: 289 LOSS_Generator: 4.264628887176514 LOSS_Discriminator: 0.10378004610538483\n",
            "ITERATION_NO.: 290 LOSS_Generator: 3.7317004203796387 LOSS_Discriminator: 0.057688236236572266\n",
            "ITERATION_NO.: 291 LOSS_Generator: 4.399421691894531 LOSS_Discriminator: 0.06414283812046051\n",
            "ITERATION_NO.: 292 LOSS_Generator: 4.560487747192383 LOSS_Discriminator: 0.13278156518936157\n",
            "ITERATION_NO.: 293 LOSS_Generator: 4.961319923400879 LOSS_Discriminator: 0.09160389006137848\n",
            "ITERATION_NO.: 294 LOSS_Generator: 4.25001859664917 LOSS_Discriminator: 0.18456429243087769\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.010217189788818 LOSS_Discriminator: 0.06324358284473419\n",
            "ITERATION_NO.: 296 LOSS_Generator: 4.983549118041992 LOSS_Discriminator: 0.050738103687763214\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.418980121612549 LOSS_Discriminator: 0.14115303754806519\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.602432727813721 LOSS_Discriminator: 0.024515239521861076\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.265611171722412 LOSS_Discriminator: 0.14812549948692322\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.645502090454102 LOSS_Discriminator: 0.059289075434207916\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.955456733703613 LOSS_Discriminator: 0.16400769352912903\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.008574962615967 LOSS_Discriminator: 0.2774501442909241\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.687209129333496 LOSS_Discriminator: 0.33285126090049744\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.3458476066589355 LOSS_Discriminator: 0.15930163860321045\n",
            "ITERATION_NO.: 305 LOSS_Generator: 4.203606128692627 LOSS_Discriminator: 0.03560315817594528\n",
            "ITERATION_NO.: 306 LOSS_Generator: 4.489083290100098 LOSS_Discriminator: 0.0667029395699501\n",
            "ITERATION_NO.: 307 LOSS_Generator: 4.6202921867370605 LOSS_Discriminator: 0.044681817293167114\n",
            "ITERATION_NO.: 308 LOSS_Generator: 4.406278133392334 LOSS_Discriminator: 0.04129887744784355\n",
            "ITERATION_NO.: 309 LOSS_Generator: 4.552197456359863 LOSS_Discriminator: 0.15818071365356445\n",
            "ITERATION_NO.: 310 LOSS_Generator: 4.74717378616333 LOSS_Discriminator: 0.045039013028144836\n",
            "ITERATION_NO.: 311 LOSS_Generator: 4.934840679168701 LOSS_Discriminator: 0.0505465567111969\n",
            "ITERATION_NO.: 312 LOSS_Generator: 4.9886674880981445 LOSS_Discriminator: 0.08987163007259369\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.653197288513184 LOSS_Discriminator: 0.04090183228254318\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.021403789520264 LOSS_Discriminator: 0.09638270735740662\n",
            "ITERATION_NO.: 315 LOSS_Generator: 4.845115661621094 LOSS_Discriminator: 0.16514691710472107\n",
            "ITERATION_NO.: 316 LOSS_Generator: 4.974061489105225 LOSS_Discriminator: 0.04703103005886078\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.093687057495117 LOSS_Discriminator: 0.09108075499534607\n",
            "ITERATION_NO.: 318 LOSS_Generator: 4.734467506408691 LOSS_Discriminator: 0.14537718892097473\n",
            "ITERATION_NO.: 319 LOSS_Generator: 4.414951324462891 LOSS_Discriminator: 0.10458394885063171\n",
            "ITERATION_NO.: 320 LOSS_Generator: 4.448766231536865 LOSS_Discriminator: 0.1254054307937622\n",
            "ITERATION_NO.: 321 LOSS_Generator: 4.170206069946289 LOSS_Discriminator: 0.2580586075782776\n",
            "ITERATION_NO.: 322 LOSS_Generator: 3.6820132732391357 LOSS_Discriminator: 0.2902851700782776\n",
            "ITERATION_NO.: 323 LOSS_Generator: 3.748969078063965 LOSS_Discriminator: 0.1491146683692932\n",
            "ITERATION_NO.: 324 LOSS_Generator: 4.302742004394531 LOSS_Discriminator: 0.11739733070135117\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.222044467926025 LOSS_Discriminator: 0.11517874896526337\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.915311336517334 LOSS_Discriminator: 0.039375804364681244\n",
            "ITERATION_NO.: 327 LOSS_Generator: 6.0668044090271 LOSS_Discriminator: 0.30091118812561035\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.527862071990967 LOSS_Discriminator: 0.10169557482004166\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.891646862030029 LOSS_Discriminator: 0.2207588255405426\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.349509239196777 LOSS_Discriminator: 0.02038460597395897\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.365456581115723 LOSS_Discriminator: 0.027878984808921814\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.018294811248779 LOSS_Discriminator: 0.04034747928380966\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.067559242248535 LOSS_Discriminator: 0.11861798167228699\n",
            "ITERATION_NO.: 334 LOSS_Generator: 4.887547492980957 LOSS_Discriminator: 0.28733912110328674\n",
            "ITERATION_NO.: 335 LOSS_Generator: 4.023797512054443 LOSS_Discriminator: 0.0525628998875618\n",
            "ITERATION_NO.: 336 LOSS_Generator: 4.415322303771973 LOSS_Discriminator: 0.08526270091533661\n",
            "ITERATION_NO.: 337 LOSS_Generator: 3.9504904747009277 LOSS_Discriminator: 0.08263183385133743\n",
            "ITERATION_NO.: 338 LOSS_Generator: 4.317553520202637 LOSS_Discriminator: 0.17783552408218384\n",
            "ITERATION_NO.: 339 LOSS_Generator: 4.8763580322265625 LOSS_Discriminator: 0.22996143996715546\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.468213081359863 LOSS_Discriminator: 0.022440105676651\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.570063591003418 LOSS_Discriminator: 0.23387613892555237\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.676999092102051 LOSS_Discriminator: 0.06233047693967819\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.163206100463867 LOSS_Discriminator: 0.1296958476305008\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.446352958679199 LOSS_Discriminator: 0.07350435107946396\n",
            "ITERATION_NO.: 345 LOSS_Generator: 4.8834991455078125 LOSS_Discriminator: 0.15023012459278107\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.408599376678467 LOSS_Discriminator: 0.0515761598944664\n",
            "ITERATION_NO.: 347 LOSS_Generator: 4.470527172088623 LOSS_Discriminator: 0.17742113769054413\n",
            "ITERATION_NO.: 348 LOSS_Generator: 4.670549392700195 LOSS_Discriminator: 0.18173304200172424\n",
            "ITERATION_NO.: 349 LOSS_Generator: 4.477114677429199 LOSS_Discriminator: 0.06663881987333298\n",
            "ITERATION_NO.: 350 LOSS_Generator: 4.983660697937012 LOSS_Discriminator: 0.060332100838422775\n",
            "ITERATION_NO.: 351 LOSS_Generator: 4.869328022003174 LOSS_Discriminator: 0.05222882330417633\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.3508405685424805 LOSS_Discriminator: 0.11978866159915924\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.5678911209106445 LOSS_Discriminator: 0.09902988374233246\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.932643890380859 LOSS_Discriminator: 0.011030271649360657\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.555379867553711 LOSS_Discriminator: 0.36658719182014465\n",
            "ITERATION_NO.: 356 LOSS_Generator: 4.568469047546387 LOSS_Discriminator: 0.1435067355632782\n",
            "ITERATION_NO.: 357 LOSS_Generator: 4.151852607727051 LOSS_Discriminator: 0.25981277227401733\n",
            "ITERATION_NO.: 358 LOSS_Generator: 3.907761573791504 LOSS_Discriminator: 0.13784685730934143\n",
            "ITERATION_NO.: 359 LOSS_Generator: 4.951801300048828 LOSS_Discriminator: 0.11456845700740814\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.512106895446777 LOSS_Discriminator: 0.13047665357589722\n",
            "ITERATION_NO.: 361 LOSS_Generator: 6.093332290649414 LOSS_Discriminator: 0.11029253900051117\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.716269016265869 LOSS_Discriminator: 0.10770267993211746\n",
            "ITERATION_NO.: 363 LOSS_Generator: 5.968261241912842 LOSS_Discriminator: 0.045146338641643524\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.654560565948486 LOSS_Discriminator: 0.009033752605319023\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.543405532836914 LOSS_Discriminator: 0.043268442153930664\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.120124816894531 LOSS_Discriminator: 0.0661315843462944\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.445037364959717 LOSS_Discriminator: 0.049824923276901245\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.292387962341309 LOSS_Discriminator: 0.2810598909854889\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.326716899871826 LOSS_Discriminator: 0.05378798395395279\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.322988510131836 LOSS_Discriminator: 0.0604732483625412\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.00724983215332 LOSS_Discriminator: 0.0931701734662056\n",
            "ITERATION_NO.: 372 LOSS_Generator: 4.470922470092773 LOSS_Discriminator: 0.08961555361747742\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.204646110534668 LOSS_Discriminator: 0.14785884320735931\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.109378337860107 LOSS_Discriminator: 0.20907263457775116\n",
            "ITERATION_NO.: 375 LOSS_Generator: 4.574702262878418 LOSS_Discriminator: 0.13523085415363312\n",
            "ITERATION_NO.: 376 LOSS_Generator: 4.011083602905273 LOSS_Discriminator: 0.2811506986618042\n",
            "ITERATION_NO.: 377 LOSS_Generator: 3.575817108154297 LOSS_Discriminator: 0.3929678201675415\n",
            "ITERATION_NO.: 378 LOSS_Generator: 3.18727970123291 LOSS_Discriminator: 0.24245703220367432\n",
            "ITERATION_NO.: 379 LOSS_Generator: 3.529153823852539 LOSS_Discriminator: 0.18293660879135132\n",
            "ITERATION_NO.: 380 LOSS_Generator: 4.706212043762207 LOSS_Discriminator: 0.11410864442586899\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.446526527404785 LOSS_Discriminator: 0.1292773336172104\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.882186412811279 LOSS_Discriminator: 0.019508685916662216\n",
            "ITERATION_NO.: 383 LOSS_Generator: 7.022592544555664 LOSS_Discriminator: 0.02937161922454834\n",
            "ITERATION_NO.: 384 LOSS_Generator: 7.131077766418457 LOSS_Discriminator: 0.13629037141799927\n",
            "ITERATION_NO.: 385 LOSS_Generator: 6.802022933959961 LOSS_Discriminator: 0.3346431851387024\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.968660354614258 LOSS_Discriminator: 0.07049541920423508\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.931280136108398 LOSS_Discriminator: 0.13840636610984802\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.5632758140563965 LOSS_Discriminator: 0.1846872717142105\n",
            "ITERATION_NO.: 389 LOSS_Generator: 3.9880406856536865 LOSS_Discriminator: 0.3849124312400818\n",
            "ITERATION_NO.: 390 LOSS_Generator: 3.4981935024261475 LOSS_Discriminator: 0.23166057467460632\n",
            "ITERATION_NO.: 391 LOSS_Generator: 3.984278678894043 LOSS_Discriminator: 0.09642481803894043\n",
            "ITERATION_NO.: 392 LOSS_Generator: 4.121407985687256 LOSS_Discriminator: 0.2156960666179657\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.062314033508301 LOSS_Discriminator: 0.10624602437019348\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.716083526611328 LOSS_Discriminator: 0.26117798686027527\n",
            "ITERATION_NO.: 395 LOSS_Generator: 6.113232135772705 LOSS_Discriminator: 0.019841089844703674\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.078997611999512 LOSS_Discriminator: 0.16738946735858917\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.54447078704834 LOSS_Discriminator: 0.14081959426403046\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.675384521484375 LOSS_Discriminator: 0.15048335492610931\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.015746593475342 LOSS_Discriminator: 0.11511949449777603\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.398077011108398 LOSS_Discriminator: 0.016864977777004242\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.178897380828857 LOSS_Discriminator: 0.04217859357595444\n",
            "ITERATION_NO.: 402 LOSS_Generator: 4.51051139831543 LOSS_Discriminator: 0.08383850008249283\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.050455093383789 LOSS_Discriminator: 0.22992092370986938\n",
            "ITERATION_NO.: 404 LOSS_Generator: 4.120314598083496 LOSS_Discriminator: 0.06150875985622406\n",
            "ITERATION_NO.: 405 LOSS_Generator: 4.125970840454102 LOSS_Discriminator: 0.0321933850646019\n",
            "ITERATION_NO.: 406 LOSS_Generator: 4.645814418792725 LOSS_Discriminator: 0.07938918471336365\n",
            "ITERATION_NO.: 407 LOSS_Generator: 4.494726181030273 LOSS_Discriminator: 0.10716819763183594\n",
            "ITERATION_NO.: 408 LOSS_Generator: 4.259896755218506 LOSS_Discriminator: 0.17857614159584045\n",
            "ITERATION_NO.: 409 LOSS_Generator: 4.700667381286621 LOSS_Discriminator: 0.03220575302839279\n",
            "ITERATION_NO.: 410 LOSS_Generator: 4.688999176025391 LOSS_Discriminator: 0.051019083708524704\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.110849857330322 LOSS_Discriminator: 0.1369202584028244\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.265506267547607 LOSS_Discriminator: 0.0824374258518219\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.023731231689453 LOSS_Discriminator: 0.030106395483016968\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.161778450012207 LOSS_Discriminator: 0.016964059323072433\n",
            "ITERATION_NO.: 415 LOSS_Generator: 4.99920654296875 LOSS_Discriminator: 0.21216829121112823\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.474116325378418 LOSS_Discriminator: 0.13824281096458435\n",
            "ITERATION_NO.: 417 LOSS_Generator: 4.71900749206543 LOSS_Discriminator: 0.2407214194536209\n",
            "ITERATION_NO.: 418 LOSS_Generator: 3.851483106613159 LOSS_Discriminator: 0.13491007685661316\n",
            "ITERATION_NO.: 419 LOSS_Generator: 3.7981133460998535 LOSS_Discriminator: 0.1020595133304596\n",
            "ITERATION_NO.: 420 LOSS_Generator: 3.6610982418060303 LOSS_Discriminator: 0.07985576242208481\n",
            "ITERATION_NO.: 421 LOSS_Generator: 3.839625120162964 LOSS_Discriminator: 0.09488509595394135\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.328467845916748 LOSS_Discriminator: 0.07629039883613586\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.6910719871521 LOSS_Discriminator: 0.11186504364013672\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.411338806152344 LOSS_Discriminator: 0.09898947179317474\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.017858028411865 LOSS_Discriminator: 0.011863412335515022\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.445115089416504 LOSS_Discriminator: 0.04430821165442467\n",
            "ITERATION_NO.: 427 LOSS_Generator: 4.980025768280029 LOSS_Discriminator: 0.12752017378807068\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.324965000152588 LOSS_Discriminator: 0.01604665443301201\n",
            "ITERATION_NO.: 429 LOSS_Generator: 4.674774169921875 LOSS_Discriminator: 0.508993923664093\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.202889442443848 LOSS_Discriminator: 0.03444055840373039\n",
            "ITERATION_NO.: 431 LOSS_Generator: 3.646906852722168 LOSS_Discriminator: 0.16663996875286102\n",
            "ITERATION_NO.: 432 LOSS_Generator: 3.8239805698394775 LOSS_Discriminator: 0.06806813925504684\n",
            "ITERATION_NO.: 433 LOSS_Generator: 4.644893646240234 LOSS_Discriminator: 0.10178659856319427\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.784914970397949 LOSS_Discriminator: 0.21058008074760437\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.691287994384766 LOSS_Discriminator: 0.03873736411333084\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.494515419006348 LOSS_Discriminator: 0.09295423328876495\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.198468208312988 LOSS_Discriminator: 0.24390074610710144\n",
            "ITERATION_NO.: 438 LOSS_Generator: 4.753471374511719 LOSS_Discriminator: 0.11896446347236633\n",
            "ITERATION_NO.: 439 LOSS_Generator: 4.507580757141113 LOSS_Discriminator: 0.236161470413208\n",
            "ITERATION_NO.: 440 LOSS_Generator: 4.348176002502441 LOSS_Discriminator: 0.2138187140226364\n",
            "ITERATION_NO.: 441 LOSS_Generator: 4.245312690734863 LOSS_Discriminator: 0.1770319789648056\n",
            "ITERATION_NO.: 442 LOSS_Generator: 3.850572109222412 LOSS_Discriminator: 0.1574573516845703\n",
            "ITERATION_NO.: 443 LOSS_Generator: 4.050962924957275 LOSS_Discriminator: 0.14999976754188538\n",
            "ITERATION_NO.: 444 LOSS_Generator: 4.585540771484375 LOSS_Discriminator: 0.11483795940876007\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.135000228881836 LOSS_Discriminator: 0.08426927775144577\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.239763259887695 LOSS_Discriminator: 0.17491018772125244\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.60014009475708 LOSS_Discriminator: 0.0788620263338089\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.615448951721191 LOSS_Discriminator: 0.10367383062839508\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.248319625854492 LOSS_Discriminator: 0.03579624742269516\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.459900379180908 LOSS_Discriminator: 0.08168689906597137\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.069751262664795 LOSS_Discriminator: 0.02913409098982811\n",
            "ITERATION_NO.: 452 LOSS_Generator: 4.709606647491455 LOSS_Discriminator: 0.11794786155223846\n",
            "ITERATION_NO.: 453 LOSS_Generator: 4.631439208984375 LOSS_Discriminator: 0.15494820475578308\n",
            "ITERATION_NO.: 454 LOSS_Generator: 4.261444091796875 LOSS_Discriminator: 0.0266649778932333\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.414005279541016 LOSS_Discriminator: 0.09238100051879883\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.4042582511901855 LOSS_Discriminator: 0.18317373096942902\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.842196464538574 LOSS_Discriminator: 0.24370139837265015\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.534292221069336 LOSS_Discriminator: 0.016539009287953377\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.1676483154296875 LOSS_Discriminator: 0.16751037538051605\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.540663719177246 LOSS_Discriminator: 0.16753727197647095\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.770925521850586 LOSS_Discriminator: 0.25848668813705444\n",
            "ITERATION_NO.: 462 LOSS_Generator: 3.9884817600250244 LOSS_Discriminator: 0.11069133132696152\n",
            "ITERATION_NO.: 463 LOSS_Generator: 4.7716755867004395 LOSS_Discriminator: 0.10216323286294937\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.889047622680664 LOSS_Discriminator: 0.035825036466121674\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.01789665222168 LOSS_Discriminator: 0.20069409906864166\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.550592422485352 LOSS_Discriminator: 0.15110360085964203\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.465719223022461 LOSS_Discriminator: 0.16571931540966034\n",
            "ITERATION_NO.: 468 LOSS_Generator: 4.550048351287842 LOSS_Discriminator: 0.08555593341588974\n",
            "ITERATION_NO.: 469 LOSS_Generator: 4.480907440185547 LOSS_Discriminator: 0.07024172693490982\n",
            "ITERATION_NO.: 470 LOSS_Generator: 4.366181373596191 LOSS_Discriminator: 0.03958726301789284\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.800171852111816 LOSS_Discriminator: 0.04446094483137131\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.465570449829102 LOSS_Discriminator: 0.10859102755784988\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.511748313903809 LOSS_Discriminator: 0.27187395095825195\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.2260637283325195 LOSS_Discriminator: 0.10796874761581421\n",
            "ITERATION_NO.: 475 LOSS_Generator: 4.0370659828186035 LOSS_Discriminator: 0.11736969649791718\n",
            "ITERATION_NO.: 476 LOSS_Generator: 4.841053009033203 LOSS_Discriminator: 0.0838344395160675\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.100996971130371 LOSS_Discriminator: 0.04353197664022446\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.269270896911621 LOSS_Discriminator: 0.09735246002674103\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.036518573760986 LOSS_Discriminator: 0.1067357137799263\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.762173652648926 LOSS_Discriminator: 0.01857263222336769\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.953610420227051 LOSS_Discriminator: 0.07410719245672226\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.620680809020996 LOSS_Discriminator: 0.08807577937841415\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.377388954162598 LOSS_Discriminator: 0.11928898841142654\n",
            "ITERATION_NO.: 484 LOSS_Generator: 4.732874870300293 LOSS_Discriminator: 0.15169066190719604\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.114368915557861 LOSS_Discriminator: 0.1264701783657074\n",
            "ITERATION_NO.: 486 LOSS_Generator: 4.080901145935059 LOSS_Discriminator: 0.15879538655281067\n",
            "ITERATION_NO.: 487 LOSS_Generator: 4.681683540344238 LOSS_Discriminator: 0.18510833382606506\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.504220962524414 LOSS_Discriminator: 0.20469525456428528\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.291024208068848 LOSS_Discriminator: 0.20775634050369263\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.444155693054199 LOSS_Discriminator: 0.019469065591692924\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.29963493347168 LOSS_Discriminator: 0.1759118139743805\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.304157257080078 LOSS_Discriminator: 0.21126118302345276\n",
            "ITERATION_NO.: 493 LOSS_Generator: 4.968018531799316 LOSS_Discriminator: 0.058962076902389526\n",
            "ITERATION_NO.: 494 LOSS_Generator: 4.910295009613037 LOSS_Discriminator: 0.13921809196472168\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.176107406616211 LOSS_Discriminator: 0.08717064559459686\n",
            "ITERATION_NO.: 496 LOSS_Generator: 4.786008834838867 LOSS_Discriminator: 0.08546602725982666\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.168128967285156 LOSS_Discriminator: 0.22967028617858887\n",
            "ITERATION_NO.: 498 LOSS_Generator: 4.811993598937988 LOSS_Discriminator: 0.031605467200279236\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.659631252288818 LOSS_Discriminator: 0.043525490909814835\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.590055465698242 LOSS_Discriminator: 0.0766180157661438\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.098354816436768 LOSS_Discriminator: 0.13969413936138153\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.014856338500977 LOSS_Discriminator: 0.22210094332695007\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.934749603271484 LOSS_Discriminator: 0.12097609043121338\n",
            "ITERATION_NO.: 504 LOSS_Generator: 4.71442174911499 LOSS_Discriminator: 0.030677001923322678\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.423434734344482 LOSS_Discriminator: 0.039038337767124176\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.1309814453125 LOSS_Discriminator: 0.03542634844779968\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.485244274139404 LOSS_Discriminator: 0.15629929304122925\n",
            "ITERATION_NO.: 508 LOSS_Generator: 4.680043697357178 LOSS_Discriminator: 0.01766897365450859\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.195730686187744 LOSS_Discriminator: 0.36778345704078674\n",
            "ITERATION_NO.: 510 LOSS_Generator: 4.806621551513672 LOSS_Discriminator: 0.1609434187412262\n",
            "ITERATION_NO.: 511 LOSS_Generator: 4.555034160614014 LOSS_Discriminator: 0.1653323471546173\n",
            "ITERATION_NO.: 512 LOSS_Generator: 3.6570658683776855 LOSS_Discriminator: 0.08754695951938629\n",
            "ITERATION_NO.: 513 LOSS_Generator: 4.058746337890625 LOSS_Discriminator: 0.17883369326591492\n",
            "ITERATION_NO.: 514 LOSS_Generator: 4.003396034240723 LOSS_Discriminator: 0.19530537724494934\n",
            "ITERATION_NO.: 515 LOSS_Generator: 4.607706069946289 LOSS_Discriminator: 0.14799976348876953\n",
            "ITERATION_NO.: 516 LOSS_Generator: 4.870087623596191 LOSS_Discriminator: 0.05310044065117836\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.001855850219727 LOSS_Discriminator: 0.13298383355140686\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.349667549133301 LOSS_Discriminator: 0.06894764304161072\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.1517839431762695 LOSS_Discriminator: 0.028989402577280998\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.8354034423828125 LOSS_Discriminator: 0.08160765469074249\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.479973793029785 LOSS_Discriminator: 0.013987218029797077\n",
            "ITERATION_NO.: 522 LOSS_Generator: 6.217252731323242 LOSS_Discriminator: 0.07764656841754913\n",
            "ITERATION_NO.: 523 LOSS_Generator: 6.281877040863037 LOSS_Discriminator: 0.06287622451782227\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.954118728637695 LOSS_Discriminator: 0.0995149239897728\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.78637170791626 LOSS_Discriminator: 0.009684745222330093\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.1963348388671875 LOSS_Discriminator: 0.4001966714859009\n",
            "ITERATION_NO.: 527 LOSS_Generator: 4.237667083740234 LOSS_Discriminator: 0.1955176144838333\n",
            "ITERATION_NO.: 528 LOSS_Generator: 3.656148910522461 LOSS_Discriminator: 0.18263617157936096\n",
            "ITERATION_NO.: 529 LOSS_Generator: 3.8360517024993896 LOSS_Discriminator: 0.24453186988830566\n",
            "ITERATION_NO.: 530 LOSS_Generator: 4.313837051391602 LOSS_Discriminator: 0.34921538829803467\n",
            "ITERATION_NO.: 531 LOSS_Generator: 4.7476067543029785 LOSS_Discriminator: 0.17250624299049377\n",
            "ITERATION_NO.: 532 LOSS_Generator: 4.484724521636963 LOSS_Discriminator: 0.24299457669258118\n",
            "ITERATION_NO.: 533 LOSS_Generator: 4.171250820159912 LOSS_Discriminator: 0.11377127468585968\n",
            "ITERATION_NO.: 534 LOSS_Generator: 4.660618782043457 LOSS_Discriminator: 0.1426602602005005\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.004979610443115 LOSS_Discriminator: 0.0651339590549469\n",
            "ITERATION_NO.: 536 LOSS_Generator: 3.7488205432891846 LOSS_Discriminator: 0.29449665546417236\n",
            "ITERATION_NO.: 537 LOSS_Generator: 4.674007892608643 LOSS_Discriminator: 0.06413708627223969\n",
            "ITERATION_NO.: 538 LOSS_Generator: 3.8515377044677734 LOSS_Discriminator: 0.1774548888206482\n",
            "ITERATION_NO.: 539 LOSS_Generator: 4.0049519538879395 LOSS_Discriminator: 0.12548358738422394\n",
            "ITERATION_NO.: 540 LOSS_Generator: 4.637154579162598 LOSS_Discriminator: 0.08297854661941528\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.459153175354004 LOSS_Discriminator: 0.09270817041397095\n",
            "ITERATION_NO.: 542 LOSS_Generator: 6.161444664001465 LOSS_Discriminator: 0.0684988871216774\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.406272888183594 LOSS_Discriminator: 0.12435837090015411\n",
            "ITERATION_NO.: 544 LOSS_Generator: 6.577825546264648 LOSS_Discriminator: 0.051050592213869095\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.216706275939941 LOSS_Discriminator: 0.03482641279697418\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.325398921966553 LOSS_Discriminator: 0.028252501040697098\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.692584991455078 LOSS_Discriminator: 0.11977654695510864\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.417976379394531 LOSS_Discriminator: 0.06478466838598251\n",
            "ITERATION_NO.: 549 LOSS_Generator: 4.191864967346191 LOSS_Discriminator: 0.20100080966949463\n",
            "ITERATION_NO.: 550 LOSS_Generator: 3.659623146057129 LOSS_Discriminator: 0.12598001956939697\n",
            "ITERATION_NO.: 551 LOSS_Generator: 3.611971616744995 LOSS_Discriminator: 0.10981326550245285\n",
            "ITERATION_NO.: 552 LOSS_Generator: 4.773677825927734 LOSS_Discriminator: 0.07206108421087265\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.2486724853515625 LOSS_Discriminator: 0.03194378316402435\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.5978288650512695 LOSS_Discriminator: 0.09058067202568054\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.753835201263428 LOSS_Discriminator: 0.057658348232507706\n",
            "ITERATION_NO.: 556 LOSS_Generator: 6.442975997924805 LOSS_Discriminator: 0.01447361521422863\n",
            "ITERATION_NO.: 557 LOSS_Generator: 6.525307655334473 LOSS_Discriminator: 0.2259974181652069\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.858273983001709 LOSS_Discriminator: 0.0070329271256923676\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.175686836242676 LOSS_Discriminator: 0.1331540048122406\n",
            "ITERATION_NO.: 560 LOSS_Generator: 6.164272308349609 LOSS_Discriminator: 0.022711900994181633\n",
            "ITERATION_NO.: 561 LOSS_Generator: 6.280240058898926 LOSS_Discriminator: 0.18836750090122223\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.901582717895508 LOSS_Discriminator: 0.3356788754463196\n",
            "ITERATION_NO.: 563 LOSS_Generator: 4.443065166473389 LOSS_Discriminator: 0.08934713155031204\n",
            "ITERATION_NO.: 564 LOSS_Generator: 4.449613571166992 LOSS_Discriminator: 0.06443402171134949\n",
            "ITERATION_NO.: 565 LOSS_Generator: 3.756070137023926 LOSS_Discriminator: 0.141734778881073\n",
            "ITERATION_NO.: 566 LOSS_Generator: 4.083686828613281 LOSS_Discriminator: 0.35291919112205505\n",
            "ITERATION_NO.: 567 LOSS_Generator: 4.400515556335449 LOSS_Discriminator: 0.14092454314231873\n",
            "ITERATION_NO.: 568 LOSS_Generator: 4.453333377838135 LOSS_Discriminator: 0.1276368498802185\n",
            "ITERATION_NO.: 569 LOSS_Generator: 4.199296951293945 LOSS_Discriminator: 0.22180724143981934\n",
            "ITERATION_NO.: 570 LOSS_Generator: 4.542266845703125 LOSS_Discriminator: 0.3131617605686188\n",
            "ITERATION_NO.: 571 LOSS_Generator: 4.167476177215576 LOSS_Discriminator: 0.060475535690784454\n",
            "ITERATION_NO.: 572 LOSS_Generator: 3.77380633354187 LOSS_Discriminator: 0.2088196575641632\n",
            "ITERATION_NO.: 573 LOSS_Generator: 3.5463294982910156 LOSS_Discriminator: 0.07019627094268799\n",
            "ITERATION_NO.: 574 LOSS_Generator: 4.00193977355957 LOSS_Discriminator: 0.09289899468421936\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.031778335571289 LOSS_Discriminator: 0.15353479981422424\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.369536399841309 LOSS_Discriminator: 0.027875684201717377\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.1986870765686035 LOSS_Discriminator: 0.0288047194480896\n",
            "ITERATION_NO.: 578 LOSS_Generator: 6.158679962158203 LOSS_Discriminator: 0.38909655809402466\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.808832168579102 LOSS_Discriminator: 0.14429064095020294\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.779971599578857 LOSS_Discriminator: 0.2748507857322693\n",
            "ITERATION_NO.: 581 LOSS_Generator: 4.911560535430908 LOSS_Discriminator: 0.17394113540649414\n",
            "ITERATION_NO.: 582 LOSS_Generator: 4.587398529052734 LOSS_Discriminator: 0.14342182874679565\n",
            "ITERATION_NO.: 583 LOSS_Generator: 3.5164761543273926 LOSS_Discriminator: 0.1800609529018402\n",
            "ITERATION_NO.: 584 LOSS_Generator: 4.152615547180176 LOSS_Discriminator: 0.26819348335266113\n",
            "ITERATION_NO.: 585 LOSS_Generator: 4.291017532348633 LOSS_Discriminator: 0.18198028206825256\n",
            "ITERATION_NO.: 586 LOSS_Generator: 4.931923866271973 LOSS_Discriminator: 0.10214027017354965\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.048703193664551 LOSS_Discriminator: 0.12966790795326233\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.035682201385498 LOSS_Discriminator: 0.12198948860168457\n",
            "ITERATION_NO.: 589 LOSS_Generator: 4.557613849639893 LOSS_Discriminator: 0.1656319499015808\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.649672031402588 LOSS_Discriminator: 0.13552293181419373\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.106147289276123 LOSS_Discriminator: 0.10166150331497192\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.049603462219238 LOSS_Discriminator: 0.11121300607919693\n",
            "ITERATION_NO.: 593 LOSS_Generator: 4.870732307434082 LOSS_Discriminator: 0.19769452512264252\n",
            "ITERATION_NO.: 594 LOSS_Generator: 4.250237464904785 LOSS_Discriminator: 0.11279693245887756\n",
            "ITERATION_NO.: 595 LOSS_Generator: 3.992547035217285 LOSS_Discriminator: 0.08039075881242752\n",
            "ITERATION_NO.: 596 LOSS_Generator: 4.931308746337891 LOSS_Discriminator: 0.14764490723609924\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.234908103942871 LOSS_Discriminator: 0.05786304920911789\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.1641011238098145 LOSS_Discriminator: 0.1780380755662918\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.420173168182373 LOSS_Discriminator: 0.03564099594950676\n",
            "ITERATION_NO.: 600 LOSS_Generator: 4.885213851928711 LOSS_Discriminator: 0.3241019546985626\n",
            "ITERATION_NO.: 601 LOSS_Generator: 3.7083816528320312 LOSS_Discriminator: 0.270560622215271\n",
            "ITERATION_NO.: 602 LOSS_Generator: 3.824672222137451 LOSS_Discriminator: 0.1332317292690277\n",
            "ITERATION_NO.: 603 LOSS_Generator: 3.437565803527832 LOSS_Discriminator: 0.11264466494321823\n",
            "ITERATION_NO.: 604 LOSS_Generator: 3.7555251121520996 LOSS_Discriminator: 0.19387799501419067\n",
            "ITERATION_NO.: 605 LOSS_Generator: 4.107659339904785 LOSS_Discriminator: 0.3076176047325134\n",
            "ITERATION_NO.: 606 LOSS_Generator: 5.148737907409668 LOSS_Discriminator: 0.08894447982311249\n",
            "ITERATION_NO.: 607 LOSS_Generator: 5.817048072814941 LOSS_Discriminator: 0.09678046405315399\n",
            "ITERATION_NO.: 608 LOSS_Generator: 5.887029647827148 LOSS_Discriminator: 0.034796327352523804\n",
            "ITERATION_NO.: 609 LOSS_Generator: 5.128139972686768 LOSS_Discriminator: 0.15008234977722168\n",
            "ITERATION_NO.: 610 LOSS_Generator: 4.958179950714111 LOSS_Discriminator: 0.1534605622291565\n",
            "ITERATION_NO.: 611 LOSS_Generator: 4.7770795822143555 LOSS_Discriminator: 0.1001640111207962\n",
            "ITERATION_NO.: 612 LOSS_Generator: 4.113498687744141 LOSS_Discriminator: 0.1262718290090561\n",
            "ITERATION_NO.: 613 LOSS_Generator: 4.300066947937012 LOSS_Discriminator: 0.11951690912246704\n",
            "ITERATION_NO.: 614 LOSS_Generator: 4.558121681213379 LOSS_Discriminator: 0.15725380182266235\n",
            "ITERATION_NO.: 615 LOSS_Generator: 4.679732799530029 LOSS_Discriminator: 0.06843861937522888\n",
            "ITERATION_NO.: 616 LOSS_Generator: 5.081110000610352 LOSS_Discriminator: 0.08635352551937103\n",
            "ITERATION_NO.: 617 LOSS_Generator: 5.872166633605957 LOSS_Discriminator: 0.4523479640483856\n",
            "ITERATION_NO.: 618 LOSS_Generator: 3.7805044651031494 LOSS_Discriminator: 0.2233676314353943\n",
            "ITERATION_NO.: 619 LOSS_Generator: 3.9573798179626465 LOSS_Discriminator: 0.21480080485343933\n",
            "ITERATION_NO.: 620 LOSS_Generator: 4.238394737243652 LOSS_Discriminator: 0.15155883133411407\n",
            "ITERATION_NO.: 621 LOSS_Generator: 4.633609294891357 LOSS_Discriminator: 0.21876507997512817\n",
            "ITERATION_NO.: 622 LOSS_Generator: 5.133919715881348 LOSS_Discriminator: 0.1693265438079834\n",
            "ITERATION_NO.: 623 LOSS_Generator: 5.561887741088867 LOSS_Discriminator: 0.19969689846038818\n",
            "ITERATION_NO.: 624 LOSS_Generator: 5.223602294921875 LOSS_Discriminator: 0.16961362957954407\n",
            "ITERATION_NO.: 625 LOSS_Generator: 4.685565948486328 LOSS_Discriminator: 0.1030552089214325\n",
            "ITERATION_NO.: 626 LOSS_Generator: 4.573747158050537 LOSS_Discriminator: 0.21500259637832642\n",
            "ITERATION_NO.: 627 LOSS_Generator: 4.387265682220459 LOSS_Discriminator: 0.20790407061576843\n",
            "ITERATION_NO.: 628 LOSS_Generator: 4.190624237060547 LOSS_Discriminator: 0.19301587343215942\n",
            "ITERATION_NO.: 629 LOSS_Generator: 3.5470571517944336 LOSS_Discriminator: 0.16196073591709137\n",
            "ITERATION_NO.: 630 LOSS_Generator: 3.771124839782715 LOSS_Discriminator: 0.15251347422599792\n",
            "ITERATION_NO.: 631 LOSS_Generator: 5.172340393066406 LOSS_Discriminator: 0.10287652164697647\n",
            "ITERATION_NO.: 632 LOSS_Generator: 4.990391254425049 LOSS_Discriminator: 0.1386062651872635\n",
            "ITERATION_NO.: 633 LOSS_Generator: 5.276714324951172 LOSS_Discriminator: 0.08109617233276367\n",
            "ITERATION_NO.: 634 LOSS_Generator: 5.229765892028809 LOSS_Discriminator: 0.18148575723171234\n",
            "ITERATION_NO.: 635 LOSS_Generator: 5.127803802490234 LOSS_Discriminator: 0.12388461828231812\n",
            "ITERATION_NO.: 636 LOSS_Generator: 4.939448356628418 LOSS_Discriminator: 0.03292368724942207\n",
            "ITERATION_NO.: 637 LOSS_Generator: 4.894613265991211 LOSS_Discriminator: 0.032761767506599426\n",
            "ITERATION_NO.: 638 LOSS_Generator: 4.955532550811768 LOSS_Discriminator: 0.026726339012384415\n",
            "ITERATION_NO.: 639 LOSS_Generator: 5.019959449768066 LOSS_Discriminator: 0.19804078340530396\n",
            "ITERATION_NO.: 640 LOSS_Generator: 4.157378196716309 LOSS_Discriminator: 0.20567917823791504\n",
            "ITERATION_NO.: 641 LOSS_Generator: 4.1541008949279785 LOSS_Discriminator: 0.08942001312971115\n",
            "ITERATION_NO.: 642 LOSS_Generator: 4.154531478881836 LOSS_Discriminator: 0.07909458875656128\n",
            "ITERATION_NO.: 643 LOSS_Generator: 3.4747939109802246 LOSS_Discriminator: 0.05497425049543381\n",
            "ITERATION_NO.: 644 LOSS_Generator: 4.264776229858398 LOSS_Discriminator: 0.0781129002571106\n",
            "ITERATION_NO.: 645 LOSS_Generator: 4.640576362609863 LOSS_Discriminator: 0.06846626847982407\n",
            "ITERATION_NO.: 646 LOSS_Generator: 5.551420211791992 LOSS_Discriminator: 0.16480812430381775\n",
            "ITERATION_NO.: 647 LOSS_Generator: 5.363171577453613 LOSS_Discriminator: 0.2275296002626419\n",
            "ITERATION_NO.: 648 LOSS_Generator: 5.76035213470459 LOSS_Discriminator: 0.07056822627782822\n",
            "ITERATION_NO.: 649 LOSS_Generator: 5.318419456481934 LOSS_Discriminator: 0.1797647476196289\n",
            "ITERATION_NO.: 650 LOSS_Generator: 4.779452323913574 LOSS_Discriminator: 0.14542068541049957\n",
            "ITERATION_NO.: 651 LOSS_Generator: 4.982466697692871 LOSS_Discriminator: 0.13856853544712067\n",
            "ITERATION_NO.: 652 LOSS_Generator: 4.532668113708496 LOSS_Discriminator: 0.10625466704368591\n",
            "ITERATION_NO.: 653 LOSS_Generator: 4.49799919128418 LOSS_Discriminator: 0.13016891479492188\n",
            "ITERATION_NO.: 654 LOSS_Generator: 3.8913846015930176 LOSS_Discriminator: 0.26869291067123413\n",
            "ITERATION_NO.: 655 LOSS_Generator: 3.9561822414398193 LOSS_Discriminator: 0.24171963334083557\n",
            "ITERATION_NO.: 656 LOSS_Generator: 3.6848409175872803 LOSS_Discriminator: 0.19334372878074646\n",
            "ITERATION_NO.: 657 LOSS_Generator: 3.7375569343566895 LOSS_Discriminator: 0.08945399522781372\n",
            "ITERATION_NO.: 658 LOSS_Generator: 3.96598219871521 LOSS_Discriminator: 0.1935049295425415\n",
            "ITERATION_NO.: 659 LOSS_Generator: 4.400229454040527 LOSS_Discriminator: 0.17486345767974854\n",
            "ITERATION_NO.: 660 LOSS_Generator: 4.56562614440918 LOSS_Discriminator: 0.15554797649383545\n",
            "ITERATION_NO.: 661 LOSS_Generator: 4.9521708488464355 LOSS_Discriminator: 0.10631056129932404\n",
            "ITERATION_NO.: 662 LOSS_Generator: 5.270101070404053 LOSS_Discriminator: 0.020711608231067657\n",
            "ITERATION_NO.: 663 LOSS_Generator: 5.162773609161377 LOSS_Discriminator: 0.07359389215707779\n",
            "ITERATION_NO.: 664 LOSS_Generator: 4.974289894104004 LOSS_Discriminator: 0.4587300717830658\n",
            "ITERATION_NO.: 665 LOSS_Generator: 3.5490212440490723 LOSS_Discriminator: 0.1584964394569397\n",
            "ITERATION_NO.: 666 LOSS_Generator: 4.117992877960205 LOSS_Discriminator: 0.07539287209510803\n",
            "ITERATION_NO.: 667 LOSS_Generator: 4.278334140777588 LOSS_Discriminator: 0.2613426446914673\n",
            "ITERATION_NO.: 668 LOSS_Generator: 5.312953472137451 LOSS_Discriminator: 0.08248183131217957\n",
            "ITERATION_NO.: 669 LOSS_Generator: 5.409064292907715 LOSS_Discriminator: 0.07093344628810883\n",
            "ITERATION_NO.: 670 LOSS_Generator: 6.409732818603516 LOSS_Discriminator: 0.2514239549636841\n",
            "ITERATION_NO.: 671 LOSS_Generator: 6.595235824584961 LOSS_Discriminator: 0.1336451768875122\n",
            "ITERATION_NO.: 672 LOSS_Generator: 6.855338096618652 LOSS_Discriminator: 0.012294430285692215\n",
            "ITERATION_NO.: 673 LOSS_Generator: 7.018078804016113 LOSS_Discriminator: 0.0409800224006176\n",
            "ITERATION_NO.: 674 LOSS_Generator: 6.36255407333374 LOSS_Discriminator: 0.0703384280204773\n",
            "ITERATION_NO.: 675 LOSS_Generator: 5.600057601928711 LOSS_Discriminator: 0.35344260931015015\n",
            "ITERATION_NO.: 676 LOSS_Generator: 5.870357513427734 LOSS_Discriminator: 0.018867477774620056\n",
            "ITERATION_NO.: 677 LOSS_Generator: 5.486458778381348 LOSS_Discriminator: 0.06786815822124481\n",
            "ITERATION_NO.: 678 LOSS_Generator: 4.629728317260742 LOSS_Discriminator: 0.14758199453353882\n",
            "ITERATION_NO.: 679 LOSS_Generator: 4.28642463684082 LOSS_Discriminator: 0.04345041513442993\n",
            "ITERATION_NO.: 680 LOSS_Generator: 3.697582244873047 LOSS_Discriminator: 0.08739228546619415\n",
            "ITERATION_NO.: 681 LOSS_Generator: 3.837447166442871 LOSS_Discriminator: 0.15734270215034485\n",
            "ITERATION_NO.: 682 LOSS_Generator: 3.732456922531128 LOSS_Discriminator: 0.15656235814094543\n",
            "ITERATION_NO.: 683 LOSS_Generator: 4.44803524017334 LOSS_Discriminator: 0.3391808867454529\n",
            "ITERATION_NO.: 684 LOSS_Generator: 4.499398231506348 LOSS_Discriminator: 0.15783216059207916\n",
            "ITERATION_NO.: 685 LOSS_Generator: 4.774073600769043 LOSS_Discriminator: 0.03795094043016434\n",
            "ITERATION_NO.: 686 LOSS_Generator: 4.858122825622559 LOSS_Discriminator: 0.15231241285800934\n",
            "ITERATION_NO.: 687 LOSS_Generator: 5.51513147354126 LOSS_Discriminator: 0.038471005856990814\n",
            "ITERATION_NO.: 688 LOSS_Generator: 5.687739372253418 LOSS_Discriminator: 0.08447533845901489\n",
            "ITERATION_NO.: 689 LOSS_Generator: 5.19588565826416 LOSS_Discriminator: 0.05698587745428085\n",
            "ITERATION_NO.: 690 LOSS_Generator: 6.087163925170898 LOSS_Discriminator: 0.10417090356349945\n",
            "ITERATION_NO.: 691 LOSS_Generator: 4.795684337615967 LOSS_Discriminator: 0.16667243838310242\n",
            "ITERATION_NO.: 692 LOSS_Generator: 4.562335014343262 LOSS_Discriminator: 0.06991072744131088\n",
            "ITERATION_NO.: 693 LOSS_Generator: 4.854367256164551 LOSS_Discriminator: 0.17493899166584015\n",
            "ITERATION_NO.: 694 LOSS_Generator: 4.066719055175781 LOSS_Discriminator: 0.2415647655725479\n",
            "ITERATION_NO.: 695 LOSS_Generator: 3.8606200218200684 LOSS_Discriminator: 0.17118242383003235\n",
            "ITERATION_NO.: 696 LOSS_Generator: 4.206210136413574 LOSS_Discriminator: 0.11904339492321014\n",
            "ITERATION_NO.: 697 LOSS_Generator: 4.944305896759033 LOSS_Discriminator: 0.221366286277771\n",
            "ITERATION_NO.: 698 LOSS_Generator: 5.566987991333008 LOSS_Discriminator: 0.09274446964263916\n",
            "ITERATION_NO.: 699 LOSS_Generator: 6.0305256843566895 LOSS_Discriminator: 0.23662540316581726\n",
            "ITERATION_NO.: 700 LOSS_Generator: 6.081418037414551 LOSS_Discriminator: 0.21354259550571442\n",
            "ITERATION_NO.: 701 LOSS_Generator: 5.318483352661133 LOSS_Discriminator: 0.22946015000343323\n",
            "ITERATION_NO.: 702 LOSS_Generator: 4.478273391723633 LOSS_Discriminator: 0.043179381638765335\n",
            "ITERATION_NO.: 703 LOSS_Generator: 3.7956299781799316 LOSS_Discriminator: 0.13852514326572418\n",
            "ITERATION_NO.: 704 LOSS_Generator: 3.146541118621826 LOSS_Discriminator: 0.09236763417720795\n",
            "ITERATION_NO.: 705 LOSS_Generator: 4.235889434814453 LOSS_Discriminator: 0.24797052145004272\n",
            "ITERATION_NO.: 706 LOSS_Generator: 4.304498672485352 LOSS_Discriminator: 0.08720289170742035\n",
            "ITERATION_NO.: 707 LOSS_Generator: 5.160306930541992 LOSS_Discriminator: 0.101924829185009\n",
            "ITERATION_NO.: 708 LOSS_Generator: 5.486005783081055 LOSS_Discriminator: 0.14231929183006287\n",
            "ITERATION_NO.: 709 LOSS_Generator: 5.812870979309082 LOSS_Discriminator: 0.16392016410827637\n",
            "ITERATION_NO.: 710 LOSS_Generator: 5.8548150062561035 LOSS_Discriminator: 0.04779188334941864\n",
            "ITERATION_NO.: 711 LOSS_Generator: 5.804224491119385 LOSS_Discriminator: 0.10449506342411041\n",
            "ITERATION_NO.: 712 LOSS_Generator: 5.641748428344727 LOSS_Discriminator: 0.2123866081237793\n",
            "ITERATION_NO.: 713 LOSS_Generator: 4.658982276916504 LOSS_Discriminator: 0.179470956325531\n",
            "ITERATION_NO.: 714 LOSS_Generator: 4.094516277313232 LOSS_Discriminator: 0.2602890431880951\n",
            "ITERATION_NO.: 715 LOSS_Generator: 3.76552152633667 LOSS_Discriminator: 0.07542537152767181\n",
            "ITERATION_NO.: 716 LOSS_Generator: 3.1993579864501953 LOSS_Discriminator: 0.10855956375598907\n",
            "ITERATION_NO.: 717 LOSS_Generator: 3.5569941997528076 LOSS_Discriminator: 0.097227543592453\n",
            "ITERATION_NO.: 718 LOSS_Generator: 4.276108741760254 LOSS_Discriminator: 0.244954913854599\n",
            "ITERATION_NO.: 719 LOSS_Generator: 4.455517768859863 LOSS_Discriminator: 0.1714886575937271\n",
            "ITERATION_NO.: 720 LOSS_Generator: 4.0928449630737305 LOSS_Discriminator: 0.15438053011894226\n",
            "ITERATION_NO.: 721 LOSS_Generator: 4.6742939949035645 LOSS_Discriminator: 0.2819851040840149\n",
            "ITERATION_NO.: 722 LOSS_Generator: 4.473241329193115 LOSS_Discriminator: 0.16986951231956482\n",
            "ITERATION_NO.: 723 LOSS_Generator: 4.2505717277526855 LOSS_Discriminator: 0.0961640328168869\n",
            "ITERATION_NO.: 724 LOSS_Generator: 4.092899322509766 LOSS_Discriminator: 0.0816161036491394\n",
            "ITERATION_NO.: 725 LOSS_Generator: 4.634951114654541 LOSS_Discriminator: 0.05676489323377609\n",
            "ITERATION_NO.: 726 LOSS_Generator: 4.698275089263916 LOSS_Discriminator: 0.045430202037096024\n",
            "ITERATION_NO.: 727 LOSS_Generator: 5.732495307922363 LOSS_Discriminator: 0.03687262907624245\n",
            "ITERATION_NO.: 728 LOSS_Generator: 5.614400386810303 LOSS_Discriminator: 0.20457762479782104\n",
            "ITERATION_NO.: 729 LOSS_Generator: 6.154565334320068 LOSS_Discriminator: 0.1206858828663826\n",
            "ITERATION_NO.: 730 LOSS_Generator: 5.33231258392334 LOSS_Discriminator: 0.48558953404426575\n",
            "ITERATION_NO.: 731 LOSS_Generator: 5.403131484985352 LOSS_Discriminator: 0.12520357966423035\n",
            "ITERATION_NO.: 732 LOSS_Generator: 3.950543165206909 LOSS_Discriminator: 0.2173815369606018\n",
            "ITERATION_NO.: 733 LOSS_Generator: 3.5176305770874023 LOSS_Discriminator: 0.09036283195018768\n",
            "ITERATION_NO.: 734 LOSS_Generator: 3.4896020889282227 LOSS_Discriminator: 0.1345852017402649\n",
            "ITERATION_NO.: 735 LOSS_Generator: 3.9541492462158203 LOSS_Discriminator: 0.17517484724521637\n",
            "ITERATION_NO.: 736 LOSS_Generator: 4.448910713195801 LOSS_Discriminator: 0.13809162378311157\n",
            "ITERATION_NO.: 737 LOSS_Generator: 4.757009983062744 LOSS_Discriminator: 0.04505141079425812\n",
            "ITERATION_NO.: 738 LOSS_Generator: 5.562264442443848 LOSS_Discriminator: 0.1646384298801422\n",
            "ITERATION_NO.: 739 LOSS_Generator: 4.868517875671387 LOSS_Discriminator: 0.23443526029586792\n",
            "ITERATION_NO.: 740 LOSS_Generator: 4.664718151092529 LOSS_Discriminator: 0.059492096304893494\n",
            "ITERATION_NO.: 741 LOSS_Generator: 4.751914024353027 LOSS_Discriminator: 0.06866230070590973\n",
            "ITERATION_NO.: 742 LOSS_Generator: 4.820035934448242 LOSS_Discriminator: 0.08328311145305634\n",
            "ITERATION_NO.: 743 LOSS_Generator: 5.146186828613281 LOSS_Discriminator: 0.057042304426431656\n",
            "ITERATION_NO.: 744 LOSS_Generator: 4.587294578552246 LOSS_Discriminator: 0.14717066287994385\n",
            "ITERATION_NO.: 745 LOSS_Generator: 4.667079925537109 LOSS_Discriminator: 0.05182415992021561\n",
            "ITERATION_NO.: 746 LOSS_Generator: 4.495484352111816 LOSS_Discriminator: 0.06554241478443146\n",
            "ITERATION_NO.: 747 LOSS_Generator: 4.635808944702148 LOSS_Discriminator: 0.12250027060508728\n",
            "ITERATION_NO.: 748 LOSS_Generator: 4.812358856201172 LOSS_Discriminator: 0.09616581350564957\n",
            "ITERATION_NO.: 749 LOSS_Generator: 4.7463531494140625 LOSS_Discriminator: 0.2019197940826416\n",
            "ITERATION_NO.: 750 LOSS_Generator: 4.739386558532715 LOSS_Discriminator: 0.0671783909201622\n",
            "ITERATION_NO.: 751 LOSS_Generator: 4.894049644470215 LOSS_Discriminator: 0.1453455090522766\n",
            "ITERATION_NO.: 752 LOSS_Generator: 4.262006759643555 LOSS_Discriminator: 0.3069057762622833\n",
            "ITERATION_NO.: 753 LOSS_Generator: 3.7095987796783447 LOSS_Discriminator: 0.17522718012332916\n",
            "ITERATION_NO.: 754 LOSS_Generator: 3.572145462036133 LOSS_Discriminator: 0.2102988064289093\n",
            "ITERATION_NO.: 755 LOSS_Generator: 4.242990493774414 LOSS_Discriminator: 0.39565110206604004\n",
            "ITERATION_NO.: 756 LOSS_Generator: 3.985558032989502 LOSS_Discriminator: 0.11584912985563278\n",
            "ITERATION_NO.: 757 LOSS_Generator: 4.683885097503662 LOSS_Discriminator: 0.09039577096700668\n",
            "ITERATION_NO.: 758 LOSS_Generator: 5.722169876098633 LOSS_Discriminator: 0.03023041971027851\n",
            "ITERATION_NO.: 759 LOSS_Generator: 5.649089813232422 LOSS_Discriminator: 0.15696600079536438\n",
            "ITERATION_NO.: 760 LOSS_Generator: 5.746067047119141 LOSS_Discriminator: 0.07399360835552216\n",
            "ITERATION_NO.: 761 LOSS_Generator: 5.464887619018555 LOSS_Discriminator: 0.06927333027124405\n",
            "ITERATION_NO.: 762 LOSS_Generator: 4.953282356262207 LOSS_Discriminator: 0.3041303753852844\n",
            "ITERATION_NO.: 763 LOSS_Generator: 4.036182403564453 LOSS_Discriminator: 0.06277599930763245\n",
            "ITERATION_NO.: 764 LOSS_Generator: 4.1753010749816895 LOSS_Discriminator: 0.13528594374656677\n",
            "ITERATION_NO.: 765 LOSS_Generator: 3.69667387008667 LOSS_Discriminator: 0.11165773868560791\n",
            "ITERATION_NO.: 766 LOSS_Generator: 4.752621650695801 LOSS_Discriminator: 0.2891446352005005\n",
            "ITERATION_NO.: 767 LOSS_Generator: 4.900635719299316 LOSS_Discriminator: 0.11905889213085175\n",
            "ITERATION_NO.: 768 LOSS_Generator: 4.2507734298706055 LOSS_Discriminator: 0.08834678679704666\n",
            "ITERATION_NO.: 769 LOSS_Generator: 5.083700656890869 LOSS_Discriminator: 0.060072168707847595\n",
            "ITERATION_NO.: 770 LOSS_Generator: 4.952393531799316 LOSS_Discriminator: 0.09633560478687286\n",
            "ITERATION_NO.: 771 LOSS_Generator: 4.248623371124268 LOSS_Discriminator: 0.2542048394680023\n",
            "ITERATION_NO.: 772 LOSS_Generator: 3.8539061546325684 LOSS_Discriminator: 0.2131384313106537\n",
            "ITERATION_NO.: 773 LOSS_Generator: 4.318626403808594 LOSS_Discriminator: 0.1527794599533081\n",
            "ITERATION_NO.: 774 LOSS_Generator: 3.9085841178894043 LOSS_Discriminator: 0.08700616657733917\n",
            "ITERATION_NO.: 775 LOSS_Generator: 4.7053985595703125 LOSS_Discriminator: 0.16306845843791962\n",
            "ITERATION_NO.: 776 LOSS_Generator: 5.480646133422852 LOSS_Discriminator: 0.12190572917461395\n",
            "ITERATION_NO.: 777 LOSS_Generator: 5.635801792144775 LOSS_Discriminator: 0.16699430346488953\n",
            "ITERATION_NO.: 778 LOSS_Generator: 5.562708854675293 LOSS_Discriminator: 0.11702406406402588\n",
            "ITERATION_NO.: 779 LOSS_Generator: 5.477113246917725 LOSS_Discriminator: 0.030854247510433197\n",
            "ITERATION_NO.: 780 LOSS_Generator: 4.673248291015625 LOSS_Discriminator: 0.24119767546653748\n",
            "ITERATION_NO.: 781 LOSS_Generator: 4.964009761810303 LOSS_Discriminator: 0.09400074183940887\n",
            "ITERATION_NO.: 782 LOSS_Generator: 4.315577983856201 LOSS_Discriminator: 0.04558389261364937\n",
            "ITERATION_NO.: 783 LOSS_Generator: 5.069561958312988 LOSS_Discriminator: 0.05644679814577103\n",
            "ITERATION_NO.: 784 LOSS_Generator: 5.078179836273193 LOSS_Discriminator: 0.14852222800254822\n",
            "ITERATION_NO.: 785 LOSS_Generator: 4.870487213134766 LOSS_Discriminator: 0.11451657116413116\n",
            "ITERATION_NO.: 786 LOSS_Generator: 4.886533737182617 LOSS_Discriminator: 0.024595897644758224\n",
            "ITERATION_NO.: 787 LOSS_Generator: 5.526961326599121 LOSS_Discriminator: 0.0701376274228096\n",
            "ITERATION_NO.: 788 LOSS_Generator: 4.9532928466796875 LOSS_Discriminator: 0.08940942585468292\n",
            "ITERATION_NO.: 789 LOSS_Generator: 5.304546356201172 LOSS_Discriminator: 0.02340482920408249\n",
            "ITERATION_NO.: 790 LOSS_Generator: 5.157513618469238 LOSS_Discriminator: 0.05442450940608978\n",
            "ITERATION_NO.: 791 LOSS_Generator: 5.666317462921143 LOSS_Discriminator: 0.08983913064002991\n",
            "ITERATION_NO.: 792 LOSS_Generator: 5.601180076599121 LOSS_Discriminator: 0.077201709151268\n",
            "ITERATION_NO.: 793 LOSS_Generator: 4.953926086425781 LOSS_Discriminator: 0.1815659999847412\n",
            "ITERATION_NO.: 794 LOSS_Generator: 4.842898368835449 LOSS_Discriminator: 0.05883472412824631\n",
            "ITERATION_NO.: 795 LOSS_Generator: 5.2258100509643555 LOSS_Discriminator: 0.021693579852581024\n",
            "ITERATION_NO.: 796 LOSS_Generator: 4.4464311599731445 LOSS_Discriminator: 0.0730728730559349\n",
            "ITERATION_NO.: 797 LOSS_Generator: 4.682344436645508 LOSS_Discriminator: 0.0818958729505539\n",
            "ITERATION_NO.: 798 LOSS_Generator: 5.2292351722717285 LOSS_Discriminator: 0.05543115735054016\n",
            "ITERATION_NO.: 799 LOSS_Generator: 5.386788368225098 LOSS_Discriminator: 0.08675023913383484\n",
            "ITERATION_NO.: 800 LOSS_Generator: 5.17926025390625 LOSS_Discriminator: 0.07890872657299042\n",
            "ITERATION_NO.: 801 LOSS_Generator: 5.329118728637695 LOSS_Discriminator: 0.1275182068347931\n",
            "ITERATION_NO.: 802 LOSS_Generator: 6.053983688354492 LOSS_Discriminator: 0.06905388832092285\n",
            "ITERATION_NO.: 803 LOSS_Generator: 6.331143379211426 LOSS_Discriminator: 0.0600733645260334\n",
            "ITERATION_NO.: 804 LOSS_Generator: 6.6698808670043945 LOSS_Discriminator: 0.05050024762749672\n",
            "ITERATION_NO.: 805 LOSS_Generator: 5.753999710083008 LOSS_Discriminator: 0.06386075913906097\n",
            "ITERATION_NO.: 806 LOSS_Generator: 5.883502006530762 LOSS_Discriminator: 0.14873197674751282\n",
            "ITERATION_NO.: 807 LOSS_Generator: 5.761500358581543 LOSS_Discriminator: 0.13477805256843567\n",
            "ITERATION_NO.: 808 LOSS_Generator: 5.114443302154541 LOSS_Discriminator: 0.020050520077347755\n",
            "ITERATION_NO.: 809 LOSS_Generator: 4.440482139587402 LOSS_Discriminator: 0.32474464178085327\n",
            "ITERATION_NO.: 810 LOSS_Generator: 3.35996413230896 LOSS_Discriminator: 0.5144623517990112\n",
            "ITERATION_NO.: 811 LOSS_Generator: 3.5466160774230957 LOSS_Discriminator: 0.10553857684135437\n",
            "ITERATION_NO.: 812 LOSS_Generator: 4.002013683319092 LOSS_Discriminator: 0.16399911046028137\n",
            "ITERATION_NO.: 813 LOSS_Generator: 4.642105579376221 LOSS_Discriminator: 0.05254726484417915\n",
            "ITERATION_NO.: 814 LOSS_Generator: 5.287186622619629 LOSS_Discriminator: 0.15277419984340668\n",
            "ITERATION_NO.: 815 LOSS_Generator: 5.41439151763916 LOSS_Discriminator: 0.2288314551115036\n",
            "ITERATION_NO.: 816 LOSS_Generator: 4.976096153259277 LOSS_Discriminator: 0.08339124917984009\n",
            "ITERATION_NO.: 817 LOSS_Generator: 4.586458206176758 LOSS_Discriminator: 0.31101205945014954\n",
            "ITERATION_NO.: 818 LOSS_Generator: 4.132207870483398 LOSS_Discriminator: 0.11573440581560135\n",
            "ITERATION_NO.: 819 LOSS_Generator: 3.912075996398926 LOSS_Discriminator: 0.16534778475761414\n",
            "ITERATION_NO.: 820 LOSS_Generator: 3.504631519317627 LOSS_Discriminator: 0.16161292791366577\n",
            "ITERATION_NO.: 821 LOSS_Generator: 4.624242305755615 LOSS_Discriminator: 0.0717516615986824\n",
            "ITERATION_NO.: 822 LOSS_Generator: 5.477028846740723 LOSS_Discriminator: 0.2148016393184662\n",
            "ITERATION_NO.: 823 LOSS_Generator: 5.773977756500244 LOSS_Discriminator: 0.05164206773042679\n",
            "ITERATION_NO.: 824 LOSS_Generator: 6.031006813049316 LOSS_Discriminator: 0.07855696976184845\n",
            "ITERATION_NO.: 825 LOSS_Generator: 5.9499006271362305 LOSS_Discriminator: 0.12180566787719727\n",
            "ITERATION_NO.: 826 LOSS_Generator: 5.89986515045166 LOSS_Discriminator: 0.128347247838974\n",
            "ITERATION_NO.: 827 LOSS_Generator: 5.366294860839844 LOSS_Discriminator: 0.02967972308397293\n",
            "ITERATION_NO.: 828 LOSS_Generator: 5.2453155517578125 LOSS_Discriminator: 0.02822117507457733\n",
            "ITERATION_NO.: 829 LOSS_Generator: 5.324027061462402 LOSS_Discriminator: 0.2070138305425644\n",
            "ITERATION_NO.: 830 LOSS_Generator: 5.251581192016602 LOSS_Discriminator: 0.047243982553482056\n",
            "ITERATION_NO.: 831 LOSS_Generator: 4.50885009765625 LOSS_Discriminator: 0.11249548941850662\n",
            "ITERATION_NO.: 832 LOSS_Generator: 4.527935981750488 LOSS_Discriminator: 0.10280582308769226\n",
            "ITERATION_NO.: 833 LOSS_Generator: 4.88040828704834 LOSS_Discriminator: 0.09968061745166779\n",
            "ITERATION_NO.: 834 LOSS_Generator: 4.503046989440918 LOSS_Discriminator: 0.2746157646179199\n",
            "ITERATION_NO.: 835 LOSS_Generator: 4.665035724639893 LOSS_Discriminator: 0.14988337457180023\n",
            "ITERATION_NO.: 836 LOSS_Generator: 4.411340713500977 LOSS_Discriminator: 0.1649380773305893\n",
            "ITERATION_NO.: 837 LOSS_Generator: 4.497592926025391 LOSS_Discriminator: 0.1938890814781189\n",
            "ITERATION_NO.: 838 LOSS_Generator: 4.397008895874023 LOSS_Discriminator: 0.16524937748908997\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.669022560119629 LOSS_Discriminator: 0.09140284359455109\n",
            "ITERATION_NO.: 840 LOSS_Generator: 5.1250386238098145 LOSS_Discriminator: 0.05495167523622513\n",
            "ITERATION_NO.: 841 LOSS_Generator: 5.215681076049805 LOSS_Discriminator: 0.2453591525554657\n",
            "ITERATION_NO.: 842 LOSS_Generator: 4.627808570861816 LOSS_Discriminator: 0.03590881824493408\n",
            "ITERATION_NO.: 843 LOSS_Generator: 4.591653823852539 LOSS_Discriminator: 0.08906523883342743\n",
            "ITERATION_NO.: 844 LOSS_Generator: 4.740926742553711 LOSS_Discriminator: 0.025677509605884552\n",
            "ITERATION_NO.: 845 LOSS_Generator: 4.71990442276001 LOSS_Discriminator: 0.11497195065021515\n",
            "ITERATION_NO.: 846 LOSS_Generator: 5.244589805603027 LOSS_Discriminator: 0.02119671180844307\n",
            "ITERATION_NO.: 847 LOSS_Generator: 4.772607803344727 LOSS_Discriminator: 0.12295573949813843\n",
            "ITERATION_NO.: 848 LOSS_Generator: 4.342433929443359 LOSS_Discriminator: 0.1233745813369751\n",
            "ITERATION_NO.: 849 LOSS_Generator: 4.784282684326172 LOSS_Discriminator: 0.04986321181058884\n",
            "ITERATION_NO.: 850 LOSS_Generator: 4.667390823364258 LOSS_Discriminator: 0.06654800474643707\n",
            "ITERATION_NO.: 851 LOSS_Generator: 5.4125823974609375 LOSS_Discriminator: 0.16142144799232483\n",
            "ITERATION_NO.: 852 LOSS_Generator: 4.64199161529541 LOSS_Discriminator: 0.08773490786552429\n",
            "ITERATION_NO.: 853 LOSS_Generator: 4.122394561767578 LOSS_Discriminator: 0.2143997848033905\n",
            "ITERATION_NO.: 854 LOSS_Generator: 4.413325309753418 LOSS_Discriminator: 0.06129755824804306\n",
            "ITERATION_NO.: 855 LOSS_Generator: 5.803627967834473 LOSS_Discriminator: 0.05899243801832199\n",
            "ITERATION_NO.: 856 LOSS_Generator: 5.570044994354248 LOSS_Discriminator: 0.12461324036121368\n",
            "ITERATION_NO.: 857 LOSS_Generator: 6.051831245422363 LOSS_Discriminator: 0.08015783131122589\n",
            "ITERATION_NO.: 858 LOSS_Generator: 6.574057579040527 LOSS_Discriminator: 0.1738542914390564\n",
            "ITERATION_NO.: 859 LOSS_Generator: 5.731603145599365 LOSS_Discriminator: 0.12921038269996643\n",
            "ITERATION_NO.: 860 LOSS_Generator: 5.649160861968994 LOSS_Discriminator: 0.1444844901561737\n",
            "ITERATION_NO.: 861 LOSS_Generator: 4.782458305358887 LOSS_Discriminator: 0.11269158124923706\n",
            "ITERATION_NO.: 862 LOSS_Generator: 4.365765571594238 LOSS_Discriminator: 0.04807636886835098\n",
            "ITERATION_NO.: 863 LOSS_Generator: 4.853557586669922 LOSS_Discriminator: 0.16996100544929504\n",
            "ITERATION_NO.: 864 LOSS_Generator: 4.791609287261963 LOSS_Discriminator: 0.055142346769571304\n",
            "ITERATION_NO.: 865 LOSS_Generator: 5.581107139587402 LOSS_Discriminator: 0.06663909554481506\n",
            "ITERATION_NO.: 866 LOSS_Generator: 5.167635917663574 LOSS_Discriminator: 0.28576475381851196\n",
            "ITERATION_NO.: 867 LOSS_Generator: 5.138383865356445 LOSS_Discriminator: 0.06780341267585754\n",
            "ITERATION_NO.: 868 LOSS_Generator: 5.537646293640137 LOSS_Discriminator: 0.031120119616389275\n",
            "ITERATION_NO.: 869 LOSS_Generator: 5.7862982749938965 LOSS_Discriminator: 0.03340461105108261\n",
            "ITERATION_NO.: 870 LOSS_Generator: 5.8451385498046875 LOSS_Discriminator: 0.1013140082359314\n",
            "ITERATION_NO.: 871 LOSS_Generator: 5.845949172973633 LOSS_Discriminator: 0.061031389981508255\n",
            "ITERATION_NO.: 872 LOSS_Generator: 5.567009925842285 LOSS_Discriminator: 0.1995980441570282\n",
            "ITERATION_NO.: 873 LOSS_Generator: 5.002289295196533 LOSS_Discriminator: 0.022566383704543114\n",
            "ITERATION_NO.: 874 LOSS_Generator: 5.248028755187988 LOSS_Discriminator: 0.06602878868579865\n",
            "ITERATION_NO.: 875 LOSS_Generator: 5.194199562072754 LOSS_Discriminator: 0.04392992705106735\n",
            "ITERATION_NO.: 876 LOSS_Generator: 4.94899845123291 LOSS_Discriminator: 0.1785699874162674\n",
            "ITERATION_NO.: 877 LOSS_Generator: 4.890060901641846 LOSS_Discriminator: 0.03322635218501091\n",
            "ITERATION_NO.: 878 LOSS_Generator: 4.506855010986328 LOSS_Discriminator: 0.28600209951400757\n",
            "ITERATION_NO.: 879 LOSS_Generator: 3.8294475078582764 LOSS_Discriminator: 0.19031494855880737\n",
            "ITERATION_NO.: 880 LOSS_Generator: 4.243197441101074 LOSS_Discriminator: 0.10918579250574112\n",
            "ITERATION_NO.: 881 LOSS_Generator: 4.449806213378906 LOSS_Discriminator: 0.1348341405391693\n",
            "ITERATION_NO.: 882 LOSS_Generator: 4.867952346801758 LOSS_Discriminator: 0.148297980427742\n",
            "ITERATION_NO.: 883 LOSS_Generator: 5.040258884429932 LOSS_Discriminator: 0.03214701637625694\n",
            "ITERATION_NO.: 884 LOSS_Generator: 5.852349281311035 LOSS_Discriminator: 0.036279112100601196\n",
            "ITERATION_NO.: 885 LOSS_Generator: 5.85249137878418 LOSS_Discriminator: 0.05117267370223999\n",
            "ITERATION_NO.: 886 LOSS_Generator: 6.025602340698242 LOSS_Discriminator: 0.1563522219657898\n",
            "ITERATION_NO.: 887 LOSS_Generator: 5.491630554199219 LOSS_Discriminator: 0.04006151854991913\n",
            "ITERATION_NO.: 888 LOSS_Generator: 5.302302360534668 LOSS_Discriminator: 0.19940944015979767\n",
            "ITERATION_NO.: 889 LOSS_Generator: 4.749995231628418 LOSS_Discriminator: 0.15442708134651184\n",
            "ITERATION_NO.: 890 LOSS_Generator: 4.10048770904541 LOSS_Discriminator: 0.07487001270055771\n",
            "ITERATION_NO.: 891 LOSS_Generator: 4.848573207855225 LOSS_Discriminator: 0.049565065652132034\n",
            "ITERATION_NO.: 892 LOSS_Generator: 5.245125770568848 LOSS_Discriminator: 0.07540220022201538\n",
            "ITERATION_NO.: 893 LOSS_Generator: 5.9477314949035645 LOSS_Discriminator: 0.10008059442043304\n",
            "ITERATION_NO.: 894 LOSS_Generator: 5.652512073516846 LOSS_Discriminator: 0.02570236101746559\n",
            "ITERATION_NO.: 895 LOSS_Generator: 6.096600532531738 LOSS_Discriminator: 0.32830777764320374\n",
            "ITERATION_NO.: 896 LOSS_Generator: 6.3349151611328125 LOSS_Discriminator: 0.0193826612085104\n",
            "ITERATION_NO.: 897 LOSS_Generator: 6.159816741943359 LOSS_Discriminator: 0.08750349283218384\n",
            "ITERATION_NO.: 898 LOSS_Generator: 6.184300422668457 LOSS_Discriminator: 0.06355677545070648\n",
            "ITERATION_NO.: 899 LOSS_Generator: 6.042233943939209 LOSS_Discriminator: 0.0798383429646492\n",
            "ITERATION_NO.: 900 LOSS_Generator: 6.896713733673096 LOSS_Discriminator: 0.050689198076725006\n",
            "ITERATION_NO.: 901 LOSS_Generator: 5.962132453918457 LOSS_Discriminator: 0.04054133966565132\n",
            "ITERATION_NO.: 902 LOSS_Generator: 6.382213592529297 LOSS_Discriminator: 0.307238906621933\n",
            "ITERATION_NO.: 903 LOSS_Generator: 5.379907608032227 LOSS_Discriminator: 0.29138338565826416\n",
            "ITERATION_NO.: 904 LOSS_Generator: 5.299426078796387 LOSS_Discriminator: 0.06651283800601959\n",
            "ITERATION_NO.: 905 LOSS_Generator: 4.652202606201172 LOSS_Discriminator: 0.06831979751586914\n",
            "ITERATION_NO.: 906 LOSS_Generator: 4.753717422485352 LOSS_Discriminator: 0.20570027828216553\n",
            "ITERATION_NO.: 907 LOSS_Generator: 4.033392429351807 LOSS_Discriminator: 0.11449691653251648\n",
            "ITERATION_NO.: 908 LOSS_Generator: 3.6847405433654785 LOSS_Discriminator: 0.07845579832792282\n",
            "ITERATION_NO.: 909 LOSS_Generator: 4.340775489807129 LOSS_Discriminator: 0.08694224059581757\n",
            "ITERATION_NO.: 910 LOSS_Generator: 5.654928207397461 LOSS_Discriminator: 0.04639771580696106\n",
            "ITERATION_NO.: 911 LOSS_Generator: 6.284717559814453 LOSS_Discriminator: 0.09699822962284088\n",
            "ITERATION_NO.: 912 LOSS_Generator: 6.350323677062988 LOSS_Discriminator: 0.022198466584086418\n",
            "ITERATION_NO.: 913 LOSS_Generator: 6.651244163513184 LOSS_Discriminator: 0.17635038495063782\n",
            "ITERATION_NO.: 914 LOSS_Generator: 6.204586505889893 LOSS_Discriminator: 0.18433797359466553\n",
            "ITERATION_NO.: 915 LOSS_Generator: 6.042078495025635 LOSS_Discriminator: 0.10938820242881775\n",
            "ITERATION_NO.: 916 LOSS_Generator: 5.665937900543213 LOSS_Discriminator: 0.1936608850955963\n",
            "ITERATION_NO.: 917 LOSS_Generator: 4.837155818939209 LOSS_Discriminator: 0.1555732935667038\n",
            "ITERATION_NO.: 918 LOSS_Generator: 4.304938793182373 LOSS_Discriminator: 0.10667263716459274\n",
            "ITERATION_NO.: 919 LOSS_Generator: 4.011116981506348 LOSS_Discriminator: 0.18232619762420654\n",
            "ITERATION_NO.: 920 LOSS_Generator: 3.839261054992676 LOSS_Discriminator: 0.11747448146343231\n",
            "ITERATION_NO.: 921 LOSS_Generator: 4.150783538818359 LOSS_Discriminator: 0.22250688076019287\n",
            "ITERATION_NO.: 922 LOSS_Generator: 4.645595073699951 LOSS_Discriminator: 0.0720415711402893\n",
            "ITERATION_NO.: 923 LOSS_Generator: 5.564280986785889 LOSS_Discriminator: 0.27437907457351685\n",
            "ITERATION_NO.: 924 LOSS_Generator: 5.631707191467285 LOSS_Discriminator: 0.19428548216819763\n",
            "ITERATION_NO.: 925 LOSS_Generator: 5.669941425323486 LOSS_Discriminator: 0.006010433193296194\n",
            "ITERATION_NO.: 926 LOSS_Generator: 5.905851364135742 LOSS_Discriminator: 0.0669972151517868\n",
            "ITERATION_NO.: 927 LOSS_Generator: 6.00716495513916 LOSS_Discriminator: 0.057940974831581116\n",
            "ITERATION_NO.: 928 LOSS_Generator: 5.678142547607422 LOSS_Discriminator: 0.09023075550794601\n",
            "ITERATION_NO.: 929 LOSS_Generator: 5.344785690307617 LOSS_Discriminator: 0.08377276360988617\n",
            "ITERATION_NO.: 930 LOSS_Generator: 5.455328464508057 LOSS_Discriminator: 0.15123215317726135\n",
            "ITERATION_NO.: 931 LOSS_Generator: 4.758727073669434 LOSS_Discriminator: 0.3048121929168701\n",
            "ITERATION_NO.: 932 LOSS_Generator: 4.252412796020508 LOSS_Discriminator: 0.1443568617105484\n",
            "ITERATION_NO.: 933 LOSS_Generator: 4.612542152404785 LOSS_Discriminator: 0.1404791921377182\n",
            "ITERATION_NO.: 934 LOSS_Generator: 4.972559928894043 LOSS_Discriminator: 0.09660203754901886\n",
            "ITERATION_NO.: 935 LOSS_Generator: 5.393852710723877 LOSS_Discriminator: 0.07790696620941162\n",
            "ITERATION_NO.: 936 LOSS_Generator: 5.928033828735352 LOSS_Discriminator: 0.07050325721502304\n",
            "ITERATION_NO.: 937 LOSS_Generator: 6.081215858459473 LOSS_Discriminator: 0.21691343188285828\n",
            "ITERATION_NO.: 938 LOSS_Generator: 5.734663486480713 LOSS_Discriminator: 0.10858790576457977\n",
            "ITERATION_NO.: 939 LOSS_Generator: 4.974721431732178 LOSS_Discriminator: 0.09917294979095459\n",
            "ITERATION_NO.: 940 LOSS_Generator: 4.306395530700684 LOSS_Discriminator: 0.14586976170539856\n",
            "ITERATION_NO.: 941 LOSS_Generator: 4.271844387054443 LOSS_Discriminator: 0.12404301762580872\n",
            "ITERATION_NO.: 942 LOSS_Generator: 4.796695709228516 LOSS_Discriminator: 0.12283208966255188\n",
            "ITERATION_NO.: 943 LOSS_Generator: 4.305540084838867 LOSS_Discriminator: 0.06386423110961914\n",
            "ITERATION_NO.: 944 LOSS_Generator: 5.121531963348389 LOSS_Discriminator: 0.05965482443571091\n",
            "ITERATION_NO.: 945 LOSS_Generator: 4.894454002380371 LOSS_Discriminator: 0.09901241958141327\n",
            "ITERATION_NO.: 946 LOSS_Generator: 5.217738628387451 LOSS_Discriminator: 0.22569385170936584\n",
            "ITERATION_NO.: 947 LOSS_Generator: 5.4308061599731445 LOSS_Discriminator: 0.11888862401247025\n",
            "ITERATION_NO.: 948 LOSS_Generator: 5.079189300537109 LOSS_Discriminator: 0.03114190883934498\n",
            "ITERATION_NO.: 949 LOSS_Generator: 5.247675895690918 LOSS_Discriminator: 0.03457775339484215\n",
            "ITERATION_NO.: 950 LOSS_Generator: 5.355860710144043 LOSS_Discriminator: 0.03234253078699112\n",
            "ITERATION_NO.: 951 LOSS_Generator: 5.197060585021973 LOSS_Discriminator: 0.2241886407136917\n",
            "ITERATION_NO.: 952 LOSS_Generator: 5.355368614196777 LOSS_Discriminator: 0.10769008100032806\n",
            "ITERATION_NO.: 953 LOSS_Generator: 4.930160045623779 LOSS_Discriminator: 0.10301873832941055\n",
            "ITERATION_NO.: 954 LOSS_Generator: 5.375057220458984 LOSS_Discriminator: 0.07942843437194824\n",
            "ITERATION_NO.: 955 LOSS_Generator: 5.3079657554626465 LOSS_Discriminator: 0.04255463927984238\n",
            "ITERATION_NO.: 956 LOSS_Generator: 6.158758640289307 LOSS_Discriminator: 0.1642943173646927\n",
            "ITERATION_NO.: 957 LOSS_Generator: 5.133221626281738 LOSS_Discriminator: 0.09376181662082672\n",
            "ITERATION_NO.: 958 LOSS_Generator: 5.493190765380859 LOSS_Discriminator: 0.13590440154075623\n",
            "ITERATION_NO.: 959 LOSS_Generator: 5.375632286071777 LOSS_Discriminator: 0.11769678443670273\n",
            "ITERATION_NO.: 960 LOSS_Generator: 5.1222381591796875 LOSS_Discriminator: 0.0834493637084961\n",
            "ITERATION_NO.: 961 LOSS_Generator: 4.230376243591309 LOSS_Discriminator: 0.1418631672859192\n",
            "ITERATION_NO.: 962 LOSS_Generator: 5.0676069259643555 LOSS_Discriminator: 0.04525723308324814\n",
            "ITERATION_NO.: 963 LOSS_Generator: 5.5713605880737305 LOSS_Discriminator: 0.07335412502288818\n",
            "ITERATION_NO.: 964 LOSS_Generator: 5.317930221557617 LOSS_Discriminator: 0.09106022864580154\n",
            "ITERATION_NO.: 965 LOSS_Generator: 5.422016143798828 LOSS_Discriminator: 0.08943062275648117\n",
            "ITERATION_NO.: 966 LOSS_Generator: 5.330096244812012 LOSS_Discriminator: 0.03846808150410652\n",
            "ITERATION_NO.: 967 LOSS_Generator: 5.3714189529418945 LOSS_Discriminator: 0.294933021068573\n",
            "ITERATION_NO.: 968 LOSS_Generator: 4.203286170959473 LOSS_Discriminator: 0.20062217116355896\n",
            "ITERATION_NO.: 969 LOSS_Generator: 4.886813163757324 LOSS_Discriminator: 0.25250840187072754\n",
            "ITERATION_NO.: 970 LOSS_Generator: 4.151308536529541 LOSS_Discriminator: 0.06666577607393265\n",
            "ITERATION_NO.: 971 LOSS_Generator: 4.165093898773193 LOSS_Discriminator: 0.20588991045951843\n",
            "ITERATION_NO.: 972 LOSS_Generator: 4.59920072555542 LOSS_Discriminator: 0.101754330098629\n",
            "ITERATION_NO.: 973 LOSS_Generator: 6.308871269226074 LOSS_Discriminator: 0.03615741804242134\n",
            "ITERATION_NO.: 974 LOSS_Generator: 7.058844566345215 LOSS_Discriminator: 0.12497303634881973\n",
            "ITERATION_NO.: 975 LOSS_Generator: 7.208512783050537 LOSS_Discriminator: 0.008209824562072754\n",
            "ITERATION_NO.: 976 LOSS_Generator: 7.108859062194824 LOSS_Discriminator: 0.3673788011074066\n",
            "ITERATION_NO.: 977 LOSS_Generator: 6.876397609710693 LOSS_Discriminator: 0.16482219099998474\n",
            "ITERATION_NO.: 978 LOSS_Generator: 5.919491767883301 LOSS_Discriminator: 0.13582053780555725\n",
            "ITERATION_NO.: 979 LOSS_Generator: 5.077795028686523 LOSS_Discriminator: 0.09718179702758789\n",
            "ITERATION_NO.: 980 LOSS_Generator: 5.412939548492432 LOSS_Discriminator: 0.19725069403648376\n",
            "ITERATION_NO.: 981 LOSS_Generator: 5.256317138671875 LOSS_Discriminator: 0.1868017017841339\n",
            "ITERATION_NO.: 982 LOSS_Generator: 6.396965026855469 LOSS_Discriminator: 0.17341911792755127\n",
            "ITERATION_NO.: 983 LOSS_Generator: 6.774614334106445 LOSS_Discriminator: 0.017410576343536377\n",
            "ITERATION_NO.: 984 LOSS_Generator: 6.2108869552612305 LOSS_Discriminator: 0.42380937933921814\n",
            "ITERATION_NO.: 985 LOSS_Generator: 6.128375053405762 LOSS_Discriminator: 0.07780075073242188\n",
            "ITERATION_NO.: 986 LOSS_Generator: 5.333017826080322 LOSS_Discriminator: 0.18455076217651367\n",
            "ITERATION_NO.: 987 LOSS_Generator: 5.0700297355651855 LOSS_Discriminator: 0.14936760067939758\n",
            "ITERATION_NO.: 988 LOSS_Generator: 4.77086877822876 LOSS_Discriminator: 0.09920866042375565\n",
            "ITERATION_NO.: 989 LOSS_Generator: 4.134621620178223 LOSS_Discriminator: 0.15432967245578766\n",
            "ITERATION_NO.: 990 LOSS_Generator: 5.0996198654174805 LOSS_Discriminator: 0.13755686581134796\n",
            "ITERATION_NO.: 991 LOSS_Generator: 5.489948749542236 LOSS_Discriminator: 0.05727836489677429\n",
            "ITERATION_NO.: 992 LOSS_Generator: 5.607230186462402 LOSS_Discriminator: 0.08640233427286148\n",
            "ITERATION_NO.: 993 LOSS_Generator: 6.452375411987305 LOSS_Discriminator: 0.034472960978746414\n",
            "ITERATION_NO.: 994 LOSS_Generator: 6.555398941040039 LOSS_Discriminator: 0.6318992972373962\n",
            "ITERATION_NO.: 995 LOSS_Generator: 6.0158586502075195 LOSS_Discriminator: 0.14174389839172363\n",
            "ITERATION_NO.: 996 LOSS_Generator: 4.2243499755859375 LOSS_Discriminator: 0.09515368938446045\n",
            "ITERATION_NO.: 997 LOSS_Generator: 3.022124767303467 LOSS_Discriminator: 0.3898140788078308\n",
            "ITERATION_NO.: 998 LOSS_Generator: 3.2970900535583496 LOSS_Discriminator: 0.23503485321998596\n",
            "ITERATION_NO.: 999 LOSS_Generator: 3.8228583335876465 LOSS_Discriminator: 0.23988749086856842\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 4.974327087402344 LOSS_Discriminator: 0.1027507409453392\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 5.23019552230835 LOSS_Discriminator: 0.13291320204734802\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 5.889923095703125 LOSS_Discriminator: 0.011898931115865707\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 6.669277191162109 LOSS_Discriminator: 0.13768577575683594\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 6.550339698791504 LOSS_Discriminator: 0.0407620444893837\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 6.76129674911499 LOSS_Discriminator: 0.11713694036006927\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 6.750576972961426 LOSS_Discriminator: 0.013914098963141441\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 6.754112243652344 LOSS_Discriminator: 0.1351385861635208\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 6.491729736328125 LOSS_Discriminator: 0.019172754138708115\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 6.2923784255981445 LOSS_Discriminator: 0.03498658910393715\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 5.979104995727539 LOSS_Discriminator: 0.20504818856716156\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 4.609936714172363 LOSS_Discriminator: 0.6398489475250244\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 2.8887393474578857 LOSS_Discriminator: 0.05486862733960152\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 2.6302762031555176 LOSS_Discriminator: 0.30601853132247925\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 3.73970890045166 LOSS_Discriminator: 0.405364066362381\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 5.586544990539551 LOSS_Discriminator: 0.14363408088684082\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 6.347846984863281 LOSS_Discriminator: 0.16961586475372314\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 6.716819763183594 LOSS_Discriminator: 0.25156643986701965\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 6.641538143157959 LOSS_Discriminator: 0.3630216121673584\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 6.228525161743164 LOSS_Discriminator: 0.5732712745666504\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 4.4830403327941895 LOSS_Discriminator: 0.2872244715690613\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 3.6559031009674072 LOSS_Discriminator: 0.17044329643249512\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 2.8116953372955322 LOSS_Discriminator: 0.18692736327648163\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 3.6398727893829346 LOSS_Discriminator: 0.32949477434158325\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 4.053781032562256 LOSS_Discriminator: 0.11271494626998901\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 5.172005653381348 LOSS_Discriminator: 0.03939596191048622\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 5.059964179992676 LOSS_Discriminator: 0.1437397003173828\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 5.293219089508057 LOSS_Discriminator: 0.1584491729736328\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 4.935523986816406 LOSS_Discriminator: 0.052888672798871994\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 5.138092994689941 LOSS_Discriminator: 0.08127346634864807\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 4.939326286315918 LOSS_Discriminator: 0.09003283083438873\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 4.925039291381836 LOSS_Discriminator: 0.05289830267429352\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 4.631677627563477 LOSS_Discriminator: 0.09153847396373749\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 4.590481758117676 LOSS_Discriminator: 0.1111101359128952\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 4.823657989501953 LOSS_Discriminator: 0.044494688510894775\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 5.156510353088379 LOSS_Discriminator: 0.14198142290115356\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 5.058175086975098 LOSS_Discriminator: 0.01517984364181757\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 5.626519680023193 LOSS_Discriminator: 0.09053891897201538\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 5.750017166137695 LOSS_Discriminator: 0.10876559466123581\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 5.299988746643066 LOSS_Discriminator: 0.040470004081726074\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 5.051589012145996 LOSS_Discriminator: 0.3586850166320801\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 4.584195137023926 LOSS_Discriminator: 0.10551393032073975\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 4.050899505615234 LOSS_Discriminator: 0.04376307129859924\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 4.037539482116699 LOSS_Discriminator: 0.10541227459907532\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 3.6609482765197754 LOSS_Discriminator: 0.17749831080436707\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 3.638000011444092 LOSS_Discriminator: 0.1189158484339714\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 4.707650661468506 LOSS_Discriminator: 0.0942092165350914\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 5.823906898498535 LOSS_Discriminator: 0.03616300970315933\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 5.868932247161865 LOSS_Discriminator: 0.059564631432294846\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 6.762027740478516 LOSS_Discriminator: 0.1777232438325882\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 6.114404678344727 LOSS_Discriminator: 0.005164647474884987\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 6.143436431884766 LOSS_Discriminator: 0.024438200518488884\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 6.5746049880981445 LOSS_Discriminator: 0.024025505408644676\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 6.757100582122803 LOSS_Discriminator: 0.01735447533428669\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 6.2478227615356445 LOSS_Discriminator: 0.22740131616592407\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 5.720625877380371 LOSS_Discriminator: 0.03680495172739029\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 4.904357433319092 LOSS_Discriminator: 0.13892337679862976\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 4.376687049865723 LOSS_Discriminator: 0.25030767917633057\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 3.678600788116455 LOSS_Discriminator: 0.06693218648433685\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 3.7724926471710205 LOSS_Discriminator: 0.06906181573867798\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 3.760392665863037 LOSS_Discriminator: 0.10717656463384628\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 3.8124637603759766 LOSS_Discriminator: 0.2129521667957306\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 4.975211143493652 LOSS_Discriminator: 0.18080084025859833\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 4.8752336502075195 LOSS_Discriminator: 0.06751706451177597\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 5.0907697677612305 LOSS_Discriminator: 0.1788858026266098\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 5.297055244445801 LOSS_Discriminator: 0.19419394433498383\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 4.631748199462891 LOSS_Discriminator: 0.30898427963256836\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 4.345773220062256 LOSS_Discriminator: 0.02791590243577957\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 4.063392639160156 LOSS_Discriminator: 0.3194998502731323\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 3.5427088737487793 LOSS_Discriminator: 0.3500213623046875\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.728018760681152 LOSS_Discriminator: 0.1078612431883812\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 5.517014026641846 LOSS_Discriminator: 0.258597731590271\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 5.342642784118652 LOSS_Discriminator: 0.10952362418174744\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 5.1508026123046875 LOSS_Discriminator: 0.10606662184000015\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 5.354983329772949 LOSS_Discriminator: 0.08650662750005722\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 5.197636604309082 LOSS_Discriminator: 0.11225961148738861\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 5.401256561279297 LOSS_Discriminator: 0.26489537954330444\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 4.19706392288208 LOSS_Discriminator: 0.10679478943347931\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 3.610124349594116 LOSS_Discriminator: 0.20713600516319275\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 4.117889404296875 LOSS_Discriminator: 0.1734868884086609\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 4.065016746520996 LOSS_Discriminator: 0.1689969152212143\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 4.178460121154785 LOSS_Discriminator: 0.1621573567390442\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 4.786783218383789 LOSS_Discriminator: 0.1497625708580017\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 4.957882881164551 LOSS_Discriminator: 0.07851502299308777\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 4.576810836791992 LOSS_Discriminator: 0.16964268684387207\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 5.085986137390137 LOSS_Discriminator: 0.1307946741580963\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 3.5555009841918945 LOSS_Discriminator: 0.5311197638511658\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 3.563598871231079 LOSS_Discriminator: 0.24249383807182312\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 3.674877166748047 LOSS_Discriminator: 0.10648229718208313\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 3.3402578830718994 LOSS_Discriminator: 0.20692554116249084\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 4.017400741577148 LOSS_Discriminator: 0.1862800121307373\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 4.110260009765625 LOSS_Discriminator: 0.15725094079971313\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 4.191903114318848 LOSS_Discriminator: 0.1347062587738037\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 4.088952541351318 LOSS_Discriminator: 0.10445178300142288\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 5.0911359786987305 LOSS_Discriminator: 0.07871837913990021\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 5.517488956451416 LOSS_Discriminator: 0.08468066900968552\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 4.492378234863281 LOSS_Discriminator: 0.23887360095977783\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 4.8130340576171875 LOSS_Discriminator: 0.03892146050930023\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 4.295937538146973 LOSS_Discriminator: 0.07471714168787003\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 4.796812057495117 LOSS_Discriminator: 0.21217064559459686\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 4.595951080322266 LOSS_Discriminator: 0.14585036039352417\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 4.018773078918457 LOSS_Discriminator: 0.10202983766794205\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 4.211557388305664 LOSS_Discriminator: 0.1468334197998047\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 4.956934928894043 LOSS_Discriminator: 0.07115346193313599\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 4.998126029968262 LOSS_Discriminator: 0.02654411271214485\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 4.941143989562988 LOSS_Discriminator: 0.11698286235332489\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 5.520340919494629 LOSS_Discriminator: 0.015494001097977161\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 5.8431267738342285 LOSS_Discriminator: 0.09604260325431824\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 5.208981513977051 LOSS_Discriminator: 0.036520905792713165\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 5.0530853271484375 LOSS_Discriminator: 0.13420671224594116\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 4.729160308837891 LOSS_Discriminator: 0.3756401538848877\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 5.061771392822266 LOSS_Discriminator: 0.03901149332523346\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 4.577323913574219 LOSS_Discriminator: 0.10931212455034256\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 4.205133438110352 LOSS_Discriminator: 0.14826877415180206\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 4.268298625946045 LOSS_Discriminator: 0.10250887274742126\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 4.91126012802124 LOSS_Discriminator: 0.18645770847797394\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 4.620121479034424 LOSS_Discriminator: 0.09369509667158127\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.261148452758789 LOSS_Discriminator: 0.0989673063158989\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 4.1603851318359375 LOSS_Discriminator: 0.12756222486495972\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 4.145651817321777 LOSS_Discriminator: 0.12984031438827515\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 3.3960695266723633 LOSS_Discriminator: 0.08644605427980423\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 3.514927625656128 LOSS_Discriminator: 0.23341383039951324\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 4.0411176681518555 LOSS_Discriminator: 0.09884200990200043\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 4.769762992858887 LOSS_Discriminator: 0.2720835208892822\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 4.456701755523682 LOSS_Discriminator: 0.26820647716522217\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 4.7389678955078125 LOSS_Discriminator: 0.07723690569400787\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 4.412705421447754 LOSS_Discriminator: 0.14499303698539734\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 4.616117477416992 LOSS_Discriminator: 0.08148126304149628\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 4.710199356079102 LOSS_Discriminator: 0.1464916467666626\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 4.739566802978516 LOSS_Discriminator: 0.03335687145590782\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 5.865056037902832 LOSS_Discriminator: 0.30374038219451904\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 5.278177261352539 LOSS_Discriminator: 0.03424935042858124\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 4.491085529327393 LOSS_Discriminator: 0.16250911355018616\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 3.734487771987915 LOSS_Discriminator: 0.11878786981105804\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 3.8660943508148193 LOSS_Discriminator: 0.08741418272256851\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 4.028109550476074 LOSS_Discriminator: 0.11865875124931335\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 4.427590370178223 LOSS_Discriminator: 0.13631467521190643\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 5.087651252746582 LOSS_Discriminator: 0.028309686109423637\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 4.727926254272461 LOSS_Discriminator: 0.13569852709770203\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 5.202341556549072 LOSS_Discriminator: 0.037962906062603\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 5.261751651763916 LOSS_Discriminator: 0.11201995611190796\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 5.511300086975098 LOSS_Discriminator: 0.09326965361833572\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 5.771704196929932 LOSS_Discriminator: 0.06579264998435974\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 5.065105438232422 LOSS_Discriminator: 0.024997789412736893\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 5.0054216384887695 LOSS_Discriminator: 0.2982158362865448\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 4.957862854003906 LOSS_Discriminator: 0.12027887254953384\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 4.395037651062012 LOSS_Discriminator: 0.14468026161193848\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 4.308794975280762 LOSS_Discriminator: 0.033808737993240356\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 5.433437347412109 LOSS_Discriminator: 0.16184458136558533\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 5.594627380371094 LOSS_Discriminator: 0.04477916285395622\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 6.262866020202637 LOSS_Discriminator: 0.03563915938138962\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 6.364816665649414 LOSS_Discriminator: 0.20357511937618256\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 6.029891490936279 LOSS_Discriminator: 0.10245979577302933\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 5.84756326675415 LOSS_Discriminator: 0.08320048451423645\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 5.596019744873047 LOSS_Discriminator: 0.12811154127120972\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 4.923474311828613 LOSS_Discriminator: 0.10664400458335876\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 5.019684314727783 LOSS_Discriminator: 0.15939629077911377\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 5.436193943023682 LOSS_Discriminator: 0.14413702487945557\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 5.472412109375 LOSS_Discriminator: 0.07212892174720764\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 5.354759693145752 LOSS_Discriminator: 0.4008830785751343\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 4.063665390014648 LOSS_Discriminator: 0.1909981071949005\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 3.2451558113098145 LOSS_Discriminator: 0.1610228717327118\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 4.006417751312256 LOSS_Discriminator: 0.11749151349067688\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 4.5833539962768555 LOSS_Discriminator: 0.06455136835575104\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 5.9408111572265625 LOSS_Discriminator: 0.11178398877382278\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 6.261662483215332 LOSS_Discriminator: 0.46229687333106995\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 6.025729179382324 LOSS_Discriminator: 0.14884048700332642\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 5.633840560913086 LOSS_Discriminator: 0.10692539811134338\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 5.40305233001709 LOSS_Discriminator: 0.16489151120185852\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 5.266561031341553 LOSS_Discriminator: 0.15722504258155823\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 4.5922441482543945 LOSS_Discriminator: 0.1003018319606781\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 5.065273761749268 LOSS_Discriminator: 0.30703943967819214\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 5.224452495574951 LOSS_Discriminator: 0.034168921411037445\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 4.457862377166748 LOSS_Discriminator: 0.37661629915237427\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 4.801986217498779 LOSS_Discriminator: 0.10499770194292068\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 4.9347639083862305 LOSS_Discriminator: 0.05213288962841034\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 4.841440200805664 LOSS_Discriminator: 0.09154820442199707\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 5.518847465515137 LOSS_Discriminator: 0.18529276549816132\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 6.005590438842773 LOSS_Discriminator: 0.08704642206430435\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 6.321673393249512 LOSS_Discriminator: 0.16308936476707458\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 6.537790298461914 LOSS_Discriminator: 0.10745285451412201\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 6.046037673950195 LOSS_Discriminator: 0.08920001983642578\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 5.1681365966796875 LOSS_Discriminator: 0.10297322273254395\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 4.753852844238281 LOSS_Discriminator: 0.06953476369380951\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 5.051107883453369 LOSS_Discriminator: 0.09757819771766663\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 4.900028705596924 LOSS_Discriminator: 0.14709687232971191\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 4.677143573760986 LOSS_Discriminator: 0.19143453240394592\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 4.685669422149658 LOSS_Discriminator: 0.1183864027261734\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 4.226953506469727 LOSS_Discriminator: 0.029514871537685394\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 4.174074172973633 LOSS_Discriminator: 0.1426875740289688\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 3.707777738571167 LOSS_Discriminator: 0.16930660605430603\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 4.103658199310303 LOSS_Discriminator: 0.10753016173839569\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 4.718448638916016 LOSS_Discriminator: 0.1688718944787979\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 4.319094181060791 LOSS_Discriminator: 0.05475062131881714\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 4.417364597320557 LOSS_Discriminator: 0.12657614052295685\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 4.096814155578613 LOSS_Discriminator: 0.1251199096441269\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 4.234006881713867 LOSS_Discriminator: 0.08245076984167099\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 4.520862579345703 LOSS_Discriminator: 0.13525927066802979\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 4.496006011962891 LOSS_Discriminator: 0.15276986360549927\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 4.881685256958008 LOSS_Discriminator: 0.02053678408265114\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 4.894277095794678 LOSS_Discriminator: 0.10077278316020966\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 4.780786514282227 LOSS_Discriminator: 0.04587462544441223\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 4.705404281616211 LOSS_Discriminator: 0.10089616477489471\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 4.148410320281982 LOSS_Discriminator: 0.11736837774515152\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 4.938971519470215 LOSS_Discriminator: 0.15711277723312378\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 4.761384010314941 LOSS_Discriminator: 0.07960471510887146\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 4.672857284545898 LOSS_Discriminator: 0.15059266984462738\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 4.273961544036865 LOSS_Discriminator: 0.16739970445632935\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 4.575679779052734 LOSS_Discriminator: 0.11988437175750732\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 5.049077033996582 LOSS_Discriminator: 0.08658475428819656\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 4.489774703979492 LOSS_Discriminator: 0.13238143920898438\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 3.8921523094177246 LOSS_Discriminator: 0.15478557348251343\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 4.722738742828369 LOSS_Discriminator: 0.1915382444858551\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 4.581345558166504 LOSS_Discriminator: 0.22882583737373352\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 5.30558443069458 LOSS_Discriminator: 0.0861569344997406\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 5.163768768310547 LOSS_Discriminator: 0.17479830980300903\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 5.121878147125244 LOSS_Discriminator: 0.2575025260448456\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 4.558743953704834 LOSS_Discriminator: 0.21714696288108826\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 3.7721176147460938 LOSS_Discriminator: 0.35821855068206787\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 3.1477556228637695 LOSS_Discriminator: 0.16439655423164368\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 3.8649392127990723 LOSS_Discriminator: 0.23758509755134583\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 4.353843688964844 LOSS_Discriminator: 0.0794367641210556\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 5.410953521728516 LOSS_Discriminator: 0.1460818201303482\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 6.322445869445801 LOSS_Discriminator: 0.015388265252113342\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 6.831904888153076 LOSS_Discriminator: 0.16442087292671204\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 6.7958784103393555 LOSS_Discriminator: 0.017648201435804367\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 6.216193675994873 LOSS_Discriminator: 0.023881951346993446\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 6.358306407928467 LOSS_Discriminator: 0.013103701174259186\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 6.1091437339782715 LOSS_Discriminator: 0.13550573587417603\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 5.86018180847168 LOSS_Discriminator: 0.11426334828138351\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 5.3386406898498535 LOSS_Discriminator: 0.14827634394168854\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 4.86182975769043 LOSS_Discriminator: 0.07904107123613358\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 5.023087978363037 LOSS_Discriminator: 0.14076486229896545\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 5.056325912475586 LOSS_Discriminator: 0.1341027468442917\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 4.816625118255615 LOSS_Discriminator: 0.2699955701828003\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 3.806478261947632 LOSS_Discriminator: 0.10055795311927795\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 3.7818074226379395 LOSS_Discriminator: 0.2511669397354126\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 3.7655391693115234 LOSS_Discriminator: 0.20482824742794037\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 3.7974605560302734 LOSS_Discriminator: 0.14772099256515503\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 4.6268181800842285 LOSS_Discriminator: 0.12897658348083496\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 5.458704948425293 LOSS_Discriminator: 0.09109342098236084\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 5.7121171951293945 LOSS_Discriminator: 0.06570044159889221\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 6.406863689422607 LOSS_Discriminator: 0.025183673948049545\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 6.9573259353637695 LOSS_Discriminator: 0.18972432613372803\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 6.200956344604492 LOSS_Discriminator: 0.4084717035293579\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 6.061019420623779 LOSS_Discriminator: 0.04509267956018448\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 5.691723823547363 LOSS_Discriminator: 0.08685579150915146\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 5.190136909484863 LOSS_Discriminator: 0.0933796763420105\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 5.052605628967285 LOSS_Discriminator: 0.07121265679597855\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 5.291676998138428 LOSS_Discriminator: 0.0875094011425972\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 4.493595123291016 LOSS_Discriminator: 0.11686191707849503\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 4.539466857910156 LOSS_Discriminator: 0.18360668420791626\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 4.817624092102051 LOSS_Discriminator: 0.07589729130268097\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 4.616661071777344 LOSS_Discriminator: 0.06332522630691528\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 4.932567119598389 LOSS_Discriminator: 0.08341687172651291\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 5.512594223022461 LOSS_Discriminator: 0.04989686608314514\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 5.453648567199707 LOSS_Discriminator: 0.044602904468774796\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 5.89582633972168 LOSS_Discriminator: 0.03896370902657509\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 5.731016635894775 LOSS_Discriminator: 0.19678151607513428\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.601734638214111 LOSS_Discriminator: 0.1377916932106018\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 5.190744400024414 LOSS_Discriminator: 0.03352496773004532\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 4.699433326721191 LOSS_Discriminator: 0.28618866205215454\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 4.559647560119629 LOSS_Discriminator: 0.039234891533851624\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 4.9758100509643555 LOSS_Discriminator: 0.2074442356824875\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 3.9971632957458496 LOSS_Discriminator: 0.2226438820362091\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 3.9411466121673584 LOSS_Discriminator: 0.059106647968292236\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 4.242486000061035 LOSS_Discriminator: 0.09659974277019501\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 4.598789215087891 LOSS_Discriminator: 0.14266732335090637\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 4.365460395812988 LOSS_Discriminator: 0.13239946961402893\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 4.819073677062988 LOSS_Discriminator: 0.1296531707048416\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 4.644932746887207 LOSS_Discriminator: 0.05427580699324608\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 5.008162498474121 LOSS_Discriminator: 0.08419949561357498\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 5.374665260314941 LOSS_Discriminator: 0.07259691506624222\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 5.482237815856934 LOSS_Discriminator: 0.06691648811101913\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 4.484350681304932 LOSS_Discriminator: 0.028326038271188736\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 4.990788459777832 LOSS_Discriminator: 0.0396738164126873\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.167160987854004 LOSS_Discriminator: 0.12461370974779129\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 5.315057754516602 LOSS_Discriminator: 0.1558753401041031\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 4.600339412689209 LOSS_Discriminator: 0.3026532828807831\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 3.9072742462158203 LOSS_Discriminator: 0.034525468945503235\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 4.14302396774292 LOSS_Discriminator: 0.12463101744651794\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 5.500882148742676 LOSS_Discriminator: 0.06603633612394333\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 5.662529468536377 LOSS_Discriminator: 0.11252489686012268\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 5.9819416999816895 LOSS_Discriminator: 0.24165815114974976\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 5.629778861999512 LOSS_Discriminator: 0.06549034267663956\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 5.045189380645752 LOSS_Discriminator: 0.0857202559709549\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 4.811771392822266 LOSS_Discriminator: 0.12662874162197113\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 5.206062316894531 LOSS_Discriminator: 0.034118425101041794\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 4.586250305175781 LOSS_Discriminator: 0.1149822324514389\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 5.326743125915527 LOSS_Discriminator: 0.09362592548131943\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 5.478207588195801 LOSS_Discriminator: 0.08290442824363708\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 4.783919334411621 LOSS_Discriminator: 0.040742166340351105\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 5.775532245635986 LOSS_Discriminator: 0.05394478142261505\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 5.906331539154053 LOSS_Discriminator: 0.14004524052143097\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 5.977970123291016 LOSS_Discriminator: 0.14627282321453094\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 5.838935852050781 LOSS_Discriminator: 0.10828829556703568\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 5.608255386352539 LOSS_Discriminator: 0.16704463958740234\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 4.9508514404296875 LOSS_Discriminator: 0.10039574652910233\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 4.629502296447754 LOSS_Discriminator: 0.09465059638023376\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 5.786654472351074 LOSS_Discriminator: 0.07114709913730621\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 5.677854537963867 LOSS_Discriminator: 0.16892872750759125\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 6.2546257972717285 LOSS_Discriminator: 0.1166786253452301\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 5.251874923706055 LOSS_Discriminator: 0.1082039475440979\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 4.800259590148926 LOSS_Discriminator: 0.1088935136795044\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 4.364667892456055 LOSS_Discriminator: 0.13343623280525208\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 4.212802886962891 LOSS_Discriminator: 0.09130341559648514\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 5.0202484130859375 LOSS_Discriminator: 0.16358783841133118\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 4.201632022857666 LOSS_Discriminator: 0.1437099426984787\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 5.212392807006836 LOSS_Discriminator: 0.0720202699303627\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 5.3953447341918945 LOSS_Discriminator: 0.08284550160169601\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 4.855622291564941 LOSS_Discriminator: 0.2870529890060425\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 5.552920341491699 LOSS_Discriminator: 0.05188147723674774\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 5.101016998291016 LOSS_Discriminator: 0.15040868520736694\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 5.4763336181640625 LOSS_Discriminator: 0.040378667414188385\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 5.721071243286133 LOSS_Discriminator: 0.16538837552070618\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 5.499098777770996 LOSS_Discriminator: 0.09191881120204926\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 5.773094177246094 LOSS_Discriminator: 0.0871656984090805\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 6.072534561157227 LOSS_Discriminator: 0.20516487956047058\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 5.075810432434082 LOSS_Discriminator: 0.13481569290161133\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 4.676540374755859 LOSS_Discriminator: 0.1378396451473236\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 4.727520942687988 LOSS_Discriminator: 0.27156487107276917\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 4.917718887329102 LOSS_Discriminator: 0.1561584770679474\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 6.329342842102051 LOSS_Discriminator: 0.09245924651622772\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 6.1072845458984375 LOSS_Discriminator: 0.1166643500328064\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 5.995217323303223 LOSS_Discriminator: 0.010856986977159977\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 6.093916893005371 LOSS_Discriminator: 0.15550407767295837\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 6.173027038574219 LOSS_Discriminator: 0.013513945043087006\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 5.967552185058594 LOSS_Discriminator: 0.16260427236557007\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 5.380645275115967 LOSS_Discriminator: 0.13411790132522583\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 6.325279235839844 LOSS_Discriminator: 0.13521426916122437\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 6.16075325012207 LOSS_Discriminator: 0.11676375567913055\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 6.063150405883789 LOSS_Discriminator: 0.2628943622112274\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 6.007767677307129 LOSS_Discriminator: 0.10851873457431793\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 5.632884979248047 LOSS_Discriminator: 0.09137630462646484\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 5.836371421813965 LOSS_Discriminator: 0.07688538730144501\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 5.133989334106445 LOSS_Discriminator: 0.31188133358955383\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 4.975821495056152 LOSS_Discriminator: 0.30421411991119385\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 4.662595748901367 LOSS_Discriminator: 0.1152617484331131\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 4.611166000366211 LOSS_Discriminator: 0.10631833970546722\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 5.070171356201172 LOSS_Discriminator: 0.11883256584405899\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 5.309930801391602 LOSS_Discriminator: 0.05446115881204605\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 6.595950126647949 LOSS_Discriminator: 0.12249782681465149\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 6.395316123962402 LOSS_Discriminator: 0.31950125098228455\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 6.072020530700684 LOSS_Discriminator: 0.020759034901857376\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 5.266386985778809 LOSS_Discriminator: 0.22720405459403992\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 4.1118268966674805 LOSS_Discriminator: 0.1649332344532013\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 4.218900680541992 LOSS_Discriminator: 0.08353130519390106\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 3.9319257736206055 LOSS_Discriminator: 0.1309971958398819\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 4.526987552642822 LOSS_Discriminator: 0.14871656894683838\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 5.348079681396484 LOSS_Discriminator: 0.14970141649246216\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 5.823836326599121 LOSS_Discriminator: 0.1587386131286621\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 5.471278190612793 LOSS_Discriminator: 0.03723972663283348\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 6.944728851318359 LOSS_Discriminator: 0.06746042519807816\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 5.876653671264648 LOSS_Discriminator: 0.32002246379852295\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 5.466488838195801 LOSS_Discriminator: 0.101960688829422\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 4.858643531799316 LOSS_Discriminator: 0.18552535772323608\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.2102251052856445 LOSS_Discriminator: 0.15737095475196838\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 4.056002140045166 LOSS_Discriminator: 0.12773016095161438\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 5.702907562255859 LOSS_Discriminator: 0.1264258176088333\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 6.368833541870117 LOSS_Discriminator: 0.09439016878604889\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 6.622646331787109 LOSS_Discriminator: 0.16788963973522186\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 5.947645664215088 LOSS_Discriminator: 0.2013588696718216\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 5.768733501434326 LOSS_Discriminator: 0.11008399724960327\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 6.051844120025635 LOSS_Discriminator: 0.22624343633651733\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 5.505467414855957 LOSS_Discriminator: 0.17460718750953674\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 5.858768463134766 LOSS_Discriminator: 0.04323907941579819\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 5.938259601593018 LOSS_Discriminator: 0.16215085983276367\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 5.463309288024902 LOSS_Discriminator: 0.136650949716568\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 5.0922956466674805 LOSS_Discriminator: 0.0352177657186985\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 5.0982890129089355 LOSS_Discriminator: 0.10204298794269562\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 4.888047695159912 LOSS_Discriminator: 0.17483928799629211\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 4.476881980895996 LOSS_Discriminator: 0.17215320467948914\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 5.228414535522461 LOSS_Discriminator: 0.0596773624420166\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 5.708318710327148 LOSS_Discriminator: 0.2148849368095398\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 5.481677055358887 LOSS_Discriminator: 0.26906323432922363\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 5.976644515991211 LOSS_Discriminator: 0.13094168901443481\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 4.966326713562012 LOSS_Discriminator: 0.25620144605636597\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 4.668463230133057 LOSS_Discriminator: 0.23281675577163696\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 3.3600540161132812 LOSS_Discriminator: 0.15319818258285522\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 4.803928375244141 LOSS_Discriminator: 0.07218071818351746\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 4.190595626831055 LOSS_Discriminator: 0.2163545936346054\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 5.04014778137207 LOSS_Discriminator: 0.10054203122854233\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 5.451162338256836 LOSS_Discriminator: 0.39452141523361206\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 6.125158786773682 LOSS_Discriminator: 0.24235562980175018\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 5.801909923553467 LOSS_Discriminator: 0.07941651344299316\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 5.679365634918213 LOSS_Discriminator: 0.19279101490974426\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 4.796013355255127 LOSS_Discriminator: 0.4008760154247284\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 3.786539316177368 LOSS_Discriminator: 0.23499982059001923\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 3.0944020748138428 LOSS_Discriminator: 0.11043642461299896\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 3.5998172760009766 LOSS_Discriminator: 0.2512047588825226\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 4.577548027038574 LOSS_Discriminator: 0.21861982345581055\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 4.966856479644775 LOSS_Discriminator: 0.20365336537361145\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 5.264364242553711 LOSS_Discriminator: 0.09767930209636688\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 5.499844551086426 LOSS_Discriminator: 0.13887782394886017\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 5.949227333068848 LOSS_Discriminator: 0.11530110239982605\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 5.174918174743652 LOSS_Discriminator: 0.1450449377298355\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 4.515296459197998 LOSS_Discriminator: 0.13586288690567017\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 4.719648361206055 LOSS_Discriminator: 0.3141268193721771\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 4.5985918045043945 LOSS_Discriminator: 0.043397367000579834\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 4.548994541168213 LOSS_Discriminator: 0.05714165046811104\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 4.346478462219238 LOSS_Discriminator: 0.11412312835454941\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 4.277699947357178 LOSS_Discriminator: 0.11100359261035919\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 4.429536819458008 LOSS_Discriminator: 0.1844058781862259\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 4.983151435852051 LOSS_Discriminator: 0.11434269696474075\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 5.320803642272949 LOSS_Discriminator: 0.1742066740989685\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 4.817282676696777 LOSS_Discriminator: 0.18047678470611572\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 4.7417192459106445 LOSS_Discriminator: 0.2571890950202942\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 4.770667552947998 LOSS_Discriminator: 0.07292038202285767\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 4.213257789611816 LOSS_Discriminator: 0.19144979119300842\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 4.542055606842041 LOSS_Discriminator: 0.07810670137405396\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 4.345478534698486 LOSS_Discriminator: 0.05415687710046768\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 5.103338241577148 LOSS_Discriminator: 0.19068235158920288\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 4.833866119384766 LOSS_Discriminator: 0.05109403282403946\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 4.898837089538574 LOSS_Discriminator: 0.04404015839099884\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 5.327426910400391 LOSS_Discriminator: 0.0767185389995575\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 5.54890251159668 LOSS_Discriminator: 0.04965828359127045\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 5.626170635223389 LOSS_Discriminator: 0.1511228382587433\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 5.005075931549072 LOSS_Discriminator: 0.05990664288401604\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 4.493015766143799 LOSS_Discriminator: 0.0843169093132019\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 3.872347831726074 LOSS_Discriminator: 0.2095438838005066\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 3.588968515396118 LOSS_Discriminator: 0.21518345177173615\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 3.959472417831421 LOSS_Discriminator: 0.09504330158233643\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 4.835621356964111 LOSS_Discriminator: 0.21133893728256226\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 4.803254127502441 LOSS_Discriminator: 0.12482713907957077\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 5.2105560302734375 LOSS_Discriminator: 0.19015343487262726\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 4.668694496154785 LOSS_Discriminator: 0.09551767259836197\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 4.7985944747924805 LOSS_Discriminator: 0.07109634578227997\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 4.215947151184082 LOSS_Discriminator: 0.1933547556400299\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 4.517132759094238 LOSS_Discriminator: 0.07128932327032089\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 4.577761650085449 LOSS_Discriminator: 0.0768207311630249\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 4.71699857711792 LOSS_Discriminator: 0.046054407954216\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 5.539154529571533 LOSS_Discriminator: 0.04047653451561928\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 5.8818888664245605 LOSS_Discriminator: 0.04643648862838745\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 5.717037200927734 LOSS_Discriminator: 0.1658441126346588\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 5.695183753967285 LOSS_Discriminator: 0.06292840093374252\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 5.554269790649414 LOSS_Discriminator: 0.023542363196611404\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 5.662251949310303 LOSS_Discriminator: 0.024157974869012833\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 5.4045491218566895 LOSS_Discriminator: 0.051226817071437836\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 5.445431709289551 LOSS_Discriminator: 0.17079193890094757\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 5.3255205154418945 LOSS_Discriminator: 0.04928218945860863\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 5.670470237731934 LOSS_Discriminator: 0.10640525817871094\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 4.883590221405029 LOSS_Discriminator: 0.3065725564956665\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 3.8228759765625 LOSS_Discriminator: 0.13868829607963562\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 3.4409475326538086 LOSS_Discriminator: 0.16764713823795319\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 3.5145838260650635 LOSS_Discriminator: 0.1983615607023239\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 4.243105888366699 LOSS_Discriminator: 0.2026582956314087\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 4.741877555847168 LOSS_Discriminator: 0.16351597011089325\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 4.441079139709473 LOSS_Discriminator: 0.07386907935142517\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 5.599766731262207 LOSS_Discriminator: 0.22161760926246643\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 4.585500240325928 LOSS_Discriminator: 0.22844567894935608\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 4.1580023765563965 LOSS_Discriminator: 0.05672780051827431\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 4.451427459716797 LOSS_Discriminator: 0.11164772510528564\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 4.45358943939209 LOSS_Discriminator: 0.07678064703941345\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 4.028237342834473 LOSS_Discriminator: 0.26457375288009644\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 4.341913223266602 LOSS_Discriminator: 0.24606108665466309\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 4.602710247039795 LOSS_Discriminator: 0.051673803478479385\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 4.582914352416992 LOSS_Discriminator: 0.153458833694458\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 4.828863620758057 LOSS_Discriminator: 0.2169448435306549\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 4.951216697692871 LOSS_Discriminator: 0.08668900281190872\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 5.248832702636719 LOSS_Discriminator: 0.12698525190353394\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 5.362078666687012 LOSS_Discriminator: 0.11968239396810532\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 4.774889945983887 LOSS_Discriminator: 0.2781357169151306\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 3.75941801071167 LOSS_Discriminator: 0.09645102918148041\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 3.783350944519043 LOSS_Discriminator: 0.16134430468082428\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 4.385433197021484 LOSS_Discriminator: 0.16840624809265137\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 6.220901012420654 LOSS_Discriminator: 0.057455889880657196\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 6.1746416091918945 LOSS_Discriminator: 0.01031702570617199\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 7.028487205505371 LOSS_Discriminator: 0.052026160061359406\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 7.701082229614258 LOSS_Discriminator: 0.0746876522898674\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 7.408443450927734 LOSS_Discriminator: 0.2885644733905792\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 6.781319618225098 LOSS_Discriminator: 0.0018102137837558985\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 5.980519771575928 LOSS_Discriminator: 0.19188152253627777\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 5.143387794494629 LOSS_Discriminator: 0.01878909207880497\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 4.2018866539001465 LOSS_Discriminator: 0.08521867543458939\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 4.878147602081299 LOSS_Discriminator: 0.04169980436563492\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 3.4269890785217285 LOSS_Discriminator: 0.06435135751962662\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 3.8264830112457275 LOSS_Discriminator: 0.2609724998474121\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 4.716922283172607 LOSS_Discriminator: 0.12444029748439789\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 5.108026504516602 LOSS_Discriminator: 0.09059721976518631\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 5.750173568725586 LOSS_Discriminator: 0.11500918865203857\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 5.748682975769043 LOSS_Discriminator: 0.13320142030715942\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 5.4838762283325195 LOSS_Discriminator: 0.1397332400083542\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 5.308816909790039 LOSS_Discriminator: 0.105768583714962\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 5.261597633361816 LOSS_Discriminator: 0.10583718121051788\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 4.928310394287109 LOSS_Discriminator: 0.08077632635831833\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 4.809783458709717 LOSS_Discriminator: 0.0749327689409256\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 4.100027084350586 LOSS_Discriminator: 0.08936658501625061\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 4.666824817657471 LOSS_Discriminator: 0.12448644638061523\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 4.931612014770508 LOSS_Discriminator: 0.13031598925590515\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 5.336363792419434 LOSS_Discriminator: 0.03050333261489868\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 5.811220169067383 LOSS_Discriminator: 0.09210358560085297\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 5.846147537231445 LOSS_Discriminator: 0.012708095833659172\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 6.384716033935547 LOSS_Discriminator: 0.09914571046829224\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 6.0522027015686035 LOSS_Discriminator: 0.18845704197883606\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 5.28947639465332 LOSS_Discriminator: 0.21886570751667023\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 4.919704437255859 LOSS_Discriminator: 0.04405004158616066\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 4.564944267272949 LOSS_Discriminator: 0.08230255544185638\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 4.705216407775879 LOSS_Discriminator: 0.16028857231140137\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 4.521566390991211 LOSS_Discriminator: 0.1271335631608963\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 4.786435604095459 LOSS_Discriminator: 0.0370648019015789\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 5.6554856300354 LOSS_Discriminator: 0.07598401606082916\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 5.280540466308594 LOSS_Discriminator: 0.24435225129127502\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 6.059842109680176 LOSS_Discriminator: 0.12020346522331238\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 5.901847839355469 LOSS_Discriminator: 0.1100805476307869\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 5.48680305480957 LOSS_Discriminator: 0.10214214026927948\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 5.09881591796875 LOSS_Discriminator: 0.16828569769859314\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 4.14531946182251 LOSS_Discriminator: 0.11624753475189209\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 4.027270317077637 LOSS_Discriminator: 0.2407316118478775\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 3.471109628677368 LOSS_Discriminator: 0.1451483815908432\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 3.8413989543914795 LOSS_Discriminator: 0.1335420161485672\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 4.616916656494141 LOSS_Discriminator: 0.05831485986709595\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 5.454885482788086 LOSS_Discriminator: 0.18400749564170837\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 5.151008605957031 LOSS_Discriminator: 0.08841902017593384\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 5.688141822814941 LOSS_Discriminator: 0.1044493168592453\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 5.4647955894470215 LOSS_Discriminator: 0.05112464725971222\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 4.891356468200684 LOSS_Discriminator: 0.19923962652683258\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 4.515769004821777 LOSS_Discriminator: 0.06450193375349045\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 4.742241859436035 LOSS_Discriminator: 0.10032139718532562\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 4.641708850860596 LOSS_Discriminator: 0.12685732543468475\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 4.642498016357422 LOSS_Discriminator: 0.16805699467658997\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 5.206690788269043 LOSS_Discriminator: 0.08367526531219482\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 5.698907375335693 LOSS_Discriminator: 0.3322925865650177\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 6.136160850524902 LOSS_Discriminator: 0.06566736102104187\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 5.43697452545166 LOSS_Discriminator: 0.08754421770572662\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 5.518908500671387 LOSS_Discriminator: 0.19090095162391663\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 4.372595310211182 LOSS_Discriminator: 0.14256277680397034\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 4.275540828704834 LOSS_Discriminator: 0.0739869475364685\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 3.986088991165161 LOSS_Discriminator: 0.13290181756019592\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 3.9439663887023926 LOSS_Discriminator: 0.1277911514043808\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 3.5333516597747803 LOSS_Discriminator: 0.19390296936035156\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 4.429318428039551 LOSS_Discriminator: 0.12088902294635773\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 4.505239009857178 LOSS_Discriminator: 0.12079402804374695\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 4.855848789215088 LOSS_Discriminator: 0.12545925378799438\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 5.312283992767334 LOSS_Discriminator: 0.038541264832019806\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 5.503941535949707 LOSS_Discriminator: 0.03350697085261345\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 6.1307692527771 LOSS_Discriminator: 0.06673571467399597\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 6.481618404388428 LOSS_Discriminator: 0.21567624807357788\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 5.715110778808594 LOSS_Discriminator: 0.38830840587615967\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 5.012111663818359 LOSS_Discriminator: 0.4519939720630646\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 3.6589791774749756 LOSS_Discriminator: 0.06731586158275604\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 3.267716407775879 LOSS_Discriminator: 0.16923436522483826\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 4.02916145324707 LOSS_Discriminator: 0.17941054701805115\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 4.7057342529296875 LOSS_Discriminator: 0.11974100768566132\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 5.826859474182129 LOSS_Discriminator: 0.252606600522995\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 6.106840133666992 LOSS_Discriminator: 0.25789105892181396\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 5.383140563964844 LOSS_Discriminator: 0.07624281197786331\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 5.454623699188232 LOSS_Discriminator: 0.20860567688941956\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 5.395659446716309 LOSS_Discriminator: 0.1100122258067131\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 4.21466588973999 LOSS_Discriminator: 0.12608665227890015\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 4.368437767028809 LOSS_Discriminator: 0.06247728690505028\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 3.93148136138916 LOSS_Discriminator: 0.11627262830734253\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 4.511579513549805 LOSS_Discriminator: 0.10310886055231094\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 4.508810043334961 LOSS_Discriminator: 0.04348060488700867\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 5.34657096862793 LOSS_Discriminator: 0.07266022264957428\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 5.063276290893555 LOSS_Discriminator: 0.09742698073387146\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 5.426409721374512 LOSS_Discriminator: 0.3429688811302185\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 5.5048136711120605 LOSS_Discriminator: 0.18287092447280884\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 4.699432849884033 LOSS_Discriminator: 0.03778049349784851\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 4.475345134735107 LOSS_Discriminator: 0.0455222986638546\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 4.424276828765869 LOSS_Discriminator: 0.019992291927337646\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 4.645606517791748 LOSS_Discriminator: 0.14739035069942474\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 4.9770121574401855 LOSS_Discriminator: 0.10105208307504654\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 4.810091018676758 LOSS_Discriminator: 0.1047612726688385\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 4.5041279792785645 LOSS_Discriminator: 0.19290244579315186\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 3.9043939113616943 LOSS_Discriminator: 0.1314547210931778\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 4.530608177185059 LOSS_Discriminator: 0.057852603495121\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 4.4043378829956055 LOSS_Discriminator: 0.18187588453292847\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.137087821960449 LOSS_Discriminator: 0.06685156375169754\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 5.425779819488525 LOSS_Discriminator: 0.09139901399612427\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 5.51193904876709 LOSS_Discriminator: 0.02420732006430626\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 5.64729642868042 LOSS_Discriminator: 0.3763931393623352\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 5.240947723388672 LOSS_Discriminator: 0.058807019144296646\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 5.132227420806885 LOSS_Discriminator: 0.0826297253370285\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 4.920633792877197 LOSS_Discriminator: 0.07833768427371979\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 4.318599700927734 LOSS_Discriminator: 0.14847055077552795\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 4.235718250274658 LOSS_Discriminator: 0.13719311356544495\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 4.884538650512695 LOSS_Discriminator: 0.1154913529753685\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 4.922359466552734 LOSS_Discriminator: 0.19046004116535187\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 4.461865425109863 LOSS_Discriminator: 0.14538675546646118\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 4.914137840270996 LOSS_Discriminator: 0.04733379930257797\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 4.5675835609436035 LOSS_Discriminator: 0.2640571594238281\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 4.290186405181885 LOSS_Discriminator: 0.19295483827590942\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 3.5843095779418945 LOSS_Discriminator: 0.1201242133975029\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 3.0878779888153076 LOSS_Discriminator: 0.14970114827156067\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 4.11422872543335 LOSS_Discriminator: 0.1620505452156067\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.598539352416992 LOSS_Discriminator: 0.24025163054466248\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 4.6363983154296875 LOSS_Discriminator: 0.0742950588464737\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 5.313895225524902 LOSS_Discriminator: 0.14715519547462463\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 5.386975288391113 LOSS_Discriminator: 0.05186353623867035\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 5.434878826141357 LOSS_Discriminator: 0.11949851363897324\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 5.850289821624756 LOSS_Discriminator: 0.03301551192998886\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 5.606266975402832 LOSS_Discriminator: 0.09183333069086075\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 5.433399200439453 LOSS_Discriminator: 0.24811169505119324\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 5.247162818908691 LOSS_Discriminator: 0.07319982349872589\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 4.804325103759766 LOSS_Discriminator: 0.17949023842811584\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 4.266040802001953 LOSS_Discriminator: 0.10582542419433594\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 4.679150104522705 LOSS_Discriminator: 0.09751522541046143\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 5.093285083770752 LOSS_Discriminator: 0.18082717061042786\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 5.429043769836426 LOSS_Discriminator: 0.18872427940368652\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 5.510985851287842 LOSS_Discriminator: 0.10103069245815277\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 4.808441638946533 LOSS_Discriminator: 0.14310415089130402\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 4.725655555725098 LOSS_Discriminator: 0.11596551537513733\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 5.225334167480469 LOSS_Discriminator: 0.12679649889469147\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 4.966255187988281 LOSS_Discriminator: 0.1481247842311859\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 4.995261192321777 LOSS_Discriminator: 0.18515589833259583\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 4.613776206970215 LOSS_Discriminator: 0.08319585025310516\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 5.600721836090088 LOSS_Discriminator: 0.08526219427585602\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 5.393928050994873 LOSS_Discriminator: 0.07038703560829163\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 5.7624640464782715 LOSS_Discriminator: 0.10267888009548187\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 5.033577919006348 LOSS_Discriminator: 0.08446958661079407\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 4.901968002319336 LOSS_Discriminator: 0.022502094507217407\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 4.460740089416504 LOSS_Discriminator: 0.35411009192466736\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 3.7087624073028564 LOSS_Discriminator: 0.11547748744487762\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 4.061717987060547 LOSS_Discriminator: 0.11748087406158447\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 4.979259490966797 LOSS_Discriminator: 0.1389542818069458\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 4.4687604904174805 LOSS_Discriminator: 0.218146413564682\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 4.71209716796875 LOSS_Discriminator: 0.10506574809551239\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 4.7161970138549805 LOSS_Discriminator: 0.14752663671970367\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 4.287037372589111 LOSS_Discriminator: 0.045511335134506226\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 5.066154479980469 LOSS_Discriminator: 0.10172773897647858\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 5.397791862487793 LOSS_Discriminator: 0.13792943954467773\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 5.370527267456055 LOSS_Discriminator: 0.1045408844947815\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 5.250666618347168 LOSS_Discriminator: 0.047467201948165894\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 5.458763599395752 LOSS_Discriminator: 0.10088002681732178\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 5.582180023193359 LOSS_Discriminator: 0.023724470287561417\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 5.132384300231934 LOSS_Discriminator: 0.11838938295841217\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 4.913697242736816 LOSS_Discriminator: 0.11294673383235931\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 4.597470760345459 LOSS_Discriminator: 0.034578487277030945\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 5.011270046234131 LOSS_Discriminator: 0.047981277108192444\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 4.754383563995361 LOSS_Discriminator: 0.05290401726961136\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 4.612479209899902 LOSS_Discriminator: 0.023361094295978546\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 5.327912330627441 LOSS_Discriminator: 0.09840596467256546\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 4.900664329528809 LOSS_Discriminator: 0.04649880528450012\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 4.78066873550415 LOSS_Discriminator: 0.10191038995981216\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 4.805964469909668 LOSS_Discriminator: 0.05519482493400574\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 4.233189582824707 LOSS_Discriminator: 0.21945060789585114\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 5.13906192779541 LOSS_Discriminator: 0.07812351733446121\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 4.632288932800293 LOSS_Discriminator: 0.08154173195362091\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 4.937577724456787 LOSS_Discriminator: 0.10197832435369492\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 4.914947032928467 LOSS_Discriminator: 0.09893450886011124\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 4.953877925872803 LOSS_Discriminator: 0.11943116784095764\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 5.301048278808594 LOSS_Discriminator: 0.13332045078277588\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 5.967679500579834 LOSS_Discriminator: 0.1842116415500641\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 4.175981521606445 LOSS_Discriminator: 0.14773520827293396\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 5.021524429321289 LOSS_Discriminator: 0.10892893373966217\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 6.827968597412109 LOSS_Discriminator: 0.26198697090148926\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 5.652554512023926 LOSS_Discriminator: 0.19348812103271484\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 6.426122665405273 LOSS_Discriminator: 0.2110195755958557\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 5.483791351318359 LOSS_Discriminator: 0.12924668192863464\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 5.61873722076416 LOSS_Discriminator: 0.11549854278564453\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 4.7757368087768555 LOSS_Discriminator: 0.23346073925495148\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 4.751967906951904 LOSS_Discriminator: 0.14240989089012146\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 4.7985310554504395 LOSS_Discriminator: 0.1630547195672989\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 3.9339241981506348 LOSS_Discriminator: 0.15338391065597534\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 4.684959411621094 LOSS_Discriminator: 0.13744232058525085\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 5.367730140686035 LOSS_Discriminator: 0.11405722051858902\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 5.037293434143066 LOSS_Discriminator: 0.1917564421892166\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 5.216488838195801 LOSS_Discriminator: 0.12031431496143341\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 5.381436824798584 LOSS_Discriminator: 0.08814875036478043\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 5.076107978820801 LOSS_Discriminator: 0.12219667434692383\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 4.190082550048828 LOSS_Discriminator: 0.10236027836799622\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 3.8266894817352295 LOSS_Discriminator: 0.14949403703212738\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 3.9838638305664062 LOSS_Discriminator: 0.1276865005493164\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 4.078452110290527 LOSS_Discriminator: 0.14862006902694702\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 5.337378978729248 LOSS_Discriminator: 0.11788775026798248\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 6.059216499328613 LOSS_Discriminator: 0.14739573001861572\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 6.488424777984619 LOSS_Discriminator: 0.22881874442100525\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 6.4250078201293945 LOSS_Discriminator: 0.011832435615360737\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 6.5640363693237305 LOSS_Discriminator: 0.015914790332317352\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 6.080020904541016 LOSS_Discriminator: 0.009700547903776169\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 6.113597869873047 LOSS_Discriminator: 0.3286919593811035\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 5.517880916595459 LOSS_Discriminator: 0.20180942118167877\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 4.724781513214111 LOSS_Discriminator: 0.12088343501091003\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 4.085249423980713 LOSS_Discriminator: 0.14504314959049225\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 3.833376884460449 LOSS_Discriminator: 0.20227086544036865\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 3.7622146606445312 LOSS_Discriminator: 0.1739879846572876\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 3.61202335357666 LOSS_Discriminator: 0.30159658193588257\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 4.942198276519775 LOSS_Discriminator: 0.09849459677934647\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 4.778964996337891 LOSS_Discriminator: 0.060145676136016846\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 5.678881645202637 LOSS_Discriminator: 0.025320081040263176\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 5.691872596740723 LOSS_Discriminator: 0.27310848236083984\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 5.608946800231934 LOSS_Discriminator: 0.29606157541275024\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 5.289241790771484 LOSS_Discriminator: 0.3194371461868286\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 3.826516628265381 LOSS_Discriminator: 0.2668384909629822\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 3.6786880493164062 LOSS_Discriminator: 0.1750173270702362\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 3.7785751819610596 LOSS_Discriminator: 0.3184915781021118\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 4.782840728759766 LOSS_Discriminator: 0.17571492493152618\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 4.7046613693237305 LOSS_Discriminator: 0.07607437670230865\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 5.507545471191406 LOSS_Discriminator: 0.3022448420524597\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 5.434715270996094 LOSS_Discriminator: 0.2334340661764145\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 5.906150817871094 LOSS_Discriminator: 0.1295245736837387\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 4.924891471862793 LOSS_Discriminator: 0.07413019239902496\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 4.591758728027344 LOSS_Discriminator: 0.13487175107002258\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 4.541847229003906 LOSS_Discriminator: 0.053319331258535385\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 4.21045446395874 LOSS_Discriminator: 0.2565573453903198\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 4.682648658752441 LOSS_Discriminator: 0.1881973147392273\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 4.604059219360352 LOSS_Discriminator: 0.41164565086364746\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 4.269933700561523 LOSS_Discriminator: 0.14536744356155396\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 4.371303558349609 LOSS_Discriminator: 0.18796977400779724\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 4.340153694152832 LOSS_Discriminator: 0.10247574746608734\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 4.627157211303711 LOSS_Discriminator: 0.06189969182014465\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 4.978592395782471 LOSS_Discriminator: 0.18974202871322632\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 5.007245063781738 LOSS_Discriminator: 0.06062065437436104\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 5.147853851318359 LOSS_Discriminator: 0.2156035304069519\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 4.987261772155762 LOSS_Discriminator: 0.04330600053071976\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 4.205532073974609 LOSS_Discriminator: 0.14141318202018738\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 3.9711551666259766 LOSS_Discriminator: 0.10985229909420013\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 4.349836349487305 LOSS_Discriminator: 0.03476211428642273\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 4.690601348876953 LOSS_Discriminator: 0.05630768835544586\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 4.571054935455322 LOSS_Discriminator: 0.08498173952102661\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 4.934200286865234 LOSS_Discriminator: 0.08165350556373596\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 5.657333850860596 LOSS_Discriminator: 0.05193419009447098\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 4.83499813079834 LOSS_Discriminator: 0.08775415271520615\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 5.344574928283691 LOSS_Discriminator: 0.19204798340797424\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 4.917287349700928 LOSS_Discriminator: 0.1629866659641266\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 4.390321731567383 LOSS_Discriminator: 0.036683328449726105\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 4.669096946716309 LOSS_Discriminator: 0.060801561921834946\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 4.477869987487793 LOSS_Discriminator: 0.10428960621356964\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 5.546689033508301 LOSS_Discriminator: 0.08225499093532562\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 5.556580543518066 LOSS_Discriminator: 0.2338332235813141\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 5.527692794799805 LOSS_Discriminator: 0.03253047168254852\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 5.289622783660889 LOSS_Discriminator: 0.08575253188610077\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 5.1794915199279785 LOSS_Discriminator: 0.09461164474487305\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 5.385542392730713 LOSS_Discriminator: 0.17142066359519958\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.738213539123535 LOSS_Discriminator: 0.05861934274435043\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 5.364468574523926 LOSS_Discriminator: 0.26315680146217346\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 4.859971046447754 LOSS_Discriminator: 0.0848931223154068\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 4.036249160766602 LOSS_Discriminator: 0.14753088355064392\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 3.736070156097412 LOSS_Discriminator: 0.15657593309879303\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 3.9275169372558594 LOSS_Discriminator: 0.06076112389564514\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 4.976960182189941 LOSS_Discriminator: 0.10801053792238235\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 5.381109714508057 LOSS_Discriminator: 0.11051572859287262\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 5.708819389343262 LOSS_Discriminator: 0.11362192034721375\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 5.980842590332031 LOSS_Discriminator: 0.04488993436098099\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 6.079159259796143 LOSS_Discriminator: 0.2235606610774994\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 5.633945465087891 LOSS_Discriminator: 0.05511860176920891\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 5.769231796264648 LOSS_Discriminator: 0.0740133747458458\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 4.884283065795898 LOSS_Discriminator: 0.1694081723690033\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 3.831110715866089 LOSS_Discriminator: 0.2791208028793335\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 3.3668627738952637 LOSS_Discriminator: 0.296337753534317\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 3.5703930854797363 LOSS_Discriminator: 0.2431814819574356\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 4.616684913635254 LOSS_Discriminator: 0.23742184042930603\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 6.1216583251953125 LOSS_Discriminator: 0.12442800402641296\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 7.407487869262695 LOSS_Discriminator: 0.2771078944206238\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 7.277606010437012 LOSS_Discriminator: 0.13235658407211304\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 6.305498123168945 LOSS_Discriminator: 0.425134539604187\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 6.2339372634887695 LOSS_Discriminator: 0.26785174012184143\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 4.534965515136719 LOSS_Discriminator: 0.29070550203323364\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 3.3477530479431152 LOSS_Discriminator: 0.27311983704566956\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 3.5551631450653076 LOSS_Discriminator: 0.16733318567276\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 4.209540367126465 LOSS_Discriminator: 0.1591191589832306\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 4.26721715927124 LOSS_Discriminator: 0.13042865693569183\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 5.700863838195801 LOSS_Discriminator: 0.032421138137578964\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 5.973762512207031 LOSS_Discriminator: 0.08305123448371887\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 5.739188194274902 LOSS_Discriminator: 0.2769777774810791\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 6.273616790771484 LOSS_Discriminator: 0.21004216372966766\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 6.029417514801025 LOSS_Discriminator: 0.030015993863344193\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 5.418034076690674 LOSS_Discriminator: 0.13625222444534302\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 5.59564733505249 LOSS_Discriminator: 0.08757144957780838\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 5.474190711975098 LOSS_Discriminator: 0.18537725508213043\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 5.174625396728516 LOSS_Discriminator: 0.10849679261445999\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 4.669958114624023 LOSS_Discriminator: 0.07006792724132538\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 5.042024612426758 LOSS_Discriminator: 0.08367689698934555\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 4.895077705383301 LOSS_Discriminator: 0.24151654541492462\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 4.857182502746582 LOSS_Discriminator: 0.0654883086681366\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 4.822358131408691 LOSS_Discriminator: 0.04637958109378815\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 4.705095291137695 LOSS_Discriminator: 0.11180773377418518\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 4.499909400939941 LOSS_Discriminator: 0.216455340385437\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 4.625336647033691 LOSS_Discriminator: 0.1757659912109375\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 4.1790008544921875 LOSS_Discriminator: 0.09353742748498917\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 3.710226058959961 LOSS_Discriminator: 0.13027144968509674\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 3.5233049392700195 LOSS_Discriminator: 0.06220225989818573\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 3.7662317752838135 LOSS_Discriminator: 0.18276271224021912\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 4.68765926361084 LOSS_Discriminator: 0.09905321896076202\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 4.149471282958984 LOSS_Discriminator: 0.0575844869017601\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 4.855329990386963 LOSS_Discriminator: 0.19529196619987488\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 4.398957252502441 LOSS_Discriminator: 0.1027754545211792\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 4.693800449371338 LOSS_Discriminator: 0.14352451264858246\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 5.18199348449707 LOSS_Discriminator: 0.1404426097869873\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 5.156044960021973 LOSS_Discriminator: 0.028864923864603043\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 5.2210693359375 LOSS_Discriminator: 0.096138134598732\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 5.56920862197876 LOSS_Discriminator: 0.051808781921863556\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 5.495192527770996 LOSS_Discriminator: 0.11287131160497665\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 5.49110746383667 LOSS_Discriminator: 0.04955410212278366\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 5.830500602722168 LOSS_Discriminator: 0.18581362068653107\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 5.322551727294922 LOSS_Discriminator: 0.14214439690113068\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 4.978132724761963 LOSS_Discriminator: 0.21765203773975372\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 3.9071927070617676 LOSS_Discriminator: 0.08417339622974396\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 5.0957417488098145 LOSS_Discriminator: 0.16590330004692078\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 4.6849045753479 LOSS_Discriminator: 0.0715654194355011\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 4.854719161987305 LOSS_Discriminator: 0.08143575489521027\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 5.822966575622559 LOSS_Discriminator: 0.15788456797599792\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 5.838837623596191 LOSS_Discriminator: 0.21468906104564667\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 6.098635673522949 LOSS_Discriminator: 0.06724917143583298\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 5.122920989990234 LOSS_Discriminator: 0.158197820186615\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 4.587500095367432 LOSS_Discriminator: 0.1377171277999878\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 4.779881000518799 LOSS_Discriminator: 0.08916281908750534\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 4.914267539978027 LOSS_Discriminator: 0.21079868078231812\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 4.320842742919922 LOSS_Discriminator: 0.17792057991027832\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 4.402142524719238 LOSS_Discriminator: 0.16870436072349548\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 4.593265533447266 LOSS_Discriminator: 0.03463967517018318\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 5.095003128051758 LOSS_Discriminator: 0.19913485646247864\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 4.7433013916015625 LOSS_Discriminator: 0.17075300216674805\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 5.5531206130981445 LOSS_Discriminator: 0.14076471328735352\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 5.373537540435791 LOSS_Discriminator: 0.26065367460250854\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 5.125370502471924 LOSS_Discriminator: 0.07133310288190842\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 5.14434289932251 LOSS_Discriminator: 0.15563616156578064\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 4.807164192199707 LOSS_Discriminator: 0.16587191820144653\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 3.988326072692871 LOSS_Discriminator: 0.15714873373508453\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 4.103245735168457 LOSS_Discriminator: 0.04481919854879379\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 3.850728988647461 LOSS_Discriminator: 0.13189682364463806\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 4.15043830871582 LOSS_Discriminator: 0.13936978578567505\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 4.81866455078125 LOSS_Discriminator: 0.08184607326984406\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 5.353138446807861 LOSS_Discriminator: 0.05032027140259743\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 5.4133477210998535 LOSS_Discriminator: 0.10999677330255508\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 5.746437072753906 LOSS_Discriminator: 0.08827660977840424\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 5.592360496520996 LOSS_Discriminator: 0.11238618195056915\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 5.15252161026001 LOSS_Discriminator: 0.1256500482559204\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 4.811328887939453 LOSS_Discriminator: 0.11148103326559067\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 4.4877729415893555 LOSS_Discriminator: 0.2307930290699005\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 4.085391521453857 LOSS_Discriminator: 0.24685479700565338\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 3.6940433979034424 LOSS_Discriminator: 0.14400240778923035\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 3.9181432723999023 LOSS_Discriminator: 0.10691265761852264\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 4.6113457679748535 LOSS_Discriminator: 0.1929788738489151\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 4.95501184463501 LOSS_Discriminator: 0.14080654084682465\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 5.356431007385254 LOSS_Discriminator: 0.0497346930205822\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 5.102181911468506 LOSS_Discriminator: 0.08517996966838837\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 5.551276206970215 LOSS_Discriminator: 0.07860428094863892\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 5.04398250579834 LOSS_Discriminator: 0.17032857239246368\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 4.788048267364502 LOSS_Discriminator: 0.1355176866054535\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 4.9564361572265625 LOSS_Discriminator: 0.12865470349788666\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 4.544036865234375 LOSS_Discriminator: 0.0876462534070015\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 5.279531002044678 LOSS_Discriminator: 0.06177845597267151\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 5.603185653686523 LOSS_Discriminator: 0.06898763030767441\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 5.381444931030273 LOSS_Discriminator: 0.18530108034610748\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 5.432906627655029 LOSS_Discriminator: 0.22331848740577698\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 4.83657169342041 LOSS_Discriminator: 0.19327253103256226\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 5.21852970123291 LOSS_Discriminator: 0.09934311360120773\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 5.002127170562744 LOSS_Discriminator: 0.01453823409974575\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 5.3946380615234375 LOSS_Discriminator: 0.06745517253875732\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 5.85893440246582 LOSS_Discriminator: 0.026225563138723373\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 5.295313835144043 LOSS_Discriminator: 0.5106451511383057\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 5.109145641326904 LOSS_Discriminator: 0.11110750585794449\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 4.996389389038086 LOSS_Discriminator: 0.04941947013139725\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 4.663097381591797 LOSS_Discriminator: 0.11379675567150116\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 4.648289680480957 LOSS_Discriminator: 0.2351406365633011\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 4.09679651260376 LOSS_Discriminator: 0.050813935697078705\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 4.419597148895264 LOSS_Discriminator: 0.06615474820137024\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 4.112462043762207 LOSS_Discriminator: 0.38990694284439087\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 4.39829158782959 LOSS_Discriminator: 0.10016794502735138\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 5.266104698181152 LOSS_Discriminator: 0.13111698627471924\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 5.2967753410339355 LOSS_Discriminator: 0.049909818917512894\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 5.790768623352051 LOSS_Discriminator: 0.2501986026763916\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 4.973263740539551 LOSS_Discriminator: 0.26449987292289734\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 4.88852596282959 LOSS_Discriminator: 0.17179682850837708\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 3.969902515411377 LOSS_Discriminator: 0.21766701340675354\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 2.9153265953063965 LOSS_Discriminator: 0.11450134962797165\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 3.253795862197876 LOSS_Discriminator: 0.08587395399808884\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 3.511899948120117 LOSS_Discriminator: 0.1688837856054306\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 4.179965019226074 LOSS_Discriminator: 0.10875028371810913\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 5.08922004699707 LOSS_Discriminator: 0.13381771743297577\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 5.146317481994629 LOSS_Discriminator: 0.18134288489818573\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 5.214256763458252 LOSS_Discriminator: 0.0867764800786972\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 5.515188217163086 LOSS_Discriminator: 0.1668224334716797\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 5.173754692077637 LOSS_Discriminator: 0.25477996468544006\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 4.213656425476074 LOSS_Discriminator: 0.12638644874095917\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 3.979191303253174 LOSS_Discriminator: 0.2668743133544922\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 4.464897155761719 LOSS_Discriminator: 0.24604499340057373\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 5.009387969970703 LOSS_Discriminator: 0.27647507190704346\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 4.728353023529053 LOSS_Discriminator: 0.15470542013645172\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 5.186925888061523 LOSS_Discriminator: 0.1784881055355072\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 5.135970115661621 LOSS_Discriminator: 0.19712568819522858\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 4.860306739807129 LOSS_Discriminator: 0.3567590117454529\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 4.8724846839904785 LOSS_Discriminator: 0.05943697690963745\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 5.153416156768799 LOSS_Discriminator: 0.18017292022705078\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 4.436100482940674 LOSS_Discriminator: 0.04961271584033966\n",
            "EPOCH OVER: 18\n",
            "ITERATION_NO.: 1 LOSS_Generator: 4.507964134216309 LOSS_Discriminator: 0.04521834850311279\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.142723083496094 LOSS_Discriminator: 0.17538288235664368\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.094985008239746 LOSS_Discriminator: 0.0957866683602333\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.793747425079346 LOSS_Discriminator: 0.0822213664650917\n",
            "ITERATION_NO.: 5 LOSS_Generator: 4.981308937072754 LOSS_Discriminator: 0.06142643839120865\n",
            "ITERATION_NO.: 6 LOSS_Generator: 4.868792533874512 LOSS_Discriminator: 0.2374526411294937\n",
            "ITERATION_NO.: 7 LOSS_Generator: 4.797208786010742 LOSS_Discriminator: 0.10489489883184433\n",
            "ITERATION_NO.: 8 LOSS_Generator: 4.804296016693115 LOSS_Discriminator: 0.21677568554878235\n",
            "ITERATION_NO.: 9 LOSS_Generator: 4.234936714172363 LOSS_Discriminator: 0.04892788827419281\n",
            "ITERATION_NO.: 10 LOSS_Generator: 4.656214714050293 LOSS_Discriminator: 0.15562881529331207\n",
            "ITERATION_NO.: 11 LOSS_Generator: 4.345325469970703 LOSS_Discriminator: 0.1619771271944046\n",
            "ITERATION_NO.: 12 LOSS_Generator: 4.4162750244140625 LOSS_Discriminator: 0.13140852749347687\n",
            "ITERATION_NO.: 13 LOSS_Generator: 4.318325519561768 LOSS_Discriminator: 0.04395517706871033\n",
            "ITERATION_NO.: 14 LOSS_Generator: 4.537431716918945 LOSS_Discriminator: 0.15798836946487427\n",
            "ITERATION_NO.: 15 LOSS_Generator: 4.8790178298950195 LOSS_Discriminator: 0.027000021189451218\n",
            "ITERATION_NO.: 16 LOSS_Generator: 4.4007182121276855 LOSS_Discriminator: 0.25872302055358887\n",
            "ITERATION_NO.: 17 LOSS_Generator: 3.7140469551086426 LOSS_Discriminator: 0.11927810311317444\n",
            "ITERATION_NO.: 18 LOSS_Generator: 3.699676990509033 LOSS_Discriminator: 0.09007702022790909\n",
            "ITERATION_NO.: 19 LOSS_Generator: 3.802194118499756 LOSS_Discriminator: 0.11413494497537613\n",
            "ITERATION_NO.: 20 LOSS_Generator: 4.054708957672119 LOSS_Discriminator: 0.08361119776964188\n",
            "ITERATION_NO.: 21 LOSS_Generator: 3.970304250717163 LOSS_Discriminator: 0.07325054705142975\n",
            "ITERATION_NO.: 22 LOSS_Generator: 4.545914649963379 LOSS_Discriminator: 0.20387078821659088\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.050449371337891 LOSS_Discriminator: 0.10639985650777817\n",
            "ITERATION_NO.: 24 LOSS_Generator: 4.874215126037598 LOSS_Discriminator: 0.37147343158721924\n",
            "ITERATION_NO.: 25 LOSS_Generator: 4.446952819824219 LOSS_Discriminator: 0.14378856122493744\n",
            "ITERATION_NO.: 26 LOSS_Generator: 4.155240058898926 LOSS_Discriminator: 0.1232587918639183\n",
            "ITERATION_NO.: 27 LOSS_Generator: 4.303994178771973 LOSS_Discriminator: 0.06568622589111328\n",
            "ITERATION_NO.: 28 LOSS_Generator: 3.965466022491455 LOSS_Discriminator: 0.1803065836429596\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.154149055480957 LOSS_Discriminator: 0.09880156815052032\n",
            "ITERATION_NO.: 30 LOSS_Generator: 4.757481575012207 LOSS_Discriminator: 0.17923811078071594\n",
            "ITERATION_NO.: 31 LOSS_Generator: 4.652173042297363 LOSS_Discriminator: 0.07621237635612488\n",
            "ITERATION_NO.: 32 LOSS_Generator: 4.498369216918945 LOSS_Discriminator: 0.18128108978271484\n",
            "ITERATION_NO.: 33 LOSS_Generator: 4.640014171600342 LOSS_Discriminator: 0.1446920931339264\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.3351898193359375 LOSS_Discriminator: 0.021698568016290665\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.4333109855651855 LOSS_Discriminator: 0.04174474999308586\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.598719596862793 LOSS_Discriminator: 0.027689725160598755\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.372166633605957 LOSS_Discriminator: 0.32793766260147095\n",
            "ITERATION_NO.: 38 LOSS_Generator: 4.739556312561035 LOSS_Discriminator: 0.043783776462078094\n",
            "ITERATION_NO.: 39 LOSS_Generator: 4.712464332580566 LOSS_Discriminator: 0.04382433369755745\n",
            "ITERATION_NO.: 40 LOSS_Generator: 4.291861534118652 LOSS_Discriminator: 0.12100287526845932\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.360543251037598 LOSS_Discriminator: 0.04640726372599602\n",
            "ITERATION_NO.: 42 LOSS_Generator: 6.139040470123291 LOSS_Discriminator: 0.12970119714736938\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.249917030334473 LOSS_Discriminator: 0.07900246977806091\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.254203796386719 LOSS_Discriminator: 0.07888084650039673\n",
            "ITERATION_NO.: 45 LOSS_Generator: 5.391531944274902 LOSS_Discriminator: 0.17724190652370453\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.528697967529297 LOSS_Discriminator: 0.1666702777147293\n",
            "ITERATION_NO.: 47 LOSS_Generator: 4.624223709106445 LOSS_Discriminator: 0.027330979704856873\n",
            "ITERATION_NO.: 48 LOSS_Generator: 4.581493377685547 LOSS_Discriminator: 0.05886336416006088\n",
            "ITERATION_NO.: 49 LOSS_Generator: 4.105103969573975 LOSS_Discriminator: 0.20282834768295288\n",
            "ITERATION_NO.: 50 LOSS_Generator: 4.364419937133789 LOSS_Discriminator: 0.04075584188103676\n",
            "ITERATION_NO.: 51 LOSS_Generator: 4.122970104217529 LOSS_Discriminator: 0.2395596206188202\n",
            "ITERATION_NO.: 52 LOSS_Generator: 3.4235024452209473 LOSS_Discriminator: 0.12835079431533813\n",
            "ITERATION_NO.: 53 LOSS_Generator: 3.787247657775879 LOSS_Discriminator: 0.17164084315299988\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.073116302490234 LOSS_Discriminator: 0.15000879764556885\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.59057092666626 LOSS_Discriminator: 0.08610737323760986\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.493051528930664 LOSS_Discriminator: 0.09246190637350082\n",
            "ITERATION_NO.: 57 LOSS_Generator: 6.220182418823242 LOSS_Discriminator: 0.11015667766332626\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.5617218017578125 LOSS_Discriminator: 0.20488989353179932\n",
            "ITERATION_NO.: 59 LOSS_Generator: 4.0251145362854 LOSS_Discriminator: 0.13538940250873566\n",
            "ITERATION_NO.: 60 LOSS_Generator: 4.842103958129883 LOSS_Discriminator: 0.0935441106557846\n",
            "ITERATION_NO.: 61 LOSS_Generator: 3.664349317550659 LOSS_Discriminator: 0.20146819949150085\n",
            "ITERATION_NO.: 62 LOSS_Generator: 4.248875617980957 LOSS_Discriminator: 0.2057577669620514\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.0218095779418945 LOSS_Discriminator: 0.07767331600189209\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.208247184753418 LOSS_Discriminator: 0.1401086449623108\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.57241153717041 LOSS_Discriminator: 0.19841358065605164\n",
            "ITERATION_NO.: 66 LOSS_Generator: 6.400733947753906 LOSS_Discriminator: 0.04243756830692291\n",
            "ITERATION_NO.: 67 LOSS_Generator: 6.415868282318115 LOSS_Discriminator: 0.1147192120552063\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.752896785736084 LOSS_Discriminator: 0.33894991874694824\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.77539587020874 LOSS_Discriminator: 0.021289564669132233\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.316517353057861 LOSS_Discriminator: 0.03527175262570381\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.3295416831970215 LOSS_Discriminator: 0.1358056664466858\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.3894853591918945 LOSS_Discriminator: 0.19316798448562622\n",
            "ITERATION_NO.: 73 LOSS_Generator: 4.541589736938477 LOSS_Discriminator: 0.17826086282730103\n",
            "ITERATION_NO.: 74 LOSS_Generator: 4.373396873474121 LOSS_Discriminator: 0.07532118260860443\n",
            "ITERATION_NO.: 75 LOSS_Generator: 4.69517707824707 LOSS_Discriminator: 0.12182804942131042\n",
            "ITERATION_NO.: 76 LOSS_Generator: 4.999879837036133 LOSS_Discriminator: 0.14993228018283844\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.275963306427002 LOSS_Discriminator: 0.055145569145679474\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.916950225830078 LOSS_Discriminator: 0.0768369510769844\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.85172700881958 LOSS_Discriminator: 0.06468494236469269\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.473302841186523 LOSS_Discriminator: 0.08654831349849701\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.525875091552734 LOSS_Discriminator: 0.04347424954175949\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.375633716583252 LOSS_Discriminator: 0.19715343415737152\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.077526092529297 LOSS_Discriminator: 0.22169169783592224\n",
            "ITERATION_NO.: 84 LOSS_Generator: 4.567827224731445 LOSS_Discriminator: 0.06856284290552139\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.0556535720825195 LOSS_Discriminator: 0.09668219834566116\n",
            "ITERATION_NO.: 86 LOSS_Generator: 4.910197734832764 LOSS_Discriminator: 0.05323323607444763\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.409998416900635 LOSS_Discriminator: 0.052239760756492615\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.348885536193848 LOSS_Discriminator: 0.05191386863589287\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.164236068725586 LOSS_Discriminator: 0.030813410878181458\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.7280707359313965 LOSS_Discriminator: 0.05901499092578888\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.409706115722656 LOSS_Discriminator: 0.13525143265724182\n",
            "ITERATION_NO.: 92 LOSS_Generator: 6.134568214416504 LOSS_Discriminator: 0.02044929936528206\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.434805870056152 LOSS_Discriminator: 0.06238820403814316\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.819650650024414 LOSS_Discriminator: 0.14628149569034576\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.014274597167969 LOSS_Discriminator: 0.06855020672082901\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.47938871383667 LOSS_Discriminator: 0.14543180167675018\n",
            "ITERATION_NO.: 97 LOSS_Generator: 4.581325054168701 LOSS_Discriminator: 0.09343079477548599\n",
            "ITERATION_NO.: 98 LOSS_Generator: 4.377708435058594 LOSS_Discriminator: 0.124189093708992\n",
            "ITERATION_NO.: 99 LOSS_Generator: 4.076581001281738 LOSS_Discriminator: 0.08417554944753647\n",
            "ITERATION_NO.: 100 LOSS_Generator: 4.607335090637207 LOSS_Discriminator: 0.07938161492347717\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.094170093536377 LOSS_Discriminator: 0.038524869829416275\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.7310285568237305 LOSS_Discriminator: 0.1079232394695282\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.880054473876953 LOSS_Discriminator: 0.06525478512048721\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.568127632141113 LOSS_Discriminator: 0.029267333447933197\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.283381462097168 LOSS_Discriminator: 0.1641901284456253\n",
            "ITERATION_NO.: 106 LOSS_Generator: 4.907059192657471 LOSS_Discriminator: 0.054149433970451355\n",
            "ITERATION_NO.: 107 LOSS_Generator: 4.338818550109863 LOSS_Discriminator: 0.1663070172071457\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.486556053161621 LOSS_Discriminator: 0.032282259315252304\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.374969482421875 LOSS_Discriminator: 0.14437806606292725\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.518115520477295 LOSS_Discriminator: 0.07595101743936539\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.762150764465332 LOSS_Discriminator: 0.1852778196334839\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.618094444274902 LOSS_Discriminator: 0.1421166956424713\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.507737636566162 LOSS_Discriminator: 0.040434591472148895\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.280093669891357 LOSS_Discriminator: 0.18699851632118225\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.495431423187256 LOSS_Discriminator: 0.06951110064983368\n",
            "ITERATION_NO.: 116 LOSS_Generator: 4.996935844421387 LOSS_Discriminator: 0.2805887460708618\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.268622398376465 LOSS_Discriminator: 0.26490673422813416\n",
            "ITERATION_NO.: 118 LOSS_Generator: 4.739340782165527 LOSS_Discriminator: 0.2900204658508301\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.19219970703125 LOSS_Discriminator: 0.1181110367178917\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.069033145904541 LOSS_Discriminator: 0.05310424417257309\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.113856315612793 LOSS_Discriminator: 0.10711626708507538\n",
            "ITERATION_NO.: 122 LOSS_Generator: 6.293001174926758 LOSS_Discriminator: 0.034117963165044785\n",
            "ITERATION_NO.: 123 LOSS_Generator: 6.582178115844727 LOSS_Discriminator: 0.2897062599658966\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.6678996086120605 LOSS_Discriminator: 0.07815183699131012\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.205384731292725 LOSS_Discriminator: 0.08714781701564789\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.709128379821777 LOSS_Discriminator: 0.19033929705619812\n",
            "ITERATION_NO.: 127 LOSS_Generator: 4.908843517303467 LOSS_Discriminator: 0.07325657457113266\n",
            "ITERATION_NO.: 128 LOSS_Generator: 4.977444648742676 LOSS_Discriminator: 0.2921833097934723\n",
            "ITERATION_NO.: 129 LOSS_Generator: 4.182311058044434 LOSS_Discriminator: 0.235065758228302\n",
            "ITERATION_NO.: 130 LOSS_Generator: 3.8315069675445557 LOSS_Discriminator: 0.303572416305542\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.134330749511719 LOSS_Discriminator: 0.20423412322998047\n",
            "ITERATION_NO.: 132 LOSS_Generator: 4.297208786010742 LOSS_Discriminator: 0.10807307809591293\n",
            "ITERATION_NO.: 133 LOSS_Generator: 4.719369411468506 LOSS_Discriminator: 0.18912726640701294\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.5792460441589355 LOSS_Discriminator: 0.17960426211357117\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.788276195526123 LOSS_Discriminator: 0.08718296885490417\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.971810340881348 LOSS_Discriminator: 0.20986230671405792\n",
            "ITERATION_NO.: 137 LOSS_Generator: 4.921728134155273 LOSS_Discriminator: 0.07406427711248398\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.327288627624512 LOSS_Discriminator: 0.1136808842420578\n",
            "ITERATION_NO.: 139 LOSS_Generator: 4.987031936645508 LOSS_Discriminator: 0.023334652185440063\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.000916481018066 LOSS_Discriminator: 0.08310368657112122\n",
            "ITERATION_NO.: 141 LOSS_Generator: 4.742870807647705 LOSS_Discriminator: 0.09818971157073975\n",
            "ITERATION_NO.: 142 LOSS_Generator: 4.426103591918945 LOSS_Discriminator: 0.23271191120147705\n",
            "ITERATION_NO.: 143 LOSS_Generator: 4.997292995452881 LOSS_Discriminator: 0.10008274018764496\n",
            "ITERATION_NO.: 144 LOSS_Generator: 4.253605842590332 LOSS_Discriminator: 0.1932910978794098\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.351367473602295 LOSS_Discriminator: 0.050637632608413696\n",
            "ITERATION_NO.: 146 LOSS_Generator: 4.961524486541748 LOSS_Discriminator: 0.08500432223081589\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.211966514587402 LOSS_Discriminator: 0.26560354232788086\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.718803405761719 LOSS_Discriminator: 0.29303276538848877\n",
            "ITERATION_NO.: 149 LOSS_Generator: 4.078377723693848 LOSS_Discriminator: 0.22389575839042664\n",
            "ITERATION_NO.: 150 LOSS_Generator: 3.411776304244995 LOSS_Discriminator: 0.16553491353988647\n",
            "ITERATION_NO.: 151 LOSS_Generator: 3.55527400970459 LOSS_Discriminator: 0.22270433604717255\n",
            "ITERATION_NO.: 152 LOSS_Generator: 4.661887168884277 LOSS_Discriminator: 0.17239834368228912\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.484479904174805 LOSS_Discriminator: 0.09794173389673233\n",
            "ITERATION_NO.: 154 LOSS_Generator: 6.0885772705078125 LOSS_Discriminator: 0.0950487032532692\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.997476100921631 LOSS_Discriminator: 0.395648717880249\n",
            "ITERATION_NO.: 156 LOSS_Generator: 6.0700507164001465 LOSS_Discriminator: 0.05439206585288048\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.855498313903809 LOSS_Discriminator: 0.18158414959907532\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.333305358886719 LOSS_Discriminator: 0.15731531381607056\n",
            "ITERATION_NO.: 159 LOSS_Generator: 4.6958818435668945 LOSS_Discriminator: 0.14273416996002197\n",
            "ITERATION_NO.: 160 LOSS_Generator: 4.528980731964111 LOSS_Discriminator: 0.14812633395195007\n",
            "ITERATION_NO.: 161 LOSS_Generator: 3.6423733234405518 LOSS_Discriminator: 0.12178055942058563\n",
            "ITERATION_NO.: 162 LOSS_Generator: 3.6146769523620605 LOSS_Discriminator: 0.181749165058136\n",
            "ITERATION_NO.: 163 LOSS_Generator: 3.088911294937134 LOSS_Discriminator: 0.311550498008728\n",
            "ITERATION_NO.: 164 LOSS_Generator: 3.78092622756958 LOSS_Discriminator: 0.1225244328379631\n",
            "ITERATION_NO.: 165 LOSS_Generator: 3.9347681999206543 LOSS_Discriminator: 0.14753711223602295\n",
            "ITERATION_NO.: 166 LOSS_Generator: 4.932669639587402 LOSS_Discriminator: 0.1670791208744049\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.060439109802246 LOSS_Discriminator: 0.17970341444015503\n",
            "ITERATION_NO.: 168 LOSS_Generator: 4.760862350463867 LOSS_Discriminator: 0.10645172744989395\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.126751899719238 LOSS_Discriminator: 0.06464211642742157\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.046303749084473 LOSS_Discriminator: 0.07150591909885406\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.722991466522217 LOSS_Discriminator: 0.10576977580785751\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.140877723693848 LOSS_Discriminator: 0.0988985151052475\n",
            "ITERATION_NO.: 173 LOSS_Generator: 4.926287651062012 LOSS_Discriminator: 0.1288357675075531\n",
            "ITERATION_NO.: 174 LOSS_Generator: 4.044637680053711 LOSS_Discriminator: 0.025319185107946396\n",
            "ITERATION_NO.: 175 LOSS_Generator: 4.951530456542969 LOSS_Discriminator: 0.033298149704933167\n",
            "ITERATION_NO.: 176 LOSS_Generator: 4.211781978607178 LOSS_Discriminator: 0.10901221632957458\n",
            "ITERATION_NO.: 177 LOSS_Generator: 4.044100761413574 LOSS_Discriminator: 0.06839410960674286\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.158552169799805 LOSS_Discriminator: 0.07649944722652435\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.3938517570495605 LOSS_Discriminator: 0.03587854653596878\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.434787273406982 LOSS_Discriminator: 0.14389139413833618\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.599287509918213 LOSS_Discriminator: 0.05253615975379944\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.520803928375244 LOSS_Discriminator: 0.12779995799064636\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.2738447189331055 LOSS_Discriminator: 0.031009119004011154\n",
            "ITERATION_NO.: 184 LOSS_Generator: 4.981675148010254 LOSS_Discriminator: 0.15422046184539795\n",
            "ITERATION_NO.: 185 LOSS_Generator: 4.873569011688232 LOSS_Discriminator: 0.10010843724012375\n",
            "ITERATION_NO.: 186 LOSS_Generator: 4.549006938934326 LOSS_Discriminator: 0.1463555246591568\n",
            "ITERATION_NO.: 187 LOSS_Generator: 4.559394836425781 LOSS_Discriminator: 0.20034649968147278\n",
            "ITERATION_NO.: 188 LOSS_Generator: 3.4990272521972656 LOSS_Discriminator: 0.16963794827461243\n",
            "ITERATION_NO.: 189 LOSS_Generator: 4.004558563232422 LOSS_Discriminator: 0.12316285073757172\n",
            "ITERATION_NO.: 190 LOSS_Generator: 4.055303573608398 LOSS_Discriminator: 0.2006790041923523\n",
            "ITERATION_NO.: 191 LOSS_Generator: 4.451750755310059 LOSS_Discriminator: 0.05691900849342346\n",
            "ITERATION_NO.: 192 LOSS_Generator: 4.899589538574219 LOSS_Discriminator: 0.10644636303186417\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.17520809173584 LOSS_Discriminator: 0.13871711492538452\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.39343786239624 LOSS_Discriminator: 0.09487836807966232\n",
            "ITERATION_NO.: 195 LOSS_Generator: 4.802768707275391 LOSS_Discriminator: 0.055691663175821304\n",
            "ITERATION_NO.: 196 LOSS_Generator: 4.924363136291504 LOSS_Discriminator: 0.04629784822463989\n",
            "ITERATION_NO.: 197 LOSS_Generator: 4.962519645690918 LOSS_Discriminator: 0.0556439533829689\n",
            "ITERATION_NO.: 198 LOSS_Generator: 4.915276527404785 LOSS_Discriminator: 0.0768170952796936\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.4789276123046875 LOSS_Discriminator: 0.13534413278102875\n",
            "ITERATION_NO.: 200 LOSS_Generator: 4.812262058258057 LOSS_Discriminator: 0.3223811984062195\n",
            "ITERATION_NO.: 201 LOSS_Generator: 4.335526943206787 LOSS_Discriminator: 0.07967310398817062\n",
            "ITERATION_NO.: 202 LOSS_Generator: 3.937952756881714 LOSS_Discriminator: 0.06921809911727905\n",
            "ITERATION_NO.: 203 LOSS_Generator: 3.250274658203125 LOSS_Discriminator: 0.10958348214626312\n",
            "ITERATION_NO.: 204 LOSS_Generator: 3.5013587474823 LOSS_Discriminator: 0.2564302384853363\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.204443454742432 LOSS_Discriminator: 0.10372702777385712\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.146007537841797 LOSS_Discriminator: 0.06641029566526413\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.650259971618652 LOSS_Discriminator: 0.10731187462806702\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.290962219238281 LOSS_Discriminator: 0.1125430166721344\n",
            "ITERATION_NO.: 209 LOSS_Generator: 6.067187309265137 LOSS_Discriminator: 0.011689875274896622\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.7972307205200195 LOSS_Discriminator: 0.02632627636194229\n",
            "ITERATION_NO.: 211 LOSS_Generator: 6.298328399658203 LOSS_Discriminator: 0.026781214401125908\n",
            "ITERATION_NO.: 212 LOSS_Generator: 6.1493377685546875 LOSS_Discriminator: 0.1743898093700409\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.487195014953613 LOSS_Discriminator: 0.021739069372415543\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.187018871307373 LOSS_Discriminator: 0.016008146107196808\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.402758598327637 LOSS_Discriminator: 0.10309901833534241\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.3299665451049805 LOSS_Discriminator: 0.06464391201734543\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.797547817230225 LOSS_Discriminator: 0.19345960021018982\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.48490047454834 LOSS_Discriminator: 0.015814613550901413\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.595860481262207 LOSS_Discriminator: 0.35030192136764526\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.0370025634765625 LOSS_Discriminator: 0.1451091766357422\n",
            "ITERATION_NO.: 221 LOSS_Generator: 4.4499969482421875 LOSS_Discriminator: 0.06102706491947174\n",
            "ITERATION_NO.: 222 LOSS_Generator: 3.7926735877990723 LOSS_Discriminator: 0.2652348279953003\n",
            "ITERATION_NO.: 223 LOSS_Generator: 4.151653289794922 LOSS_Discriminator: 0.18029630184173584\n",
            "ITERATION_NO.: 224 LOSS_Generator: 4.176685333251953 LOSS_Discriminator: 0.3446674048900604\n",
            "ITERATION_NO.: 225 LOSS_Generator: 4.3832244873046875 LOSS_Discriminator: 0.15098229050636292\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.23940372467041 LOSS_Discriminator: 0.14163242280483246\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.50071907043457 LOSS_Discriminator: 0.08128653466701508\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.0681610107421875 LOSS_Discriminator: 0.023145074024796486\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.682608604431152 LOSS_Discriminator: 0.25128480792045593\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.680922508239746 LOSS_Discriminator: 0.16381195187568665\n",
            "ITERATION_NO.: 231 LOSS_Generator: 4.866857051849365 LOSS_Discriminator: 0.32771050930023193\n",
            "ITERATION_NO.: 232 LOSS_Generator: 4.251166343688965 LOSS_Discriminator: 0.13501405715942383\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.050349235534668 LOSS_Discriminator: 0.16463467478752136\n",
            "ITERATION_NO.: 234 LOSS_Generator: 4.780681610107422 LOSS_Discriminator: 0.3532642722129822\n",
            "ITERATION_NO.: 235 LOSS_Generator: 4.855947971343994 LOSS_Discriminator: 0.06477239727973938\n",
            "ITERATION_NO.: 236 LOSS_Generator: 4.606942176818848 LOSS_Discriminator: 0.1636791229248047\n",
            "ITERATION_NO.: 237 LOSS_Generator: 4.383822917938232 LOSS_Discriminator: 0.08589167892932892\n",
            "ITERATION_NO.: 238 LOSS_Generator: 4.323708534240723 LOSS_Discriminator: 0.08199737966060638\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.439561367034912 LOSS_Discriminator: 0.24123826622962952\n",
            "ITERATION_NO.: 240 LOSS_Generator: 4.682833194732666 LOSS_Discriminator: 0.2528659999370575\n",
            "ITERATION_NO.: 241 LOSS_Generator: 4.398899078369141 LOSS_Discriminator: 0.10177114605903625\n",
            "ITERATION_NO.: 242 LOSS_Generator: 4.688941955566406 LOSS_Discriminator: 0.14190752804279327\n",
            "ITERATION_NO.: 243 LOSS_Generator: 4.571320533752441 LOSS_Discriminator: 0.14051559567451477\n",
            "ITERATION_NO.: 244 LOSS_Generator: 4.221597194671631 LOSS_Discriminator: 0.062204889953136444\n",
            "ITERATION_NO.: 245 LOSS_Generator: 4.697545051574707 LOSS_Discriminator: 0.20004180073738098\n",
            "ITERATION_NO.: 246 LOSS_Generator: 3.6069188117980957 LOSS_Discriminator: 0.2717955410480499\n",
            "ITERATION_NO.: 247 LOSS_Generator: 3.253074884414673 LOSS_Discriminator: 0.1550910919904709\n",
            "ITERATION_NO.: 248 LOSS_Generator: 3.406337261199951 LOSS_Discriminator: 0.38867735862731934\n",
            "ITERATION_NO.: 249 LOSS_Generator: 3.266953468322754 LOSS_Discriminator: 0.2016828954219818\n",
            "ITERATION_NO.: 250 LOSS_Generator: 4.9547119140625 LOSS_Discriminator: 0.14344918727874756\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.033652305603027 LOSS_Discriminator: 0.14351408183574677\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.693116188049316 LOSS_Discriminator: 0.08635526895523071\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.781343936920166 LOSS_Discriminator: 0.05400095880031586\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.61824893951416 LOSS_Discriminator: 0.25812679529190063\n",
            "ITERATION_NO.: 255 LOSS_Generator: 4.857508182525635 LOSS_Discriminator: 0.09720957279205322\n",
            "ITERATION_NO.: 256 LOSS_Generator: 4.305255889892578 LOSS_Discriminator: 0.42505407333374023\n",
            "ITERATION_NO.: 257 LOSS_Generator: 3.0303077697753906 LOSS_Discriminator: 0.18190431594848633\n",
            "ITERATION_NO.: 258 LOSS_Generator: 3.1614203453063965 LOSS_Discriminator: 0.1817430853843689\n",
            "ITERATION_NO.: 259 LOSS_Generator: 4.392744541168213 LOSS_Discriminator: 0.232582688331604\n",
            "ITERATION_NO.: 260 LOSS_Generator: 4.6688127517700195 LOSS_Discriminator: 0.09306290745735168\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.839853286743164 LOSS_Discriminator: 0.01970864087343216\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.700927734375 LOSS_Discriminator: 0.11783529072999954\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.974323272705078 LOSS_Discriminator: 0.3635673522949219\n",
            "ITERATION_NO.: 264 LOSS_Generator: 6.6376190185546875 LOSS_Discriminator: 0.009115317836403847\n",
            "ITERATION_NO.: 265 LOSS_Generator: 5.915256023406982 LOSS_Discriminator: 0.06419245898723602\n",
            "ITERATION_NO.: 266 LOSS_Generator: 5.150573253631592 LOSS_Discriminator: 0.06060757488012314\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.464080810546875 LOSS_Discriminator: 0.15715646743774414\n",
            "ITERATION_NO.: 268 LOSS_Generator: 4.219097137451172 LOSS_Discriminator: 0.08563373982906342\n",
            "ITERATION_NO.: 269 LOSS_Generator: 3.7417590618133545 LOSS_Discriminator: 0.06499484926462173\n",
            "ITERATION_NO.: 270 LOSS_Generator: 4.093523025512695 LOSS_Discriminator: 0.111514151096344\n",
            "ITERATION_NO.: 271 LOSS_Generator: 4.418135166168213 LOSS_Discriminator: 0.27247747778892517\n",
            "ITERATION_NO.: 272 LOSS_Generator: 4.831857681274414 LOSS_Discriminator: 0.23210376501083374\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.497345924377441 LOSS_Discriminator: 0.16300347447395325\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.2992706298828125 LOSS_Discriminator: 0.18730449676513672\n",
            "ITERATION_NO.: 275 LOSS_Generator: 4.749663352966309 LOSS_Discriminator: 0.11567828059196472\n",
            "ITERATION_NO.: 276 LOSS_Generator: 4.583432674407959 LOSS_Discriminator: 0.07570904493331909\n",
            "ITERATION_NO.: 277 LOSS_Generator: 3.9624550342559814 LOSS_Discriminator: 0.03510032221674919\n",
            "ITERATION_NO.: 278 LOSS_Generator: 4.653405666351318 LOSS_Discriminator: 0.058066632598638535\n",
            "ITERATION_NO.: 279 LOSS_Generator: 4.1047749519348145 LOSS_Discriminator: 0.11097822338342667\n",
            "ITERATION_NO.: 280 LOSS_Generator: 4.923983573913574 LOSS_Discriminator: 0.260452002286911\n",
            "ITERATION_NO.: 281 LOSS_Generator: 4.60085916519165 LOSS_Discriminator: 0.07499353587627411\n",
            "ITERATION_NO.: 282 LOSS_Generator: 4.736023902893066 LOSS_Discriminator: 0.05359046906232834\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.35670280456543 LOSS_Discriminator: 0.04315057396888733\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.4367265701293945 LOSS_Discriminator: 0.3367137908935547\n",
            "ITERATION_NO.: 285 LOSS_Generator: 4.918468952178955 LOSS_Discriminator: 0.14489184319972992\n",
            "ITERATION_NO.: 286 LOSS_Generator: 3.787794589996338 LOSS_Discriminator: 0.18983140587806702\n",
            "ITERATION_NO.: 287 LOSS_Generator: 3.8877501487731934 LOSS_Discriminator: 0.09496477246284485\n",
            "ITERATION_NO.: 288 LOSS_Generator: 3.9549343585968018 LOSS_Discriminator: 0.18504147231578827\n",
            "ITERATION_NO.: 289 LOSS_Generator: 4.347146511077881 LOSS_Discriminator: 0.18432047963142395\n",
            "ITERATION_NO.: 290 LOSS_Generator: 4.594288349151611 LOSS_Discriminator: 0.09731420874595642\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.0165252685546875 LOSS_Discriminator: 0.08304120600223541\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.617161750793457 LOSS_Discriminator: 0.01014913059771061\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.792235374450684 LOSS_Discriminator: 0.11058339476585388\n",
            "ITERATION_NO.: 294 LOSS_Generator: 6.008294105529785 LOSS_Discriminator: 0.14258229732513428\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.274994850158691 LOSS_Discriminator: 0.017785055562853813\n",
            "ITERATION_NO.: 296 LOSS_Generator: 4.846761703491211 LOSS_Discriminator: 0.30940523743629456\n",
            "ITERATION_NO.: 297 LOSS_Generator: 3.358044385910034 LOSS_Discriminator: 0.1418967992067337\n",
            "ITERATION_NO.: 298 LOSS_Generator: 3.145714521408081 LOSS_Discriminator: 0.13366691768169403\n",
            "ITERATION_NO.: 299 LOSS_Generator: 4.162965297698975 LOSS_Discriminator: 0.14679057896137238\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.232215404510498 LOSS_Discriminator: 0.09904316812753677\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.950159072875977 LOSS_Discriminator: 0.08426875621080399\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.484816551208496 LOSS_Discriminator: 0.021763518452644348\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.617741107940674 LOSS_Discriminator: 0.09590816497802734\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.155604839324951 LOSS_Discriminator: 0.21985217928886414\n",
            "ITERATION_NO.: 305 LOSS_Generator: 5.467375755310059 LOSS_Discriminator: 0.044966526329517365\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.993984699249268 LOSS_Discriminator: 0.10906273871660233\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.534881591796875 LOSS_Discriminator: 0.05831993371248245\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.593497276306152 LOSS_Discriminator: 0.010800032876431942\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.788015365600586 LOSS_Discriminator: 0.09350990504026413\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.493696689605713 LOSS_Discriminator: 0.13832364976406097\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.015032768249512 LOSS_Discriminator: 0.1986820548772812\n",
            "ITERATION_NO.: 312 LOSS_Generator: 4.584477424621582 LOSS_Discriminator: 0.13018406927585602\n",
            "ITERATION_NO.: 313 LOSS_Generator: 4.413047790527344 LOSS_Discriminator: 0.1558864712715149\n",
            "ITERATION_NO.: 314 LOSS_Generator: 4.182692527770996 LOSS_Discriminator: 0.08254864066839218\n",
            "ITERATION_NO.: 315 LOSS_Generator: 4.648200988769531 LOSS_Discriminator: 0.14479824900627136\n",
            "ITERATION_NO.: 316 LOSS_Generator: 4.38043737411499 LOSS_Discriminator: 0.06301428377628326\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.464090347290039 LOSS_Discriminator: 0.04978285729885101\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.793264865875244 LOSS_Discriminator: 0.07278969883918762\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.620466709136963 LOSS_Discriminator: 0.18410158157348633\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.986180305480957 LOSS_Discriminator: 0.05681810528039932\n",
            "ITERATION_NO.: 321 LOSS_Generator: 6.239129066467285 LOSS_Discriminator: 0.1289706528186798\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.077424049377441 LOSS_Discriminator: 0.18728139996528625\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.471668243408203 LOSS_Discriminator: 0.1367420256137848\n",
            "ITERATION_NO.: 324 LOSS_Generator: 4.569615840911865 LOSS_Discriminator: 0.05941621959209442\n",
            "ITERATION_NO.: 325 LOSS_Generator: 4.843832969665527 LOSS_Discriminator: 0.045532628893852234\n",
            "ITERATION_NO.: 326 LOSS_Generator: 4.589786529541016 LOSS_Discriminator: 0.12317053973674774\n",
            "ITERATION_NO.: 327 LOSS_Generator: 4.453128814697266 LOSS_Discriminator: 0.03416631370782852\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.1578521728515625 LOSS_Discriminator: 0.06586640328168869\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.414548397064209 LOSS_Discriminator: 0.227120503783226\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.235494613647461 LOSS_Discriminator: 0.08235912770032883\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.297918319702148 LOSS_Discriminator: 0.025471681728959084\n",
            "ITERATION_NO.: 332 LOSS_Generator: 4.624442100524902 LOSS_Discriminator: 0.0776778906583786\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.509352684020996 LOSS_Discriminator: 0.09152039885520935\n",
            "ITERATION_NO.: 334 LOSS_Generator: 4.950231552124023 LOSS_Discriminator: 0.17376388609409332\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.97834587097168 LOSS_Discriminator: 0.154414102435112\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.05993127822876 LOSS_Discriminator: 0.13628321886062622\n",
            "ITERATION_NO.: 337 LOSS_Generator: 4.530773162841797 LOSS_Discriminator: 0.15487389266490936\n",
            "ITERATION_NO.: 338 LOSS_Generator: 4.289186000823975 LOSS_Discriminator: 0.10524480044841766\n",
            "ITERATION_NO.: 339 LOSS_Generator: 4.482021331787109 LOSS_Discriminator: 0.1292642056941986\n",
            "ITERATION_NO.: 340 LOSS_Generator: 4.543953895568848 LOSS_Discriminator: 0.11606872081756592\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.122063636779785 LOSS_Discriminator: 0.12225103378295898\n",
            "ITERATION_NO.: 342 LOSS_Generator: 4.904974460601807 LOSS_Discriminator: 0.18695679306983948\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.145928382873535 LOSS_Discriminator: 0.12155145406723022\n",
            "ITERATION_NO.: 344 LOSS_Generator: 4.639273166656494 LOSS_Discriminator: 0.46091338992118835\n",
            "ITERATION_NO.: 345 LOSS_Generator: 3.974792242050171 LOSS_Discriminator: 0.07688790559768677\n",
            "ITERATION_NO.: 346 LOSS_Generator: 4.390181541442871 LOSS_Discriminator: 0.12615089118480682\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.123568058013916 LOSS_Discriminator: 0.07625621557235718\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.530673980712891 LOSS_Discriminator: 0.023786388337612152\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.329097270965576 LOSS_Discriminator: 0.07693753391504288\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.849611759185791 LOSS_Discriminator: 0.11284514516592026\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.54135274887085 LOSS_Discriminator: 0.13796894252300262\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.002702713012695 LOSS_Discriminator: 0.023007340729236603\n",
            "ITERATION_NO.: 353 LOSS_Generator: 4.734407901763916 LOSS_Discriminator: 0.17216473817825317\n",
            "ITERATION_NO.: 354 LOSS_Generator: 4.980660438537598 LOSS_Discriminator: 0.24208374321460724\n",
            "ITERATION_NO.: 355 LOSS_Generator: 4.092713356018066 LOSS_Discriminator: 0.1063273623585701\n",
            "ITERATION_NO.: 356 LOSS_Generator: 4.458028793334961 LOSS_Discriminator: 0.0412735790014267\n",
            "ITERATION_NO.: 357 LOSS_Generator: 4.442256450653076 LOSS_Discriminator: 0.310749351978302\n",
            "ITERATION_NO.: 358 LOSS_Generator: 4.411707401275635 LOSS_Discriminator: 0.09513697773218155\n",
            "ITERATION_NO.: 359 LOSS_Generator: 4.730788230895996 LOSS_Discriminator: 0.06293682754039764\n",
            "ITERATION_NO.: 360 LOSS_Generator: 4.264719486236572 LOSS_Discriminator: 0.29857850074768066\n",
            "ITERATION_NO.: 361 LOSS_Generator: 4.3999738693237305 LOSS_Discriminator: 0.2820223867893219\n",
            "ITERATION_NO.: 362 LOSS_Generator: 4.25619649887085 LOSS_Discriminator: 0.1448541283607483\n",
            "ITERATION_NO.: 363 LOSS_Generator: 4.004275798797607 LOSS_Discriminator: 0.18153169751167297\n",
            "ITERATION_NO.: 364 LOSS_Generator: 4.023613452911377 LOSS_Discriminator: 0.043571587651968\n",
            "ITERATION_NO.: 365 LOSS_Generator: 4.71124792098999 LOSS_Discriminator: 0.10386045277118683\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.159868240356445 LOSS_Discriminator: 0.26043701171875\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.631555080413818 LOSS_Discriminator: 0.15919572114944458\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.809284210205078 LOSS_Discriminator: 0.11905509233474731\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.710835933685303 LOSS_Discriminator: 0.16468699276447296\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.690679550170898 LOSS_Discriminator: 0.03676027059555054\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.215744972229004 LOSS_Discriminator: 0.07775352895259857\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.027817726135254 LOSS_Discriminator: 0.050953373312950134\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.606995582580566 LOSS_Discriminator: 0.10184238851070404\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.078149795532227 LOSS_Discriminator: 0.024436671286821365\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.792258262634277 LOSS_Discriminator: 0.16440236568450928\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.546598434448242 LOSS_Discriminator: 0.04412498325109482\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.120899677276611 LOSS_Discriminator: 0.1860976219177246\n",
            "ITERATION_NO.: 378 LOSS_Generator: 4.395547389984131 LOSS_Discriminator: 0.3890363276004791\n",
            "ITERATION_NO.: 379 LOSS_Generator: 3.893181800842285 LOSS_Discriminator: 0.091768279671669\n",
            "ITERATION_NO.: 380 LOSS_Generator: 4.170111656188965 LOSS_Discriminator: 0.04243852570652962\n",
            "ITERATION_NO.: 381 LOSS_Generator: 4.335620880126953 LOSS_Discriminator: 0.10972872376441956\n",
            "ITERATION_NO.: 382 LOSS_Generator: 4.959044456481934 LOSS_Discriminator: 0.14632758498191833\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.242422103881836 LOSS_Discriminator: 0.21857906877994537\n",
            "ITERATION_NO.: 384 LOSS_Generator: 4.849801540374756 LOSS_Discriminator: 0.2350035309791565\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.798586845397949 LOSS_Discriminator: 0.27550625801086426\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.123459339141846 LOSS_Discriminator: 0.059661488980054855\n",
            "ITERATION_NO.: 387 LOSS_Generator: 4.497024059295654 LOSS_Discriminator: 0.1440354883670807\n",
            "ITERATION_NO.: 388 LOSS_Generator: 4.354783058166504 LOSS_Discriminator: 0.02761968970298767\n",
            "ITERATION_NO.: 389 LOSS_Generator: 4.103615760803223 LOSS_Discriminator: 0.13761454820632935\n",
            "ITERATION_NO.: 390 LOSS_Generator: 4.1606221199035645 LOSS_Discriminator: 0.08237758278846741\n",
            "ITERATION_NO.: 391 LOSS_Generator: 4.913850784301758 LOSS_Discriminator: 0.06256242096424103\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.445808410644531 LOSS_Discriminator: 0.12253715097904205\n",
            "ITERATION_NO.: 393 LOSS_Generator: 4.961535930633545 LOSS_Discriminator: 0.10563136637210846\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.590337753295898 LOSS_Discriminator: 0.0829891636967659\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.522398948669434 LOSS_Discriminator: 0.14902958273887634\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.091148376464844 LOSS_Discriminator: 0.0552789643406868\n",
            "ITERATION_NO.: 397 LOSS_Generator: 4.816177845001221 LOSS_Discriminator: 0.06529192626476288\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.098711013793945 LOSS_Discriminator: 0.06485600769519806\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.539196968078613 LOSS_Discriminator: 0.04538266733288765\n",
            "ITERATION_NO.: 400 LOSS_Generator: 4.504661560058594 LOSS_Discriminator: 0.047896575182676315\n",
            "ITERATION_NO.: 401 LOSS_Generator: 4.99796199798584 LOSS_Discriminator: 0.06295059621334076\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.619856834411621 LOSS_Discriminator: 0.055388834327459335\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.537460803985596 LOSS_Discriminator: 0.041300542652606964\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.752262592315674 LOSS_Discriminator: 0.1774740219116211\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.843111515045166 LOSS_Discriminator: 0.40676450729370117\n",
            "ITERATION_NO.: 406 LOSS_Generator: 4.803659439086914 LOSS_Discriminator: 0.10417909175157547\n",
            "ITERATION_NO.: 407 LOSS_Generator: 4.038640975952148 LOSS_Discriminator: 0.2220315784215927\n",
            "ITERATION_NO.: 408 LOSS_Generator: 3.7510271072387695 LOSS_Discriminator: 0.1982388198375702\n",
            "ITERATION_NO.: 409 LOSS_Generator: 4.332575798034668 LOSS_Discriminator: 0.2178153693675995\n",
            "ITERATION_NO.: 410 LOSS_Generator: 4.720314979553223 LOSS_Discriminator: 0.14790451526641846\n",
            "ITERATION_NO.: 411 LOSS_Generator: 4.598954200744629 LOSS_Discriminator: 0.10336343199014664\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.7010498046875 LOSS_Discriminator: 0.1504155844449997\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.9861578941345215 LOSS_Discriminator: 0.05933663249015808\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.055935859680176 LOSS_Discriminator: 0.10205543041229248\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.553723335266113 LOSS_Discriminator: 0.13123789429664612\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.494924068450928 LOSS_Discriminator: 0.3193041980266571\n",
            "ITERATION_NO.: 417 LOSS_Generator: 4.666868209838867 LOSS_Discriminator: 0.23224502801895142\n",
            "ITERATION_NO.: 418 LOSS_Generator: 3.9849467277526855 LOSS_Discriminator: 0.17923381924629211\n",
            "ITERATION_NO.: 419 LOSS_Generator: 3.3412439823150635 LOSS_Discriminator: 0.1952008157968521\n",
            "ITERATION_NO.: 420 LOSS_Generator: 3.764906644821167 LOSS_Discriminator: 0.07855463027954102\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.069939613342285 LOSS_Discriminator: 0.09599524736404419\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.349337577819824 LOSS_Discriminator: 0.044904544949531555\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.721381187438965 LOSS_Discriminator: 0.18934957683086395\n",
            "ITERATION_NO.: 424 LOSS_Generator: 6.347273826599121 LOSS_Discriminator: 0.10315589606761932\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.969076156616211 LOSS_Discriminator: 0.005796264857053757\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.695018768310547 LOSS_Discriminator: 0.13316872715950012\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.11953067779541 LOSS_Discriminator: 0.033273715525865555\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.130946159362793 LOSS_Discriminator: 0.06918862462043762\n",
            "ITERATION_NO.: 429 LOSS_Generator: 6.372609615325928 LOSS_Discriminator: 0.07922829687595367\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.623777389526367 LOSS_Discriminator: 0.05212131515145302\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.185126781463623 LOSS_Discriminator: 0.1467689573764801\n",
            "ITERATION_NO.: 432 LOSS_Generator: 4.882177352905273 LOSS_Discriminator: 0.018879484385252\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.069768905639648 LOSS_Discriminator: 0.06634382158517838\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.953947067260742 LOSS_Discriminator: 0.21808582544326782\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.331535339355469 LOSS_Discriminator: 0.05710037797689438\n",
            "ITERATION_NO.: 436 LOSS_Generator: 4.891243934631348 LOSS_Discriminator: 0.1419389694929123\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.834175109863281 LOSS_Discriminator: 0.04648245498538017\n",
            "ITERATION_NO.: 438 LOSS_Generator: 5.826294898986816 LOSS_Discriminator: 0.04377559572458267\n",
            "ITERATION_NO.: 439 LOSS_Generator: 5.215043067932129 LOSS_Discriminator: 0.1253213733434677\n",
            "ITERATION_NO.: 440 LOSS_Generator: 5.701916694641113 LOSS_Discriminator: 0.23587925732135773\n",
            "ITERATION_NO.: 441 LOSS_Generator: 3.9423751831054688 LOSS_Discriminator: 0.17889145016670227\n",
            "ITERATION_NO.: 442 LOSS_Generator: 4.0192999839782715 LOSS_Discriminator: 0.16697297990322113\n",
            "ITERATION_NO.: 443 LOSS_Generator: 3.74861478805542 LOSS_Discriminator: 0.17957159876823425\n",
            "ITERATION_NO.: 444 LOSS_Generator: 4.3459930419921875 LOSS_Discriminator: 0.13174478709697723\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.2990007400512695 LOSS_Discriminator: 0.09917470812797546\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.797426223754883 LOSS_Discriminator: 0.03776541352272034\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.923923492431641 LOSS_Discriminator: 0.2633759081363678\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.180042266845703 LOSS_Discriminator: 0.22258055210113525\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.172684669494629 LOSS_Discriminator: 0.21143043041229248\n",
            "ITERATION_NO.: 450 LOSS_Generator: 4.82590389251709 LOSS_Discriminator: 0.09433979541063309\n",
            "ITERATION_NO.: 451 LOSS_Generator: 4.6237359046936035 LOSS_Discriminator: 0.1106138676404953\n",
            "ITERATION_NO.: 452 LOSS_Generator: 4.193343162536621 LOSS_Discriminator: 0.14314480125904083\n",
            "ITERATION_NO.: 453 LOSS_Generator: 4.003546237945557 LOSS_Discriminator: 0.16856200993061066\n",
            "ITERATION_NO.: 454 LOSS_Generator: 4.402050018310547 LOSS_Discriminator: 0.10945093631744385\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.012200832366943 LOSS_Discriminator: 0.18959954380989075\n",
            "ITERATION_NO.: 456 LOSS_Generator: 4.854649543762207 LOSS_Discriminator: 0.21979331970214844\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.151060104370117 LOSS_Discriminator: 0.09662087261676788\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.438985347747803 LOSS_Discriminator: 0.07239551842212677\n",
            "ITERATION_NO.: 459 LOSS_Generator: 3.8663363456726074 LOSS_Discriminator: 0.07791798561811447\n",
            "ITERATION_NO.: 460 LOSS_Generator: 4.945880889892578 LOSS_Discriminator: 0.0997946634888649\n",
            "ITERATION_NO.: 461 LOSS_Generator: 4.4630327224731445 LOSS_Discriminator: 0.14479616284370422\n",
            "ITERATION_NO.: 462 LOSS_Generator: 3.9242730140686035 LOSS_Discriminator: 0.07898475229740143\n",
            "ITERATION_NO.: 463 LOSS_Generator: 3.894989490509033 LOSS_Discriminator: 0.06196443736553192\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.0750226974487305 LOSS_Discriminator: 0.05956639349460602\n",
            "ITERATION_NO.: 465 LOSS_Generator: 6.01495885848999 LOSS_Discriminator: 0.04644528031349182\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.877456188201904 LOSS_Discriminator: 0.04533042013645172\n",
            "ITERATION_NO.: 467 LOSS_Generator: 6.123005390167236 LOSS_Discriminator: 0.13709694147109985\n",
            "ITERATION_NO.: 468 LOSS_Generator: 6.216867923736572 LOSS_Discriminator: 0.031545452773571014\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.617650985717773 LOSS_Discriminator: 0.2448442578315735\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.02556037902832 LOSS_Discriminator: 0.12929919362068176\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.616300106048584 LOSS_Discriminator: 0.2475341409444809\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.548986434936523 LOSS_Discriminator: 0.08459167182445526\n",
            "ITERATION_NO.: 473 LOSS_Generator: 4.626199722290039 LOSS_Discriminator: 0.3141658306121826\n",
            "ITERATION_NO.: 474 LOSS_Generator: 3.5965819358825684 LOSS_Discriminator: 0.14386829733848572\n",
            "ITERATION_NO.: 475 LOSS_Generator: 4.8268327713012695 LOSS_Discriminator: 0.20346175134181976\n",
            "ITERATION_NO.: 476 LOSS_Generator: 4.742806434631348 LOSS_Discriminator: 0.08238814026117325\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.177298545837402 LOSS_Discriminator: 0.09708017110824585\n",
            "ITERATION_NO.: 478 LOSS_Generator: 4.989819049835205 LOSS_Discriminator: 0.10649076104164124\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.0525031089782715 LOSS_Discriminator: 0.29727843403816223\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.864985466003418 LOSS_Discriminator: 0.20909979939460754\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.31125545501709 LOSS_Discriminator: 0.07798624038696289\n",
            "ITERATION_NO.: 482 LOSS_Generator: 4.741305351257324 LOSS_Discriminator: 0.23985978960990906\n",
            "ITERATION_NO.: 483 LOSS_Generator: 4.6129889488220215 LOSS_Discriminator: 0.15773792564868927\n",
            "ITERATION_NO.: 484 LOSS_Generator: 4.494171619415283 LOSS_Discriminator: 0.2539358139038086\n",
            "ITERATION_NO.: 485 LOSS_Generator: 4.116477012634277 LOSS_Discriminator: 0.18375296890735626\n",
            "ITERATION_NO.: 486 LOSS_Generator: 4.9025397300720215 LOSS_Discriminator: 0.11255834251642227\n",
            "ITERATION_NO.: 487 LOSS_Generator: 4.2529802322387695 LOSS_Discriminator: 0.12587344646453857\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.092259883880615 LOSS_Discriminator: 0.07611034065485\n",
            "ITERATION_NO.: 489 LOSS_Generator: 3.9853219985961914 LOSS_Discriminator: 0.2095060646533966\n",
            "ITERATION_NO.: 490 LOSS_Generator: 4.690587997436523 LOSS_Discriminator: 0.1795087456703186\n",
            "ITERATION_NO.: 491 LOSS_Generator: 4.0824809074401855 LOSS_Discriminator: 0.11055243760347366\n",
            "ITERATION_NO.: 492 LOSS_Generator: 3.7651381492614746 LOSS_Discriminator: 0.06591752916574478\n",
            "ITERATION_NO.: 493 LOSS_Generator: 4.261666774749756 LOSS_Discriminator: 0.13069038093090057\n",
            "ITERATION_NO.: 494 LOSS_Generator: 4.4654741287231445 LOSS_Discriminator: 0.17428605258464813\n",
            "ITERATION_NO.: 495 LOSS_Generator: 4.3391313552856445 LOSS_Discriminator: 0.09257210791110992\n",
            "ITERATION_NO.: 496 LOSS_Generator: 4.958266258239746 LOSS_Discriminator: 0.06029865890741348\n",
            "ITERATION_NO.: 497 LOSS_Generator: 4.912017822265625 LOSS_Discriminator: 0.040446557104587555\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.050826549530029 LOSS_Discriminator: 0.10941886901855469\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.30483341217041 LOSS_Discriminator: 0.17076033353805542\n",
            "ITERATION_NO.: 500 LOSS_Generator: 4.983121395111084 LOSS_Discriminator: 0.15651699900627136\n",
            "ITERATION_NO.: 501 LOSS_Generator: 4.588314533233643 LOSS_Discriminator: 0.22768431901931763\n",
            "ITERATION_NO.: 502 LOSS_Generator: 3.761930227279663 LOSS_Discriminator: 0.0930408388376236\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.325429916381836 LOSS_Discriminator: 0.1101415827870369\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.115405559539795 LOSS_Discriminator: 0.3353842496871948\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.3718976974487305 LOSS_Discriminator: 0.03505591303110123\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.196623802185059 LOSS_Discriminator: 0.06282054632902145\n",
            "ITERATION_NO.: 507 LOSS_Generator: 6.138688087463379 LOSS_Discriminator: 0.03935746103525162\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.532351493835449 LOSS_Discriminator: 0.1343626230955124\n",
            "ITERATION_NO.: 509 LOSS_Generator: 6.15794563293457 LOSS_Discriminator: 0.09090173989534378\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.779198169708252 LOSS_Discriminator: 0.10223472863435745\n",
            "ITERATION_NO.: 511 LOSS_Generator: 4.875838756561279 LOSS_Discriminator: 0.22115030884742737\n",
            "ITERATION_NO.: 512 LOSS_Generator: 4.573727607727051 LOSS_Discriminator: 0.1237274631857872\n",
            "ITERATION_NO.: 513 LOSS_Generator: 3.924125909805298 LOSS_Discriminator: 0.08333288133144379\n",
            "ITERATION_NO.: 514 LOSS_Generator: 4.448963165283203 LOSS_Discriminator: 0.15159936249256134\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.102063179016113 LOSS_Discriminator: 0.07688490301370621\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.790239334106445 LOSS_Discriminator: 0.02405521273612976\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.538414478302002 LOSS_Discriminator: 0.2054174542427063\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.246328353881836 LOSS_Discriminator: 0.15478640794754028\n",
            "ITERATION_NO.: 519 LOSS_Generator: 4.855916976928711 LOSS_Discriminator: 0.061761628836393356\n",
            "ITERATION_NO.: 520 LOSS_Generator: 4.398523330688477 LOSS_Discriminator: 0.03895401209592819\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.108965873718262 LOSS_Discriminator: 0.04032890126109123\n",
            "ITERATION_NO.: 522 LOSS_Generator: 4.634091377258301 LOSS_Discriminator: 0.12686121463775635\n",
            "ITERATION_NO.: 523 LOSS_Generator: 4.909388542175293 LOSS_Discriminator: 0.05486033484339714\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.288506507873535 LOSS_Discriminator: 0.03878568485379219\n",
            "ITERATION_NO.: 525 LOSS_Generator: 4.989917755126953 LOSS_Discriminator: 0.08987808227539062\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.177642822265625 LOSS_Discriminator: 0.15657752752304077\n",
            "ITERATION_NO.: 527 LOSS_Generator: 6.266575813293457 LOSS_Discriminator: 0.12174711376428604\n",
            "ITERATION_NO.: 528 LOSS_Generator: 4.81013822555542 LOSS_Discriminator: 0.08074475824832916\n",
            "ITERATION_NO.: 529 LOSS_Generator: 4.879056453704834 LOSS_Discriminator: 0.13625916838645935\n",
            "ITERATION_NO.: 530 LOSS_Generator: 4.007758140563965 LOSS_Discriminator: 0.16407924890518188\n",
            "ITERATION_NO.: 531 LOSS_Generator: 3.7821831703186035 LOSS_Discriminator: 0.28634893894195557\n",
            "ITERATION_NO.: 532 LOSS_Generator: 3.0481574535369873 LOSS_Discriminator: 0.172220379114151\n",
            "ITERATION_NO.: 533 LOSS_Generator: 4.3645710945129395 LOSS_Discriminator: 0.23252178728580475\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.608711242675781 LOSS_Discriminator: 0.06359691917896271\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.90268087387085 LOSS_Discriminator: 0.1933003067970276\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.293946743011475 LOSS_Discriminator: 0.2566676139831543\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.847902297973633 LOSS_Discriminator: 0.01805565506219864\n",
            "ITERATION_NO.: 538 LOSS_Generator: 6.436622619628906 LOSS_Discriminator: 0.05072379112243652\n",
            "ITERATION_NO.: 539 LOSS_Generator: 6.246270656585693 LOSS_Discriminator: 0.11666618287563324\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.722261428833008 LOSS_Discriminator: 0.14773815870285034\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.692826271057129 LOSS_Discriminator: 0.1839945912361145\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.362575054168701 LOSS_Discriminator: 0.12343044579029083\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.032832145690918 LOSS_Discriminator: 0.24720491468906403\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.698817253112793 LOSS_Discriminator: 0.11427585780620575\n",
            "ITERATION_NO.: 545 LOSS_Generator: 4.911701679229736 LOSS_Discriminator: 0.19633400440216064\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.132107734680176 LOSS_Discriminator: 0.1933981478214264\n",
            "ITERATION_NO.: 547 LOSS_Generator: 4.190118312835693 LOSS_Discriminator: 0.032596051692962646\n",
            "ITERATION_NO.: 548 LOSS_Generator: 3.968262195587158 LOSS_Discriminator: 0.08174324035644531\n",
            "ITERATION_NO.: 549 LOSS_Generator: 4.369487762451172 LOSS_Discriminator: 0.10466130822896957\n",
            "ITERATION_NO.: 550 LOSS_Generator: 4.626180648803711 LOSS_Discriminator: 0.1266273856163025\n",
            "ITERATION_NO.: 551 LOSS_Generator: 4.907809257507324 LOSS_Discriminator: 0.09047143906354904\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.242382049560547 LOSS_Discriminator: 0.10250511765480042\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.062261581420898 LOSS_Discriminator: 0.1294921189546585\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.140477180480957 LOSS_Discriminator: 0.04469281807541847\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.910151481628418 LOSS_Discriminator: 0.03206159546971321\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.877216339111328 LOSS_Discriminator: 0.05585359036922455\n",
            "ITERATION_NO.: 557 LOSS_Generator: 6.229475498199463 LOSS_Discriminator: 0.14709050953388214\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.741105556488037 LOSS_Discriminator: 0.06934147328138351\n",
            "ITERATION_NO.: 559 LOSS_Generator: 4.689947128295898 LOSS_Discriminator: 0.045604437589645386\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.107795715332031 LOSS_Discriminator: 0.14236918091773987\n",
            "ITERATION_NO.: 561 LOSS_Generator: 4.253424167633057 LOSS_Discriminator: 0.10608096420764923\n",
            "ITERATION_NO.: 562 LOSS_Generator: 4.338197708129883 LOSS_Discriminator: 0.061354897916316986\n",
            "ITERATION_NO.: 563 LOSS_Generator: 4.808811187744141 LOSS_Discriminator: 0.29532620310783386\n",
            "ITERATION_NO.: 564 LOSS_Generator: 4.681130409240723 LOSS_Discriminator: 0.16708266735076904\n",
            "ITERATION_NO.: 565 LOSS_Generator: 4.233473777770996 LOSS_Discriminator: 0.3071575462818146\n",
            "ITERATION_NO.: 566 LOSS_Generator: 4.08791971206665 LOSS_Discriminator: 0.1307632029056549\n",
            "ITERATION_NO.: 567 LOSS_Generator: 4.362504005432129 LOSS_Discriminator: 0.1366492509841919\n",
            "ITERATION_NO.: 568 LOSS_Generator: 4.131536483764648 LOSS_Discriminator: 0.16753137111663818\n",
            "ITERATION_NO.: 569 LOSS_Generator: 4.428780555725098 LOSS_Discriminator: 0.1659686118364334\n",
            "ITERATION_NO.: 570 LOSS_Generator: 4.900422096252441 LOSS_Discriminator: 0.18217876553535461\n",
            "ITERATION_NO.: 571 LOSS_Generator: 4.978666305541992 LOSS_Discriminator: 0.07399776577949524\n",
            "ITERATION_NO.: 572 LOSS_Generator: 6.189303398132324 LOSS_Discriminator: 0.038499876856803894\n",
            "ITERATION_NO.: 573 LOSS_Generator: 6.543127059936523 LOSS_Discriminator: 0.009797651320695877\n",
            "ITERATION_NO.: 574 LOSS_Generator: 6.256409645080566 LOSS_Discriminator: 0.3362141251564026\n",
            "ITERATION_NO.: 575 LOSS_Generator: 6.187923431396484 LOSS_Discriminator: 0.31617867946624756\n",
            "ITERATION_NO.: 576 LOSS_Generator: 6.168116092681885 LOSS_Discriminator: 0.17849744856357574\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.085904121398926 LOSS_Discriminator: 0.0572960190474987\n",
            "ITERATION_NO.: 578 LOSS_Generator: 4.4835710525512695 LOSS_Discriminator: 0.20380686223506927\n",
            "ITERATION_NO.: 579 LOSS_Generator: 3.8985514640808105 LOSS_Discriminator: 0.11465105414390564\n",
            "ITERATION_NO.: 580 LOSS_Generator: 3.145425796508789 LOSS_Discriminator: 0.14069168269634247\n",
            "ITERATION_NO.: 581 LOSS_Generator: 3.5554912090301514 LOSS_Discriminator: 0.30987024307250977\n",
            "ITERATION_NO.: 582 LOSS_Generator: 4.955270767211914 LOSS_Discriminator: 0.2679406404495239\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.106025695800781 LOSS_Discriminator: 0.07766853272914886\n",
            "ITERATION_NO.: 584 LOSS_Generator: 5.282849311828613 LOSS_Discriminator: 0.15146169066429138\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.15090274810791 LOSS_Discriminator: 0.1606222540140152\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.205412864685059 LOSS_Discriminator: 0.22804653644561768\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.241891860961914 LOSS_Discriminator: 0.19038456678390503\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.252151012420654 LOSS_Discriminator: 0.07551532983779907\n",
            "ITERATION_NO.: 589 LOSS_Generator: 4.80816125869751 LOSS_Discriminator: 0.24937795102596283\n",
            "ITERATION_NO.: 590 LOSS_Generator: 3.7268924713134766 LOSS_Discriminator: 0.09066451340913773\n",
            "ITERATION_NO.: 591 LOSS_Generator: 3.729365348815918 LOSS_Discriminator: 0.17445634305477142\n",
            "ITERATION_NO.: 592 LOSS_Generator: 3.3088412284851074 LOSS_Discriminator: 0.09549590945243835\n",
            "ITERATION_NO.: 593 LOSS_Generator: 4.50663948059082 LOSS_Discriminator: 0.2007623016834259\n",
            "ITERATION_NO.: 594 LOSS_Generator: 4.483999252319336 LOSS_Discriminator: 0.055143266916275024\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.988646030426025 LOSS_Discriminator: 0.13955077528953552\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.825784206390381 LOSS_Discriminator: 0.03501075878739357\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.779892444610596 LOSS_Discriminator: 0.03606017306447029\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.281394958496094 LOSS_Discriminator: 0.23267972469329834\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.45182991027832 LOSS_Discriminator: 0.1544347107410431\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.220968246459961 LOSS_Discriminator: 0.10313889384269714\n",
            "ITERATION_NO.: 601 LOSS_Generator: 5.242586135864258 LOSS_Discriminator: 0.11700396239757538\n",
            "ITERATION_NO.: 602 LOSS_Generator: 4.541232109069824 LOSS_Discriminator: 0.06246691197156906\n",
            "ITERATION_NO.: 603 LOSS_Generator: 4.127632141113281 LOSS_Discriminator: 0.08247631043195724\n",
            "ITERATION_NO.: 604 LOSS_Generator: 4.257437705993652 LOSS_Discriminator: 0.379701167345047\n",
            "ITERATION_NO.: 605 LOSS_Generator: 4.812898635864258 LOSS_Discriminator: 0.04588188976049423\n",
            "ITERATION_NO.: 606 LOSS_Generator: 4.9957709312438965 LOSS_Discriminator: 0.09707281738519669\n",
            "ITERATION_NO.: 607 LOSS_Generator: 4.792016983032227 LOSS_Discriminator: 0.08275596797466278\n",
            "ITERATION_NO.: 608 LOSS_Generator: 4.72389554977417 LOSS_Discriminator: 0.07651927322149277\n",
            "ITERATION_NO.: 609 LOSS_Generator: 5.184983253479004 LOSS_Discriminator: 0.09109704196453094\n",
            "ITERATION_NO.: 610 LOSS_Generator: 5.316685676574707 LOSS_Discriminator: 0.10606878995895386\n",
            "ITERATION_NO.: 611 LOSS_Generator: 5.725759029388428 LOSS_Discriminator: 0.03056171163916588\n",
            "ITERATION_NO.: 612 LOSS_Generator: 5.730562210083008 LOSS_Discriminator: 0.034196190536022186\n",
            "ITERATION_NO.: 613 LOSS_Generator: 5.174287796020508 LOSS_Discriminator: 0.025800269097089767\n",
            "ITERATION_NO.: 614 LOSS_Generator: 5.580312728881836 LOSS_Discriminator: 0.02109316736459732\n",
            "ITERATION_NO.: 615 LOSS_Generator: 5.249477386474609 LOSS_Discriminator: 0.290210098028183\n",
            "ITERATION_NO.: 616 LOSS_Generator: 5.354997634887695 LOSS_Discriminator: 0.16185931861400604\n",
            "ITERATION_NO.: 617 LOSS_Generator: 3.886739730834961 LOSS_Discriminator: 0.12958386540412903\n",
            "ITERATION_NO.: 618 LOSS_Generator: 3.9727916717529297 LOSS_Discriminator: 0.14439666271209717\n",
            "ITERATION_NO.: 619 LOSS_Generator: 4.720683574676514 LOSS_Discriminator: 0.10355720669031143\n",
            "ITERATION_NO.: 620 LOSS_Generator: 4.308539390563965 LOSS_Discriminator: 0.3056579828262329\n",
            "ITERATION_NO.: 621 LOSS_Generator: 4.261871337890625 LOSS_Discriminator: 0.18193025887012482\n",
            "ITERATION_NO.: 622 LOSS_Generator: 4.887625217437744 LOSS_Discriminator: 0.09376479685306549\n",
            "ITERATION_NO.: 623 LOSS_Generator: 5.365216255187988 LOSS_Discriminator: 0.1304604560136795\n",
            "ITERATION_NO.: 624 LOSS_Generator: 5.5778584480285645 LOSS_Discriminator: 0.11546070873737335\n",
            "ITERATION_NO.: 625 LOSS_Generator: 5.242736339569092 LOSS_Discriminator: 0.19779440760612488\n",
            "ITERATION_NO.: 626 LOSS_Generator: 4.501796722412109 LOSS_Discriminator: 0.08901528269052505\n",
            "ITERATION_NO.: 627 LOSS_Generator: 4.060576438903809 LOSS_Discriminator: 0.1961638629436493\n",
            "ITERATION_NO.: 628 LOSS_Generator: 3.8137528896331787 LOSS_Discriminator: 0.05842454731464386\n",
            "ITERATION_NO.: 629 LOSS_Generator: 4.27001953125 LOSS_Discriminator: 0.14235766232013702\n",
            "ITERATION_NO.: 630 LOSS_Generator: 3.6627111434936523 LOSS_Discriminator: 0.3152907192707062\n",
            "ITERATION_NO.: 631 LOSS_Generator: 4.007192611694336 LOSS_Discriminator: 0.12913507223129272\n",
            "ITERATION_NO.: 632 LOSS_Generator: 3.630802631378174 LOSS_Discriminator: 0.09521207213401794\n",
            "ITERATION_NO.: 633 LOSS_Generator: 3.7677364349365234 LOSS_Discriminator: 0.14793801307678223\n",
            "ITERATION_NO.: 634 LOSS_Generator: 4.881404876708984 LOSS_Discriminator: 0.08419601619243622\n",
            "ITERATION_NO.: 635 LOSS_Generator: 4.799412727355957 LOSS_Discriminator: 0.025650925934314728\n",
            "ITERATION_NO.: 636 LOSS_Generator: 6.180815696716309 LOSS_Discriminator: 0.018367044627666473\n",
            "ITERATION_NO.: 637 LOSS_Generator: 6.773439884185791 LOSS_Discriminator: 0.10955536365509033\n",
            "ITERATION_NO.: 638 LOSS_Generator: 7.111246109008789 LOSS_Discriminator: 0.0112947141751647\n",
            "ITERATION_NO.: 639 LOSS_Generator: 6.4166083335876465 LOSS_Discriminator: 0.43192288279533386\n",
            "ITERATION_NO.: 640 LOSS_Generator: 6.589254379272461 LOSS_Discriminator: 0.053002335131168365\n",
            "ITERATION_NO.: 641 LOSS_Generator: 5.762436866760254 LOSS_Discriminator: 0.07133837044239044\n",
            "ITERATION_NO.: 642 LOSS_Generator: 5.3011555671691895 LOSS_Discriminator: 0.08441723883152008\n",
            "ITERATION_NO.: 643 LOSS_Generator: 4.849533557891846 LOSS_Discriminator: 0.0644751489162445\n",
            "ITERATION_NO.: 644 LOSS_Generator: 4.604770183563232 LOSS_Discriminator: 0.09706674516201019\n",
            "ITERATION_NO.: 645 LOSS_Generator: 4.571922302246094 LOSS_Discriminator: 0.07186269760131836\n",
            "ITERATION_NO.: 646 LOSS_Generator: 4.5170698165893555 LOSS_Discriminator: 0.1842796504497528\n",
            "ITERATION_NO.: 647 LOSS_Generator: 4.46712589263916 LOSS_Discriminator: 0.16297508776187897\n",
            "ITERATION_NO.: 648 LOSS_Generator: 4.756918430328369 LOSS_Discriminator: 0.1002645269036293\n",
            "ITERATION_NO.: 649 LOSS_Generator: 4.502048492431641 LOSS_Discriminator: 0.08534769713878632\n",
            "ITERATION_NO.: 650 LOSS_Generator: 4.575355529785156 LOSS_Discriminator: 0.1317744255065918\n",
            "ITERATION_NO.: 651 LOSS_Generator: 4.435778617858887 LOSS_Discriminator: 0.11061875522136688\n",
            "ITERATION_NO.: 652 LOSS_Generator: 4.44159460067749 LOSS_Discriminator: 0.031903766095638275\n",
            "ITERATION_NO.: 653 LOSS_Generator: 5.105093955993652 LOSS_Discriminator: 0.05929813161492348\n",
            "ITERATION_NO.: 654 LOSS_Generator: 4.586125373840332 LOSS_Discriminator: 0.11640927940607071\n",
            "ITERATION_NO.: 655 LOSS_Generator: 5.179640769958496 LOSS_Discriminator: 0.12662860751152039\n",
            "ITERATION_NO.: 656 LOSS_Generator: 4.053747177124023 LOSS_Discriminator: 0.21729952096939087\n",
            "ITERATION_NO.: 657 LOSS_Generator: 4.876062393188477 LOSS_Discriminator: 0.17497733235359192\n",
            "ITERATION_NO.: 658 LOSS_Generator: 4.758762359619141 LOSS_Discriminator: 0.09690897166728973\n",
            "ITERATION_NO.: 659 LOSS_Generator: 4.769789695739746 LOSS_Discriminator: 0.33032190799713135\n",
            "ITERATION_NO.: 660 LOSS_Generator: 5.278384208679199 LOSS_Discriminator: 0.0338706448674202\n",
            "ITERATION_NO.: 661 LOSS_Generator: 5.53400993347168 LOSS_Discriminator: 0.24163229763507843\n",
            "ITERATION_NO.: 662 LOSS_Generator: 5.640685081481934 LOSS_Discriminator: 0.14824113249778748\n",
            "ITERATION_NO.: 663 LOSS_Generator: 4.801939010620117 LOSS_Discriminator: 0.27695947885513306\n",
            "ITERATION_NO.: 664 LOSS_Generator: 4.353321075439453 LOSS_Discriminator: 0.17114533483982086\n",
            "ITERATION_NO.: 665 LOSS_Generator: 3.6689209938049316 LOSS_Discriminator: 0.07336296141147614\n",
            "ITERATION_NO.: 666 LOSS_Generator: 4.829209327697754 LOSS_Discriminator: 0.10222820937633514\n",
            "ITERATION_NO.: 667 LOSS_Generator: 5.300302505493164 LOSS_Discriminator: 0.17912499606609344\n",
            "ITERATION_NO.: 668 LOSS_Generator: 5.166027545928955 LOSS_Discriminator: 0.11002696305513382\n",
            "ITERATION_NO.: 669 LOSS_Generator: 5.704850196838379 LOSS_Discriminator: 0.10606610029935837\n",
            "ITERATION_NO.: 670 LOSS_Generator: 5.503009796142578 LOSS_Discriminator: 0.08721017837524414\n",
            "ITERATION_NO.: 671 LOSS_Generator: 5.85203742980957 LOSS_Discriminator: 0.17572931945323944\n",
            "ITERATION_NO.: 672 LOSS_Generator: 4.911325454711914 LOSS_Discriminator: 0.1846209168434143\n",
            "ITERATION_NO.: 673 LOSS_Generator: 4.203010559082031 LOSS_Discriminator: 0.26137489080429077\n",
            "ITERATION_NO.: 674 LOSS_Generator: 3.4261789321899414 LOSS_Discriminator: 0.29923275113105774\n",
            "ITERATION_NO.: 675 LOSS_Generator: 3.6022415161132812 LOSS_Discriminator: 0.1890137642621994\n",
            "ITERATION_NO.: 676 LOSS_Generator: 3.9512691497802734 LOSS_Discriminator: 0.14231720566749573\n",
            "ITERATION_NO.: 677 LOSS_Generator: 5.503920078277588 LOSS_Discriminator: 0.06379370391368866\n",
            "ITERATION_NO.: 678 LOSS_Generator: 6.0863471031188965 LOSS_Discriminator: 0.1758948266506195\n",
            "ITERATION_NO.: 679 LOSS_Generator: 6.489161491394043 LOSS_Discriminator: 0.016756456345319748\n",
            "ITERATION_NO.: 680 LOSS_Generator: 6.199294090270996 LOSS_Discriminator: 0.3549868166446686\n",
            "ITERATION_NO.: 681 LOSS_Generator: 5.780464172363281 LOSS_Discriminator: 0.27048373222351074\n",
            "ITERATION_NO.: 682 LOSS_Generator: 4.896948337554932 LOSS_Discriminator: 0.23782125115394592\n",
            "ITERATION_NO.: 683 LOSS_Generator: 4.008939266204834 LOSS_Discriminator: 0.13195860385894775\n",
            "ITERATION_NO.: 684 LOSS_Generator: 3.849581003189087 LOSS_Discriminator: 0.15492337942123413\n",
            "ITERATION_NO.: 685 LOSS_Generator: 3.486572742462158 LOSS_Discriminator: 0.1166103184223175\n",
            "ITERATION_NO.: 686 LOSS_Generator: 4.357500076293945 LOSS_Discriminator: 0.14620649814605713\n",
            "ITERATION_NO.: 687 LOSS_Generator: 4.59857177734375 LOSS_Discriminator: 0.12009784579277039\n",
            "ITERATION_NO.: 688 LOSS_Generator: 5.377842903137207 LOSS_Discriminator: 0.12453332543373108\n",
            "ITERATION_NO.: 689 LOSS_Generator: 5.7006120681762695 LOSS_Discriminator: 0.033503636717796326\n",
            "ITERATION_NO.: 690 LOSS_Generator: 5.51870059967041 LOSS_Discriminator: 0.2521747052669525\n",
            "ITERATION_NO.: 691 LOSS_Generator: 4.910804748535156 LOSS_Discriminator: 0.17589285969734192\n",
            "ITERATION_NO.: 692 LOSS_Generator: 4.800533294677734 LOSS_Discriminator: 0.07858439534902573\n",
            "ITERATION_NO.: 693 LOSS_Generator: 4.035666465759277 LOSS_Discriminator: 0.1154242604970932\n",
            "ITERATION_NO.: 694 LOSS_Generator: 3.8538784980773926 LOSS_Discriminator: 0.16868610680103302\n",
            "ITERATION_NO.: 695 LOSS_Generator: 3.7800991535186768 LOSS_Discriminator: 0.07620817422866821\n",
            "ITERATION_NO.: 696 LOSS_Generator: 4.136455059051514 LOSS_Discriminator: 0.1069796159863472\n",
            "ITERATION_NO.: 697 LOSS_Generator: 4.694661617279053 LOSS_Discriminator: 0.10713629424571991\n",
            "ITERATION_NO.: 698 LOSS_Generator: 5.224590301513672 LOSS_Discriminator: 0.20961560308933258\n",
            "ITERATION_NO.: 699 LOSS_Generator: 5.8451337814331055 LOSS_Discriminator: 0.098385751247406\n",
            "ITERATION_NO.: 700 LOSS_Generator: 5.880393981933594 LOSS_Discriminator: 0.02556898444890976\n",
            "ITERATION_NO.: 701 LOSS_Generator: 5.779784202575684 LOSS_Discriminator: 0.08191888779401779\n",
            "ITERATION_NO.: 702 LOSS_Generator: 6.156291484832764 LOSS_Discriminator: 0.1053398996591568\n",
            "ITERATION_NO.: 703 LOSS_Generator: 5.676962852478027 LOSS_Discriminator: 0.19186092913150787\n",
            "ITERATION_NO.: 704 LOSS_Generator: 5.409452438354492 LOSS_Discriminator: 0.08449715375900269\n",
            "ITERATION_NO.: 705 LOSS_Generator: 3.899099588394165 LOSS_Discriminator: 0.19293606281280518\n",
            "ITERATION_NO.: 706 LOSS_Generator: 4.402978897094727 LOSS_Discriminator: 0.05183977633714676\n",
            "ITERATION_NO.: 707 LOSS_Generator: 3.7721433639526367 LOSS_Discriminator: 0.11361213028430939\n",
            "ITERATION_NO.: 708 LOSS_Generator: 4.149244785308838 LOSS_Discriminator: 0.19174736738204956\n",
            "ITERATION_NO.: 709 LOSS_Generator: 3.79176664352417 LOSS_Discriminator: 0.07280682772397995\n",
            "ITERATION_NO.: 710 LOSS_Generator: 4.776162147521973 LOSS_Discriminator: 0.05984003096818924\n",
            "ITERATION_NO.: 711 LOSS_Generator: 5.0785369873046875 LOSS_Discriminator: 0.1775520145893097\n",
            "ITERATION_NO.: 712 LOSS_Generator: 5.5236124992370605 LOSS_Discriminator: 0.11337023973464966\n",
            "ITERATION_NO.: 713 LOSS_Generator: 4.685508728027344 LOSS_Discriminator: 0.17947514355182648\n",
            "ITERATION_NO.: 714 LOSS_Generator: 4.513197898864746 LOSS_Discriminator: 0.1679125428199768\n",
            "ITERATION_NO.: 715 LOSS_Generator: 3.9797139167785645 LOSS_Discriminator: 0.03570924699306488\n",
            "ITERATION_NO.: 716 LOSS_Generator: 3.665010929107666 LOSS_Discriminator: 0.18316535651683807\n",
            "ITERATION_NO.: 717 LOSS_Generator: 4.192020893096924 LOSS_Discriminator: 0.08466541022062302\n",
            "ITERATION_NO.: 718 LOSS_Generator: 4.4845967292785645 LOSS_Discriminator: 0.08544133603572845\n",
            "ITERATION_NO.: 719 LOSS_Generator: 5.249747276306152 LOSS_Discriminator: 0.09614460170269012\n",
            "ITERATION_NO.: 720 LOSS_Generator: 6.0670166015625 LOSS_Discriminator: 0.10319472849369049\n",
            "ITERATION_NO.: 721 LOSS_Generator: 5.282459259033203 LOSS_Discriminator: 0.3495838940143585\n",
            "ITERATION_NO.: 722 LOSS_Generator: 5.24263858795166 LOSS_Discriminator: 0.07360075414180756\n",
            "ITERATION_NO.: 723 LOSS_Generator: 4.009883880615234 LOSS_Discriminator: 0.21598587930202484\n",
            "ITERATION_NO.: 724 LOSS_Generator: 3.0112266540527344 LOSS_Discriminator: 0.18170633912086487\n",
            "ITERATION_NO.: 725 LOSS_Generator: 3.108072280883789 LOSS_Discriminator: 0.18760094046592712\n",
            "ITERATION_NO.: 726 LOSS_Generator: 4.763509750366211 LOSS_Discriminator: 0.21670441329479218\n",
            "ITERATION_NO.: 727 LOSS_Generator: 5.007277488708496 LOSS_Discriminator: 0.07575315982103348\n",
            "ITERATION_NO.: 728 LOSS_Generator: 6.458297252655029 LOSS_Discriminator: 0.21852412819862366\n",
            "ITERATION_NO.: 729 LOSS_Generator: 5.73698091506958 LOSS_Discriminator: 0.25823140144348145\n",
            "ITERATION_NO.: 730 LOSS_Generator: 6.1983489990234375 LOSS_Discriminator: 0.18759453296661377\n",
            "ITERATION_NO.: 731 LOSS_Generator: 5.45503044128418 LOSS_Discriminator: 0.05615193396806717\n",
            "ITERATION_NO.: 732 LOSS_Generator: 5.594545364379883 LOSS_Discriminator: 0.09328737109899521\n",
            "ITERATION_NO.: 733 LOSS_Generator: 4.763944625854492 LOSS_Discriminator: 0.10123798996210098\n",
            "ITERATION_NO.: 734 LOSS_Generator: 4.023723125457764 LOSS_Discriminator: 0.19721567630767822\n",
            "ITERATION_NO.: 735 LOSS_Generator: 3.251481056213379 LOSS_Discriminator: 0.2240232527256012\n",
            "ITERATION_NO.: 736 LOSS_Generator: 4.081638813018799 LOSS_Discriminator: 0.2110232412815094\n",
            "ITERATION_NO.: 737 LOSS_Generator: 4.6862993240356445 LOSS_Discriminator: 0.2377907633781433\n",
            "ITERATION_NO.: 738 LOSS_Generator: 5.2000203132629395 LOSS_Discriminator: 0.05317944288253784\n",
            "ITERATION_NO.: 739 LOSS_Generator: 7.1416778564453125 LOSS_Discriminator: 0.13474315404891968\n",
            "ITERATION_NO.: 740 LOSS_Generator: 7.096845626831055 LOSS_Discriminator: 0.20678091049194336\n",
            "ITERATION_NO.: 741 LOSS_Generator: 6.183315753936768 LOSS_Discriminator: 0.16944614052772522\n",
            "ITERATION_NO.: 742 LOSS_Generator: 5.0961198806762695 LOSS_Discriminator: 0.13330979645252228\n",
            "ITERATION_NO.: 743 LOSS_Generator: 4.549274921417236 LOSS_Discriminator: 0.019289888441562653\n",
            "ITERATION_NO.: 744 LOSS_Generator: 4.026432991027832 LOSS_Discriminator: 0.0326460525393486\n",
            "ITERATION_NO.: 745 LOSS_Generator: 4.772953033447266 LOSS_Discriminator: 0.10103665292263031\n",
            "ITERATION_NO.: 746 LOSS_Generator: 4.732476234436035 LOSS_Discriminator: 0.15529796481132507\n",
            "ITERATION_NO.: 747 LOSS_Generator: 4.588451385498047 LOSS_Discriminator: 0.2523179054260254\n",
            "ITERATION_NO.: 748 LOSS_Generator: 5.2093987464904785 LOSS_Discriminator: 0.0346485897898674\n",
            "ITERATION_NO.: 749 LOSS_Generator: 5.1823272705078125 LOSS_Discriminator: 0.08779667317867279\n",
            "ITERATION_NO.: 750 LOSS_Generator: 5.783580780029297 LOSS_Discriminator: 0.18081974983215332\n",
            "ITERATION_NO.: 751 LOSS_Generator: 5.176518440246582 LOSS_Discriminator: 0.32608067989349365\n",
            "ITERATION_NO.: 752 LOSS_Generator: 4.967711925506592 LOSS_Discriminator: 0.1255892813205719\n",
            "ITERATION_NO.: 753 LOSS_Generator: 4.761909484863281 LOSS_Discriminator: 0.14360378682613373\n",
            "ITERATION_NO.: 754 LOSS_Generator: 4.22285270690918 LOSS_Discriminator: 0.12421216070652008\n",
            "ITERATION_NO.: 755 LOSS_Generator: 4.32204008102417 LOSS_Discriminator: 0.0646466612815857\n",
            "ITERATION_NO.: 756 LOSS_Generator: 4.120936393737793 LOSS_Discriminator: 0.10809967666864395\n",
            "ITERATION_NO.: 757 LOSS_Generator: 4.065595626831055 LOSS_Discriminator: 0.050924114882946014\n",
            "ITERATION_NO.: 758 LOSS_Generator: 4.758026123046875 LOSS_Discriminator: 0.046141788363456726\n",
            "ITERATION_NO.: 759 LOSS_Generator: 4.859332084655762 LOSS_Discriminator: 0.1192493587732315\n",
            "ITERATION_NO.: 760 LOSS_Generator: 5.02402400970459 LOSS_Discriminator: 0.18338802456855774\n",
            "ITERATION_NO.: 761 LOSS_Generator: 4.725134372711182 LOSS_Discriminator: 0.11140970885753632\n",
            "ITERATION_NO.: 762 LOSS_Generator: 4.167649269104004 LOSS_Discriminator: 0.050800587981939316\n",
            "ITERATION_NO.: 763 LOSS_Generator: 4.433235168457031 LOSS_Discriminator: 0.11472627520561218\n",
            "ITERATION_NO.: 764 LOSS_Generator: 5.099476337432861 LOSS_Discriminator: 0.10580025613307953\n",
            "ITERATION_NO.: 765 LOSS_Generator: 4.746067047119141 LOSS_Discriminator: 0.12266093492507935\n",
            "ITERATION_NO.: 766 LOSS_Generator: 5.14457893371582 LOSS_Discriminator: 0.029274191707372665\n",
            "ITERATION_NO.: 767 LOSS_Generator: 4.991586208343506 LOSS_Discriminator: 0.09219823032617569\n",
            "ITERATION_NO.: 768 LOSS_Generator: 5.371573448181152 LOSS_Discriminator: 0.19438189268112183\n",
            "ITERATION_NO.: 769 LOSS_Generator: 5.107863426208496 LOSS_Discriminator: 0.33931565284729004\n",
            "ITERATION_NO.: 770 LOSS_Generator: 4.17649507522583 LOSS_Discriminator: 0.24813461303710938\n",
            "ITERATION_NO.: 771 LOSS_Generator: 3.562533140182495 LOSS_Discriminator: 0.24034707248210907\n",
            "ITERATION_NO.: 772 LOSS_Generator: 3.8289809226989746 LOSS_Discriminator: 0.3101910650730133\n",
            "ITERATION_NO.: 773 LOSS_Generator: 5.079636573791504 LOSS_Discriminator: 0.144585520029068\n",
            "ITERATION_NO.: 774 LOSS_Generator: 4.9301371574401855 LOSS_Discriminator: 0.23233433067798615\n",
            "ITERATION_NO.: 775 LOSS_Generator: 4.398392677307129 LOSS_Discriminator: 0.07748542726039886\n",
            "ITERATION_NO.: 776 LOSS_Generator: 4.992585182189941 LOSS_Discriminator: 0.08285528421401978\n",
            "ITERATION_NO.: 777 LOSS_Generator: 5.357126235961914 LOSS_Discriminator: 0.022958695888519287\n",
            "ITERATION_NO.: 778 LOSS_Generator: 5.359427452087402 LOSS_Discriminator: 0.25693240761756897\n",
            "ITERATION_NO.: 779 LOSS_Generator: 4.989534854888916 LOSS_Discriminator: 0.20358923077583313\n",
            "ITERATION_NO.: 780 LOSS_Generator: 5.406786918640137 LOSS_Discriminator: 0.07990790903568268\n",
            "ITERATION_NO.: 781 LOSS_Generator: 5.021902084350586 LOSS_Discriminator: 0.046062469482421875\n",
            "ITERATION_NO.: 782 LOSS_Generator: 4.875667572021484 LOSS_Discriminator: 0.31635552644729614\n",
            "ITERATION_NO.: 783 LOSS_Generator: 4.613528251647949 LOSS_Discriminator: 0.26335105299949646\n",
            "ITERATION_NO.: 784 LOSS_Generator: 4.408888816833496 LOSS_Discriminator: 0.20187783241271973\n",
            "ITERATION_NO.: 785 LOSS_Generator: 4.186958312988281 LOSS_Discriminator: 0.325581818819046\n",
            "ITERATION_NO.: 786 LOSS_Generator: 5.286046028137207 LOSS_Discriminator: 0.13310915231704712\n",
            "ITERATION_NO.: 787 LOSS_Generator: 5.129180431365967 LOSS_Discriminator: 0.15771493315696716\n",
            "ITERATION_NO.: 788 LOSS_Generator: 6.917665004730225 LOSS_Discriminator: 0.13087451457977295\n",
            "ITERATION_NO.: 789 LOSS_Generator: 6.047482013702393 LOSS_Discriminator: 0.05906860530376434\n",
            "ITERATION_NO.: 790 LOSS_Generator: 6.361908435821533 LOSS_Discriminator: 0.22063446044921875\n",
            "ITERATION_NO.: 791 LOSS_Generator: 5.441448211669922 LOSS_Discriminator: 0.21518853306770325\n",
            "ITERATION_NO.: 792 LOSS_Generator: 4.432892799377441 LOSS_Discriminator: 0.2345527708530426\n",
            "ITERATION_NO.: 793 LOSS_Generator: 3.969282388687134 LOSS_Discriminator: 0.1865268051624298\n",
            "ITERATION_NO.: 794 LOSS_Generator: 3.39894962310791 LOSS_Discriminator: 0.24750621616840363\n",
            "ITERATION_NO.: 795 LOSS_Generator: 4.855212211608887 LOSS_Discriminator: 0.19339250028133392\n",
            "ITERATION_NO.: 796 LOSS_Generator: 5.433027267456055 LOSS_Discriminator: 0.028527649119496346\n",
            "ITERATION_NO.: 797 LOSS_Generator: 5.940008163452148 LOSS_Discriminator: 0.1271285116672516\n",
            "ITERATION_NO.: 798 LOSS_Generator: 6.458579063415527 LOSS_Discriminator: 0.09919609129428864\n",
            "ITERATION_NO.: 799 LOSS_Generator: 6.285325527191162 LOSS_Discriminator: 0.13586673140525818\n",
            "ITERATION_NO.: 800 LOSS_Generator: 5.603388786315918 LOSS_Discriminator: 0.1283033937215805\n",
            "ITERATION_NO.: 801 LOSS_Generator: 5.557450771331787 LOSS_Discriminator: 0.05197504907846451\n",
            "ITERATION_NO.: 802 LOSS_Generator: 4.872412204742432 LOSS_Discriminator: 0.024688515812158585\n",
            "ITERATION_NO.: 803 LOSS_Generator: 4.924826622009277 LOSS_Discriminator: 0.039757221937179565\n",
            "ITERATION_NO.: 804 LOSS_Generator: 4.527984619140625 LOSS_Discriminator: 0.09670211374759674\n",
            "ITERATION_NO.: 805 LOSS_Generator: 4.260031700134277 LOSS_Discriminator: 0.1439913809299469\n",
            "ITERATION_NO.: 806 LOSS_Generator: 3.856990337371826 LOSS_Discriminator: 0.08225138485431671\n",
            "ITERATION_NO.: 807 LOSS_Generator: 3.7352070808410645 LOSS_Discriminator: 0.09825165569782257\n",
            "ITERATION_NO.: 808 LOSS_Generator: 4.0588788986206055 LOSS_Discriminator: 0.06008448451757431\n",
            "ITERATION_NO.: 809 LOSS_Generator: 4.277294158935547 LOSS_Discriminator: 0.1237947940826416\n",
            "ITERATION_NO.: 810 LOSS_Generator: 4.482879161834717 LOSS_Discriminator: 0.18678095936775208\n",
            "ITERATION_NO.: 811 LOSS_Generator: 4.578272819519043 LOSS_Discriminator: 0.15842415392398834\n",
            "ITERATION_NO.: 812 LOSS_Generator: 4.429304599761963 LOSS_Discriminator: 0.02914382703602314\n",
            "ITERATION_NO.: 813 LOSS_Generator: 5.060891151428223 LOSS_Discriminator: 0.06966988742351532\n",
            "ITERATION_NO.: 814 LOSS_Generator: 5.5248703956604 LOSS_Discriminator: 0.019456785172224045\n",
            "ITERATION_NO.: 815 LOSS_Generator: 5.812599182128906 LOSS_Discriminator: 0.015624885447323322\n",
            "ITERATION_NO.: 816 LOSS_Generator: 5.357305526733398 LOSS_Discriminator: 0.06720803678035736\n",
            "ITERATION_NO.: 817 LOSS_Generator: 6.180690288543701 LOSS_Discriminator: 0.0718969851732254\n",
            "ITERATION_NO.: 818 LOSS_Generator: 6.247478008270264 LOSS_Discriminator: 0.17411693930625916\n",
            "ITERATION_NO.: 819 LOSS_Generator: 5.066122055053711 LOSS_Discriminator: 0.40927207469940186\n",
            "ITERATION_NO.: 820 LOSS_Generator: 4.373403072357178 LOSS_Discriminator: 0.05841121822595596\n",
            "ITERATION_NO.: 821 LOSS_Generator: 4.059539318084717 LOSS_Discriminator: 0.09059806168079376\n",
            "ITERATION_NO.: 822 LOSS_Generator: 3.714998722076416 LOSS_Discriminator: 0.10272431373596191\n",
            "ITERATION_NO.: 823 LOSS_Generator: 3.9212493896484375 LOSS_Discriminator: 0.13057291507720947\n",
            "ITERATION_NO.: 824 LOSS_Generator: 3.929340362548828 LOSS_Discriminator: 0.13723021745681763\n",
            "ITERATION_NO.: 825 LOSS_Generator: 4.452337265014648 LOSS_Discriminator: 0.03221053630113602\n",
            "ITERATION_NO.: 826 LOSS_Generator: 4.642724990844727 LOSS_Discriminator: 0.12413762509822845\n",
            "ITERATION_NO.: 827 LOSS_Generator: 4.487067222595215 LOSS_Discriminator: 0.08152468502521515\n",
            "ITERATION_NO.: 828 LOSS_Generator: 4.909088134765625 LOSS_Discriminator: 0.07076192647218704\n",
            "ITERATION_NO.: 829 LOSS_Generator: 4.846010208129883 LOSS_Discriminator: 0.12177486717700958\n",
            "ITERATION_NO.: 830 LOSS_Generator: 4.605171203613281 LOSS_Discriminator: 0.07728222012519836\n",
            "ITERATION_NO.: 831 LOSS_Generator: 4.7181901931762695 LOSS_Discriminator: 0.3184356689453125\n",
            "ITERATION_NO.: 832 LOSS_Generator: 4.614794731140137 LOSS_Discriminator: 0.16846545040607452\n",
            "ITERATION_NO.: 833 LOSS_Generator: 4.511805534362793 LOSS_Discriminator: 0.10846679657697678\n",
            "ITERATION_NO.: 834 LOSS_Generator: 5.163909912109375 LOSS_Discriminator: 0.053057558834552765\n",
            "ITERATION_NO.: 835 LOSS_Generator: 5.254545211791992 LOSS_Discriminator: 0.2505347430706024\n",
            "ITERATION_NO.: 836 LOSS_Generator: 5.214413642883301 LOSS_Discriminator: 0.16989439725875854\n",
            "ITERATION_NO.: 837 LOSS_Generator: 4.920050621032715 LOSS_Discriminator: 0.14691261947155\n",
            "ITERATION_NO.: 838 LOSS_Generator: 4.729517936706543 LOSS_Discriminator: 0.21566081047058105\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.30531120300293 LOSS_Discriminator: 0.1627148687839508\n",
            "ITERATION_NO.: 840 LOSS_Generator: 4.090350151062012 LOSS_Discriminator: 0.14513705670833588\n",
            "ITERATION_NO.: 841 LOSS_Generator: 4.011566162109375 LOSS_Discriminator: 0.1712421476840973\n",
            "ITERATION_NO.: 842 LOSS_Generator: 5.5226569175720215 LOSS_Discriminator: 0.08192265778779984\n",
            "ITERATION_NO.: 843 LOSS_Generator: 5.8841729164123535 LOSS_Discriminator: 0.26651278138160706\n",
            "ITERATION_NO.: 844 LOSS_Generator: 6.378339767456055 LOSS_Discriminator: 0.10068410634994507\n",
            "ITERATION_NO.: 845 LOSS_Generator: 6.237712383270264 LOSS_Discriminator: 0.012320765294134617\n",
            "ITERATION_NO.: 846 LOSS_Generator: 6.241939544677734 LOSS_Discriminator: 0.05135457217693329\n",
            "ITERATION_NO.: 847 LOSS_Generator: 7.057870388031006 LOSS_Discriminator: 0.13103225827217102\n",
            "ITERATION_NO.: 848 LOSS_Generator: 5.868306636810303 LOSS_Discriminator: 0.030112305656075478\n",
            "ITERATION_NO.: 849 LOSS_Generator: 5.811474800109863 LOSS_Discriminator: 0.4657917022705078\n",
            "ITERATION_NO.: 850 LOSS_Generator: 4.638311386108398 LOSS_Discriminator: 0.06365245580673218\n",
            "ITERATION_NO.: 851 LOSS_Generator: 4.250207424163818 LOSS_Discriminator: 0.12831437587738037\n",
            "ITERATION_NO.: 852 LOSS_Generator: 3.997852325439453 LOSS_Discriminator: 0.12174884974956512\n",
            "ITERATION_NO.: 853 LOSS_Generator: 4.014670372009277 LOSS_Discriminator: 0.1736447513103485\n",
            "ITERATION_NO.: 854 LOSS_Generator: 5.213529109954834 LOSS_Discriminator: 0.13137832283973694\n",
            "ITERATION_NO.: 855 LOSS_Generator: 5.816778182983398 LOSS_Discriminator: 0.20237305760383606\n",
            "ITERATION_NO.: 856 LOSS_Generator: 5.574119567871094 LOSS_Discriminator: 0.5319825410842896\n",
            "ITERATION_NO.: 857 LOSS_Generator: 5.462661266326904 LOSS_Discriminator: 0.11701852083206177\n",
            "ITERATION_NO.: 858 LOSS_Generator: 4.878376007080078 LOSS_Discriminator: 0.11257931590080261\n",
            "ITERATION_NO.: 859 LOSS_Generator: 4.324267387390137 LOSS_Discriminator: 0.30480828881263733\n",
            "ITERATION_NO.: 860 LOSS_Generator: 3.8294029235839844 LOSS_Discriminator: 0.21045637130737305\n",
            "ITERATION_NO.: 861 LOSS_Generator: 3.4964170455932617 LOSS_Discriminator: 0.11263273656368256\n",
            "ITERATION_NO.: 862 LOSS_Generator: 3.8511757850646973 LOSS_Discriminator: 0.1238098219037056\n",
            "ITERATION_NO.: 863 LOSS_Generator: 5.008331298828125 LOSS_Discriminator: 0.145625039935112\n",
            "ITERATION_NO.: 864 LOSS_Generator: 5.988339900970459 LOSS_Discriminator: 0.2585436701774597\n",
            "ITERATION_NO.: 865 LOSS_Generator: 5.746155738830566 LOSS_Discriminator: 0.1863056719303131\n",
            "ITERATION_NO.: 866 LOSS_Generator: 5.175346374511719 LOSS_Discriminator: 0.18763816356658936\n",
            "ITERATION_NO.: 867 LOSS_Generator: 4.327277183532715 LOSS_Discriminator: 0.36508360505104065\n",
            "ITERATION_NO.: 868 LOSS_Generator: 4.190988063812256 LOSS_Discriminator: 0.04367905482649803\n",
            "ITERATION_NO.: 869 LOSS_Generator: 3.3262076377868652 LOSS_Discriminator: 0.2019089162349701\n",
            "ITERATION_NO.: 870 LOSS_Generator: 3.7924625873565674 LOSS_Discriminator: 0.267961323261261\n",
            "ITERATION_NO.: 871 LOSS_Generator: 4.992277145385742 LOSS_Discriminator: 0.22843128442764282\n",
            "ITERATION_NO.: 872 LOSS_Generator: 5.163018226623535 LOSS_Discriminator: 0.05376828461885452\n",
            "ITERATION_NO.: 873 LOSS_Generator: 5.3576979637146 LOSS_Discriminator: 0.12597426772117615\n",
            "ITERATION_NO.: 874 LOSS_Generator: 5.574593544006348 LOSS_Discriminator: 0.07836601138114929\n",
            "ITERATION_NO.: 875 LOSS_Generator: 5.264190196990967 LOSS_Discriminator: 0.06282830238342285\n",
            "ITERATION_NO.: 876 LOSS_Generator: 5.247283935546875 LOSS_Discriminator: 0.07813665270805359\n",
            "ITERATION_NO.: 877 LOSS_Generator: 5.5590033531188965 LOSS_Discriminator: 0.022717930376529694\n",
            "ITERATION_NO.: 878 LOSS_Generator: 5.739607334136963 LOSS_Discriminator: 0.0365121029317379\n",
            "ITERATION_NO.: 879 LOSS_Generator: 5.112550735473633 LOSS_Discriminator: 0.23036284744739532\n",
            "ITERATION_NO.: 880 LOSS_Generator: 4.7365312576293945 LOSS_Discriminator: 0.04298577457666397\n",
            "ITERATION_NO.: 881 LOSS_Generator: 4.533089637756348 LOSS_Discriminator: 0.06660854071378708\n",
            "ITERATION_NO.: 882 LOSS_Generator: 4.229482650756836 LOSS_Discriminator: 0.1177539974451065\n",
            "ITERATION_NO.: 883 LOSS_Generator: 4.3316569328308105 LOSS_Discriminator: 0.035600192844867706\n",
            "ITERATION_NO.: 884 LOSS_Generator: 4.419757843017578 LOSS_Discriminator: 0.08417858183383942\n",
            "ITERATION_NO.: 885 LOSS_Generator: 4.843776226043701 LOSS_Discriminator: 0.08377458155155182\n",
            "ITERATION_NO.: 886 LOSS_Generator: 5.070381164550781 LOSS_Discriminator: 0.06261049211025238\n",
            "ITERATION_NO.: 887 LOSS_Generator: 5.073873996734619 LOSS_Discriminator: 0.17947642505168915\n",
            "ITERATION_NO.: 888 LOSS_Generator: 4.7924485206604 LOSS_Discriminator: 0.09216377139091492\n",
            "ITERATION_NO.: 889 LOSS_Generator: 4.655073165893555 LOSS_Discriminator: 0.0709599032998085\n",
            "ITERATION_NO.: 890 LOSS_Generator: 4.993821144104004 LOSS_Discriminator: 0.14133301377296448\n",
            "ITERATION_NO.: 891 LOSS_Generator: 5.0153656005859375 LOSS_Discriminator: 0.03860265761613846\n",
            "ITERATION_NO.: 892 LOSS_Generator: 4.917745590209961 LOSS_Discriminator: 0.13663437962532043\n",
            "ITERATION_NO.: 893 LOSS_Generator: 4.697081089019775 LOSS_Discriminator: 0.1709986925125122\n",
            "ITERATION_NO.: 894 LOSS_Generator: 4.196182727813721 LOSS_Discriminator: 0.15726269781589508\n",
            "ITERATION_NO.: 895 LOSS_Generator: 3.7888896465301514 LOSS_Discriminator: 0.3204421401023865\n",
            "ITERATION_NO.: 896 LOSS_Generator: 3.98237943649292 LOSS_Discriminator: 0.305353581905365\n",
            "ITERATION_NO.: 897 LOSS_Generator: 4.1702728271484375 LOSS_Discriminator: 0.09415490925312042\n",
            "ITERATION_NO.: 898 LOSS_Generator: 4.968050956726074 LOSS_Discriminator: 0.1878572702407837\n",
            "ITERATION_NO.: 899 LOSS_Generator: 5.846091270446777 LOSS_Discriminator: 0.07918223738670349\n",
            "ITERATION_NO.: 900 LOSS_Generator: 5.239973068237305 LOSS_Discriminator: 0.25524789094924927\n",
            "ITERATION_NO.: 901 LOSS_Generator: 4.556071758270264 LOSS_Discriminator: 0.18512606620788574\n",
            "ITERATION_NO.: 902 LOSS_Generator: 4.742844581604004 LOSS_Discriminator: 0.1532195508480072\n",
            "ITERATION_NO.: 903 LOSS_Generator: 3.7369909286499023 LOSS_Discriminator: 0.28289973735809326\n",
            "ITERATION_NO.: 904 LOSS_Generator: 4.286019802093506 LOSS_Discriminator: 0.24084441363811493\n",
            "ITERATION_NO.: 905 LOSS_Generator: 5.057684421539307 LOSS_Discriminator: 0.1342136263847351\n",
            "ITERATION_NO.: 906 LOSS_Generator: 5.7384138107299805 LOSS_Discriminator: 0.14291638135910034\n",
            "ITERATION_NO.: 907 LOSS_Generator: 5.834408760070801 LOSS_Discriminator: 0.15999528765678406\n",
            "ITERATION_NO.: 908 LOSS_Generator: 5.014054298400879 LOSS_Discriminator: 0.5685043334960938\n",
            "ITERATION_NO.: 909 LOSS_Generator: 4.88953971862793 LOSS_Discriminator: 0.0343693308532238\n",
            "ITERATION_NO.: 910 LOSS_Generator: 3.9252424240112305 LOSS_Discriminator: 0.08280855417251587\n",
            "ITERATION_NO.: 911 LOSS_Generator: 3.601764678955078 LOSS_Discriminator: 0.16730795800685883\n",
            "ITERATION_NO.: 912 LOSS_Generator: 3.777543067932129 LOSS_Discriminator: 0.08635501563549042\n",
            "ITERATION_NO.: 913 LOSS_Generator: 3.795341730117798 LOSS_Discriminator: 0.15642520785331726\n",
            "ITERATION_NO.: 914 LOSS_Generator: 4.657630920410156 LOSS_Discriminator: 0.11971193552017212\n",
            "ITERATION_NO.: 915 LOSS_Generator: 4.905221462249756 LOSS_Discriminator: 0.10750076919794083\n",
            "ITERATION_NO.: 916 LOSS_Generator: 5.609058380126953 LOSS_Discriminator: 0.02210927940905094\n",
            "ITERATION_NO.: 917 LOSS_Generator: 5.850400447845459 LOSS_Discriminator: 0.17997828125953674\n",
            "ITERATION_NO.: 918 LOSS_Generator: 6.5279951095581055 LOSS_Discriminator: 0.006807737052440643\n",
            "ITERATION_NO.: 919 LOSS_Generator: 6.386484622955322 LOSS_Discriminator: 0.04021381214261055\n",
            "ITERATION_NO.: 920 LOSS_Generator: 6.852888584136963 LOSS_Discriminator: 0.17023690044879913\n",
            "ITERATION_NO.: 921 LOSS_Generator: 5.940637111663818 LOSS_Discriminator: 0.13704712688922882\n",
            "ITERATION_NO.: 922 LOSS_Generator: 5.545985221862793 LOSS_Discriminator: 0.11555035412311554\n",
            "ITERATION_NO.: 923 LOSS_Generator: 5.430618762969971 LOSS_Discriminator: 0.02774283103644848\n",
            "ITERATION_NO.: 924 LOSS_Generator: 5.087368011474609 LOSS_Discriminator: 0.07907264679670334\n",
            "ITERATION_NO.: 925 LOSS_Generator: 4.27349853515625 LOSS_Discriminator: 0.11614808440208435\n",
            "ITERATION_NO.: 926 LOSS_Generator: 4.242116451263428 LOSS_Discriminator: 0.10871291160583496\n",
            "ITERATION_NO.: 927 LOSS_Generator: 4.035890579223633 LOSS_Discriminator: 0.14053936302661896\n",
            "ITERATION_NO.: 928 LOSS_Generator: 4.538405418395996 LOSS_Discriminator: 0.15584918856620789\n",
            "ITERATION_NO.: 929 LOSS_Generator: 4.319258689880371 LOSS_Discriminator: 0.11054873466491699\n",
            "ITERATION_NO.: 930 LOSS_Generator: 4.130590915679932 LOSS_Discriminator: 0.1543174684047699\n",
            "ITERATION_NO.: 931 LOSS_Generator: 4.269538879394531 LOSS_Discriminator: 0.1121959537267685\n",
            "ITERATION_NO.: 932 LOSS_Generator: 4.377366065979004 LOSS_Discriminator: 0.06899362802505493\n",
            "ITERATION_NO.: 933 LOSS_Generator: 4.807660102844238 LOSS_Discriminator: 0.2295321226119995\n",
            "ITERATION_NO.: 934 LOSS_Generator: 5.220798492431641 LOSS_Discriminator: 0.11348791420459747\n",
            "ITERATION_NO.: 935 LOSS_Generator: 5.0683441162109375 LOSS_Discriminator: 0.14779506623744965\n",
            "ITERATION_NO.: 936 LOSS_Generator: 5.103185653686523 LOSS_Discriminator: 0.06291183829307556\n",
            "ITERATION_NO.: 937 LOSS_Generator: 5.608037948608398 LOSS_Discriminator: 0.10369183123111725\n",
            "ITERATION_NO.: 938 LOSS_Generator: 5.05880069732666 LOSS_Discriminator: 0.2609850764274597\n",
            "ITERATION_NO.: 939 LOSS_Generator: 4.686084270477295 LOSS_Discriminator: 0.12371261417865753\n",
            "ITERATION_NO.: 940 LOSS_Generator: 4.652785778045654 LOSS_Discriminator: 0.19565412402153015\n",
            "ITERATION_NO.: 941 LOSS_Generator: 4.420287609100342 LOSS_Discriminator: 0.2864004075527191\n",
            "ITERATION_NO.: 942 LOSS_Generator: 4.637112140655518 LOSS_Discriminator: 0.16100820899009705\n",
            "ITERATION_NO.: 943 LOSS_Generator: 3.801905393600464 LOSS_Discriminator: 0.20214486122131348\n",
            "ITERATION_NO.: 944 LOSS_Generator: 4.271299362182617 LOSS_Discriminator: 0.04955461993813515\n",
            "ITERATION_NO.: 945 LOSS_Generator: 4.843755722045898 LOSS_Discriminator: 0.036604590713977814\n",
            "ITERATION_NO.: 946 LOSS_Generator: 4.635409355163574 LOSS_Discriminator: 0.33092987537384033\n",
            "ITERATION_NO.: 947 LOSS_Generator: 4.263100624084473 LOSS_Discriminator: 0.05019485950469971\n",
            "ITERATION_NO.: 948 LOSS_Generator: 4.530705451965332 LOSS_Discriminator: 0.03991086781024933\n",
            "ITERATION_NO.: 949 LOSS_Generator: 4.425732612609863 LOSS_Discriminator: 0.15951785445213318\n",
            "ITERATION_NO.: 950 LOSS_Generator: 4.003966331481934 LOSS_Discriminator: 0.24347591400146484\n",
            "ITERATION_NO.: 951 LOSS_Generator: 3.417240619659424 LOSS_Discriminator: 0.20062768459320068\n",
            "ITERATION_NO.: 952 LOSS_Generator: 3.4443302154541016 LOSS_Discriminator: 0.2560272514820099\n",
            "ITERATION_NO.: 953 LOSS_Generator: 3.787142753601074 LOSS_Discriminator: 0.24671047925949097\n",
            "ITERATION_NO.: 954 LOSS_Generator: 4.869629383087158 LOSS_Discriminator: 0.18939362466335297\n",
            "ITERATION_NO.: 955 LOSS_Generator: 5.080906391143799 LOSS_Discriminator: 0.07455914467573166\n",
            "ITERATION_NO.: 956 LOSS_Generator: 5.614594459533691 LOSS_Discriminator: 0.05312106013298035\n",
            "ITERATION_NO.: 957 LOSS_Generator: 6.201537132263184 LOSS_Discriminator: 0.011908890679478645\n",
            "ITERATION_NO.: 958 LOSS_Generator: 6.855778217315674 LOSS_Discriminator: 0.051460884511470795\n",
            "ITERATION_NO.: 959 LOSS_Generator: 6.103766441345215 LOSS_Discriminator: 0.07285693287849426\n",
            "ITERATION_NO.: 960 LOSS_Generator: 5.928313255310059 LOSS_Discriminator: 0.21102255582809448\n",
            "ITERATION_NO.: 961 LOSS_Generator: 5.688392639160156 LOSS_Discriminator: 0.1602252721786499\n",
            "ITERATION_NO.: 962 LOSS_Generator: 4.6011457443237305 LOSS_Discriminator: 0.3802385926246643\n",
            "ITERATION_NO.: 963 LOSS_Generator: 3.5682106018066406 LOSS_Discriminator: 0.15534234046936035\n",
            "ITERATION_NO.: 964 LOSS_Generator: 3.5079681873321533 LOSS_Discriminator: 0.16960451006889343\n",
            "ITERATION_NO.: 965 LOSS_Generator: 3.3297007083892822 LOSS_Discriminator: 0.13517442345619202\n",
            "ITERATION_NO.: 966 LOSS_Generator: 3.9135539531707764 LOSS_Discriminator: 0.11206673830747604\n",
            "ITERATION_NO.: 967 LOSS_Generator: 5.349488735198975 LOSS_Discriminator: 0.07204482704401016\n",
            "ITERATION_NO.: 968 LOSS_Generator: 5.2008514404296875 LOSS_Discriminator: 0.2813165783882141\n",
            "ITERATION_NO.: 969 LOSS_Generator: 5.31025505065918 LOSS_Discriminator: 0.10929687321186066\n",
            "ITERATION_NO.: 970 LOSS_Generator: 5.373895168304443 LOSS_Discriminator: 0.02967548370361328\n",
            "ITERATION_NO.: 971 LOSS_Generator: 5.453097343444824 LOSS_Discriminator: 0.15860487520694733\n",
            "ITERATION_NO.: 972 LOSS_Generator: 4.741885662078857 LOSS_Discriminator: 0.011865406297147274\n",
            "ITERATION_NO.: 973 LOSS_Generator: 4.778626441955566 LOSS_Discriminator: 0.053372822701931\n",
            "ITERATION_NO.: 974 LOSS_Generator: 4.636866569519043 LOSS_Discriminator: 0.09548186510801315\n",
            "ITERATION_NO.: 975 LOSS_Generator: 4.82780647277832 LOSS_Discriminator: 0.0799141377210617\n",
            "ITERATION_NO.: 976 LOSS_Generator: 5.329273223876953 LOSS_Discriminator: 0.2018386721611023\n",
            "ITERATION_NO.: 977 LOSS_Generator: 4.828680992126465 LOSS_Discriminator: 0.08555055409669876\n",
            "ITERATION_NO.: 978 LOSS_Generator: 5.068521022796631 LOSS_Discriminator: 0.19319450855255127\n",
            "ITERATION_NO.: 979 LOSS_Generator: 4.925323486328125 LOSS_Discriminator: 0.1559762805700302\n",
            "ITERATION_NO.: 980 LOSS_Generator: 4.895987510681152 LOSS_Discriminator: 0.2560068368911743\n",
            "ITERATION_NO.: 981 LOSS_Generator: 4.135081768035889 LOSS_Discriminator: 0.12321498245000839\n",
            "ITERATION_NO.: 982 LOSS_Generator: 5.075119972229004 LOSS_Discriminator: 0.14373290538787842\n",
            "ITERATION_NO.: 983 LOSS_Generator: 5.051440238952637 LOSS_Discriminator: 0.09605998545885086\n",
            "ITERATION_NO.: 984 LOSS_Generator: 4.950838088989258 LOSS_Discriminator: 0.04877728223800659\n",
            "ITERATION_NO.: 985 LOSS_Generator: 5.686845779418945 LOSS_Discriminator: 0.11585283279418945\n",
            "ITERATION_NO.: 986 LOSS_Generator: 5.0315656661987305 LOSS_Discriminator: 0.12906302511692047\n",
            "ITERATION_NO.: 987 LOSS_Generator: 5.532174110412598 LOSS_Discriminator: 0.1026829183101654\n",
            "ITERATION_NO.: 988 LOSS_Generator: 5.4590559005737305 LOSS_Discriminator: 0.01602129638195038\n",
            "ITERATION_NO.: 989 LOSS_Generator: 5.6073079109191895 LOSS_Discriminator: 0.16623979806900024\n",
            "ITERATION_NO.: 990 LOSS_Generator: 4.943474769592285 LOSS_Discriminator: 0.017995472997426987\n",
            "ITERATION_NO.: 991 LOSS_Generator: 5.337358474731445 LOSS_Discriminator: 0.09442786872386932\n",
            "ITERATION_NO.: 992 LOSS_Generator: 4.667154312133789 LOSS_Discriminator: 0.39529895782470703\n",
            "ITERATION_NO.: 993 LOSS_Generator: 4.08384895324707 LOSS_Discriminator: 0.15492746233940125\n",
            "ITERATION_NO.: 994 LOSS_Generator: 4.47197151184082 LOSS_Discriminator: 0.0758533775806427\n",
            "ITERATION_NO.: 995 LOSS_Generator: 3.8357667922973633 LOSS_Discriminator: 0.17066077888011932\n",
            "ITERATION_NO.: 996 LOSS_Generator: 4.690279960632324 LOSS_Discriminator: 0.3225618600845337\n",
            "ITERATION_NO.: 997 LOSS_Generator: 4.584821701049805 LOSS_Discriminator: 0.2555575966835022\n",
            "ITERATION_NO.: 998 LOSS_Generator: 4.4150390625 LOSS_Discriminator: 0.18208298087120056\n",
            "ITERATION_NO.: 999 LOSS_Generator: 4.774798393249512 LOSS_Discriminator: 0.07039771229028702\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 4.582165718078613 LOSS_Discriminator: 0.07611456513404846\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 4.777900695800781 LOSS_Discriminator: 0.13943397998809814\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 4.675442695617676 LOSS_Discriminator: 0.08102698624134064\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 4.624345779418945 LOSS_Discriminator: 0.25905388593673706\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 4.140044212341309 LOSS_Discriminator: 0.2171759009361267\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 3.9905762672424316 LOSS_Discriminator: 0.1286945343017578\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 4.461254596710205 LOSS_Discriminator: 0.08979271352291107\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 4.2307939529418945 LOSS_Discriminator: 0.12236233055591583\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 4.553663730621338 LOSS_Discriminator: 0.20565128326416016\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 4.739752769470215 LOSS_Discriminator: 0.09304336458444595\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 4.9988932609558105 LOSS_Discriminator: 0.14596077799797058\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 4.634641647338867 LOSS_Discriminator: 0.1410520225763321\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 4.919326305389404 LOSS_Discriminator: 0.09212341159582138\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 4.279884338378906 LOSS_Discriminator: 0.19126300513744354\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 3.9917101860046387 LOSS_Discriminator: 0.06622665375471115\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 4.676075458526611 LOSS_Discriminator: 0.07797524333000183\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 4.815250873565674 LOSS_Discriminator: 0.0662129819393158\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 4.770752906799316 LOSS_Discriminator: 0.08864385634660721\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 4.871807098388672 LOSS_Discriminator: 0.13307102024555206\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 4.86676025390625 LOSS_Discriminator: 0.1154162734746933\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 5.106828689575195 LOSS_Discriminator: 0.0244821235537529\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 4.965260028839111 LOSS_Discriminator: 0.060100845992565155\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 4.601654052734375 LOSS_Discriminator: 0.24372217059135437\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 4.406291961669922 LOSS_Discriminator: 0.037968557327985764\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 4.807437896728516 LOSS_Discriminator: 0.10389743745326996\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 4.573431968688965 LOSS_Discriminator: 0.15365561842918396\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 3.75394868850708 LOSS_Discriminator: 0.16619184613227844\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 4.480288028717041 LOSS_Discriminator: 0.18267707526683807\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 4.623805999755859 LOSS_Discriminator: 0.07521338760852814\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 4.531308174133301 LOSS_Discriminator: 0.27342653274536133\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 4.244540691375732 LOSS_Discriminator: 0.1741953194141388\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 4.49193000793457 LOSS_Discriminator: 0.11262424290180206\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 5.35176944732666 LOSS_Discriminator: 0.08295781165361404\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 4.4624857902526855 LOSS_Discriminator: 0.16201287508010864\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 4.4136552810668945 LOSS_Discriminator: 0.09382793307304382\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 4.90713357925415 LOSS_Discriminator: 0.05990321934223175\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 5.098579406738281 LOSS_Discriminator: 0.08268451690673828\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 4.9564971923828125 LOSS_Discriminator: 0.12733770906925201\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 4.724607467651367 LOSS_Discriminator: 0.032107237726449966\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 5.300164699554443 LOSS_Discriminator: 0.17027129232883453\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 4.787125587463379 LOSS_Discriminator: 0.18886759877204895\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 4.898268222808838 LOSS_Discriminator: 0.06861454248428345\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 4.910469055175781 LOSS_Discriminator: 0.21847620606422424\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 4.9659929275512695 LOSS_Discriminator: 0.09884355962276459\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 4.735067367553711 LOSS_Discriminator: 0.04727772995829582\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 4.791833877563477 LOSS_Discriminator: 0.181380033493042\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 4.755997180938721 LOSS_Discriminator: 0.043079786002635956\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 5.116357803344727 LOSS_Discriminator: 0.04268387332558632\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 4.212578773498535 LOSS_Discriminator: 0.16430681943893433\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 5.006030559539795 LOSS_Discriminator: 0.10081986337900162\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 5.363189697265625 LOSS_Discriminator: 0.037188343703746796\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 5.685735702514648 LOSS_Discriminator: 0.03223876282572746\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 5.731697082519531 LOSS_Discriminator: 0.235579714179039\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 5.742106914520264 LOSS_Discriminator: 0.14145715534687042\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 5.43060302734375 LOSS_Discriminator: 0.015134159475564957\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 5.21583890914917 LOSS_Discriminator: 0.21722030639648438\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 4.266378879547119 LOSS_Discriminator: 0.09118032455444336\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 4.486128807067871 LOSS_Discriminator: 0.07303431630134583\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 4.862394332885742 LOSS_Discriminator: 0.22929704189300537\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 4.017335891723633 LOSS_Discriminator: 0.2142084538936615\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 4.153352737426758 LOSS_Discriminator: 0.0940498635172844\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 4.518955230712891 LOSS_Discriminator: 0.0815558210015297\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 4.757372856140137 LOSS_Discriminator: 0.18225908279418945\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 4.879135608673096 LOSS_Discriminator: 0.1253015697002411\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 5.243954181671143 LOSS_Discriminator: 0.03997848555445671\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 5.242168426513672 LOSS_Discriminator: 0.036943405866622925\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 6.2580060958862305 LOSS_Discriminator: 0.08408627659082413\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 5.357328414916992 LOSS_Discriminator: 0.17236453294754028\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 6.138547897338867 LOSS_Discriminator: 0.18405473232269287\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 5.52523136138916 LOSS_Discriminator: 0.23906704783439636\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.76300573348999 LOSS_Discriminator: 0.03218775987625122\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 3.965569019317627 LOSS_Discriminator: 0.1830540895462036\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 3.6372063159942627 LOSS_Discriminator: 0.1984776258468628\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 3.8241734504699707 LOSS_Discriminator: 0.25973424315452576\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 4.065068244934082 LOSS_Discriminator: 0.21441048383712769\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 4.457014560699463 LOSS_Discriminator: 0.16172567009925842\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 5.463678359985352 LOSS_Discriminator: 0.06666053831577301\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 5.131380081176758 LOSS_Discriminator: 0.2790164053440094\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 4.945213317871094 LOSS_Discriminator: 0.04668787121772766\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 5.044980049133301 LOSS_Discriminator: 0.08269815891981125\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 4.778317451477051 LOSS_Discriminator: 0.26830384135246277\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 4.603437423706055 LOSS_Discriminator: 0.05360407754778862\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 4.560774803161621 LOSS_Discriminator: 0.067038893699646\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 4.766668796539307 LOSS_Discriminator: 0.039911650121212006\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 4.485780715942383 LOSS_Discriminator: 0.22407272458076477\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 5.134100914001465 LOSS_Discriminator: 0.05912990868091583\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 4.9029436111450195 LOSS_Discriminator: 0.03902660310268402\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 5.933780670166016 LOSS_Discriminator: 0.014575958251953125\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 5.857898235321045 LOSS_Discriminator: 0.3248961567878723\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 5.042600631713867 LOSS_Discriminator: 0.1991044282913208\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 4.564558982849121 LOSS_Discriminator: 0.03374103084206581\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 4.4424309730529785 LOSS_Discriminator: 0.09440190345048904\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 3.980060577392578 LOSS_Discriminator: 0.07089465856552124\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 5.2113213539123535 LOSS_Discriminator: 0.12493380904197693\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 5.468219757080078 LOSS_Discriminator: 0.08973561972379684\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 5.71722936630249 LOSS_Discriminator: 0.12038546055555344\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 5.287358283996582 LOSS_Discriminator: 0.0170608963817358\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 6.368365287780762 LOSS_Discriminator: 0.03168660029768944\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 5.050052165985107 LOSS_Discriminator: 0.09312177449464798\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 5.494215965270996 LOSS_Discriminator: 0.14192192256450653\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 5.315201282501221 LOSS_Discriminator: 0.008021763525903225\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 5.049304962158203 LOSS_Discriminator: 0.029145631939172745\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 4.942916393280029 LOSS_Discriminator: 0.1872309297323227\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 4.35389518737793 LOSS_Discriminator: 0.12958776950836182\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 4.556611061096191 LOSS_Discriminator: 0.22481298446655273\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 4.2855753898620605 LOSS_Discriminator: 0.20126599073410034\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 4.412304401397705 LOSS_Discriminator: 0.34024888277053833\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 4.809982776641846 LOSS_Discriminator: 0.06301828473806381\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 4.795618057250977 LOSS_Discriminator: 0.08457624912261963\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 5.4146881103515625 LOSS_Discriminator: 0.15566495060920715\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 5.291041374206543 LOSS_Discriminator: 0.05876965820789337\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 5.404932022094727 LOSS_Discriminator: 0.1568610966205597\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 4.7269158363342285 LOSS_Discriminator: 0.17440424859523773\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 4.857569217681885 LOSS_Discriminator: 0.2285068780183792\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 4.559872627258301 LOSS_Discriminator: 0.05660846084356308\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 4.21498966217041 LOSS_Discriminator: 0.11916842311620712\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 5.168903350830078 LOSS_Discriminator: 0.07774363458156586\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.5812883377075195 LOSS_Discriminator: 0.02507559023797512\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 6.151594161987305 LOSS_Discriminator: 0.14969927072525024\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 6.570822715759277 LOSS_Discriminator: 0.012995719909667969\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 6.1062397956848145 LOSS_Discriminator: 0.1346019059419632\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 6.797065734863281 LOSS_Discriminator: 0.014844671823084354\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 6.438230991363525 LOSS_Discriminator: 0.04644901677966118\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 6.216785907745361 LOSS_Discriminator: 0.053340598940849304\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 5.753596782684326 LOSS_Discriminator: 0.05204378813505173\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 5.358908176422119 LOSS_Discriminator: 0.2816338539123535\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 5.080480575561523 LOSS_Discriminator: 0.03222552686929703\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 4.464076042175293 LOSS_Discriminator: 0.10371414572000504\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 4.589325904846191 LOSS_Discriminator: 0.129878968000412\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 4.645853042602539 LOSS_Discriminator: 0.2911240756511688\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 5.33088493347168 LOSS_Discriminator: 0.20956389605998993\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 5.488755226135254 LOSS_Discriminator: 0.027278471738100052\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 5.566720962524414 LOSS_Discriminator: 0.18209144473075867\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 5.5569376945495605 LOSS_Discriminator: 0.06436991691589355\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 5.062825679779053 LOSS_Discriminator: 0.09732204675674438\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 5.251302242279053 LOSS_Discriminator: 0.06190435588359833\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 5.191875457763672 LOSS_Discriminator: 0.12389202415943146\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 4.908024787902832 LOSS_Discriminator: 0.28972476720809937\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 4.913370132446289 LOSS_Discriminator: 0.07573388516902924\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 5.518578052520752 LOSS_Discriminator: 0.22641178965568542\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 4.639226913452148 LOSS_Discriminator: 0.4541189968585968\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 4.822195053100586 LOSS_Discriminator: 0.19411741197109222\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 3.8414998054504395 LOSS_Discriminator: 0.12857946753501892\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 3.9280600547790527 LOSS_Discriminator: 0.20553934574127197\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 3.964426040649414 LOSS_Discriminator: 0.17607760429382324\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 4.3175153732299805 LOSS_Discriminator: 0.06453574448823929\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 4.812661170959473 LOSS_Discriminator: 0.15823735296726227\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 4.563909530639648 LOSS_Discriminator: 0.13592679798603058\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 4.714304447174072 LOSS_Discriminator: 0.08190909028053284\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 4.5030012130737305 LOSS_Discriminator: 0.1500110924243927\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 4.141956329345703 LOSS_Discriminator: 0.10963299870491028\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 5.295581817626953 LOSS_Discriminator: 0.048595163971185684\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 5.178469657897949 LOSS_Discriminator: 0.16928018629550934\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 5.717672824859619 LOSS_Discriminator: 0.053290922194719315\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 5.084399223327637 LOSS_Discriminator: 0.023256585001945496\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 5.631464004516602 LOSS_Discriminator: 0.11709743738174438\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 5.906771659851074 LOSS_Discriminator: 0.07800552248954773\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 5.673996448516846 LOSS_Discriminator: 0.17326641082763672\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 5.290456771850586 LOSS_Discriminator: 0.1717643439769745\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 5.207855224609375 LOSS_Discriminator: 0.13298530876636505\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 4.4128828048706055 LOSS_Discriminator: 0.14949581027030945\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 4.127679824829102 LOSS_Discriminator: 0.16770419478416443\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 3.7149417400360107 LOSS_Discriminator: 0.12535271048545837\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 3.9502363204956055 LOSS_Discriminator: 0.2212991714477539\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 4.6032257080078125 LOSS_Discriminator: 0.17904235422611237\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 5.777984619140625 LOSS_Discriminator: 0.10813185572624207\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 5.159275054931641 LOSS_Discriminator: 0.14947667717933655\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 6.121628761291504 LOSS_Discriminator: 0.08636954426765442\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 5.2484941482543945 LOSS_Discriminator: 0.18090805411338806\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 5.619784355163574 LOSS_Discriminator: 0.16558165848255157\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 5.188889503479004 LOSS_Discriminator: 0.05570239573717117\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 5.40546989440918 LOSS_Discriminator: 0.17487381398677826\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 5.136092185974121 LOSS_Discriminator: 0.09334549307823181\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 5.258361339569092 LOSS_Discriminator: 0.1429484635591507\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 5.210179805755615 LOSS_Discriminator: 0.13631990551948547\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 4.914036750793457 LOSS_Discriminator: 0.07232338935136795\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 5.907621383666992 LOSS_Discriminator: 0.05571858957409859\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 5.9773759841918945 LOSS_Discriminator: 0.1461428999900818\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 6.306282043457031 LOSS_Discriminator: 0.11393225938081741\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 5.813997745513916 LOSS_Discriminator: 0.18850955367088318\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 5.481141090393066 LOSS_Discriminator: 0.396411269903183\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 5.012701034545898 LOSS_Discriminator: 0.3005790710449219\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 4.052181243896484 LOSS_Discriminator: 0.1921062469482422\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 4.288905143737793 LOSS_Discriminator: 0.19221444427967072\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 4.928119659423828 LOSS_Discriminator: 0.057700641453266144\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 4.697842597961426 LOSS_Discriminator: 0.37960344552993774\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 5.062151908874512 LOSS_Discriminator: 0.2039625644683838\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 4.700495719909668 LOSS_Discriminator: 0.21884015202522278\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 4.2900261878967285 LOSS_Discriminator: 0.09725473821163177\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 5.231137275695801 LOSS_Discriminator: 0.08072414994239807\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 6.223334312438965 LOSS_Discriminator: 0.06942742317914963\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 5.79286003112793 LOSS_Discriminator: 0.10826097428798676\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 6.161224842071533 LOSS_Discriminator: 0.09534325450658798\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 6.382331848144531 LOSS_Discriminator: 0.16150283813476562\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 6.127081394195557 LOSS_Discriminator: 0.009694928303360939\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 5.3718109130859375 LOSS_Discriminator: 0.1296311467885971\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 5.077000141143799 LOSS_Discriminator: 0.05165255814790726\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 4.866681098937988 LOSS_Discriminator: 0.07111912965774536\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 4.437983512878418 LOSS_Discriminator: 0.25293153524398804\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 4.766600131988525 LOSS_Discriminator: 0.11290553212165833\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 5.094738960266113 LOSS_Discriminator: 0.101045161485672\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 4.204634189605713 LOSS_Discriminator: 0.14930792152881622\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 4.913509845733643 LOSS_Discriminator: 0.0815347209572792\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 4.224891662597656 LOSS_Discriminator: 0.22674646973609924\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 5.271475791931152 LOSS_Discriminator: 0.1164105162024498\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 4.882713794708252 LOSS_Discriminator: 0.09178899228572845\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 5.212309837341309 LOSS_Discriminator: 0.027308344841003418\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 5.5183634757995605 LOSS_Discriminator: 0.15462546050548553\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 5.66412353515625 LOSS_Discriminator: 0.11971446871757507\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 5.739521026611328 LOSS_Discriminator: 0.023352261632680893\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 5.315512180328369 LOSS_Discriminator: 0.03521125763654709\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 5.44636344909668 LOSS_Discriminator: 0.14228187501430511\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 5.86207389831543 LOSS_Discriminator: 0.09108490496873856\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 5.633352279663086 LOSS_Discriminator: 0.16483700275421143\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 5.414963722229004 LOSS_Discriminator: 0.19504857063293457\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 4.228271007537842 LOSS_Discriminator: 0.22189021110534668\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 4.280762195587158 LOSS_Discriminator: 0.16548863053321838\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 4.408195495605469 LOSS_Discriminator: 0.11148566752672195\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 5.2760910987854 LOSS_Discriminator: 0.0993405282497406\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 5.410805702209473 LOSS_Discriminator: 0.1737757921218872\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 5.685037136077881 LOSS_Discriminator: 0.037269167602062225\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 6.098598957061768 LOSS_Discriminator: 0.20328646898269653\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 5.187112331390381 LOSS_Discriminator: 0.41206100583076477\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 4.462459564208984 LOSS_Discriminator: 0.1418171226978302\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 3.5839176177978516 LOSS_Discriminator: 0.23458737134933472\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 3.52038836479187 LOSS_Discriminator: 0.25881683826446533\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 4.599301338195801 LOSS_Discriminator: 0.14260059595108032\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 5.191450119018555 LOSS_Discriminator: 0.2300395965576172\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 5.162440776824951 LOSS_Discriminator: 0.0381900891661644\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 5.1594953536987305 LOSS_Discriminator: 0.18062281608581543\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 4.800178050994873 LOSS_Discriminator: 0.04205525666475296\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 4.336202621459961 LOSS_Discriminator: 0.15132394433021545\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 3.874415397644043 LOSS_Discriminator: 0.17787408828735352\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 4.014956951141357 LOSS_Discriminator: 0.19270692765712738\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 4.297772407531738 LOSS_Discriminator: 0.09533808380365372\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 3.7688159942626953 LOSS_Discriminator: 0.03136955201625824\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 4.332825183868408 LOSS_Discriminator: 0.07907567918300629\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 4.2680816650390625 LOSS_Discriminator: 0.09915106743574142\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 4.586625099182129 LOSS_Discriminator: 0.034163832664489746\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 4.824031829833984 LOSS_Discriminator: 0.0720796063542366\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 5.238507270812988 LOSS_Discriminator: 0.05548480898141861\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 5.1538496017456055 LOSS_Discriminator: 0.033881090581417084\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 5.5017852783203125 LOSS_Discriminator: 0.05260905995965004\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 5.9008073806762695 LOSS_Discriminator: 0.22293248772621155\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 5.622076034545898 LOSS_Discriminator: 0.19154620170593262\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 5.379145622253418 LOSS_Discriminator: 0.2109805941581726\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 4.778559684753418 LOSS_Discriminator: 0.13269490003585815\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 4.251524925231934 LOSS_Discriminator: 0.06625721603631973\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 3.820739984512329 LOSS_Discriminator: 0.05960545688867569\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 4.796679496765137 LOSS_Discriminator: 0.1466219425201416\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 5.404597282409668 LOSS_Discriminator: 0.029234182089567184\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 5.235021591186523 LOSS_Discriminator: 0.06730186939239502\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 5.513438701629639 LOSS_Discriminator: 0.15456315875053406\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 4.859514236450195 LOSS_Discriminator: 0.08049183338880539\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 5.088267803192139 LOSS_Discriminator: 0.11114086210727692\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 4.839634895324707 LOSS_Discriminator: 0.17424046993255615\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 4.516938209533691 LOSS_Discriminator: 0.3642173409461975\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 3.734720468521118 LOSS_Discriminator: 0.12441956996917725\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 4.406816482543945 LOSS_Discriminator: 0.21202589571475983\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.352861404418945 LOSS_Discriminator: 0.13908369839191437\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 4.854866027832031 LOSS_Discriminator: 0.1588895618915558\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 6.466923713684082 LOSS_Discriminator: 0.04688594490289688\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 6.199922561645508 LOSS_Discriminator: 0.12847544252872467\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 6.52669620513916 LOSS_Discriminator: 0.03921020030975342\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 6.211472511291504 LOSS_Discriminator: 0.09799942374229431\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 7.167440414428711 LOSS_Discriminator: 0.0072695063427090645\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 6.2083024978637695 LOSS_Discriminator: 0.28701251745224\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 5.192452907562256 LOSS_Discriminator: 0.28559303283691406\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 4.5573811531066895 LOSS_Discriminator: 0.10950542241334915\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 4.504113674163818 LOSS_Discriminator: 0.13946853578090668\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 5.830636501312256 LOSS_Discriminator: 0.2068120539188385\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 5.135568141937256 LOSS_Discriminator: 0.12129450589418411\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 5.241281509399414 LOSS_Discriminator: 0.1605309098958969\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 5.130326271057129 LOSS_Discriminator: 0.12153004109859467\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 5.13542366027832 LOSS_Discriminator: 0.02070062980055809\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 4.769102573394775 LOSS_Discriminator: 0.12499234825372696\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.1530256271362305 LOSS_Discriminator: 0.0855749174952507\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 5.563969612121582 LOSS_Discriminator: 0.38280636072158813\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 5.540345668792725 LOSS_Discriminator: 0.12843574583530426\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 5.943638324737549 LOSS_Discriminator: 0.042783670127391815\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 5.5547637939453125 LOSS_Discriminator: 0.33718645572662354\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 4.677862167358398 LOSS_Discriminator: 0.13197264075279236\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 4.336626052856445 LOSS_Discriminator: 0.26740318536758423\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 3.9425621032714844 LOSS_Discriminator: 0.16343523561954498\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 3.694181442260742 LOSS_Discriminator: 0.2079131007194519\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 3.456576347351074 LOSS_Discriminator: 0.24658548831939697\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 4.387789726257324 LOSS_Discriminator: 0.30623573064804077\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 4.520882606506348 LOSS_Discriminator: 0.114069864153862\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 5.098403453826904 LOSS_Discriminator: 0.12991422414779663\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 4.768738269805908 LOSS_Discriminator: 0.30017465353012085\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 3.946417808532715 LOSS_Discriminator: 0.21810778975486755\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 4.091245174407959 LOSS_Discriminator: 0.05724523961544037\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 3.5196609497070312 LOSS_Discriminator: 0.16906645894050598\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 3.2082362174987793 LOSS_Discriminator: 0.17169880867004395\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 3.628912925720215 LOSS_Discriminator: 0.14999155700206757\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 4.485854148864746 LOSS_Discriminator: 0.08908490836620331\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 4.942255020141602 LOSS_Discriminator: 0.02594418078660965\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 6.000017166137695 LOSS_Discriminator: 0.11840496212244034\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 5.930896759033203 LOSS_Discriminator: 0.25023865699768066\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 5.131166458129883 LOSS_Discriminator: 0.16883602738380432\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 4.971689701080322 LOSS_Discriminator: 0.013080760836601257\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 4.102909564971924 LOSS_Discriminator: 0.11904561519622803\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 4.04110050201416 LOSS_Discriminator: 0.06972098350524902\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 4.006635665893555 LOSS_Discriminator: 0.07823634892702103\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 4.5185394287109375 LOSS_Discriminator: 0.09228876978158951\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 5.057962417602539 LOSS_Discriminator: 0.15149027109146118\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 5.181774139404297 LOSS_Discriminator: 0.14513036608695984\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 5.313392639160156 LOSS_Discriminator: 0.08501626551151276\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 5.720273017883301 LOSS_Discriminator: 0.1177651509642601\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 4.914227485656738 LOSS_Discriminator: 0.14770695567131042\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 4.952059745788574 LOSS_Discriminator: 0.13460484147071838\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 4.435592174530029 LOSS_Discriminator: 0.05736711621284485\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 4.651113510131836 LOSS_Discriminator: 0.04088882729411125\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 4.204110145568848 LOSS_Discriminator: 0.18617644906044006\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 4.957831859588623 LOSS_Discriminator: 0.15263986587524414\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 4.9722700119018555 LOSS_Discriminator: 0.1251242756843567\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 4.21850061416626 LOSS_Discriminator: 0.20670269429683685\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 3.477271795272827 LOSS_Discriminator: 0.10121405124664307\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 4.1203508377075195 LOSS_Discriminator: 0.10802237689495087\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 4.828763008117676 LOSS_Discriminator: 0.09757769852876663\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 5.636374473571777 LOSS_Discriminator: 0.14890703558921814\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 6.049566745758057 LOSS_Discriminator: 0.07339386641979218\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 5.970252990722656 LOSS_Discriminator: 0.0225481279194355\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 6.498274803161621 LOSS_Discriminator: 0.05297531187534332\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 7.066650390625 LOSS_Discriminator: 0.01452638953924179\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 7.066368103027344 LOSS_Discriminator: 0.49581050872802734\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 6.115767478942871 LOSS_Discriminator: 0.3306885361671448\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 4.8530988693237305 LOSS_Discriminator: 0.07078517228364944\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 4.134350776672363 LOSS_Discriminator: 0.09816022962331772\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 3.8526673316955566 LOSS_Discriminator: 0.2734849750995636\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 4.236231327056885 LOSS_Discriminator: 0.18683481216430664\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 4.848911285400391 LOSS_Discriminator: 0.14085756242275238\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 4.852714538574219 LOSS_Discriminator: 0.3291657567024231\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 4.469992637634277 LOSS_Discriminator: 0.06513538956642151\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 4.525789260864258 LOSS_Discriminator: 0.10134182870388031\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 4.751198768615723 LOSS_Discriminator: 0.1507207304239273\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 5.283455848693848 LOSS_Discriminator: 0.1657411903142929\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 5.38823127746582 LOSS_Discriminator: 0.14674945175647736\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 5.740983009338379 LOSS_Discriminator: 0.1114361584186554\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 5.4778242111206055 LOSS_Discriminator: 0.14110907912254333\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 5.379863739013672 LOSS_Discriminator: 0.22300639748573303\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 4.710838794708252 LOSS_Discriminator: 0.06400032341480255\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 4.562699317932129 LOSS_Discriminator: 0.0908297598361969\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 4.922910690307617 LOSS_Discriminator: 0.03676173463463783\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 4.813189506530762 LOSS_Discriminator: 0.08558126538991928\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 4.318997383117676 LOSS_Discriminator: 0.2721634805202484\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 4.3444085121154785 LOSS_Discriminator: 0.11226048320531845\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 4.210744857788086 LOSS_Discriminator: 0.14678756892681122\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 4.724704265594482 LOSS_Discriminator: 0.11823590099811554\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 4.735363006591797 LOSS_Discriminator: 0.09384669363498688\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 4.831636428833008 LOSS_Discriminator: 0.11095646023750305\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 4.85472297668457 LOSS_Discriminator: 0.0923563688993454\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 5.561736106872559 LOSS_Discriminator: 0.22511833906173706\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 4.659879684448242 LOSS_Discriminator: 0.11265908926725388\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 4.223965644836426 LOSS_Discriminator: 0.11982140690088272\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 4.246593475341797 LOSS_Discriminator: 0.26036450266838074\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.303505897521973 LOSS_Discriminator: 0.07390862703323364\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 4.4267964363098145 LOSS_Discriminator: 0.15778285264968872\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 4.659905910491943 LOSS_Discriminator: 0.19306635856628418\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 4.540886878967285 LOSS_Discriminator: 0.08638159930706024\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 4.905284404754639 LOSS_Discriminator: 0.13309291005134583\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 5.175540924072266 LOSS_Discriminator: 0.0731443539261818\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 4.502766132354736 LOSS_Discriminator: 0.346801221370697\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 5.102591037750244 LOSS_Discriminator: 0.04748653993010521\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 4.777817726135254 LOSS_Discriminator: 0.13084836304187775\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 4.547491550445557 LOSS_Discriminator: 0.062286894768476486\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 4.679013252258301 LOSS_Discriminator: 0.06093169003725052\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 5.319129943847656 LOSS_Discriminator: 0.20763853192329407\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 5.537753582000732 LOSS_Discriminator: 0.0258108451962471\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 5.654772758483887 LOSS_Discriminator: 0.050857510417699814\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 5.891115188598633 LOSS_Discriminator: 0.10269957780838013\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 5.132480144500732 LOSS_Discriminator: 0.08703404664993286\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 4.9654059410095215 LOSS_Discriminator: 0.32759252190589905\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 3.8596677780151367 LOSS_Discriminator: 0.07998311519622803\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 3.4137678146362305 LOSS_Discriminator: 0.11542179435491562\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 5.394903182983398 LOSS_Discriminator: 0.17496232688426971\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 4.907899856567383 LOSS_Discriminator: 0.3768993020057678\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 5.370024681091309 LOSS_Discriminator: 0.05458787828683853\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 6.0505876541137695 LOSS_Discriminator: 0.17950327694416046\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 6.219830513000488 LOSS_Discriminator: 0.22695772349834442\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 4.906184196472168 LOSS_Discriminator: 0.06964965164661407\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 4.816336631774902 LOSS_Discriminator: 0.12530051171779633\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 4.831884384155273 LOSS_Discriminator: 0.2533089816570282\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 4.2393269538879395 LOSS_Discriminator: 0.040241539478302\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 4.893023490905762 LOSS_Discriminator: 0.2929823398590088\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 4.823186874389648 LOSS_Discriminator: 0.09379331767559052\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 4.812128067016602 LOSS_Discriminator: 0.07097463309764862\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 5.640198707580566 LOSS_Discriminator: 0.01812109723687172\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 5.2227983474731445 LOSS_Discriminator: 0.2556301951408386\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 4.524196624755859 LOSS_Discriminator: 0.019598430022597313\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 4.328001022338867 LOSS_Discriminator: 0.20134255290031433\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 4.76102352142334 LOSS_Discriminator: 0.1631571352481842\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 3.89487361907959 LOSS_Discriminator: 0.3147769570350647\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 3.9455578327178955 LOSS_Discriminator: 0.24232710897922516\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 4.203970909118652 LOSS_Discriminator: 0.06583639234304428\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 4.278594017028809 LOSS_Discriminator: 0.33451205492019653\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 4.405930519104004 LOSS_Discriminator: 0.045954857021570206\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 5.226526260375977 LOSS_Discriminator: 0.14551743865013123\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 4.516654014587402 LOSS_Discriminator: 0.22001492977142334\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 5.004608154296875 LOSS_Discriminator: 0.053101226687431335\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 5.273848533630371 LOSS_Discriminator: 0.03248659148812294\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 5.596454620361328 LOSS_Discriminator: 0.16715335845947266\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 5.286276817321777 LOSS_Discriminator: 0.018404405564069748\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 5.557665824890137 LOSS_Discriminator: 0.02997758239507675\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 5.779207229614258 LOSS_Discriminator: 0.11493439972400665\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 5.745199203491211 LOSS_Discriminator: 0.11483828723430634\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 5.852636814117432 LOSS_Discriminator: 0.021203331649303436\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 5.624866485595703 LOSS_Discriminator: 0.09465377032756805\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 5.431521892547607 LOSS_Discriminator: 0.11462534964084625\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 5.063752174377441 LOSS_Discriminator: 0.12208037823438644\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 4.505355358123779 LOSS_Discriminator: 0.2295263409614563\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 4.269217014312744 LOSS_Discriminator: 0.0609130784869194\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 4.859462738037109 LOSS_Discriminator: 0.060609735548496246\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 5.502239227294922 LOSS_Discriminator: 0.05610578507184982\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 5.741363525390625 LOSS_Discriminator: 0.025902925059199333\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 6.145361423492432 LOSS_Discriminator: 0.12401890754699707\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 6.384548664093018 LOSS_Discriminator: 0.0814356580376625\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 5.989572525024414 LOSS_Discriminator: 0.2252730131149292\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 5.768078804016113 LOSS_Discriminator: 0.1960880160331726\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 5.063446998596191 LOSS_Discriminator: 0.16441011428833008\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 4.418613433837891 LOSS_Discriminator: 0.04872068762779236\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 4.765719413757324 LOSS_Discriminator: 0.09899342060089111\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 4.424737930297852 LOSS_Discriminator: 0.2718614339828491\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 5.013463020324707 LOSS_Discriminator: 0.15431658923625946\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 4.552453994750977 LOSS_Discriminator: 0.2435341775417328\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 4.621669292449951 LOSS_Discriminator: 0.3150334060192108\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 3.7560811042785645 LOSS_Discriminator: 0.27790331840515137\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 3.831911563873291 LOSS_Discriminator: 0.13670679926872253\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 4.279714584350586 LOSS_Discriminator: 0.06435759365558624\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 4.13626766204834 LOSS_Discriminator: 0.2353811413049698\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 4.973222732543945 LOSS_Discriminator: 0.17517025768756866\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 5.780038356781006 LOSS_Discriminator: 0.108315110206604\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 5.229351043701172 LOSS_Discriminator: 0.07521101087331772\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 5.761004447937012 LOSS_Discriminator: 0.2171131819486618\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 4.987957000732422 LOSS_Discriminator: 0.4098697304725647\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 4.304570198059082 LOSS_Discriminator: 0.1360122263431549\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 4.212851524353027 LOSS_Discriminator: 0.3901563286781311\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 3.8177669048309326 LOSS_Discriminator: 0.15961810946464539\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 4.703828811645508 LOSS_Discriminator: 0.23291923105716705\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 5.005914688110352 LOSS_Discriminator: 0.10437972843647003\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 5.333726406097412 LOSS_Discriminator: 0.08584661781787872\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 4.970344543457031 LOSS_Discriminator: 0.12869349122047424\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 4.599302291870117 LOSS_Discriminator: 0.41417545080184937\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 3.7662124633789062 LOSS_Discriminator: 0.09579399228096008\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 4.490922451019287 LOSS_Discriminator: 0.1706331968307495\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 4.809840202331543 LOSS_Discriminator: 0.13315075635910034\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 4.640433311462402 LOSS_Discriminator: 0.1143621876835823\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 5.517726421356201 LOSS_Discriminator: 0.032920293509960175\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 5.512421131134033 LOSS_Discriminator: 0.09327393770217896\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 5.932818412780762 LOSS_Discriminator: 0.03340534120798111\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 6.2950849533081055 LOSS_Discriminator: 0.16850484907627106\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 6.253530025482178 LOSS_Discriminator: 0.050880469381809235\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 5.924774646759033 LOSS_Discriminator: 0.2625107169151306\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 6.095541477203369 LOSS_Discriminator: 0.23056545853614807\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 6.062602519989014 LOSS_Discriminator: 0.09648129343986511\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 5.723326683044434 LOSS_Discriminator: 0.11456586420536041\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 5.686210632324219 LOSS_Discriminator: 0.22345876693725586\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 5.116367340087891 LOSS_Discriminator: 0.028673455119132996\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 5.63040018081665 LOSS_Discriminator: 0.04809163510799408\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 5.502927303314209 LOSS_Discriminator: 0.07310444116592407\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 5.17353630065918 LOSS_Discriminator: 0.2060178518295288\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 5.289556503295898 LOSS_Discriminator: 0.11909924447536469\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 4.819497585296631 LOSS_Discriminator: 0.16493311524391174\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 4.583777904510498 LOSS_Discriminator: 0.1118667721748352\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 4.695247650146484 LOSS_Discriminator: 0.1376221626996994\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 4.13911247253418 LOSS_Discriminator: 0.12207552790641785\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 3.7910847663879395 LOSS_Discriminator: 0.05715617537498474\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 4.2241530418396 LOSS_Discriminator: 0.20555642247200012\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 4.361706733703613 LOSS_Discriminator: 0.15426987409591675\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 4.403494834899902 LOSS_Discriminator: 0.108045294880867\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 4.262823104858398 LOSS_Discriminator: 0.13382698595523834\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 4.317972183227539 LOSS_Discriminator: 0.15432551503181458\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 4.218685150146484 LOSS_Discriminator: 0.12022259831428528\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 4.347983360290527 LOSS_Discriminator: 0.1910608857870102\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 4.4745354652404785 LOSS_Discriminator: 0.11328093707561493\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 4.645031929016113 LOSS_Discriminator: 0.16726815700531006\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 4.256280899047852 LOSS_Discriminator: 0.15014126896858215\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 4.521849632263184 LOSS_Discriminator: 0.3340302109718323\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 4.021875381469727 LOSS_Discriminator: 0.1959538757801056\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 4.652679443359375 LOSS_Discriminator: 0.07728682458400726\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 4.82990837097168 LOSS_Discriminator: 0.05412780120968819\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 5.582115173339844 LOSS_Discriminator: 0.15002629160881042\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 5.750346660614014 LOSS_Discriminator: 0.09674125164747238\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 5.148404121398926 LOSS_Discriminator: 0.2527279257774353\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 4.716094493865967 LOSS_Discriminator: 0.10706023871898651\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 5.335238456726074 LOSS_Discriminator: 0.20125040411949158\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 3.770158529281616 LOSS_Discriminator: 0.16961535811424255\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 3.364346981048584 LOSS_Discriminator: 0.1924418807029724\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 4.234885215759277 LOSS_Discriminator: 0.10205702483654022\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 4.036479473114014 LOSS_Discriminator: 0.1676267683506012\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 5.451564788818359 LOSS_Discriminator: 0.06408491730690002\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 6.265542507171631 LOSS_Discriminator: 0.14750291407108307\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 6.455799579620361 LOSS_Discriminator: 0.007603927049785852\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 6.952754974365234 LOSS_Discriminator: 0.1328643411397934\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 6.455044746398926 LOSS_Discriminator: 0.24937257170677185\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 5.343044281005859 LOSS_Discriminator: 0.3141080141067505\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 4.3152570724487305 LOSS_Discriminator: 0.07186932116746902\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 2.831178665161133 LOSS_Discriminator: 0.16429229080677032\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 3.4607980251312256 LOSS_Discriminator: 0.17109407484531403\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 3.903394937515259 LOSS_Discriminator: 0.11751003563404083\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 3.558135509490967 LOSS_Discriminator: 0.14398308098316193\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 4.300257682800293 LOSS_Discriminator: 0.09393604099750519\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 5.077456474304199 LOSS_Discriminator: 0.0971192792057991\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 4.995038032531738 LOSS_Discriminator: 0.147506982088089\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 5.073984146118164 LOSS_Discriminator: 0.17794321477413177\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 5.662652969360352 LOSS_Discriminator: 0.1304245889186859\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 5.478259086608887 LOSS_Discriminator: 0.10167963802814484\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 5.583441734313965 LOSS_Discriminator: 0.09622570872306824\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 5.25165319442749 LOSS_Discriminator: 0.1429624855518341\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 6.121723175048828 LOSS_Discriminator: 0.027551036328077316\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 5.356434345245361 LOSS_Discriminator: 0.14550015330314636\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 4.497969627380371 LOSS_Discriminator: 0.11655868589878082\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 4.419958114624023 LOSS_Discriminator: 0.11393222957849503\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 4.226357460021973 LOSS_Discriminator: 0.048020996153354645\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 4.235694885253906 LOSS_Discriminator: 0.18274980783462524\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 5.462650299072266 LOSS_Discriminator: 0.12400171160697937\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 5.437961101531982 LOSS_Discriminator: 0.18916456401348114\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 5.46239709854126 LOSS_Discriminator: 0.15953639149665833\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 5.305732727050781 LOSS_Discriminator: 0.04794398695230484\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 5.7550249099731445 LOSS_Discriminator: 0.21987614035606384\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 3.8902549743652344 LOSS_Discriminator: 0.11671954393386841\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 3.3361117839813232 LOSS_Discriminator: 0.2156226933002472\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 3.8772788047790527 LOSS_Discriminator: 0.23297885060310364\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 4.93535852432251 LOSS_Discriminator: 0.12724368274211884\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 5.914162635803223 LOSS_Discriminator: 0.08017562329769135\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 6.348807334899902 LOSS_Discriminator: 0.13750819861888885\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 6.4920244216918945 LOSS_Discriminator: 0.13612233102321625\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 6.5906219482421875 LOSS_Discriminator: 0.016506170853972435\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 6.128887176513672 LOSS_Discriminator: 0.09859414398670197\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 6.894618988037109 LOSS_Discriminator: 0.010047612711787224\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 6.459814071655273 LOSS_Discriminator: 0.19994506239891052\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 5.628177642822266 LOSS_Discriminator: 0.2183716893196106\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 4.54625129699707 LOSS_Discriminator: 0.25920742750167847\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 3.930867910385132 LOSS_Discriminator: 0.12830060720443726\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 3.8010895252227783 LOSS_Discriminator: 0.1349923014640808\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 3.4980998039245605 LOSS_Discriminator: 0.08323010802268982\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 4.0300092697143555 LOSS_Discriminator: 0.08288434892892838\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 3.9954094886779785 LOSS_Discriminator: 0.09639942646026611\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 4.435606956481934 LOSS_Discriminator: 0.2921408414840698\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 4.661783218383789 LOSS_Discriminator: 0.109331876039505\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 4.9154462814331055 LOSS_Discriminator: 0.07417257875204086\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 4.596920967102051 LOSS_Discriminator: 0.11755017191171646\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 4.058609962463379 LOSS_Discriminator: 0.09656575322151184\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 4.145697116851807 LOSS_Discriminator: 0.07431518286466599\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 4.330507278442383 LOSS_Discriminator: 0.0574575774371624\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 5.2515482902526855 LOSS_Discriminator: 0.04251767694950104\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 5.128884315490723 LOSS_Discriminator: 0.08791796863079071\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 6.178676128387451 LOSS_Discriminator: 0.026639819145202637\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 5.984828948974609 LOSS_Discriminator: 0.22475922107696533\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 5.365262985229492 LOSS_Discriminator: 0.025638803839683533\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 4.98272705078125 LOSS_Discriminator: 0.18476900458335876\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 5.202159881591797 LOSS_Discriminator: 0.11917661130428314\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 4.563797473907471 LOSS_Discriminator: 0.18923941254615784\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 3.705650568008423 LOSS_Discriminator: 0.11452051997184753\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 3.926157236099243 LOSS_Discriminator: 0.1318720579147339\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 4.4961957931518555 LOSS_Discriminator: 0.24987144768238068\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 4.768176078796387 LOSS_Discriminator: 0.19385400414466858\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 5.089651584625244 LOSS_Discriminator: 0.0991860032081604\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 5.183953762054443 LOSS_Discriminator: 0.12565672397613525\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 5.614223957061768 LOSS_Discriminator: 0.01847747713327408\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 5.803735733032227 LOSS_Discriminator: 0.11206644773483276\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 5.318826675415039 LOSS_Discriminator: 0.2403901219367981\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 4.857105731964111 LOSS_Discriminator: 0.15011169016361237\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 4.167361259460449 LOSS_Discriminator: 0.08187256753444672\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 3.728322982788086 LOSS_Discriminator: 0.18185541033744812\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 3.6863510608673096 LOSS_Discriminator: 0.164002925157547\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 4.235246658325195 LOSS_Discriminator: 0.17212647199630737\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.126047134399414 LOSS_Discriminator: 0.11032333970069885\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 5.641667366027832 LOSS_Discriminator: 0.07582522183656693\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 6.185893535614014 LOSS_Discriminator: 0.2500544786453247\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 6.178249835968018 LOSS_Discriminator: 0.004020899534225464\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 6.420965194702148 LOSS_Discriminator: 0.019549664109945297\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 6.20583438873291 LOSS_Discriminator: 0.34683680534362793\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 5.561514377593994 LOSS_Discriminator: 0.270321786403656\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 4.783176422119141 LOSS_Discriminator: 0.09105370193719864\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 4.38798713684082 LOSS_Discriminator: 0.0969420075416565\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 4.303337097167969 LOSS_Discriminator: 0.12793704867362976\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 3.74141526222229 LOSS_Discriminator: 0.10753312706947327\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 4.1767988204956055 LOSS_Discriminator: 0.07389986515045166\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 4.454049110412598 LOSS_Discriminator: 0.10048370063304901\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 5.237898349761963 LOSS_Discriminator: 0.1488671600818634\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 5.253350257873535 LOSS_Discriminator: 0.2266603708267212\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 5.644617080688477 LOSS_Discriminator: 0.025310490280389786\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 5.566357612609863 LOSS_Discriminator: 0.11023316532373428\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 4.929031848907471 LOSS_Discriminator: 0.33098340034484863\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.479095458984375 LOSS_Discriminator: 0.08334191143512726\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 4.765265464782715 LOSS_Discriminator: 0.26435011625289917\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 4.45325231552124 LOSS_Discriminator: 0.07412295043468475\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 5.128911972045898 LOSS_Discriminator: 0.07046261429786682\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 5.284259796142578 LOSS_Discriminator: 0.04978440701961517\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 5.803820610046387 LOSS_Discriminator: 0.011201238259673119\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 6.604741096496582 LOSS_Discriminator: 0.052424658089876175\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 6.110008239746094 LOSS_Discriminator: 0.18642982840538025\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 5.918846130371094 LOSS_Discriminator: 0.00908539816737175\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 5.319738864898682 LOSS_Discriminator: 0.060092002153396606\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 5.476820468902588 LOSS_Discriminator: 0.1341608464717865\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 4.792541027069092 LOSS_Discriminator: 0.2205449938774109\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 4.556601524353027 LOSS_Discriminator: 0.10978754609823227\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 3.697474718093872 LOSS_Discriminator: 0.24840104579925537\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 4.377900123596191 LOSS_Discriminator: 0.2076617032289505\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 4.521749496459961 LOSS_Discriminator: 0.16091114282608032\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 5.496950149536133 LOSS_Discriminator: 0.04470333456993103\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 5.157628059387207 LOSS_Discriminator: 0.1055183857679367\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 5.27846097946167 LOSS_Discriminator: 0.24332129955291748\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 5.3325300216674805 LOSS_Discriminator: 0.03065752051770687\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 5.764479160308838 LOSS_Discriminator: 0.05240999907255173\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 5.0992231369018555 LOSS_Discriminator: 0.3600717782974243\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 4.163081645965576 LOSS_Discriminator: 0.32553935050964355\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 4.382566928863525 LOSS_Discriminator: 0.20558255910873413\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 4.746743679046631 LOSS_Discriminator: 0.1888599693775177\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 4.357692718505859 LOSS_Discriminator: 0.15289494395256042\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 3.528913974761963 LOSS_Discriminator: 0.1375424563884735\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 4.111747741699219 LOSS_Discriminator: 0.07591970264911652\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 4.426502704620361 LOSS_Discriminator: 0.4088236093521118\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 5.007132530212402 LOSS_Discriminator: 0.15008974075317383\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 4.158759117126465 LOSS_Discriminator: 0.05598343908786774\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 4.643428802490234 LOSS_Discriminator: 0.09153468161821365\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 5.2779107093811035 LOSS_Discriminator: 0.15452051162719727\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 4.748072147369385 LOSS_Discriminator: 0.088919997215271\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 4.3199462890625 LOSS_Discriminator: 0.12968996167182922\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 4.656346797943115 LOSS_Discriminator: 0.05506640300154686\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 4.8470659255981445 LOSS_Discriminator: 0.1101507768034935\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 4.78223180770874 LOSS_Discriminator: 0.16670626401901245\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 4.50502872467041 LOSS_Discriminator: 0.10264921188354492\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 4.395027160644531 LOSS_Discriminator: 0.08591615408658981\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 4.114553451538086 LOSS_Discriminator: 0.13309518992900848\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 3.6908888816833496 LOSS_Discriminator: 0.07773564010858536\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 4.173581123352051 LOSS_Discriminator: 0.08325248956680298\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 4.74018669128418 LOSS_Discriminator: 0.08784634619951248\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 5.143509864807129 LOSS_Discriminator: 0.22484494745731354\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 4.585101127624512 LOSS_Discriminator: 0.25124651193618774\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 4.837165832519531 LOSS_Discriminator: 0.15527740120887756\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 4.918856143951416 LOSS_Discriminator: 0.07261042296886444\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 5.272805213928223 LOSS_Discriminator: 0.017498590052127838\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 5.831820011138916 LOSS_Discriminator: 0.09779134392738342\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 5.511792182922363 LOSS_Discriminator: 0.027038361877202988\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 5.525565147399902 LOSS_Discriminator: 0.13639283180236816\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 6.144880294799805 LOSS_Discriminator: 0.04304167255759239\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 6.114683151245117 LOSS_Discriminator: 0.10893677920103073\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 5.543046951293945 LOSS_Discriminator: 0.07852378487586975\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 5.885305881500244 LOSS_Discriminator: 0.3341084122657776\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 4.852336883544922 LOSS_Discriminator: 0.41011151671409607\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 3.597519874572754 LOSS_Discriminator: 0.13966549932956696\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 3.4825499057769775 LOSS_Discriminator: 0.0846986323595047\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 2.612218141555786 LOSS_Discriminator: 0.2488245666027069\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 4.534793853759766 LOSS_Discriminator: 0.14632095396518707\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 5.199245452880859 LOSS_Discriminator: 0.067970871925354\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 6.107224464416504 LOSS_Discriminator: 0.10663197934627533\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 6.171663284301758 LOSS_Discriminator: 0.2810993492603302\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 6.382676124572754 LOSS_Discriminator: 0.2765096426010132\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 5.375141143798828 LOSS_Discriminator: 0.0871497094631195\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 4.967778205871582 LOSS_Discriminator: 0.30163922905921936\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 4.023397445678711 LOSS_Discriminator: 0.15165166556835175\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 3.9801831245422363 LOSS_Discriminator: 0.30484139919281006\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 3.4223945140838623 LOSS_Discriminator: 0.04407472908496857\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 4.077883720397949 LOSS_Discriminator: 0.18466714024543762\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 4.481644630432129 LOSS_Discriminator: 0.07634495198726654\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 4.365405082702637 LOSS_Discriminator: 0.05986397713422775\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 4.603610038757324 LOSS_Discriminator: 0.3286972939968109\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 3.9732115268707275 LOSS_Discriminator: 0.2156030237674713\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 4.467527389526367 LOSS_Discriminator: 0.045405082404613495\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 4.314300537109375 LOSS_Discriminator: 0.12472940981388092\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 4.8594560623168945 LOSS_Discriminator: 0.1404295116662979\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 4.524421215057373 LOSS_Discriminator: 0.10950472950935364\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 4.693010330200195 LOSS_Discriminator: 0.14634571969509125\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 4.375713348388672 LOSS_Discriminator: 0.05363963544368744\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 4.0961198806762695 LOSS_Discriminator: 0.06309260427951813\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 4.7278337478637695 LOSS_Discriminator: 0.09478870034217834\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 4.468117713928223 LOSS_Discriminator: 0.07226430624723434\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 4.765459060668945 LOSS_Discriminator: 0.03819116950035095\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 5.615573406219482 LOSS_Discriminator: 0.062037087976932526\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 5.340660572052002 LOSS_Discriminator: 0.06042996793985367\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 4.850759506225586 LOSS_Discriminator: 0.05323868244886398\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 5.758462905883789 LOSS_Discriminator: 0.07302743196487427\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 5.482176780700684 LOSS_Discriminator: 0.17541155219078064\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 5.304977893829346 LOSS_Discriminator: 0.03461316600441933\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 5.8988447189331055 LOSS_Discriminator: 0.03130899369716644\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 5.460745811462402 LOSS_Discriminator: 0.06479141116142273\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 5.583320140838623 LOSS_Discriminator: 0.015538616105914116\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 5.362744331359863 LOSS_Discriminator: 0.214131161570549\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 4.509113311767578 LOSS_Discriminator: 0.2970903217792511\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 4.127503871917725 LOSS_Discriminator: 0.06897842884063721\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 4.666357040405273 LOSS_Discriminator: 0.13505753874778748\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 4.106553554534912 LOSS_Discriminator: 0.10973681509494781\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 3.827587842941284 LOSS_Discriminator: 0.13096725940704346\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 4.721281051635742 LOSS_Discriminator: 0.05704767256975174\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 5.919967174530029 LOSS_Discriminator: 0.14082565903663635\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 5.809856414794922 LOSS_Discriminator: 0.14635230600833893\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 5.82133150100708 LOSS_Discriminator: 0.12973269820213318\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 5.638012886047363 LOSS_Discriminator: 0.12939517199993134\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 5.801028251647949 LOSS_Discriminator: 0.1180083304643631\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 5.015000343322754 LOSS_Discriminator: 0.09194639325141907\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 4.704137325286865 LOSS_Discriminator: 0.04051974415779114\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 4.441586494445801 LOSS_Discriminator: 0.10204290598630905\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 4.144708633422852 LOSS_Discriminator: 0.16830451786518097\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 3.5690360069274902 LOSS_Discriminator: 0.09462380409240723\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 5.116399765014648 LOSS_Discriminator: 0.1162731796503067\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 5.362695693969727 LOSS_Discriminator: 0.022993609309196472\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 5.436373233795166 LOSS_Discriminator: 0.06508906185626984\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 5.837039470672607 LOSS_Discriminator: 0.14043313264846802\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 5.5272216796875 LOSS_Discriminator: 0.16357490420341492\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 5.277612209320068 LOSS_Discriminator: 0.2676032781600952\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 4.176022529602051 LOSS_Discriminator: 0.1882079839706421\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 3.6208176612854004 LOSS_Discriminator: 0.22120261192321777\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 3.2355294227600098 LOSS_Discriminator: 0.2982270419597626\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 4.239320755004883 LOSS_Discriminator: 0.26050758361816406\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 4.94748067855835 LOSS_Discriminator: 0.25548839569091797\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 5.500641345977783 LOSS_Discriminator: 0.10132205486297607\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 6.864187240600586 LOSS_Discriminator: 0.06645724177360535\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 6.409555435180664 LOSS_Discriminator: 0.06683796644210815\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 7.20371150970459 LOSS_Discriminator: 0.02558790147304535\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 7.001509189605713 LOSS_Discriminator: 0.05563326179981232\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 7.26444149017334 LOSS_Discriminator: 0.06707584112882614\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 5.640484809875488 LOSS_Discriminator: 0.09776866436004639\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 6.479890823364258 LOSS_Discriminator: 0.04091019928455353\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 5.019912242889404 LOSS_Discriminator: 0.08583521842956543\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 4.828913688659668 LOSS_Discriminator: 0.14374522864818573\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 4.281157970428467 LOSS_Discriminator: 0.3837376832962036\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 4.270696640014648 LOSS_Discriminator: 0.048927344381809235\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 4.801347255706787 LOSS_Discriminator: 0.04155055433511734\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 4.737940788269043 LOSS_Discriminator: 0.0981966033577919\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 4.5749664306640625 LOSS_Discriminator: 0.03668612241744995\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 5.031458377838135 LOSS_Discriminator: 0.11126062273979187\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 5.371852874755859 LOSS_Discriminator: 0.07962242513895035\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.694235801696777 LOSS_Discriminator: 0.12125910073518753\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 5.757504463195801 LOSS_Discriminator: 0.024761080741882324\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 5.669707298278809 LOSS_Discriminator: 0.11224223673343658\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 6.023497581481934 LOSS_Discriminator: 0.07877960056066513\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 4.862822532653809 LOSS_Discriminator: 0.15916012227535248\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 4.452348232269287 LOSS_Discriminator: 0.10908398777246475\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 4.106785774230957 LOSS_Discriminator: 0.12719230353832245\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 3.568732976913452 LOSS_Discriminator: 0.061222437769174576\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 4.312373638153076 LOSS_Discriminator: 0.12062619626522064\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 4.1297502517700195 LOSS_Discriminator: 0.24458745121955872\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 4.623953819274902 LOSS_Discriminator: 0.05265344679355621\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 4.776278972625732 LOSS_Discriminator: 0.12856939435005188\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 5.245226860046387 LOSS_Discriminator: 0.017651069909334183\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 5.136191368103027 LOSS_Discriminator: 0.11295123398303986\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 5.6501264572143555 LOSS_Discriminator: 0.1152176707983017\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 5.983301162719727 LOSS_Discriminator: 0.1792619675397873\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 5.383902072906494 LOSS_Discriminator: 0.15232133865356445\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 4.23613166809082 LOSS_Discriminator: 0.49539560079574585\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 3.525106906890869 LOSS_Discriminator: 0.07669933140277863\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 2.9787180423736572 LOSS_Discriminator: 0.1513386070728302\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 3.330313205718994 LOSS_Discriminator: 0.22248543798923492\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 4.224819183349609 LOSS_Discriminator: 0.12408576905727386\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 5.783249378204346 LOSS_Discriminator: 0.04815961793065071\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 6.320883750915527 LOSS_Discriminator: 0.0536746084690094\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 6.588154315948486 LOSS_Discriminator: 0.32353660464286804\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 6.830294609069824 LOSS_Discriminator: 0.07617084681987762\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 6.0755462646484375 LOSS_Discriminator: 0.27915486693382263\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 6.034463882446289 LOSS_Discriminator: 0.008239814080297947\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 5.2811455726623535 LOSS_Discriminator: 0.0471581369638443\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 4.652504920959473 LOSS_Discriminator: 0.06961618363857269\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 5.099389553070068 LOSS_Discriminator: 0.12045978754758835\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 4.890092372894287 LOSS_Discriminator: 0.12958188354969025\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 4.470245838165283 LOSS_Discriminator: 0.21242505311965942\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 4.861989974975586 LOSS_Discriminator: 0.12543202936649323\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 5.332702159881592 LOSS_Discriminator: 0.07010789215564728\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 5.1663689613342285 LOSS_Discriminator: 0.09135561436414719\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 5.632646560668945 LOSS_Discriminator: 0.014821257442235947\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 5.3773298263549805 LOSS_Discriminator: 0.2015089988708496\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 5.370769023895264 LOSS_Discriminator: 0.1326911747455597\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 4.339486122131348 LOSS_Discriminator: 0.14504028856754303\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 4.611410617828369 LOSS_Discriminator: 0.1373835802078247\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 3.8827126026153564 LOSS_Discriminator: 0.29855161905288696\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 3.6538798809051514 LOSS_Discriminator: 0.13796353340148926\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 3.455321788787842 LOSS_Discriminator: 0.17515142261981964\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 4.009576320648193 LOSS_Discriminator: 0.1341274082660675\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 4.885944366455078 LOSS_Discriminator: 0.0822279155254364\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 5.486051559448242 LOSS_Discriminator: 0.1358814835548401\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 5.889945983886719 LOSS_Discriminator: 0.07604075223207474\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 5.544331073760986 LOSS_Discriminator: 0.25308850407600403\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 4.64528751373291 LOSS_Discriminator: 0.14994248747825623\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 4.266783714294434 LOSS_Discriminator: 0.2335113137960434\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 3.822925090789795 LOSS_Discriminator: 0.2335943728685379\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 3.5486886501312256 LOSS_Discriminator: 0.07647309452295303\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 4.373546123504639 LOSS_Discriminator: 0.07578971982002258\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 4.623006820678711 LOSS_Discriminator: 0.0906473845243454\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 5.524709701538086 LOSS_Discriminator: 0.11633606255054474\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 5.643119812011719 LOSS_Discriminator: 0.20358604192733765\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 5.323023319244385 LOSS_Discriminator: 0.04119374603033066\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 4.976635456085205 LOSS_Discriminator: 0.05520476773381233\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 4.939460754394531 LOSS_Discriminator: 0.09087969362735748\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 5.085897445678711 LOSS_Discriminator: 0.09497752785682678\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 4.792316436767578 LOSS_Discriminator: 0.08766309171915054\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 4.650549411773682 LOSS_Discriminator: 0.16770842671394348\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 5.250938415527344 LOSS_Discriminator: 0.1264578104019165\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 5.071727275848389 LOSS_Discriminator: 0.11099021881818771\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 5.010391712188721 LOSS_Discriminator: 0.05692250281572342\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 5.328400135040283 LOSS_Discriminator: 0.04646811634302139\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 5.291877746582031 LOSS_Discriminator: 0.12699198722839355\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 5.550729274749756 LOSS_Discriminator: 0.060217250138521194\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 5.397202491760254 LOSS_Discriminator: 0.05099210888147354\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 4.965458869934082 LOSS_Discriminator: 0.013797825202345848\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 5.187777519226074 LOSS_Discriminator: 0.15533986687660217\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 4.812288761138916 LOSS_Discriminator: 0.017146112397313118\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 4.9009270668029785 LOSS_Discriminator: 0.06066690385341644\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 5.272180557250977 LOSS_Discriminator: 0.05380956456065178\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 5.587182998657227 LOSS_Discriminator: 0.05946095287799835\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 5.173162460327148 LOSS_Discriminator: 0.05968020111322403\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 5.72676944732666 LOSS_Discriminator: 0.013082408346235752\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 5.337265968322754 LOSS_Discriminator: 0.024411261081695557\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 5.978424549102783 LOSS_Discriminator: 0.06464298069477081\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 6.351520538330078 LOSS_Discriminator: 0.10227198898792267\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 5.620244979858398 LOSS_Discriminator: 0.06916190683841705\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 5.294683456420898 LOSS_Discriminator: 0.007275906391441822\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 6.007772922515869 LOSS_Discriminator: 0.23160938918590546\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 5.2019171714782715 LOSS_Discriminator: 0.05215734243392944\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 4.918732643127441 LOSS_Discriminator: 0.18922075629234314\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 4.778817176818848 LOSS_Discriminator: 0.09376867860555649\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 4.898447513580322 LOSS_Discriminator: 0.36142048239707947\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 4.185230255126953 LOSS_Discriminator: 0.21961769461631775\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 4.831118583679199 LOSS_Discriminator: 0.1595611423254013\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 5.5159382820129395 LOSS_Discriminator: 0.17658860981464386\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 5.705163955688477 LOSS_Discriminator: 0.3365217447280884\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 5.01369571685791 LOSS_Discriminator: 0.11175630986690521\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 4.988893985748291 LOSS_Discriminator: 0.13767486810684204\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 4.997801780700684 LOSS_Discriminator: 0.14080211520195007\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 5.002052307128906 LOSS_Discriminator: 0.10030069947242737\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 4.683546543121338 LOSS_Discriminator: 0.23390458524227142\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 4.8116302490234375 LOSS_Discriminator: 0.3817644715309143\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 4.903374671936035 LOSS_Discriminator: 0.11033393442630768\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 5.304495334625244 LOSS_Discriminator: 0.07079624384641647\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 5.057126998901367 LOSS_Discriminator: 0.05178850144147873\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 5.32340145111084 LOSS_Discriminator: 0.11973054707050323\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 5.016069412231445 LOSS_Discriminator: 0.28636664152145386\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 4.3202667236328125 LOSS_Discriminator: 0.2995533347129822\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 3.3561806678771973 LOSS_Discriminator: 0.12619641423225403\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 4.288957118988037 LOSS_Discriminator: 0.1739072948694229\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 4.209171295166016 LOSS_Discriminator: 0.07449411600828171\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 4.192653656005859 LOSS_Discriminator: 0.06718878448009491\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 5.265404224395752 LOSS_Discriminator: 0.1334550529718399\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 5.771720886230469 LOSS_Discriminator: 0.17768177390098572\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 5.2909674644470215 LOSS_Discriminator: 0.10679659992456436\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 5.163460731506348 LOSS_Discriminator: 0.04912864416837692\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 4.713863372802734 LOSS_Discriminator: 0.2011125385761261\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 4.7242751121521 LOSS_Discriminator: 0.02744935266673565\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 4.505163192749023 LOSS_Discriminator: 0.13059626519680023\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 4.032787322998047 LOSS_Discriminator: 0.2616382837295532\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 3.46848201751709 LOSS_Discriminator: 0.23019041121006012\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 3.643064498901367 LOSS_Discriminator: 0.1709844470024109\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 4.7619218826293945 LOSS_Discriminator: 0.1879551112651825\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 6.3815693855285645 LOSS_Discriminator: 0.016000762581825256\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 7.030727863311768 LOSS_Discriminator: 0.09976731985807419\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 7.334766387939453 LOSS_Discriminator: 0.18420955538749695\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 7.549618244171143 LOSS_Discriminator: 0.3844224810600281\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 6.777878761291504 LOSS_Discriminator: 0.16228477656841278\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 6.468411445617676 LOSS_Discriminator: 0.14648562669754028\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 4.918679237365723 LOSS_Discriminator: 0.021045630797743797\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 4.641273498535156 LOSS_Discriminator: 0.1681947261095047\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 3.886117458343506 LOSS_Discriminator: 0.060119181871414185\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 3.431368350982666 LOSS_Discriminator: 0.1380395144224167\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 3.352794885635376 LOSS_Discriminator: 0.15907809138298035\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 3.4671688079833984 LOSS_Discriminator: 0.1521901786327362\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 3.721224308013916 LOSS_Discriminator: 0.2384164035320282\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 4.005904197692871 LOSS_Discriminator: 0.0658121332526207\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 4.2719902992248535 LOSS_Discriminator: 0.06597962230443954\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 4.879555702209473 LOSS_Discriminator: 0.07344789803028107\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 5.45664119720459 LOSS_Discriminator: 0.03350677713751793\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 5.512745380401611 LOSS_Discriminator: 0.14056766033172607\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 4.907953262329102 LOSS_Discriminator: 0.10999210923910141\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 4.453566074371338 LOSS_Discriminator: 0.36102616786956787\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 4.8925981521606445 LOSS_Discriminator: 0.06473589688539505\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 4.429059028625488 LOSS_Discriminator: 0.10103702545166016\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 4.302722454071045 LOSS_Discriminator: 0.05827369913458824\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 4.469918727874756 LOSS_Discriminator: 0.044897496700286865\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 5.528565406799316 LOSS_Discriminator: 0.04204689711332321\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 5.323571681976318 LOSS_Discriminator: 0.11177344620227814\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 5.319408416748047 LOSS_Discriminator: 0.19138586521148682\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 4.608975410461426 LOSS_Discriminator: 0.175853431224823\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 4.87440299987793 LOSS_Discriminator: 0.12129954993724823\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 4.126049041748047 LOSS_Discriminator: 0.0355510339140892\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 3.8493268489837646 LOSS_Discriminator: 0.07071791589260101\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 4.893526554107666 LOSS_Discriminator: 0.0599629282951355\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 4.993781566619873 LOSS_Discriminator: 0.06833680719137192\n",
            "EPOCH OVER: 19\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwv8HOMdje49olbIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Image_no 1\n",
            "Image_no 2\n",
            "Image_no 3\n",
            "Image_no 4\n",
            "Image_no 5\n",
            "Image_no 6\n",
            "Image_no 7\n",
            "Image_no 8\n",
            "Image_no 9\n",
            "Image_no 10\n",
            "Image_no 11\n",
            "Image_no 12\n",
            "Image_no 13\n",
            "Image_no 14\n",
            "Image_no 15\n",
            "Image_no 16\n",
            "Image_no 17\n",
            "Image_no 18\n",
            "Image_no 19\n",
            "Image_no 20\n",
            "Image_no 21\n",
            "Image_no 22\n",
            "Image_no 23\n",
            "Image_no 24\n",
            "Image_no 25\n",
            "Image_no 26\n",
            "Image_no 27\n",
            "Image_no 28\n",
            "Image_no 29\n",
            "Image_no 30\n",
            "Image_no 31\n",
            "Image_no 32\n",
            "Image_no 33\n",
            "Image_no 34\n",
            "Image_no 35\n",
            "Image_no 36\n",
            "Image_no 37\n",
            "Image_no 38\n",
            "Image_no 39\n",
            "Image_no 40\n",
            "Image_no 41\n",
            "Image_no 42\n",
            "Image_no 43\n",
            "Image_no 44\n",
            "Image_no 45\n",
            "Image_no 46\n",
            "Image_no 47\n",
            "Image_no 48\n",
            "Image_no 49\n",
            "Image_no 50\n",
            "Image_no 51\n",
            "Image_no 52\n",
            "Image_no 53\n",
            "Image_no 54\n",
            "Image_no 55\n",
            "Image_no 56\n",
            "Image_no 57\n",
            "Image_no 58\n",
            "Image_no 59\n",
            "Image_no 60\n",
            "Image_no 61\n",
            "Image_no 62\n",
            "Image_no 63\n",
            "Image_no 64\n",
            "Image_no 65\n",
            "Image_no 66\n",
            "Image_no 67\n",
            "Image_no 68\n",
            "Image_no 69\n",
            "Image_no 70\n",
            "Image_no 71\n",
            "Image_no 72\n",
            "Image_no 73\n",
            "Image_no 74\n",
            "Image_no 75\n",
            "Image_no 76\n",
            "Image_no 77\n",
            "Image_no 78\n",
            "Image_no 79\n",
            "Image_no 80\n",
            "Image_no 81\n",
            "Image_no 82\n",
            "Image_no 83\n",
            "Image_no 84\n",
            "Image_no 85\n",
            "Image_no 86\n",
            "Image_no 87\n",
            "Image_no 88\n",
            "Image_no 89\n",
            "Image_no 90\n",
            "Image_no 91\n",
            "Image_no 92\n",
            "Image_no 93\n",
            "Image_no 94\n",
            "Image_no 95\n",
            "Image_no 96\n",
            "Image_no 97\n",
            "Image_no 98\n",
            "Image_no 99\n",
            "Image_no 100\n",
            "Image_no 101\n",
            "Image_no 102\n",
            "Image_no 103\n",
            "Image_no 104\n",
            "Image_no 105\n",
            "Image_no 106\n",
            "Image_no 107\n",
            "Image_no 108\n",
            "Image_no 109\n",
            "Image_no 110\n",
            "Image_no 111\n",
            "Image_no 112\n",
            "Image_no 113\n",
            "Image_no 114\n",
            "Image_no 115\n",
            "Image_no 116\n",
            "Image_no 117\n",
            "Image_no 118\n",
            "Image_no 119\n",
            "Image_no 120\n",
            "Image_no 121\n",
            "Image_no 122\n",
            "Image_no 123\n",
            "Image_no 124\n",
            "Image_no 125\n",
            "Image_no 126\n",
            "Image_no 127\n",
            "Image_no 128\n",
            "Image_no 129\n",
            "Image_no 130\n",
            "Image_no 131\n",
            "Image_no 132\n",
            "Image_no 133\n",
            "Image_no 134\n",
            "Image_no 135\n",
            "Image_no 136\n",
            "Image_no 137\n",
            "Image_no 138\n",
            "Image_no 139\n",
            "Image_no 140\n",
            "Image_no 141\n",
            "Image_no 142\n",
            "Image_no 143\n",
            "Image_no 144\n",
            "Image_no 145\n",
            "Image_no 146\n",
            "Image_no 147\n",
            "Image_no 148\n",
            "Image_no 149\n",
            "Image_no 150\n",
            "Image_no 151\n",
            "Image_no 152\n",
            "Image_no 153\n",
            "Image_no 154\n",
            "Image_no 155\n",
            "Image_no 156\n",
            "Image_no 157\n",
            "Image_no 158\n",
            "Image_no 159\n",
            "Image_no 160\n",
            "Image_no 161\n",
            "Image_no 162\n",
            "Image_no 163\n",
            "Image_no 164\n",
            "Image_no 165\n",
            "Image_no 166\n",
            "Image_no 167\n",
            "Image_no 168\n",
            "Image_no 169\n",
            "Image_no 170\n",
            "Image_no 171\n",
            "Image_no 172\n",
            "Image_no 173\n",
            "Image_no 174\n",
            "Image_no 175\n",
            "Image_no 176\n",
            "Image_no 177\n",
            "Image_no 178\n",
            "Image_no 179\n",
            "Image_no 180\n",
            "Image_no 181\n",
            "Image_no 182\n",
            "Image_no 183\n",
            "Image_no 184\n",
            "Image_no 185\n",
            "Image_no 186\n",
            "Image_no 187\n",
            "Image_no 188\n",
            "Image_no 189\n",
            "Image_no 190\n",
            "Image_no 191\n",
            "Image_no 192\n",
            "Image_no 193\n",
            "Image_no 194\n",
            "Image_no 195\n",
            "Image_no 196\n",
            "Image_no 197\n",
            "Image_no 198\n",
            "Image_no 199\n",
            "Image_no 200\n",
            "Image_no 201\n",
            "Image_no 202\n",
            "Image_no 203\n",
            "Image_no 204\n",
            "Image_no 205\n",
            "Image_no 206\n",
            "Image_no 207\n",
            "Image_no 208\n",
            "Image_no 209\n",
            "Image_no 210\n",
            "Image_no 211\n",
            "Image_no 212\n",
            "Image_no 213\n",
            "Image_no 214\n",
            "Image_no 215\n",
            "Image_no 216\n",
            "Image_no 217\n",
            "Image_no 218\n",
            "Image_no 219\n",
            "Image_no 220\n",
            "Image_no 221\n",
            "Image_no 222\n",
            "Image_no 223\n",
            "Image_no 224\n",
            "Image_no 225\n",
            "Image_no 226\n",
            "Image_no 227\n",
            "Image_no 228\n",
            "Image_no 229\n",
            "Image_no 230\n",
            "Image_no 231\n",
            "Image_no 232\n",
            "Image_no 233\n",
            "Image_no 234\n",
            "Image_no 235\n",
            "Image_no 236\n",
            "Image_no 237\n",
            "Image_no 238\n",
            "Image_no 239\n",
            "Image_no 240\n",
            "Image_no 241\n",
            "Image_no 242\n",
            "Image_no 243\n",
            "Image_no 244\n",
            "Image_no 245\n",
            "Image_no 246\n",
            "Image_no 247\n",
            "Image_no 248\n",
            "Image_no 249\n",
            "Image_no 250\n",
            "Image_no 251\n",
            "Image_no 252\n",
            "Image_no 253\n",
            "Image_no 254\n",
            "Image_no 255\n",
            "Image_no 256\n",
            "Image_no 257\n",
            "Image_no 258\n",
            "Image_no 259\n",
            "Image_no 260\n",
            "Image_no 261\n",
            "Image_no 262\n",
            "Image_no 263\n",
            "Image_no 264\n",
            "Image_no 265\n",
            "Image_no 266\n",
            "Image_no 267\n",
            "Image_no 268\n",
            "Image_no 269\n",
            "Image_no 270\n",
            "Image_no 271\n",
            "Image_no 272\n",
            "Image_no 273\n",
            "Image_no 274\n",
            "Image_no 275\n",
            "Image_no 276\n",
            "Image_no 277\n",
            "Image_no 278\n",
            "Image_no 279\n",
            "Image_no 280\n",
            "Image_no 281\n",
            "Image_no 282\n",
            "Image_no 283\n",
            "Image_no 284\n",
            "Image_no 285\n",
            "Image_no 286\n",
            "Image_no 287\n",
            "Image_no 288\n",
            "Image_no 289\n",
            "Image_no 290\n",
            "Image_no 291\n",
            "Image_no 292\n",
            "Image_no 293\n",
            "Image_no 294\n",
            "Image_no 295\n",
            "Image_no 296\n",
            "Image_no 297\n",
            "Image_no 298\n",
            "Image_no 299\n",
            "Image_no 300\n",
            "Image_no 301\n",
            "Image_no 302\n",
            "Image_no 303\n",
            "Image_no 304\n",
            "Image_no 305\n",
            "Image_no 306\n",
            "Image_no 307\n",
            "Image_no 308\n",
            "Image_no 309\n",
            "Image_no 310\n",
            "Image_no 311\n",
            "Image_no 312\n",
            "Image_no 313\n",
            "Image_no 314\n",
            "Image_no 315\n",
            "Image_no 316\n",
            "Image_no 317\n",
            "Image_no 318\n",
            "Image_no 319\n",
            "Image_no 320\n",
            "Image_no 321\n",
            "Image_no 322\n",
            "Image_no 323\n",
            "Image_no 324\n",
            "Image_no 325\n",
            "Image_no 326\n",
            "Image_no 327\n",
            "Image_no 328\n",
            "Image_no 329\n",
            "Image_no 330\n",
            "Image_no 331\n",
            "Image_no 332\n",
            "Image_no 333\n",
            "Image_no 334\n",
            "Image_no 335\n",
            "Image_no 336\n",
            "Image_no 337\n",
            "Image_no 338\n",
            "Image_no 339\n",
            "Image_no 340\n",
            "Image_no 341\n",
            "Image_no 342\n",
            "Image_no 343\n",
            "Image_no 344\n",
            "Image_no 345\n",
            "Image_no 346\n",
            "Image_no 347\n",
            "Image_no 348\n",
            "Image_no 349\n",
            "Image_no 350\n",
            "Image_no 351\n",
            "Image_no 352\n",
            "Image_no 353\n",
            "Image_no 354\n",
            "Image_no 355\n",
            "Image_no 356\n",
            "Image_no 357\n",
            "Image_no 358\n",
            "Image_no 359\n",
            "Image_no 360\n",
            "Image_no 361\n",
            "Image_no 362\n",
            "Image_no 363\n",
            "Image_no 364\n",
            "Image_no 365\n",
            "Image_no 366\n",
            "Image_no 367\n",
            "Image_no 368\n",
            "Image_no 369\n",
            "Image_no 370\n",
            "Image_no 371\n",
            "Image_no 372\n",
            "Image_no 373\n",
            "Image_no 374\n",
            "Image_no 375\n",
            "Image_no 376\n",
            "Image_no 377\n",
            "Image_no 378\n",
            "Image_no 379\n",
            "Image_no 380\n",
            "Image_no 381\n",
            "Image_no 382\n",
            "Image_no 383\n",
            "Image_no 384\n",
            "Image_no 385\n",
            "Image_no 386\n",
            "Image_no 387\n",
            "Image_no 388\n",
            "Image_no 389\n",
            "Image_no 390\n",
            "Image_no 391\n",
            "Image_no 392\n",
            "Image_no 393\n",
            "Image_no 394\n",
            "Image_no 395\n",
            "Image_no 396\n",
            "Image_no 397\n",
            "Image_no 398\n",
            "Image_no 399\n",
            "Image_no 400\n",
            "Image_no 401\n",
            "Image_no 402\n",
            "Image_no 403\n",
            "Image_no 404\n",
            "Image_no 405\n",
            "Image_no 406\n",
            "Image_no 407\n",
            "Image_no 408\n",
            "Image_no 409\n",
            "Image_no 410\n",
            "Image_no 411\n",
            "Image_no 412\n",
            "Image_no 413\n",
            "Image_no 414\n",
            "Image_no 415\n",
            "Image_no 416\n",
            "Image_no 417\n",
            "Image_no 418\n",
            "Image_no 419\n",
            "Image_no 420\n",
            "Image_no 421\n",
            "Image_no 422\n",
            "Image_no 423\n",
            "Image_no 424\n",
            "Image_no 425\n",
            "Image_no 426\n",
            "Image_no 427\n",
            "Image_no 428\n",
            "Image_no 429\n",
            "Image_no 430\n",
            "Image_no 431\n",
            "Image_no 432\n",
            "Image_no 433\n",
            "Image_no 434\n",
            "Image_no 435\n",
            "Image_no 436\n",
            "Image_no 437\n",
            "Image_no 438\n",
            "Image_no 439\n",
            "Image_no 440\n",
            "Image_no 441\n",
            "Image_no 442\n",
            "Image_no 443\n",
            "Image_no 444\n",
            "Image_no 445\n",
            "Image_no 446\n",
            "Image_no 447\n",
            "Image_no 448\n",
            "Image_no 449\n",
            "Image_no 450\n",
            "Image_no 451\n",
            "Image_no 452\n",
            "Image_no 453\n",
            "Image_no 454\n",
            "Image_no 455\n",
            "Image_no 456\n",
            "Image_no 457\n",
            "Image_no 458\n",
            "Image_no 459\n",
            "Image_no 460\n",
            "Image_no 461\n",
            "Image_no 462\n",
            "Image_no 463\n",
            "Image_no 464\n",
            "Image_no 465\n",
            "Image_no 466\n",
            "Image_no 467\n",
            "Image_no 468\n",
            "Image_no 469\n",
            "Image_no 470\n",
            "Image_no 471\n",
            "Image_no 472\n",
            "Image_no 473\n",
            "Image_no 474\n",
            "Image_no 475\n",
            "Image_no 476\n",
            "Image_no 477\n",
            "Image_no 478\n",
            "Image_no 479\n",
            "Image_no 480\n",
            "Image_no 481\n",
            "Image_no 482\n",
            "Image_no 483\n",
            "Image_no 484\n",
            "Image_no 485\n",
            "Image_no 486\n",
            "Image_no 487\n",
            "Image_no 488\n",
            "Image_no 489\n",
            "Image_no 490\n",
            "Image_no 491\n",
            "Image_no 492\n",
            "Image_no 493\n",
            "Image_no 494\n",
            "Image_no 495\n",
            "Image_no 496\n",
            "Image_no 497\n",
            "Image_no 498\n",
            "Image_no 499\n",
            "Image_no 500\n",
            "Image_no 501\n",
            "Image_no 502\n",
            "Image_no 503\n",
            "Image_no 504\n",
            "Image_no 505\n",
            "Image_no 506\n",
            "Image_no 507\n",
            "Image_no 508\n",
            "Image_no 509\n",
            "Image_no 510\n",
            "Image_no 511\n",
            "Image_no 512\n",
            "Image_no 513\n",
            "Image_no 514\n",
            "Image_no 515\n",
            "Image_no 516\n",
            "Image_no 517\n",
            "Image_no 518\n",
            "Image_no 519\n",
            "Image_no 520\n",
            "Image_no 521\n",
            "Image_no 522\n",
            "Image_no 523\n",
            "Image_no 524\n",
            "Image_no 525\n",
            "Image_no 526\n",
            "Image_no 527\n",
            "Image_no 528\n",
            "Image_no 529\n",
            "Image_no 530\n",
            "Image_no 531\n",
            "Image_no 532\n",
            "Image_no 533\n",
            "Image_no 534\n",
            "Image_no 535\n",
            "Image_no 536\n",
            "Image_no 537\n",
            "Image_no 538\n",
            "Image_no 539\n",
            "Image_no 540\n",
            "Image_no 541\n",
            "Image_no 542\n",
            "Image_no 543\n",
            "Image_no 544\n",
            "Image_no 545\n",
            "Image_no 546\n",
            "Image_no 547\n",
            "Image_no 548\n",
            "Image_no 549\n",
            "Image_no 550\n",
            "Image_no 551\n",
            "Image_no 552\n",
            "Image_no 553\n",
            "Image_no 554\n",
            "Image_no 555\n",
            "Image_no 556\n",
            "Image_no 557\n",
            "Image_no 558\n",
            "Image_no 559\n",
            "Image_no 560\n",
            "Image_no 561\n",
            "Image_no 562\n",
            "Image_no 563\n",
            "Image_no 564\n",
            "Image_no 565\n",
            "Image_no 566\n",
            "Image_no 567\n",
            "Image_no 568\n",
            "Image_no 569\n",
            "Image_no 570\n",
            "Image_no 571\n",
            "Image_no 572\n",
            "Image_no 573\n",
            "Image_no 574\n",
            "Image_no 575\n",
            "Image_no 576\n",
            "Image_no 577\n",
            "Image_no 578\n",
            "Image_no 579\n",
            "Image_no 580\n",
            "Image_no 581\n",
            "Image_no 582\n",
            "Image_no 583\n",
            "Image_no 584\n",
            "Image_no 585\n",
            "Image_no 586\n",
            "Image_no 587\n",
            "Image_no 588\n",
            "Image_no 589\n",
            "Image_no 590\n",
            "Image_no 591\n",
            "Image_no 592\n",
            "Image_no 593\n",
            "Image_no 594\n",
            "Image_no 595\n",
            "Image_no 596\n",
            "Image_no 597\n",
            "Image_no 598\n",
            "Image_no 599\n",
            "Image_no 600\n",
            "Image_no 601\n",
            "Image_no 602\n",
            "Image_no 603\n",
            "Image_no 604\n",
            "Image_no 605\n",
            "Image_no 606\n",
            "Image_no 607\n",
            "Image_no 608\n",
            "Image_no 609\n",
            "Image_no 610\n",
            "Image_no 611\n",
            "Image_no 612\n",
            "Image_no 613\n",
            "Image_no 614\n",
            "Image_no 615\n",
            "Image_no 616\n",
            "Image_no 617\n",
            "Image_no 618\n",
            "Image_no 619\n",
            "Image_no 620\n",
            "Image_no 621\n",
            "Image_no 622\n",
            "Image_no 623\n",
            "Image_no 624\n",
            "Image_no 625\n",
            "Image_no 626\n",
            "Image_no 627\n",
            "Image_no 628\n",
            "Image_no 629\n",
            "Image_no 630\n",
            "Image_no 631\n",
            "Image_no 632\n",
            "Image_no 633\n",
            "Image_no 634\n",
            "Image_no 635\n",
            "Image_no 636\n",
            "Image_no 637\n",
            "Image_no 638\n",
            "Image_no 639\n",
            "Image_no 640\n",
            "Image_no 641\n",
            "Image_no 642\n",
            "Image_no 643\n",
            "Image_no 644\n",
            "Image_no 645\n",
            "Image_no 646\n",
            "Image_no 647\n",
            "Image_no 648\n",
            "Image_no 649\n",
            "Image_no 650\n",
            "Image_no 651\n",
            "Image_no 652\n",
            "Image_no 653\n",
            "Image_no 654\n",
            "Image_no 655\n",
            "Image_no 656\n",
            "Image_no 657\n",
            "Image_no 658\n",
            "Image_no 659\n",
            "Image_no 660\n",
            "Image_no 661\n",
            "Image_no 662\n",
            "Image_no 663\n",
            "Image_no 664\n",
            "Image_no 665\n",
            "Image_no 666\n",
            "Image_no 667\n",
            "Image_no 668\n",
            "Image_no 669\n",
            "Image_no 670\n",
            "Image_no 671\n",
            "Image_no 672\n",
            "Image_no 673\n",
            "Image_no 674\n",
            "Image_no 675\n",
            "Image_no 676\n",
            "Image_no 677\n",
            "Image_no 678\n",
            "Image_no 679\n",
            "Image_no 680\n",
            "Image_no 681\n",
            "Image_no 682\n",
            "Image_no 683\n",
            "Image_no 684\n",
            "Image_no 685\n",
            "Image_no 686\n",
            "Image_no 687\n",
            "Image_no 688\n",
            "Image_no 689\n",
            "Image_no 690\n",
            "Image_no 691\n",
            "Image_no 692\n",
            "Image_no 693\n",
            "Image_no 694\n",
            "Image_no 695\n",
            "Image_no 696\n",
            "Image_no 697\n",
            "Image_no 698\n",
            "Image_no 699\n",
            "Image_no 700\n",
            "Image_no 701\n",
            "Image_no 702\n",
            "Image_no 703\n",
            "Image_no 704\n",
            "Image_no 705\n",
            "Image_no 706\n",
            "Image_no 707\n",
            "Image_no 708\n",
            "Image_no 709\n",
            "Image_no 710\n",
            "Image_no 711\n",
            "Image_no 712\n",
            "Image_no 713\n",
            "Image_no 714\n",
            "Image_no 715\n",
            "Image_no 716\n",
            "Image_no 717\n",
            "Image_no 718\n",
            "Image_no 719\n",
            "Image_no 720\n",
            "Image_no 721\n",
            "Image_no 722\n",
            "Image_no 723\n",
            "Image_no 724\n",
            "Image_no 725\n",
            "Image_no 726\n",
            "Image_no 727\n",
            "Image_no 728\n",
            "Image_no 729\n",
            "Image_no 730\n",
            "Image_no 731\n",
            "Image_no 732\n",
            "Image_no 733\n",
            "Image_no 734\n",
            "Image_no 735\n",
            "Image_no 736\n",
            "Image_no 737\n",
            "Image_no 738\n",
            "Image_no 739\n",
            "Image_no 740\n",
            "Image_no 741\n",
            "Image_no 742\n",
            "Image_no 743\n",
            "Image_no 744\n",
            "Image_no 745\n",
            "Image_no 746\n",
            "Image_no 747\n",
            "Image_no 748\n",
            "Image_no 749\n",
            "Image_no 750\n",
            "Image_no 751\n",
            "Image_no 752\n",
            "Image_no 753\n",
            "Image_no 754\n",
            "Image_no 755\n",
            "Image_no 756\n",
            "Image_no 757\n",
            "Image_no 758\n",
            "Image_no 759\n",
            "Image_no 760\n",
            "Image_no 761\n",
            "Image_no 762\n",
            "Image_no 763\n",
            "Image_no 764\n",
            "Image_no 765\n",
            "Image_no 766\n",
            "Image_no 767\n",
            "Image_no 768\n",
            "Image_no 769\n",
            "Image_no 770\n",
            "Image_no 771\n",
            "Image_no 772\n",
            "Image_no 773\n",
            "Image_no 774\n",
            "Image_no 775\n",
            "Image_no 776\n",
            "Image_no 777\n",
            "Image_no 778\n",
            "Image_no 779\n",
            "Image_no 780\n",
            "Image_no 781\n",
            "Image_no 782\n",
            "Image_no 783\n",
            "Image_no 784\n",
            "Image_no 785\n",
            "Image_no 786\n",
            "Image_no 787\n",
            "Image_no 788\n",
            "Image_no 789\n",
            "Image_no 790\n",
            "Image_no 791\n",
            "Image_no 792\n",
            "Image_no 793\n",
            "Image_no 794\n",
            "Image_no 795\n",
            "Image_no 796\n",
            "Image_no 797\n",
            "Image_no 798\n",
            "Image_no 799\n",
            "Image_no 800\n",
            "Image_no 801\n",
            "Image_no 802\n",
            "Image_no 803\n",
            "Image_no 804\n",
            "Image_no 805\n",
            "Image_no 806\n",
            "Image_no 807\n",
            "Image_no 808\n",
            "Image_no 809\n",
            "Image_no 810\n",
            "Image_no 811\n",
            "Image_no 812\n",
            "Image_no 813\n",
            "Image_no 814\n",
            "Image_no 815\n",
            "Image_no 816\n",
            "Image_no 817\n",
            "Image_no 818\n",
            "Image_no 819\n",
            "Image_no 820\n",
            "Image_no 821\n",
            "Image_no 822\n",
            "Image_no 823\n",
            "Image_no 824\n",
            "Image_no 825\n",
            "Image_no 826\n",
            "Image_no 827\n",
            "Image_no 828\n",
            "Image_no 829\n",
            "Image_no 830\n",
            "Image_no 831\n",
            "Image_no 832\n",
            "Image_no 833\n",
            "Image_no 834\n",
            "Image_no 835\n",
            "Image_no 836\n",
            "Image_no 837\n",
            "Image_no 838\n",
            "Image_no 839\n",
            "Image_no 840\n",
            "Image_no 841\n",
            "Image_no 842\n",
            "Image_no 843\n",
            "Image_no 844\n",
            "Image_no 845\n",
            "Image_no 846\n",
            "Image_no 847\n",
            "Image_no 848\n",
            "Image_no 849\n",
            "Image_no 850\n",
            "Image_no 851\n",
            "Image_no 852\n",
            "Image_no 853\n",
            "Image_no 854\n",
            "Image_no 855\n",
            "Image_no 856\n",
            "Image_no 857\n",
            "Image_no 858\n",
            "Image_no 859\n",
            "Image_no 860\n",
            "Image_no 861\n",
            "Image_no 862\n",
            "Image_no 863\n",
            "Image_no 864\n",
            "Image_no 865\n",
            "Image_no 866\n",
            "Image_no 867\n",
            "Image_no 868\n",
            "Image_no 869\n",
            "Image_no 870\n",
            "Image_no 871\n",
            "Image_no 872\n",
            "Image_no 873\n",
            "Image_no 874\n",
            "Image_no 875\n",
            "Image_no 876\n",
            "Image_no 877\n",
            "Image_no 878\n",
            "Image_no 879\n",
            "Image_no 880\n",
            "Image_no 881\n",
            "Image_no 882\n",
            "Image_no 883\n",
            "Image_no 884\n",
            "Image_no 885\n",
            "Image_no 886\n",
            "Image_no 887\n",
            "Image_no 888\n",
            "Image_no 889\n",
            "Image_no 890\n",
            "Image_no 891\n",
            "Image_no 892\n",
            "Image_no 893\n",
            "Image_no 894\n",
            "Image_no 895\n",
            "Image_no 896\n",
            "Image_no 897\n",
            "Image_no 898\n",
            "Image_no 899\n",
            "Image_no 900\n",
            "Image_no 901\n",
            "Image_no 902\n",
            "Image_no 903\n",
            "Image_no 904\n",
            "Image_no 905\n",
            "Image_no 906\n",
            "Image_no 907\n",
            "Image_no 908\n",
            "Image_no 909\n",
            "Image_no 910\n",
            "Image_no 911\n",
            "Image_no 912\n",
            "Image_no 913\n",
            "Image_no 914\n",
            "Image_no 915\n",
            "Image_no 916\n",
            "Image_no 917\n",
            "Image_no 918\n",
            "Image_no 919\n",
            "Image_no 920\n",
            "Image_no 921\n",
            "Image_no 922\n",
            "Image_no 923\n",
            "Image_no 924\n",
            "Image_no 925\n",
            "Image_no 926\n",
            "Image_no 927\n",
            "Image_no 928\n",
            "Image_no 929\n",
            "Image_no 930\n",
            "Image_no 931\n",
            "Image_no 932\n",
            "Image_no 933\n",
            "Image_no 934\n",
            "Image_no 935\n",
            "Image_no 936\n",
            "Image_no 937\n",
            "Image_no 938\n",
            "Image_no 939\n",
            "Image_no 940\n",
            "Image_no 941\n",
            "Image_no 942\n",
            "Image_no 943\n",
            "Image_no 944\n",
            "Image_no 945\n",
            "Image_no 946\n",
            "Image_no 947\n",
            "Image_no 948\n",
            "Image_no 949\n",
            "Image_no 950\n",
            "Image_no 951\n",
            "Image_no 952\n",
            "Image_no 953\n",
            "Image_no 954\n",
            "Image_no 955\n",
            "Image_no 956\n",
            "Image_no 957\n",
            "Image_no 958\n",
            "Image_no 959\n",
            "Image_no 960\n",
            "Image_no 961\n",
            "Image_no 962\n",
            "Image_no 963\n",
            "Image_no 964\n",
            "Image_no 965\n",
            "Image_no 966\n",
            "Image_no 967\n",
            "Image_no 968\n",
            "Image_no 969\n",
            "Image_no 970\n",
            "Image_no 971\n",
            "Image_no 972\n",
            "Image_no 973\n",
            "Image_no 974\n",
            "Image_no 975\n",
            "Image_no 976\n",
            "Image_no 977\n",
            "Image_no 978\n",
            "Image_no 979\n",
            "Image_no 980\n",
            "Image_no 981\n",
            "Image_no 982\n",
            "Image_no 983\n",
            "Image_no 984\n",
            "Image_no 985\n",
            "Image_no 986\n",
            "Image_no 987\n",
            "Image_no 988\n",
            "Image_no 989\n",
            "Image_no 990\n",
            "Image_no 991\n",
            "Image_no 992\n",
            "Image_no 993\n",
            "Image_no 994\n",
            "Image_no 995\n",
            "Image_no 996\n",
            "Image_no 997\n",
            "Image_no 998\n",
            "Image_no 999\n",
            "Image_no 1000\n",
            "Image_no 1001\n",
            "Image_no 1002\n",
            "Image_no 1003\n",
            "Image_no 1004\n",
            "Image_no 1005\n",
            "Image_no 1006\n",
            "Image_no 1007\n",
            "Image_no 1008\n",
            "Image_no 1009\n",
            "Image_no 1010\n",
            "Image_no 1011\n",
            "Image_no 1012\n",
            "Image_no 1013\n",
            "Image_no 1014\n",
            "Image_no 1015\n",
            "Image_no 1016\n",
            "Image_no 1017\n",
            "Image_no 1018\n",
            "Image_no 1019\n",
            "Image_no 1020\n",
            "Image_no 1021\n",
            "Image_no 1022\n",
            "Image_no 1023\n",
            "Image_no 1024\n",
            "Image_no 1025\n",
            "Image_no 1026\n",
            "Image_no 1027\n",
            "Image_no 1028\n",
            "Image_no 1029\n",
            "Image_no 1030\n",
            "Image_no 1031\n",
            "Image_no 1032\n",
            "Image_no 1033\n",
            "Image_no 1034\n",
            "Image_no 1035\n",
            "Image_no 1036\n",
            "Image_no 1037\n",
            "Image_no 1038\n",
            "Image_no 1039\n",
            "Image_no 1040\n",
            "Image_no 1041\n",
            "Image_no 1042\n",
            "Image_no 1043\n",
            "Image_no 1044\n",
            "Image_no 1045\n",
            "Image_no 1046\n",
            "Image_no 1047\n",
            "Image_no 1048\n",
            "Image_no 1049\n",
            "Image_no 1050\n",
            "Image_no 1051\n",
            "Image_no 1052\n",
            "Image_no 1053\n",
            "Image_no 1054\n",
            "Image_no 1055\n",
            "Image_no 1056\n",
            "Image_no 1057\n",
            "Image_no 1058\n",
            "Image_no 1059\n",
            "Image_no 1060\n",
            "Image_no 1061\n",
            "Image_no 1062\n",
            "Image_no 1063\n",
            "Image_no 1064\n",
            "Image_no 1065\n",
            "Image_no 1066\n",
            "Image_no 1067\n",
            "Image_no 1068\n",
            "Image_no 1069\n",
            "Image_no 1070\n",
            "Image_no 1071\n",
            "Image_no 1072\n",
            "Image_no 1073\n",
            "Image_no 1074\n",
            "Image_no 1075\n",
            "Image_no 1076\n",
            "Image_no 1077\n",
            "Image_no 1078\n",
            "Image_no 1079\n",
            "Image_no 1080\n",
            "Image_no 1081\n",
            "Image_no 1082\n",
            "Image_no 1083\n",
            "Image_no 1084\n",
            "Image_no 1085\n",
            "Image_no 1086\n",
            "Image_no 1087\n",
            "Image_no 1088\n",
            "Image_no 1089\n",
            "Image_no 1090\n",
            "Image_no 1091\n",
            "Image_no 1092\n",
            "Image_no 1093\n",
            "Image_no 1094\n",
            "Image_no 1095\n",
            "Image_no 1096\n",
            "Image_no 1097\n",
            "Image_no 1098\n",
            "Image_no 1099\n",
            "Image_no 1100\n",
            "Image_no 1101\n",
            "Image_no 1102\n",
            "Image_no 1103\n",
            "Image_no 1104\n",
            "Image_no 1105\n",
            "Image_no 1106\n",
            "Image_no 1107\n",
            "Image_no 1108\n",
            "Image_no 1109\n",
            "Image_no 1110\n",
            "Image_no 1111\n",
            "Image_no 1112\n",
            "Image_no 1113\n",
            "Image_no 1114\n",
            "Image_no 1115\n",
            "Image_no 1116\n",
            "Image_no 1117\n",
            "Image_no 1118\n",
            "Image_no 1119\n",
            "Image_no 1120\n",
            "Image_no 1121\n",
            "Image_no 1122\n",
            "Image_no 1123\n",
            "Image_no 1124\n",
            "Image_no 1125\n",
            "Image_no 1126\n",
            "Image_no 1127\n",
            "Image_no 1128\n",
            "Image_no 1129\n",
            "Image_no 1130\n",
            "Image_no 1131\n",
            "Image_no 1132\n",
            "Image_no 1133\n",
            "Image_no 1134\n",
            "Image_no 1135\n",
            "Image_no 1136\n",
            "Image_no 1137\n",
            "Image_no 1138\n",
            "Image_no 1139\n",
            "Image_no 1140\n",
            "Image_no 1141\n",
            "Image_no 1142\n",
            "Image_no 1143\n",
            "Image_no 1144\n",
            "Image_no 1145\n",
            "Image_no 1146\n",
            "Image_no 1147\n",
            "Image_no 1148\n",
            "Image_no 1149\n",
            "Image_no 1150\n",
            "Image_no 1151\n",
            "Image_no 1152\n",
            "Image_no 1153\n",
            "Image_no 1154\n",
            "Image_no 1155\n",
            "Image_no 1156\n",
            "Image_no 1157\n",
            "Image_no 1158\n",
            "Image_no 1159\n",
            "Image_no 1160\n",
            "Image_no 1161\n",
            "Image_no 1162\n",
            "Image_no 1163\n",
            "Image_no 1164\n",
            "Image_no 1165\n",
            "Image_no 1166\n",
            "Image_no 1167\n",
            "Image_no 1168\n",
            "Image_no 1169\n",
            "Image_no 1170\n",
            "Image_no 1171\n",
            "Image_no 1172\n",
            "Image_no 1173\n",
            "Image_no 1174\n",
            "Image_no 1175\n",
            "Image_no 1176\n",
            "Image_no 1177\n",
            "Image_no 1178\n",
            "Image_no 1179\n",
            "Image_no 1180\n",
            "Image_no 1181\n",
            "Image_no 1182\n",
            "Image_no 1183\n",
            "Image_no 1184\n",
            "Image_no 1185\n",
            "Image_no 1186\n",
            "Image_no 1187\n",
            "Image_no 1188\n",
            "Image_no 1189\n",
            "Image_no 1190\n",
            "Image_no 1191\n",
            "Image_no 1192\n",
            "Image_no 1193\n",
            "Image_no 1194\n",
            "Image_no 1195\n",
            "Image_no 1196\n",
            "Image_no 1197\n",
            "Image_no 1198\n",
            "Image_no 1199\n",
            "Image_no 1200\n",
            "Image_no 1201\n",
            "Image_no 1202\n",
            "Image_no 1203\n",
            "Image_no 1204\n",
            "Image_no 1205\n",
            "Image_no 1206\n",
            "Image_no 1207\n",
            "Image_no 1208\n",
            "Image_no 1209\n",
            "Image_no 1210\n",
            "Image_no 1211\n",
            "Image_no 1212\n",
            "Image_no 1213\n",
            "Image_no 1214\n",
            "Image_no 1215\n",
            "Image_no 1216\n",
            "Image_no 1217\n",
            "Image_no 1218\n",
            "Image_no 1219\n",
            "Image_no 1220\n",
            "Image_no 1221\n",
            "Image_no 1222\n",
            "Image_no 1223\n",
            "Image_no 1224\n",
            "Image_no 1225\n",
            "Image_no 1226\n",
            "Image_no 1227\n",
            "Image_no 1228\n",
            "Image_no 1229\n",
            "Image_no 1230\n",
            "Image_no 1231\n",
            "Image_no 1232\n",
            "Image_no 1233\n",
            "Image_no 1234\n",
            "Image_no 1235\n",
            "Image_no 1236\n",
            "Image_no 1237\n",
            "Image_no 1238\n",
            "Image_no 1239\n",
            "Image_no 1240\n",
            "Image_no 1241\n",
            "Image_no 1242\n",
            "Image_no 1243\n",
            "Image_no 1244\n",
            "Image_no 1245\n",
            "Image_no 1246\n",
            "Image_no 1247\n",
            "Image_no 1248\n",
            "Image_no 1249\n",
            "Image_no 1250\n",
            "Image_no 1251\n",
            "Image_no 1252\n",
            "Image_no 1253\n",
            "Image_no 1254\n",
            "Image_no 1255\n",
            "Image_no 1256\n",
            "Image_no 1257\n",
            "Image_no 1258\n",
            "Image_no 1259\n",
            "Image_no 1260\n",
            "Image_no 1261\n",
            "Image_no 1262\n",
            "Image_no 1263\n",
            "Image_no 1264\n",
            "Image_no 1265\n",
            "Image_no 1266\n",
            "Image_no 1267\n",
            "Image_no 1268\n",
            "Image_no 1269\n",
            "Image_no 1270\n",
            "Image_no 1271\n",
            "Image_no 1272\n",
            "Image_no 1273\n",
            "Image_no 1274\n",
            "Image_no 1275\n",
            "Image_no 1276\n",
            "Image_no 1277\n",
            "Image_no 1278\n",
            "Image_no 1279\n",
            "Image_no 1280\n",
            "Image_no 1281\n",
            "Image_no 1282\n",
            "Image_no 1283\n",
            "Image_no 1284\n",
            "Image_no 1285\n",
            "Image_no 1286\n",
            "Image_no 1287\n",
            "Image_no 1288\n",
            "Image_no 1289\n",
            "Image_no 1290\n",
            "Image_no 1291\n",
            "Image_no 1292\n",
            "Image_no 1293\n",
            "Image_no 1294\n",
            "Image_no 1295\n",
            "Image_no 1296\n",
            "Image_no 1297\n",
            "Image_no 1298\n",
            "Image_no 1299\n",
            "Image_no 1300\n",
            "Image_no 1301\n",
            "Image_no 1302\n",
            "Image_no 1303\n",
            "Image_no 1304\n",
            "Image_no 1305\n",
            "Image_no 1306\n",
            "Image_no 1307\n",
            "Image_no 1308\n",
            "Image_no 1309\n",
            "Image_no 1310\n",
            "Image_no 1311\n",
            "Image_no 1312\n",
            "Image_no 1313\n",
            "Image_no 1314\n",
            "Image_no 1315\n",
            "Image_no 1316\n",
            "Image_no 1317\n",
            "Image_no 1318\n",
            "Image_no 1319\n",
            "Image_no 1320\n",
            "Image_no 1321\n",
            "Image_no 1322\n",
            "Image_no 1323\n",
            "Image_no 1324\n",
            "Image_no 1325\n",
            "Image_no 1326\n",
            "Image_no 1327\n",
            "Image_no 1328\n",
            "Image_no 1329\n",
            "Image_no 1330\n",
            "Image_no 1331\n",
            "Image_no 1332\n",
            "Image_no 1333\n",
            "Image_no 1334\n",
            "Image_no 1335\n",
            "Image_no 1336\n",
            "Image_no 1337\n",
            "Image_no 1338\n",
            "Image_no 1339\n",
            "Image_no 1340\n",
            "Image_no 1341\n",
            "Image_no 1342\n",
            "Image_no 1343\n",
            "Image_no 1344\n",
            "Image_no 1345\n",
            "Image_no 1346\n",
            "Image_no 1347\n",
            "Image_no 1348\n",
            "Image_no 1349\n",
            "Image_no 1350\n",
            "Image_no 1351\n",
            "Image_no 1352\n",
            "Image_no 1353\n",
            "Image_no 1354\n",
            "Image_no 1355\n",
            "Image_no 1356\n",
            "Image_no 1357\n",
            "Image_no 1358\n",
            "Image_no 1359\n",
            "Image_no 1360\n",
            "Image_no 1361\n",
            "Image_no 1362\n",
            "Image_no 1363\n",
            "Image_no 1364\n",
            "Image_no 1365\n",
            "Image_no 1366\n",
            "Image_no 1367\n",
            "Image_no 1368\n",
            "Image_no 1369\n",
            "Image_no 1370\n",
            "Image_no 1371\n",
            "Image_no 1372\n",
            "Image_no 1373\n",
            "Image_no 1374\n",
            "Image_no 1375\n",
            "Image_no 1376\n",
            "Image_no 1377\n",
            "Image_no 1378\n",
            "Image_no 1379\n",
            "Image_no 1380\n",
            "Image_no 1381\n",
            "Image_no 1382\n",
            "Image_no 1383\n",
            "Image_no 1384\n",
            "Image_no 1385\n",
            "Image_no 1386\n",
            "Image_no 1387\n",
            "Image_no 1388\n",
            "Image_no 1389\n",
            "Image_no 1390\n",
            "Image_no 1391\n",
            "Image_no 1392\n",
            "Image_no 1393\n",
            "Image_no 1394\n",
            "Image_no 1395\n",
            "Image_no 1396\n",
            "Image_no 1397\n",
            "Image_no 1398\n",
            "Image_no 1399\n",
            "Image_no 1400\n",
            "Image_no 1401\n",
            "Image_no 1402\n",
            "Image_no 1403\n",
            "Image_no 1404\n",
            "Image_no 1405\n",
            "Image_no 1406\n",
            "Image_no 1407\n",
            "Image_no 1408\n",
            "Image_no 1409\n",
            "Image_no 1410\n",
            "Image_no 1411\n",
            "Image_no 1412\n",
            "Image_no 1413\n",
            "Image_no 1414\n",
            "Image_no 1415\n",
            "Image_no 1416\n",
            "Image_no 1417\n",
            "Image_no 1418\n",
            "Image_no 1419\n",
            "Image_no 1420\n",
            "Image_no 1421\n",
            "Image_no 1422\n",
            "Image_no 1423\n",
            "Image_no 1424\n",
            "Image_no 1425\n",
            "Image_no 1426\n",
            "Image_no 1427\n",
            "Image_no 1428\n",
            "Image_no 1429\n",
            "Image_no 1430\n",
            "Image_no 1431\n",
            "Image_no 1432\n",
            "Image_no 1433\n",
            "Image_no 1434\n",
            "Image_no 1435\n",
            "Image_no 1436\n",
            "Image_no 1437\n",
            "Image_no 1438\n",
            "Image_no 1439\n",
            "Image_no 1440\n",
            "Image_no 1441\n",
            "Image_no 1442\n",
            "Image_no 1443\n",
            "Image_no 1444\n",
            "Image_no 1445\n",
            "Image_no 1446\n",
            "Image_no 1447\n",
            "Image_no 1448\n",
            "Image_no 1449\n",
            "Image_no 1450\n",
            "Image_no 1451\n",
            "Image_no 1452\n",
            "Image_no 1453\n",
            "Image_no 1454\n",
            "Image_no 1455\n",
            "Image_no 1456\n",
            "Image_no 1457\n",
            "Image_no 1458\n",
            "Image_no 1459\n",
            "Image_no 1460\n",
            "Image_no 1461\n",
            "Image_no 1462\n",
            "Image_no 1463\n",
            "Image_no 1464\n",
            "Image_no 1465\n",
            "Image_no 1466\n",
            "Image_no 1467\n",
            "Image_no 1468\n",
            "Image_no 1469\n",
            "Image_no 1470\n",
            "Image_no 1471\n",
            "Image_no 1472\n",
            "Image_no 1473\n",
            "Image_no 1474\n",
            "Image_no 1475\n",
            "Image_no 1476\n",
            "Image_no 1477\n",
            "Image_no 1478\n",
            "Image_no 1479\n",
            "Image_no 1480\n",
            "Image_no 1481\n",
            "Image_no 1482\n",
            "Image_no 1483\n",
            "Image_no 1484\n",
            "Image_no 1485\n",
            "Image_no 1486\n",
            "Image_no 1487\n",
            "Image_no 1488\n",
            "Image_no 1489\n",
            "Image_no 1490\n",
            "Image_no 1491\n",
            "Image_no 1492\n",
            "Image_no 1493\n",
            "Image_no 1494\n",
            "Image_no 1495\n",
            "Image_no 1496\n",
            "Image_no 1497\n",
            "Image_no 1498\n",
            "Image_no 1499\n",
            "Image_no 1500\n",
            "Image_no 1501\n",
            "Image_no 1502\n",
            "Image_no 1503\n",
            "Image_no 1504\n",
            "Image_no 1505\n",
            "Image_no 1506\n",
            "Image_no 1507\n",
            "Image_no 1508\n",
            "Image_no 1509\n",
            "Image_no 1510\n",
            "Image_no 1511\n",
            "Image_no 1512\n",
            "Image_no 1513\n",
            "Image_no 1514\n",
            "Image_no 1515\n",
            "Image_no 1516\n",
            "Image_no 1517\n",
            "Image_no 1518\n",
            "Image_no 1519\n",
            "Image_no 1520\n",
            "Image_no 1521\n",
            "Image_no 1522\n",
            "Image_no 1523\n",
            "Image_no 1524\n",
            "Image_no 1525\n",
            "Image_no 1526\n",
            "Image_no 1527\n",
            "Image_no 1528\n",
            "Image_no 1529\n",
            "Image_no 1530\n",
            "Image_no 1531\n",
            "Image_no 1532\n",
            "Image_no 1533\n",
            "Image_no 1534\n",
            "Image_no 1535\n",
            "Image_no 1536\n",
            "Image_no 1537\n",
            "Image_no 1538\n",
            "Image_no 1539\n",
            "Image_no 1540\n",
            "Image_no 1541\n",
            "Image_no 1542\n",
            "Image_no 1543\n",
            "Image_no 1544\n",
            "Image_no 1545\n",
            "Image_no 1546\n",
            "Image_no 1547\n",
            "Image_no 1548\n",
            "Image_no 1549\n",
            "Image_no 1550\n",
            "Image_no 1551\n",
            "Image_no 1552\n",
            "Image_no 1553\n",
            "Image_no 1554\n",
            "Image_no 1555\n",
            "Image_no 1556\n",
            "Image_no 1557\n",
            "Image_no 1558\n",
            "Image_no 1559\n",
            "Image_no 1560\n",
            "Image_no 1561\n",
            "Image_no 1562\n",
            "Image_no 1563\n",
            "Image_no 1564\n",
            "Image_no 1565\n",
            "Image_no 1566\n",
            "Image_no 1567\n",
            "Image_no 1568\n",
            "Image_no 1569\n",
            "Image_no 1570\n",
            "Image_no 1571\n",
            "Image_no 1572\n",
            "Image_no 1573\n",
            "Image_no 1574\n",
            "Image_no 1575\n",
            "Image_no 1576\n",
            "Image_no 1577\n",
            "Image_no 1578\n",
            "Image_no 1579\n",
            "Image_no 1580\n",
            "Image_no 1581\n",
            "Image_no 1582\n",
            "Image_no 1583\n",
            "Image_no 1584\n",
            "Image_no 1585\n",
            "Image_no 1586\n",
            "Image_no 1587\n",
            "Image_no 1588\n",
            "Image_no 1589\n",
            "Image_no 1590\n",
            "Image_no 1591\n",
            "Image_no 1592\n",
            "Image_no 1593\n",
            "Image_no 1594\n",
            "Image_no 1595\n",
            "Image_no 1596\n",
            "Image_no 1597\n",
            "Image_no 1598\n",
            "Image_no 1599\n",
            "Image_no 1600\n",
            "Image_no 1601\n",
            "Image_no 1602\n",
            "Image_no 1603\n",
            "Image_no 1604\n",
            "Image_no 1605\n",
            "Image_no 1606\n",
            "Image_no 1607\n",
            "Image_no 1608\n",
            "Image_no 1609\n",
            "Image_no 1610\n",
            "Image_no 1611\n",
            "Image_no 1612\n",
            "Image_no 1613\n",
            "Image_no 1614\n",
            "Image_no 1615\n",
            "Image_no 1616\n",
            "Image_no 1617\n",
            "Image_no 1618\n",
            "Image_no 1619\n",
            "Image_no 1620\n",
            "Image_no 1621\n",
            "Image_no 1622\n",
            "Image_no 1623\n",
            "Image_no 1624\n",
            "Image_no 1625\n",
            "Image_no 1626\n",
            "Image_no 1627\n",
            "Image_no 1628\n",
            "Image_no 1629\n",
            "Image_no 1630\n",
            "Image_no 1631\n",
            "Image_no 1632\n",
            "Image_no 1633\n",
            "Image_no 1634\n",
            "Image_no 1635\n",
            "Image_no 1636\n",
            "Image_no 1637\n",
            "Image_no 1638\n",
            "Image_no 1639\n",
            "Image_no 1640\n",
            "Image_no 1641\n",
            "Image_no 1642\n",
            "Image_no 1643\n",
            "Image_no 1644\n",
            "Image_no 1645\n",
            "Image_no 1646\n",
            "Image_no 1647\n",
            "Image_no 1648\n",
            "Image_no 1649\n",
            "Image_no 1650\n",
            "Image_no 1651\n",
            "Image_no 1652\n",
            "Image_no 1653\n",
            "Image_no 1654\n",
            "Image_no 1655\n",
            "Image_no 1656\n",
            "Image_no 1657\n",
            "Image_no 1658\n",
            "Image_no 1659\n",
            "Image_no 1660\n",
            "Image_no 1661\n",
            "Image_no 1662\n",
            "Image_no 1663\n",
            "Image_no 1664\n",
            "Image_no 1665\n",
            "Image_no 1666\n",
            "Image_no 1667\n",
            "Image_no 1668\n",
            "Image_no 1669\n",
            "Image_no 1670\n",
            "Image_no 1671\n",
            "Image_no 1672\n",
            "Image_no 1673\n",
            "Image_no 1674\n",
            "Image_no 1675\n",
            "Image_no 1676\n",
            "Image_no 1677\n",
            "Image_no 1678\n",
            "Image_no 1679\n",
            "Image_no 1680\n",
            "Image_no 1681\n",
            "Image_no 1682\n",
            "Image_no 1683\n",
            "Image_no 1684\n",
            "Image_no 1685\n",
            "Image_no 1686\n",
            "Image_no 1687\n",
            "Image_no 1688\n",
            "Image_no 1689\n",
            "Image_no 1690\n",
            "Image_no 1691\n",
            "Image_no 1692\n",
            "Image_no 1693\n",
            "Image_no 1694\n",
            "Image_no 1695\n",
            "Image_no 1696\n",
            "Image_no 1697\n",
            "Image_no 1698\n",
            "Image_no 1699\n",
            "Image_no 1700\n",
            "Image_no 1701\n",
            "Image_no 1702\n",
            "Image_no 1703\n",
            "Image_no 1704\n",
            "Image_no 1705\n",
            "Image_no 1706\n",
            "Image_no 1707\n",
            "Image_no 1708\n",
            "Image_no 1709\n",
            "Image_no 1710\n",
            "Image_no 1711\n",
            "Image_no 1712\n",
            "Image_no 1713\n",
            "Image_no 1714\n",
            "Image_no 1715\n",
            "Image_no 1716\n",
            "Image_no 1717\n",
            "Image_no 1718\n",
            "Image_no 1719\n",
            "Image_no 1720\n",
            "Image_no 1721\n",
            "Image_no 1722\n",
            "Image_no 1723\n",
            "Image_no 1724\n",
            "Image_no 1725\n",
            "Image_no 1726\n",
            "Image_no 1727\n",
            "Image_no 1728\n",
            "Image_no 1729\n",
            "Image_no 1730\n",
            "Image_no 1731\n",
            "Image_no 1732\n",
            "Image_no 1733\n",
            "Image_no 1734\n",
            "Image_no 1735\n",
            "Image_no 1736\n",
            "Image_no 1737\n",
            "Image_no 1738\n",
            "Image_no 1739\n",
            "Image_no 1740\n",
            "Image_no 1741\n",
            "Image_no 1742\n",
            "Image_no 1743\n",
            "Image_no 1744\n",
            "Image_no 1745\n",
            "Image_no 1746\n",
            "Image_no 1747\n",
            "Image_no 1748\n",
            "Image_no 1749\n",
            "Image_no 1750\n",
            "Image_no 1751\n",
            "Image_no 1752\n",
            "Image_no 1753\n",
            "Image_no 1754\n",
            "Image_no 1755\n",
            "Image_no 1756\n",
            "Image_no 1757\n",
            "Image_no 1758\n",
            "Image_no 1759\n",
            "Image_no 1760\n",
            "Image_no 1761\n",
            "Image_no 1762\n",
            "Image_no 1763\n",
            "Image_no 1764\n",
            "Image_no 1765\n",
            "Image_no 1766\n",
            "Image_no 1767\n",
            "Image_no 1768\n",
            "Image_no 1769\n",
            "Image_no 1770\n",
            "Image_no 1771\n",
            "Image_no 1772\n",
            "Image_no 1773\n",
            "Image_no 1774\n",
            "Image_no 1775\n",
            "Image_no 1776\n",
            "Image_no 1777\n",
            "Image_no 1778\n",
            "Image_no 1779\n",
            "Image_no 1780\n",
            "Image_no 1781\n",
            "Image_no 1782\n",
            "Image_no 1783\n",
            "Image_no 1784\n",
            "Image_no 1785\n",
            "Image_no 1786\n",
            "Image_no 1787\n",
            "Image_no 1788\n",
            "Image_no 1789\n",
            "Image_no 1790\n",
            "Image_no 1791\n",
            "Image_no 1792\n",
            "Image_no 1793\n",
            "Image_no 1794\n",
            "Image_no 1795\n",
            "Image_no 1796\n",
            "Image_no 1797\n",
            "Image_no 1798\n",
            "Image_no 1799\n",
            "Image_no 1800\n",
            "Image_no 1801\n",
            "Image_no 1802\n",
            "Image_no 1803\n",
            "Image_no 1804\n",
            "Image_no 1805\n",
            "Image_no 1806\n",
            "Image_no 1807\n",
            "Image_no 1808\n",
            "Image_no 1809\n",
            "Image_no 1810\n",
            "Image_no 1811\n",
            "Image_no 1812\n",
            "Image_no 1813\n",
            "Image_no 1814\n",
            "Image_no 1815\n",
            "Image_no 1816\n",
            "Image_no 1817\n",
            "Image_no 1818\n",
            "Image_no 1819\n",
            "Image_no 1820\n",
            "Image_no 1821\n",
            "Image_no 1822\n",
            "Image_no 1823\n",
            "Image_no 1824\n",
            "Image_no 1825\n",
            "Image_no 1826\n",
            "Image_no 1827\n",
            "Image_no 1828\n",
            "Image_no 1829\n",
            "Image_no 1830\n",
            "Image_no 1831\n",
            "Image_no 1832\n",
            "Image_no 1833\n",
            "Image_no 1834\n",
            "Image_no 1835\n",
            "Image_no 1836\n",
            "Image_no 1837\n",
            "Image_no 1838\n",
            "Image_no 1839\n",
            "Image_no 1840\n",
            "Image_no 1841\n",
            "Image_no 1842\n",
            "Image_no 1843\n",
            "Image_no 1844\n",
            "Image_no 1845\n",
            "Image_no 1846\n",
            "Image_no 1847\n",
            "Image_no 1848\n",
            "Image_no 1849\n",
            "Image_no 1850\n",
            "Image_no 1851\n",
            "Image_no 1852\n",
            "Image_no 1853\n",
            "Image_no 1854\n",
            "Image_no 1855\n",
            "Image_no 1856\n",
            "Image_no 1857\n",
            "Image_no 1858\n",
            "Image_no 1859\n",
            "Image_no 1860\n",
            "Image_no 1861\n",
            "Image_no 1862\n",
            "Image_no 1863\n",
            "Image_no 1864\n",
            "Image_no 1865\n",
            "Image_no 1866\n",
            "Image_no 1867\n",
            "Image_no 1868\n",
            "Image_no 1869\n",
            "Image_no 1870\n",
            "Image_no 1871\n",
            "Image_no 1872\n",
            "Image_no 1873\n",
            "Image_no 1874\n",
            "Image_no 1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yz6M_8IYEdN",
        "colab_type": "code",
        "outputId": "5e797cb4-a0aa-48dd-96cc-c32fad058547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "img=cv2.imread('testing1700.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1dc0139d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADLCAYAAACVv9NEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJa_mpGvAKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer3 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 10\n",
        "rows = 10\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('testing'+str(i*100)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RalhZHlCHPeG",
        "colab_type": "text"
      },
      "source": [
        "MODE COLLAPSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQh9debltAVr",
        "colab_type": "text"
      },
      "source": [
        "WE CAN SEE THE MIXING OCCURNG IN THE LATENT SPACE AND ESPECIALLY BECAUSE OF LOW DIMENSIONALITY 522 THOUGH THE POINTS IN LATENT SPACE ARE DISCRETE AND KIND OF ONE TO ONE MAPPING BUT THEY STILL SEEM TO BE QUITE CONTINUOUS. TAKING A HIGHER DIMENSIONAL DATASET MAY SHOW THE SPARSITY IN THE LATENT IN A BETTER WAY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkKRbjiJ7AOX",
        "colab_type": "code",
        "outputId": "e0a1bfce-96be-4de5-c48b-82ce5ace2efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "#layer3 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAHTCAYAAABiN8IeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3azocy/eQwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emKAhl5VTgyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iq1TfmdTg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer2 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer2_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vZmGsyTg3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer1 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HJvYuG-CYE",
        "colab_type": "text"
      },
      "source": [
        "SO IT SEEMS THAT THE LATENT SPACE IS VERY CONTINUOUS BUT IN REALITY IT IS DISCRETE IN CASE OF A NORMAL AUTOENCODER TO VISUALIZE IT WE NEED A LARGER DIMENSION DATASET TO PROOVE IT WE NEED TO PLOT THE LATENT SPACE BY CONVERTING IT INTO TWO DIMENSIONS USING PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvIql91TgwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WZqSH9vTgtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axzG2TQeOhNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}