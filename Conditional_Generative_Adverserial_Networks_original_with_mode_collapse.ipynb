{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Generative Adverserial Networks_original with mode_collapse",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzIGuOK6mLPlcMV0kY6Z/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "230cace3947a42ea93f701174c0eef4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d14a5710e0934ba389301388ce7ec187",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_020ca3f838504287b8373369855e4d3f",
              "IPY_MODEL_feb75b362c6c428eb59b9cf6f5552355"
            ]
          }
        },
        "d14a5710e0934ba389301388ce7ec187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "020ca3f838504287b8373369855e4d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b45ed6b69aa840d9a2b29966f5aef50e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b64328d6a2f4467d8c886058cf35d3c8"
          }
        },
        "feb75b362c6c428eb59b9cf6f5552355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1dec9a45ab54f6d9596da07deb107f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:02&lt;00:00, 3367365.78it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b53a295f23be4bb78bec565adb7b3899"
          }
        },
        "b45ed6b69aa840d9a2b29966f5aef50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b64328d6a2f4467d8c886058cf35d3c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1dec9a45ab54f6d9596da07deb107f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b53a295f23be4bb78bec565adb7b3899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1f44f9ba4a242a2a43900e03bec5e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_983312e1325e4603bca5ef1776b6ce2b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65b062d085544584879e2475e5c6fcc1",
              "IPY_MODEL_3ac3291c19b64bcab0f3eb0470b2757e"
            ]
          }
        },
        "983312e1325e4603bca5ef1776b6ce2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65b062d085544584879e2475e5c6fcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a76f89a01ed94e9ea88ec78150dd4a5a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eff1d54393aa4656ac766402a51b023b"
          }
        },
        "3ac3291c19b64bcab0f3eb0470b2757e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d538d3ed9cd4e3f8012e0a1b68e47e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 118090.53it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1fbd01ef9d84e70a7819089747c682c"
          }
        },
        "a76f89a01ed94e9ea88ec78150dd4a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eff1d54393aa4656ac766402a51b023b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d538d3ed9cd4e3f8012e0a1b68e47e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1fbd01ef9d84e70a7819089747c682c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01edcec4c5bd4fb78660ff7c3560bce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3191a2ae44ac46db8ba3f15d15af0fa0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0531c7b416384f1583056922590e65b1",
              "IPY_MODEL_96310a48c2004ada82c512e5aab841d2"
            ]
          }
        },
        "3191a2ae44ac46db8ba3f15d15af0fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0531c7b416384f1583056922590e65b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9315dc158c444985ad1e5ac119e4c8f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1972cdc020b4b528ad47c2127fe004a"
          }
        },
        "96310a48c2004ada82c512e5aab841d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e853b8810914fcdb2f95f08e52ac936",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:01&lt;00:00, 1235055.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_263495c3e23847b791c14747e997dde5"
          }
        },
        "9315dc158c444985ad1e5ac119e4c8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1972cdc020b4b528ad47c2127fe004a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e853b8810914fcdb2f95f08e52ac936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "263495c3e23847b791c14747e997dde5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f2bdf61a86444e49a535a54907e906d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_919f8fc90e1640a1a4e16c8be5fa8d28",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_30bd6893653f4fb5a123b574bcce66ac",
              "IPY_MODEL_6fd1b616e7ce40208837c1c917c71b09"
            ]
          }
        },
        "919f8fc90e1640a1a4e16c8be5fa8d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30bd6893653f4fb5a123b574bcce66ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1f80d4636ad44888c84b3e512ef86f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22f37021386e4cb8969dc08da1e1d14f"
          }
        },
        "6fd1b616e7ce40208837c1c917c71b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff1b6fdbb0204acc9ed45429caf3dd83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 22994.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2090e83a20414617a90b41b6a063e432"
          }
        },
        "f1f80d4636ad44888c84b3e512ef86f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22f37021386e4cb8969dc08da1e1d14f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff1b6fdbb0204acc9ed45429caf3dd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2090e83a20414617a90b41b6a063e432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyakjain0112/Generative-models-papers-and-code/blob/master/Conditional_Generative_Adverserial_Networks_original_with_mode_collapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryF6KNvfIwsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all requirements\n",
        "import torch as torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu as Relu\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pi-2xlo2im",
        "colab_type": "code",
        "outputId": "42184512-29eb-447f-f321-33efa5f87b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "230cace3947a42ea93f701174c0eef4b",
            "d14a5710e0934ba389301388ce7ec187",
            "020ca3f838504287b8373369855e4d3f",
            "feb75b362c6c428eb59b9cf6f5552355",
            "b45ed6b69aa840d9a2b29966f5aef50e",
            "b64328d6a2f4467d8c886058cf35d3c8",
            "f1dec9a45ab54f6d9596da07deb107f1",
            "b53a295f23be4bb78bec565adb7b3899",
            "f1f44f9ba4a242a2a43900e03bec5e70",
            "983312e1325e4603bca5ef1776b6ce2b",
            "65b062d085544584879e2475e5c6fcc1",
            "3ac3291c19b64bcab0f3eb0470b2757e",
            "a76f89a01ed94e9ea88ec78150dd4a5a",
            "eff1d54393aa4656ac766402a51b023b",
            "4d538d3ed9cd4e3f8012e0a1b68e47e0",
            "f1fbd01ef9d84e70a7819089747c682c",
            "01edcec4c5bd4fb78660ff7c3560bce8",
            "3191a2ae44ac46db8ba3f15d15af0fa0",
            "0531c7b416384f1583056922590e65b1",
            "96310a48c2004ada82c512e5aab841d2",
            "9315dc158c444985ad1e5ac119e4c8f3",
            "a1972cdc020b4b528ad47c2127fe004a",
            "3e853b8810914fcdb2f95f08e52ac936",
            "263495c3e23847b791c14747e997dde5",
            "5f2bdf61a86444e49a535a54907e906d",
            "919f8fc90e1640a1a4e16c8be5fa8d28",
            "30bd6893653f4fb5a123b574bcce66ac",
            "6fd1b616e7ce40208837c1c917c71b09",
            "f1f80d4636ad44888c84b3e512ef86f5",
            "22f37021386e4cb8969dc08da1e1d14f",
            "ff1b6fdbb0204acc9ed45429caf3dd83",
            "2090e83a20414617a90b41b6a063e432"
          ]
        }
      },
      "source": [
        "#using the inbuilt Dataset class where all data is loaded into the cpu memory and using getitem we can get the data by just passing the corresponding index4\n",
        "\n",
        "#VARIABLE=DATASET\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "230cace3947a42ea93f701174c0eef4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1f44f9ba4a242a2a43900e03bec5e70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01edcec4c5bd4fb78660ff7c3560bce8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f2bdf61a86444e49a535a54907e906d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuEHh_4vpfh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading all the data using Dataloaader inbuilt class it loads all the data again into the cpu memory without making a copy because it uses iter for it and even if suffle examples is true then also no cpy is \n",
        "#required it just reshuffles the order of the iteraror in the list. \n",
        "#NOTE: only when we will load the data for training in the training class then the data will be needed to be loaded in the gpu memory\n",
        "batch_size=32\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO6OQlJluQy3",
        "colab_type": "code",
        "outputId": "10f40ac3-0c77-477d-d0ed-77a5ae52370f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_generator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=32):\n",
        "    super(model_generator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.mean=100\n",
        "    self.class_size=10\n",
        "    self.std=100\n",
        "    self.g_input_without_label=100\n",
        "    self.g_input_dim=110\n",
        "    self.g_output_dim=self.filter_size*self.filter_size\n",
        "    self.batcher=1\n",
        "    self.labeler_batch_size=torch.from_numpy(np.zeros([self.batch_size,self.class_size],dtype='float32')).cuda()\n",
        "    self.labeler_batcher=torch.from_numpy(np.zeros([self.batcher,self.class_size],dtype='float32')).cuda()\n",
        "    self.fc1 = nn.Linear(self.g_input_dim, 128)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, self.g_output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,x_input,label,latent_space=False,latent=None,marker=False,plot=False,batcher=1):\n",
        "    #taking x_input as input for coding simplicity but not using it in any way\n",
        "    if latent_space==True or marker==True or plot==True:\n",
        "      self.x=(torch.randn(batcher,self.g_input_without_label)).cuda()\n",
        "      vec=self.labeler_batcher\n",
        "      vec[0][int(label)]=1\n",
        "      self.x=torch.cat((self.x,vec),1)\n",
        "\n",
        "    else:\n",
        "      self.x=(torch.randn(self.batch_size,self.g_input_without_label)).cuda()\n",
        "      vec=self.labeler_batch_size\n",
        "      for i in range(self.batch_size):\n",
        "        vec[i][int(label[i].item())]=1\n",
        "      self.x=torch.cat((self.x,vec),1)\n",
        "    \n",
        "    x = F.leaky_relu(self.fc1(self.x), 0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    return torch.tanh(self.fc4(x))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atPKIhw3CY_M",
        "colab_type": "code",
        "outputId": "38cd1313-e628-4a41-d5a1-2fbcba32edc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_discriminator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=32):\n",
        "    super(model_discriminator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "\n",
        "    self.class_size=10\n",
        "    self.d_input_dim=self.filter_size*self.filter_size+self.class_size\n",
        "\n",
        "    self.fc1 = nn.Linear(self.d_input_dim, 1024)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "\n",
        "  def forward(self,x,latent_space=False,latent=None,marker=False,plot=False):\n",
        "      x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "      if plot==True:\n",
        "        return x\n",
        "      if latent_space==True:\n",
        "        return x\n",
        "      if marker==True:\n",
        "        x=latent\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      return torch.sigmoid(self.fc4(x))\n",
        "\"\"\"\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x=self.layer3(x)\n",
        "    x=Relu(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.layer4(x)\n",
        "    x=Relu(x)\n",
        "    x=self.layer5(x)\n",
        "    x=torch.sigmoid(x)\n",
        "    #print(x.size())\n",
        "    #x=x.cpu().detach().numpy()\n",
        "    #x=np.array(np.amax(x,axis=1))\n",
        "    \n",
        "    \n",
        "    #print(x.size())\n",
        "\n",
        "    return x\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    if plot==True:\\n      return x\\n    if latent_space==True:\\n      return x\\n    if marker==True:\\n      x=latent\\n    x=self.layer3(x)\\n    x=Relu(x)\\n    x=torch.flatten(x,1)\\n    x=self.layer4(x)\\n    x=Relu(x)\\n    x=self.layer5(x)\\n    x=torch.sigmoid(x)\\n    #print(x.size())\\n    #x=x.cpu().detach().numpy()\\n    #x=np.array(np.amax(x,axis=1))\\n    \\n    \\n    #print(x.size())\\n\\n    return x\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zeJguY2gvG",
        "colab_type": "code",
        "outputId": "d0cc453b-bf6f-4e6f-a1b3-cdc46511ad8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "class model_generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(model_generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class model_discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(model_discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))\n",
        "        \n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nclass model_generator(nn.Module):\\n    def __init__(self, g_input_dim, g_output_dim):\\n        super(model_generator, self).__init__()       \\n        self.fc1 = nn.Linear(g_input_dim, 256)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\\n\\n    \\n    # forward method\\n    def forward(self, x): \\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        return torch.tanh(self.fc4(x))\\n    \\nclass model_discriminator(nn.Module):\\n    def __init__(self, d_input_dim):\\n        super(model_discriminator, self).__init__()\\n        self.fc1 = nn.Linear(d_input_dim, 1024)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, 1)\\n    \\n    # forward method\\n    def forward(self, x):\\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        return torch.sigmoid(self.fc4(x))\\n        \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lMPEo6izHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#latent space visualization\n",
        "class modify_space(object):\n",
        "  def __init__(self,importer,test_gen,label=1,label2=2,num_latents=10):\n",
        "    self.batch_size=1\n",
        "    self.label=label\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.label2=label2\n",
        "    self.test_gen=test_gen\n",
        "    self.num_latents=num_latents\n",
        "    self.net_generator=importer.net_generator\n",
        "    self.net_discriminator=importer.net_discriminator\n",
        "    self.generate()\n",
        "    self.latent()\n",
        "\n",
        "\n",
        "  def generate(self):\n",
        "    w=0\n",
        "    e=0\n",
        "    print(1)\n",
        "    for (image,label) in self.test_gen:\n",
        "      if w==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label:\n",
        "          self.image=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "\n",
        "          self.label=label[i].cuda()\n",
        "          w=1\n",
        "          break\n",
        "    for (image,label) in self.test_gen:\n",
        "      if e==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label2:\n",
        "          self.image2=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "          self.label2=label[i].cuda()\n",
        "          e=1\n",
        "          break\n",
        "\n",
        "  def latent(self):\n",
        "    latent1=self.net_generator(self.image,latent_space=True,label=self.label)\n",
        "    latent2=self.net_generator(self.image2,latent_space=True,label=self.label2)\n",
        "    for count in range(self.num_latents+1):\n",
        "      input_latent=(latent1*(count/(self.num_latents))+latent2*(1-count/self.num_latents))\n",
        "      output=self.net_generator(self.image2,latent=input_latent,marker=True,label=self.label)\n",
        "      output=output.view(1, 1,self.filter_size ,self.filter_size ) \n",
        "      save_image(output,'latent_layer1_'+str(count)+'.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4mZAqyQgp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualizing the latent space points in two dimenions using principal component analysis\n",
        "a=0\n",
        "class plotter(object):\n",
        "  def __init__(self,render):\n",
        "    self.batch_size=1\n",
        "    self.test_gen=test_gen\n",
        "    self.net_generator=render.net_generator\n",
        "    self.net_discriminator=render.net_discriminator\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.latent=[]\n",
        "    self.latent_size=256\n",
        "    self.label_array=[]\n",
        "    self.visualize()\n",
        "   \n",
        "  def visualize(self):\n",
        "    for (image,label) in self.test_gen:\n",
        "      for i in range(len(label)):\n",
        "        latent=self.net_generator(np.reshape(image[i],[self.batch_size,self.num_layers*self.filter_size*self.filter_size]).cuda(),plot=True,label=label[i].item()).cpu().detach().numpy()\n",
        "        latent_x=np.reshape(latent,(self.latent_size))\n",
        "        self.latent.append(latent_x)\n",
        "        \n",
        "        self.label_array.append(label[i].item())\n",
        "        #feat_cols = ['feature'+str(i) for i in range(latent_x.shape[1])]\n",
        "        #latent=pd.DataFrame(latent_x,columns=feat_cols)\n",
        "        #print(latent.shape)\n",
        "        #print(latent)\n",
        "\n",
        "    pca_latent = PCA(n_components=2)\n",
        "  \n",
        "    twodim_pca = pca_latent.fit_transform(np.array(self.latent))\n",
        "    colormap = np.array(['r', 'g', 'b','pink','orange','lightblue','black','brown','yellow','white'])\n",
        "    for i in range(10):\n",
        "      temp1=[]\n",
        "      temp2=[]\n",
        "      \n",
        "      for j in range(len(self.label_array)):\n",
        "\n",
        "        if self.label_array[j]==i:\n",
        "          temp1.append(twodim_pca[j][0])\n",
        "          temp2.append(twodim_pca[j][1]) \n",
        "      plt.scatter(temp1,temp2,c=colormap[i])\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_OekwHJ5W2d",
        "colab_type": "code",
        "outputId": "90d77efd-c815-4ac1-eb50-29b62b28b711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#MAIN DO NOT TOUCH IT\n",
        "net_generator=model_generator().cuda()\n",
        "net_discriminator=model_discriminator().cuda()\n",
        "lr=0.000025\n",
        "opt_generator=torch.optim.Adam(net_generator.parameters(),lr)\n",
        "opt_discriminator=torch.optim.Adam(net_discriminator.parameters(),lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8JeXOyqY-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If we are building a class without using any functionality of any other class then object can be written it means that the class will return an object type dataset\n",
        "#However if we write like data.Datasets of Sampler then it means that it will have functionality of the superclass data.Datasets or Sampler however ultimately these super classes will also return an object.\n",
        "#so here in a way we are returning an object of the super class via the sub class.\n",
        "#BUILDING A SIMPLE AUTOENCODER WHICH JUST AIMS AT RECONSTRUCTION WITHOUT ANY STOCHASTICITY\n",
        "\n",
        "\n",
        "class training(object):\n",
        "\n",
        "  def __init__(self,epochs=30,train_g=train_gen,test_g=test_gen,generate_latent=False,plot=False,opt_generator=opt_generator,opt_discriminator=opt_discriminator):\n",
        "    self.epochs=epochs\n",
        "    self.lr=lr\n",
        "    self.train_gen=train_g\n",
        "    self.batch_size=32\n",
        "    self.test_gen=test_g\n",
        "    self.device='cuda'\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.net_generator=net_generator\n",
        "    self.net_discriminator=net_discriminator\n",
        "    self.loss=torch.nn.BCELoss()\n",
        "    self.opt_discriminator=opt_discriminator\n",
        "    self.opt_generator=opt_generator\n",
        "    self.generation_iters=1\n",
        "    self.d_iters=3\n",
        "    self.class_size=10\n",
        "    self.labeler=torch.from_numpy(np.zeros([self.batch_size,self.class_size],dtype='float32')).cuda()\n",
        "    self.train()\n",
        "    \n",
        "    if generate_latent==True:\n",
        "      modify_space(self,test_gen)\n",
        "    if plot==True:\n",
        "      plotter(self)\n",
        "    self.test()\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    for epoch_no in range(self.epochs):\n",
        "      counter=0\n",
        "      for (image,label) in self.train_gen:\n",
        "        img=image.to(self.device)\n",
        "        lab_real=torch.from_numpy(np.ones([self.batch_size,1],np.float32)).to(self.device)\n",
        "        lab_fake=torch.from_numpy(np.zeros([self.batch_size,1],np.float32)).to(self.device)\n",
        "\n",
        "        #train_discriminator\n",
        "        #self.opt_generator.zero_grad()\n",
        "\n",
        "        self.opt_discriminator.zero_grad()\n",
        "        \"\"\"random_choice=np.random.choice([0,1])\n",
        "        if random_choice==0:\n",
        "          input_label=lab_fake\n",
        "          input_img=output_generator[:,:,:27,:27]\n",
        "          print(\"fake_image\",input_img.size())\n",
        "        else:\n",
        "          input_label=lab_real\n",
        "          input_img=img[:,:,:27,:27]\n",
        "          print(\"real_image\",input_img.size())\n",
        "          \"\"\"\n",
        "        loss_d=0\n",
        "        for i in range(self.d_iters):  \n",
        "          output_generator=self.net_generator(img,label).to(self.device)\n",
        "          vec=self.labeler\n",
        "          for i in range(self.batch_size):\n",
        "            vec[i][int(label[i].item())]=1\n",
        "\n",
        "          output_generator=torch.cat((output_generator,vec),1)\n",
        "\n",
        "          logits_fake = self.net_discriminator(output_generator).to(self.device)\n",
        "\n",
        "\n",
        "          logits_real = self.net_discriminator(torch.cat((img.view(self.batch_size,self.num_layers*self.filter_size*self.filter_size),vec),1)).to(self.device)\n",
        "\n",
        "          loss_discriminator_real = self.loss(logits_real,lab_real)\n",
        "          loss_discriminator_fake = self.loss(logits_fake,lab_fake)\n",
        "          loss_discriminator=loss_discriminator_real+loss_discriminator_fake\n",
        "          loss_discriminator.backward(retain_graph=True)\n",
        "          self.opt_discriminator.step()\n",
        "          loss_d+=loss_discriminator\n",
        "        #discriminator one iteration complete\n",
        "\n",
        "        #train_generator\n",
        "        tot_loss=0\n",
        "        for i in range(self.generation_iters):\n",
        "          img=0\n",
        "          self.opt_generator.zero_grad()\n",
        "          #self.opt_discriminator.zero_grad()\n",
        "          output_generator=self.net_generator(img,label).to(self.device)\n",
        "          output_generator=torch.cat((output_generator,vec),1)\n",
        "          #print(\"fake_image\",output_generator.size())\n",
        "          logit_disc = self.net_discriminator(output_generator).to(self.device)\n",
        "          loss_generator = self.loss(logit_disc,lab_real)\n",
        "          tot_loss+=loss_generator\n",
        "          loss_generator.backward()\n",
        "          self.opt_generator.step()\n",
        "        #One generator iteration completed\n",
        "        counter+=1\n",
        "        print(\"ITERATION_NO.:\",counter ,\"LOSS_Generator:\",tot_loss.item()/self.generation_iters ,\"LOSS_Discriminator:\",loss_d.item()/self.d_iters)\n",
        "      print(\"EPOCH OVER:\",epoch_no)\n",
        "   \n",
        "  def test(self):\n",
        "    count=0\n",
        "    for (image,label) in self.train_gen:\n",
        "      img=image[0].to(self.device)\n",
        "      lab=label.to(self.device)\n",
        "      outputs = self.net_generator(img,label).view(32,1,28,28)\n",
        "      count+=1\n",
        "      print(\"Image_no\",count)\n",
        "      if count%100==0:\n",
        "        save_image(outputs,'testing'+str(count)+'.png')\n",
        "       \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUJezNR8OFw",
        "colab_type": "code",
        "outputId": "a6907577-8298-471a-8655-47bf8be9d876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True,generate_latent=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ITERATION_NO.: 631 LOSS_Generator: 5.806629180908203 LOSS_Discriminator: 0.11789703369140625\n",
            "ITERATION_NO.: 632 LOSS_Generator: 5.012133598327637 LOSS_Discriminator: 0.11223995685577393\n",
            "ITERATION_NO.: 633 LOSS_Generator: 4.970308780670166 LOSS_Discriminator: 0.02246405929327011\n",
            "ITERATION_NO.: 634 LOSS_Generator: 4.728080749511719 LOSS_Discriminator: 0.13024228811264038\n",
            "ITERATION_NO.: 635 LOSS_Generator: 4.306203842163086 LOSS_Discriminator: 0.12628276149431863\n",
            "ITERATION_NO.: 636 LOSS_Generator: 4.011298179626465 LOSS_Discriminator: 0.05422412355740865\n",
            "ITERATION_NO.: 637 LOSS_Generator: 4.734061241149902 LOSS_Discriminator: 0.09575721621513367\n",
            "ITERATION_NO.: 638 LOSS_Generator: 4.778985500335693 LOSS_Discriminator: 0.05302496751149496\n",
            "ITERATION_NO.: 639 LOSS_Generator: 5.104422092437744 LOSS_Discriminator: 0.0558294008175532\n",
            "ITERATION_NO.: 640 LOSS_Generator: 5.261897087097168 LOSS_Discriminator: 0.03660347064336141\n",
            "ITERATION_NO.: 641 LOSS_Generator: 5.939151287078857 LOSS_Discriminator: 0.035681821405887604\n",
            "ITERATION_NO.: 642 LOSS_Generator: 6.633004188537598 LOSS_Discriminator: 0.020426221191883087\n",
            "ITERATION_NO.: 643 LOSS_Generator: 6.36965274810791 LOSS_Discriminator: 0.12898496786753336\n",
            "ITERATION_NO.: 644 LOSS_Generator: 5.9624128341674805 LOSS_Discriminator: 0.014833960682153702\n",
            "ITERATION_NO.: 645 LOSS_Generator: 5.386795997619629 LOSS_Discriminator: 0.16964991887410483\n",
            "ITERATION_NO.: 646 LOSS_Generator: 4.104503631591797 LOSS_Discriminator: 0.35286569595336914\n",
            "ITERATION_NO.: 647 LOSS_Generator: 3.658421039581299 LOSS_Discriminator: 0.15462303161621094\n",
            "ITERATION_NO.: 648 LOSS_Generator: 4.510067939758301 LOSS_Discriminator: 0.11048872272173564\n",
            "ITERATION_NO.: 649 LOSS_Generator: 6.0607757568359375 LOSS_Discriminator: 0.06094969312349955\n",
            "ITERATION_NO.: 650 LOSS_Generator: 7.287711143493652 LOSS_Discriminator: 0.10988147060076396\n",
            "ITERATION_NO.: 651 LOSS_Generator: 7.841017723083496 LOSS_Discriminator: 0.07992274065812428\n",
            "ITERATION_NO.: 652 LOSS_Generator: 7.85753059387207 LOSS_Discriminator: 0.18318518002827963\n",
            "ITERATION_NO.: 653 LOSS_Generator: 7.0969953536987305 LOSS_Discriminator: 0.4053894281387329\n",
            "ITERATION_NO.: 654 LOSS_Generator: 6.1445770263671875 LOSS_Discriminator: 0.0017778555241723855\n",
            "ITERATION_NO.: 655 LOSS_Generator: 5.994830131530762 LOSS_Discriminator: 0.0032722757508357367\n",
            "ITERATION_NO.: 656 LOSS_Generator: 5.230645179748535 LOSS_Discriminator: 0.04474855959415436\n",
            "ITERATION_NO.: 657 LOSS_Generator: 5.185596466064453 LOSS_Discriminator: 0.013966004053751627\n",
            "ITERATION_NO.: 658 LOSS_Generator: 5.987057685852051 LOSS_Discriminator: 0.10945709546407063\n",
            "ITERATION_NO.: 659 LOSS_Generator: 5.105955123901367 LOSS_Discriminator: 0.015121445059776306\n",
            "ITERATION_NO.: 660 LOSS_Generator: 4.405396461486816 LOSS_Discriminator: 0.3017375667889913\n",
            "ITERATION_NO.: 661 LOSS_Generator: 3.2010889053344727 LOSS_Discriminator: 0.31618515650431317\n",
            "ITERATION_NO.: 662 LOSS_Generator: 3.460371494293213 LOSS_Discriminator: 0.2003475030263265\n",
            "ITERATION_NO.: 663 LOSS_Generator: 4.724832534790039 LOSS_Discriminator: 0.2543613115946452\n",
            "ITERATION_NO.: 664 LOSS_Generator: 5.499259948730469 LOSS_Discriminator: 0.1948323647181193\n",
            "ITERATION_NO.: 665 LOSS_Generator: 5.360139846801758 LOSS_Discriminator: 0.07540420691172282\n",
            "ITERATION_NO.: 666 LOSS_Generator: 4.9853196144104 LOSS_Discriminator: 0.18683181206385294\n",
            "ITERATION_NO.: 667 LOSS_Generator: 4.270401477813721 LOSS_Discriminator: 0.12015728155771892\n",
            "ITERATION_NO.: 668 LOSS_Generator: 4.454916954040527 LOSS_Discriminator: 0.05725687245527903\n",
            "ITERATION_NO.: 669 LOSS_Generator: 4.74191951751709 LOSS_Discriminator: 0.10437542200088501\n",
            "ITERATION_NO.: 670 LOSS_Generator: 5.789541244506836 LOSS_Discriminator: 0.018788572400808334\n",
            "ITERATION_NO.: 671 LOSS_Generator: 5.57866907119751 LOSS_Discriminator: 0.32172250747680664\n",
            "ITERATION_NO.: 672 LOSS_Generator: 5.355892658233643 LOSS_Discriminator: 0.15107337633768717\n",
            "ITERATION_NO.: 673 LOSS_Generator: 3.8130545616149902 LOSS_Discriminator: 0.04399354259173075\n",
            "ITERATION_NO.: 674 LOSS_Generator: 4.796599388122559 LOSS_Discriminator: 0.0423058420419693\n",
            "ITERATION_NO.: 675 LOSS_Generator: 5.346278190612793 LOSS_Discriminator: 0.015601107229789099\n",
            "ITERATION_NO.: 676 LOSS_Generator: 6.000100135803223 LOSS_Discriminator: 0.0340491458773613\n",
            "ITERATION_NO.: 677 LOSS_Generator: 5.942522048950195 LOSS_Discriminator: 0.005533897007505099\n",
            "ITERATION_NO.: 678 LOSS_Generator: 6.278225898742676 LOSS_Discriminator: 0.07025059560934703\n",
            "ITERATION_NO.: 679 LOSS_Generator: 5.457794189453125 LOSS_Discriminator: 0.27881399790445965\n",
            "ITERATION_NO.: 680 LOSS_Generator: 4.136465549468994 LOSS_Discriminator: 0.045061359802881874\n",
            "ITERATION_NO.: 681 LOSS_Generator: 3.7336349487304688 LOSS_Discriminator: 0.22386227051417032\n",
            "ITERATION_NO.: 682 LOSS_Generator: 3.4645237922668457 LOSS_Discriminator: 0.128381609916687\n",
            "ITERATION_NO.: 683 LOSS_Generator: 4.3269877433776855 LOSS_Discriminator: 0.16253972053527832\n",
            "ITERATION_NO.: 684 LOSS_Generator: 5.007956504821777 LOSS_Discriminator: 0.14713252584139505\n",
            "ITERATION_NO.: 685 LOSS_Generator: 5.308316230773926 LOSS_Discriminator: 0.038265724976857506\n",
            "ITERATION_NO.: 686 LOSS_Generator: 5.197857856750488 LOSS_Discriminator: 0.016069871683915455\n",
            "ITERATION_NO.: 687 LOSS_Generator: 5.249523639678955 LOSS_Discriminator: 0.10902387897173564\n",
            "ITERATION_NO.: 688 LOSS_Generator: 5.065284729003906 LOSS_Discriminator: 0.12296249469121297\n",
            "ITERATION_NO.: 689 LOSS_Generator: 4.698803901672363 LOSS_Discriminator: 0.08595206340154012\n",
            "ITERATION_NO.: 690 LOSS_Generator: 4.886384963989258 LOSS_Discriminator: 0.026846302052338917\n",
            "ITERATION_NO.: 691 LOSS_Generator: 4.576112747192383 LOSS_Discriminator: 0.0418716569741567\n",
            "ITERATION_NO.: 692 LOSS_Generator: 5.390810012817383 LOSS_Discriminator: 0.015403675536314646\n",
            "ITERATION_NO.: 693 LOSS_Generator: 5.372842788696289 LOSS_Discriminator: 0.012632772326469421\n",
            "ITERATION_NO.: 694 LOSS_Generator: 5.180209636688232 LOSS_Discriminator: 0.24519085884094238\n",
            "ITERATION_NO.: 695 LOSS_Generator: 4.450926780700684 LOSS_Discriminator: 0.05482293168703715\n",
            "ITERATION_NO.: 696 LOSS_Generator: 4.084564208984375 LOSS_Discriminator: 0.5444415807723999\n",
            "ITERATION_NO.: 697 LOSS_Generator: 2.78896427154541 LOSS_Discriminator: 0.3679538567860921\n",
            "ITERATION_NO.: 698 LOSS_Generator: 4.22344970703125 LOSS_Discriminator: 0.16925126314163208\n",
            "ITERATION_NO.: 699 LOSS_Generator: 6.27746057510376 LOSS_Discriminator: 0.10786672433217366\n",
            "ITERATION_NO.: 700 LOSS_Generator: 7.003225326538086 LOSS_Discriminator: 0.013793249924977621\n",
            "ITERATION_NO.: 701 LOSS_Generator: 7.4187517166137695 LOSS_Discriminator: 0.003504143717388312\n",
            "ITERATION_NO.: 702 LOSS_Generator: 7.557863712310791 LOSS_Discriminator: 0.06710987786451976\n",
            "ITERATION_NO.: 703 LOSS_Generator: 7.618528366088867 LOSS_Discriminator: 0.17371952533721924\n",
            "ITERATION_NO.: 704 LOSS_Generator: 7.330358982086182 LOSS_Discriminator: 0.18456363677978516\n",
            "ITERATION_NO.: 705 LOSS_Generator: 7.300750732421875 LOSS_Discriminator: 0.07811521987120311\n",
            "ITERATION_NO.: 706 LOSS_Generator: 6.753307819366455 LOSS_Discriminator: 0.31013305981953937\n",
            "ITERATION_NO.: 707 LOSS_Generator: 6.055049896240234 LOSS_Discriminator: 0.00813439612587293\n",
            "ITERATION_NO.: 708 LOSS_Generator: 5.545489311218262 LOSS_Discriminator: 0.20476730664571127\n",
            "ITERATION_NO.: 709 LOSS_Generator: 3.9326283931732178 LOSS_Discriminator: 0.03441977500915527\n",
            "ITERATION_NO.: 710 LOSS_Generator: 4.082082748413086 LOSS_Discriminator: 0.11209967732429504\n",
            "ITERATION_NO.: 711 LOSS_Generator: 4.094727516174316 LOSS_Discriminator: 0.2921221653620402\n",
            "ITERATION_NO.: 712 LOSS_Generator: 3.8730857372283936 LOSS_Discriminator: 0.08248793582121532\n",
            "ITERATION_NO.: 713 LOSS_Generator: 4.034034252166748 LOSS_Discriminator: 0.09957816203435262\n",
            "ITERATION_NO.: 714 LOSS_Generator: 4.907943248748779 LOSS_Discriminator: 0.022923648357391357\n",
            "ITERATION_NO.: 715 LOSS_Generator: 4.92154598236084 LOSS_Discriminator: 0.06708801289399464\n",
            "ITERATION_NO.: 716 LOSS_Generator: 4.7576375007629395 LOSS_Discriminator: 0.10442796349525452\n",
            "ITERATION_NO.: 717 LOSS_Generator: 4.938649654388428 LOSS_Discriminator: 0.12563663721084595\n",
            "ITERATION_NO.: 718 LOSS_Generator: 4.565785884857178 LOSS_Discriminator: 0.07304839789867401\n",
            "ITERATION_NO.: 719 LOSS_Generator: 4.505837440490723 LOSS_Discriminator: 0.14753671487172446\n",
            "ITERATION_NO.: 720 LOSS_Generator: 4.6666059494018555 LOSS_Discriminator: 0.05096071461836497\n",
            "ITERATION_NO.: 721 LOSS_Generator: 4.707292556762695 LOSS_Discriminator: 0.04432536164919535\n",
            "ITERATION_NO.: 722 LOSS_Generator: 5.2831645011901855 LOSS_Discriminator: 0.13328556219736734\n",
            "ITERATION_NO.: 723 LOSS_Generator: 4.573272705078125 LOSS_Discriminator: 0.018146480123202007\n",
            "ITERATION_NO.: 724 LOSS_Generator: 4.615604877471924 LOSS_Discriminator: 0.05477610230445862\n",
            "ITERATION_NO.: 725 LOSS_Generator: 4.850822448730469 LOSS_Discriminator: 0.12841681639353433\n",
            "ITERATION_NO.: 726 LOSS_Generator: 4.718749523162842 LOSS_Discriminator: 0.06107992927233378\n",
            "ITERATION_NO.: 727 LOSS_Generator: 4.682775497436523 LOSS_Discriminator: 0.03921728332837423\n",
            "ITERATION_NO.: 728 LOSS_Generator: 4.884200096130371 LOSS_Discriminator: 0.05361704031626383\n",
            "ITERATION_NO.: 729 LOSS_Generator: 5.368039608001709 LOSS_Discriminator: 0.05024768908818563\n",
            "ITERATION_NO.: 730 LOSS_Generator: 5.107564926147461 LOSS_Discriminator: 0.29511594772338867\n",
            "ITERATION_NO.: 731 LOSS_Generator: 4.614891529083252 LOSS_Discriminator: 0.03739443918069204\n",
            "ITERATION_NO.: 732 LOSS_Generator: 4.846232891082764 LOSS_Discriminator: 0.06756152709325154\n",
            "ITERATION_NO.: 733 LOSS_Generator: 4.335493564605713 LOSS_Discriminator: 0.06815886497497559\n",
            "ITERATION_NO.: 734 LOSS_Generator: 5.0057525634765625 LOSS_Discriminator: 0.04538016517957052\n",
            "ITERATION_NO.: 735 LOSS_Generator: 4.658114433288574 LOSS_Discriminator: 0.18371872107187906\n",
            "ITERATION_NO.: 736 LOSS_Generator: 4.644338130950928 LOSS_Discriminator: 0.08655803402264912\n",
            "ITERATION_NO.: 737 LOSS_Generator: 4.397912979125977 LOSS_Discriminator: 0.053258880972862244\n",
            "ITERATION_NO.: 738 LOSS_Generator: 4.41458797454834 LOSS_Discriminator: 0.08698105812072754\n",
            "ITERATION_NO.: 739 LOSS_Generator: 4.792144775390625 LOSS_Discriminator: 0.0958240528901418\n",
            "ITERATION_NO.: 740 LOSS_Generator: 4.501002311706543 LOSS_Discriminator: 0.3081313371658325\n",
            "ITERATION_NO.: 741 LOSS_Generator: 3.691420078277588 LOSS_Discriminator: 0.09375240405400594\n",
            "ITERATION_NO.: 742 LOSS_Generator: 4.155378341674805 LOSS_Discriminator: 0.24623948335647583\n",
            "ITERATION_NO.: 743 LOSS_Generator: 4.946193695068359 LOSS_Discriminator: 0.09904176990191142\n",
            "ITERATION_NO.: 744 LOSS_Generator: 5.739879608154297 LOSS_Discriminator: 0.06545154750347137\n",
            "ITERATION_NO.: 745 LOSS_Generator: 5.3328962326049805 LOSS_Discriminator: 0.15482248862584433\n",
            "ITERATION_NO.: 746 LOSS_Generator: 5.514589309692383 LOSS_Discriminator: 0.009896663948893547\n",
            "ITERATION_NO.: 747 LOSS_Generator: 5.372897624969482 LOSS_Discriminator: 0.03678379952907562\n",
            "ITERATION_NO.: 748 LOSS_Generator: 4.836013317108154 LOSS_Discriminator: 0.14764849344889322\n",
            "ITERATION_NO.: 749 LOSS_Generator: 3.934983730316162 LOSS_Discriminator: 0.0583613912264506\n",
            "ITERATION_NO.: 750 LOSS_Generator: 3.641307830810547 LOSS_Discriminator: 0.09630306561787923\n",
            "ITERATION_NO.: 751 LOSS_Generator: 5.030668258666992 LOSS_Discriminator: 0.06520713369051616\n",
            "ITERATION_NO.: 752 LOSS_Generator: 5.047713279724121 LOSS_Discriminator: 0.055712138613065086\n",
            "ITERATION_NO.: 753 LOSS_Generator: 5.575928688049316 LOSS_Discriminator: 0.043555488189061485\n",
            "ITERATION_NO.: 754 LOSS_Generator: 5.654455661773682 LOSS_Discriminator: 0.010854835311571756\n",
            "ITERATION_NO.: 755 LOSS_Generator: 6.412280082702637 LOSS_Discriminator: 0.16378271579742432\n",
            "ITERATION_NO.: 756 LOSS_Generator: 5.508567810058594 LOSS_Discriminator: 0.022565672794977825\n",
            "ITERATION_NO.: 757 LOSS_Generator: 5.142011642456055 LOSS_Discriminator: 0.08706472317377727\n",
            "ITERATION_NO.: 758 LOSS_Generator: 4.409673690795898 LOSS_Discriminator: 0.06134549776713053\n",
            "ITERATION_NO.: 759 LOSS_Generator: 4.329324722290039 LOSS_Discriminator: 0.041814982891082764\n",
            "ITERATION_NO.: 760 LOSS_Generator: 4.178421974182129 LOSS_Discriminator: 0.0943179726600647\n",
            "ITERATION_NO.: 761 LOSS_Generator: 4.944616317749023 LOSS_Discriminator: 0.07266863187154134\n",
            "ITERATION_NO.: 762 LOSS_Generator: 5.434135437011719 LOSS_Discriminator: 0.016902598241964977\n",
            "ITERATION_NO.: 763 LOSS_Generator: 5.731462478637695 LOSS_Discriminator: 0.19936259587605795\n",
            "ITERATION_NO.: 764 LOSS_Generator: 5.814694404602051 LOSS_Discriminator: 0.025319799780845642\n",
            "ITERATION_NO.: 765 LOSS_Generator: 5.153705596923828 LOSS_Discriminator: 0.03369271755218506\n",
            "ITERATION_NO.: 766 LOSS_Generator: 4.3668107986450195 LOSS_Discriminator: 0.18294270833333334\n",
            "ITERATION_NO.: 767 LOSS_Generator: 4.644055366516113 LOSS_Discriminator: 0.07880878448486328\n",
            "ITERATION_NO.: 768 LOSS_Generator: 4.563113212585449 LOSS_Discriminator: 0.08503463864326477\n",
            "ITERATION_NO.: 769 LOSS_Generator: 5.402135372161865 LOSS_Discriminator: 0.07523905734221141\n",
            "ITERATION_NO.: 770 LOSS_Generator: 4.848349571228027 LOSS_Discriminator: 0.06857875982920329\n",
            "ITERATION_NO.: 771 LOSS_Generator: 5.507536888122559 LOSS_Discriminator: 0.06462934613227844\n",
            "ITERATION_NO.: 772 LOSS_Generator: 5.107734680175781 LOSS_Discriminator: 0.0139242485165596\n",
            "ITERATION_NO.: 773 LOSS_Generator: 5.773036479949951 LOSS_Discriminator: 0.07456909120082855\n",
            "ITERATION_NO.: 774 LOSS_Generator: 4.823931694030762 LOSS_Discriminator: 0.1008396844069163\n",
            "ITERATION_NO.: 775 LOSS_Generator: 5.273094654083252 LOSS_Discriminator: 0.06743741035461426\n",
            "ITERATION_NO.: 776 LOSS_Generator: 4.205574035644531 LOSS_Discriminator: 0.12052607536315918\n",
            "ITERATION_NO.: 777 LOSS_Generator: 4.266877174377441 LOSS_Discriminator: 0.1281907061735789\n",
            "ITERATION_NO.: 778 LOSS_Generator: 4.726421356201172 LOSS_Discriminator: 0.06867911418279012\n",
            "ITERATION_NO.: 779 LOSS_Generator: 5.473669052124023 LOSS_Discriminator: 0.02536064386367798\n",
            "ITERATION_NO.: 780 LOSS_Generator: 5.868532180786133 LOSS_Discriminator: 0.021294179062048595\n",
            "ITERATION_NO.: 781 LOSS_Generator: 6.303916931152344 LOSS_Discriminator: 0.06850896279017131\n",
            "ITERATION_NO.: 782 LOSS_Generator: 6.173252105712891 LOSS_Discriminator: 0.1913924217224121\n",
            "ITERATION_NO.: 783 LOSS_Generator: 5.56711483001709 LOSS_Discriminator: 0.007573770980040233\n",
            "ITERATION_NO.: 784 LOSS_Generator: 4.943975925445557 LOSS_Discriminator: 0.1640763779481252\n",
            "ITERATION_NO.: 785 LOSS_Generator: 4.120561122894287 LOSS_Discriminator: 0.03676982223987579\n",
            "ITERATION_NO.: 786 LOSS_Generator: 4.154824733734131 LOSS_Discriminator: 0.08186390002568562\n",
            "ITERATION_NO.: 787 LOSS_Generator: 5.195822715759277 LOSS_Discriminator: 0.111492524544398\n",
            "ITERATION_NO.: 788 LOSS_Generator: 6.2040228843688965 LOSS_Discriminator: 0.16528606414794922\n",
            "ITERATION_NO.: 789 LOSS_Generator: 5.925428867340088 LOSS_Discriminator: 0.05289676288763682\n",
            "ITERATION_NO.: 790 LOSS_Generator: 6.079693794250488 LOSS_Discriminator: 0.11268240213394165\n",
            "ITERATION_NO.: 791 LOSS_Generator: 5.853405952453613 LOSS_Discriminator: 0.004972473407785098\n",
            "ITERATION_NO.: 792 LOSS_Generator: 6.105212211608887 LOSS_Discriminator: 0.020474712053934734\n",
            "ITERATION_NO.: 793 LOSS_Generator: 5.744478225708008 LOSS_Discriminator: 0.0977190335591634\n",
            "ITERATION_NO.: 794 LOSS_Generator: 5.447232723236084 LOSS_Discriminator: 0.06172209978103638\n",
            "ITERATION_NO.: 795 LOSS_Generator: 4.33452844619751 LOSS_Discriminator: 0.17635647455851236\n",
            "ITERATION_NO.: 796 LOSS_Generator: 4.507570266723633 LOSS_Discriminator: 0.035566379626592\n",
            "ITERATION_NO.: 797 LOSS_Generator: 4.482874870300293 LOSS_Discriminator: 0.07395355900128682\n",
            "ITERATION_NO.: 798 LOSS_Generator: 5.200833797454834 LOSS_Discriminator: 0.08053250114123027\n",
            "ITERATION_NO.: 799 LOSS_Generator: 5.919231414794922 LOSS_Discriminator: 0.18693403402964273\n",
            "ITERATION_NO.: 800 LOSS_Generator: 5.956337928771973 LOSS_Discriminator: 0.011356689035892487\n",
            "ITERATION_NO.: 801 LOSS_Generator: 6.048468589782715 LOSS_Discriminator: 0.15669304132461548\n",
            "ITERATION_NO.: 802 LOSS_Generator: 5.543389797210693 LOSS_Discriminator: 0.007991993178923925\n",
            "ITERATION_NO.: 803 LOSS_Generator: 5.2473673820495605 LOSS_Discriminator: 0.09348394473393758\n",
            "ITERATION_NO.: 804 LOSS_Generator: 5.151602745056152 LOSS_Discriminator: 0.019757761309544247\n",
            "ITERATION_NO.: 805 LOSS_Generator: 4.900528907775879 LOSS_Discriminator: 0.11635483304659526\n",
            "ITERATION_NO.: 806 LOSS_Generator: 4.417304039001465 LOSS_Discriminator: 0.16996548573176065\n",
            "ITERATION_NO.: 807 LOSS_Generator: 3.4878010749816895 LOSS_Discriminator: 0.18294556935628256\n",
            "ITERATION_NO.: 808 LOSS_Generator: 4.131093978881836 LOSS_Discriminator: 0.12576373418172201\n",
            "ITERATION_NO.: 809 LOSS_Generator: 5.254075050354004 LOSS_Discriminator: 0.09881820281346639\n",
            "ITERATION_NO.: 810 LOSS_Generator: 6.114017486572266 LOSS_Discriminator: 0.02281567205985387\n",
            "ITERATION_NO.: 811 LOSS_Generator: 7.0123395919799805 LOSS_Discriminator: 0.009217529247204462\n",
            "ITERATION_NO.: 812 LOSS_Generator: 6.901635646820068 LOSS_Discriminator: 0.4340514341990153\n",
            "ITERATION_NO.: 813 LOSS_Generator: 5.999992370605469 LOSS_Discriminator: 0.11751874287923177\n",
            "ITERATION_NO.: 814 LOSS_Generator: 5.235633373260498 LOSS_Discriminator: 0.027973671754201252\n",
            "ITERATION_NO.: 815 LOSS_Generator: 4.554222106933594 LOSS_Discriminator: 0.030286692082881927\n",
            "ITERATION_NO.: 816 LOSS_Generator: 4.8674116134643555 LOSS_Discriminator: 0.03030162552992503\n",
            "ITERATION_NO.: 817 LOSS_Generator: 5.178753852844238 LOSS_Discriminator: 0.1261883278687795\n",
            "ITERATION_NO.: 818 LOSS_Generator: 5.760302543640137 LOSS_Discriminator: 0.03930669277906418\n",
            "ITERATION_NO.: 819 LOSS_Generator: 5.251084327697754 LOSS_Discriminator: 0.14977643887201944\n",
            "ITERATION_NO.: 820 LOSS_Generator: 5.298489570617676 LOSS_Discriminator: 0.13501479228337607\n",
            "ITERATION_NO.: 821 LOSS_Generator: 4.058279991149902 LOSS_Discriminator: 0.06464017430941264\n",
            "ITERATION_NO.: 822 LOSS_Generator: 3.752638339996338 LOSS_Discriminator: 0.10003518064816792\n",
            "ITERATION_NO.: 823 LOSS_Generator: 4.981907367706299 LOSS_Discriminator: 0.14961536725362143\n",
            "ITERATION_NO.: 824 LOSS_Generator: 4.895641803741455 LOSS_Discriminator: 0.10909849405288696\n",
            "ITERATION_NO.: 825 LOSS_Generator: 5.467175483703613 LOSS_Discriminator: 0.03134240210056305\n",
            "ITERATION_NO.: 826 LOSS_Generator: 5.1247358322143555 LOSS_Discriminator: 0.0866747796535492\n",
            "ITERATION_NO.: 827 LOSS_Generator: 5.194854736328125 LOSS_Discriminator: 0.11808578173319499\n",
            "ITERATION_NO.: 828 LOSS_Generator: 4.901558876037598 LOSS_Discriminator: 0.03486757477124532\n",
            "ITERATION_NO.: 829 LOSS_Generator: 4.64153528213501 LOSS_Discriminator: 0.213455597559611\n",
            "ITERATION_NO.: 830 LOSS_Generator: 4.033123970031738 LOSS_Discriminator: 0.05679874122142792\n",
            "ITERATION_NO.: 831 LOSS_Generator: 5.0961594581604 LOSS_Discriminator: 0.06964027881622314\n",
            "ITERATION_NO.: 832 LOSS_Generator: 5.93833065032959 LOSS_Discriminator: 0.021744385361671448\n",
            "ITERATION_NO.: 833 LOSS_Generator: 6.7241058349609375 LOSS_Discriminator: 0.0060098133981227875\n",
            "ITERATION_NO.: 834 LOSS_Generator: 6.785200119018555 LOSS_Discriminator: 0.03511779010295868\n",
            "ITERATION_NO.: 835 LOSS_Generator: 6.961719512939453 LOSS_Discriminator: 0.09499307473500569\n",
            "ITERATION_NO.: 836 LOSS_Generator: 6.636100769042969 LOSS_Discriminator: 0.03403036048014959\n",
            "ITERATION_NO.: 837 LOSS_Generator: 6.315127372741699 LOSS_Discriminator: 0.07085344195365906\n",
            "ITERATION_NO.: 838 LOSS_Generator: 5.803945541381836 LOSS_Discriminator: 0.1625538965066274\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.345743179321289 LOSS_Discriminator: 0.045216684540112816\n",
            "ITERATION_NO.: 840 LOSS_Generator: 4.191479682922363 LOSS_Discriminator: 0.05435784657796224\n",
            "ITERATION_NO.: 841 LOSS_Generator: 3.715360164642334 LOSS_Discriminator: 0.0488922248284022\n",
            "ITERATION_NO.: 842 LOSS_Generator: 4.272797107696533 LOSS_Discriminator: 0.026393570005893707\n",
            "ITERATION_NO.: 843 LOSS_Generator: 5.207189083099365 LOSS_Discriminator: 0.12671523292859396\n",
            "ITERATION_NO.: 844 LOSS_Generator: 5.156392574310303 LOSS_Discriminator: 0.18719849983851114\n",
            "ITERATION_NO.: 845 LOSS_Generator: 5.463035583496094 LOSS_Discriminator: 0.06052341560522715\n",
            "ITERATION_NO.: 846 LOSS_Generator: 4.662438869476318 LOSS_Discriminator: 0.15138115485509238\n",
            "ITERATION_NO.: 847 LOSS_Generator: 4.982246398925781 LOSS_Discriminator: 0.0408174991607666\n",
            "ITERATION_NO.: 848 LOSS_Generator: 5.368343353271484 LOSS_Discriminator: 0.036749606331189476\n",
            "ITERATION_NO.: 849 LOSS_Generator: 5.282840728759766 LOSS_Discriminator: 0.020528169969717663\n",
            "ITERATION_NO.: 850 LOSS_Generator: 5.766486644744873 LOSS_Discriminator: 0.10561334093411763\n",
            "ITERATION_NO.: 851 LOSS_Generator: 4.951094627380371 LOSS_Discriminator: 0.1591770350933075\n",
            "ITERATION_NO.: 852 LOSS_Generator: 4.196817874908447 LOSS_Discriminator: 0.022812192638715107\n",
            "ITERATION_NO.: 853 LOSS_Generator: 4.250956058502197 LOSS_Discriminator: 0.12516703208287558\n",
            "ITERATION_NO.: 854 LOSS_Generator: 3.337515115737915 LOSS_Discriminator: 0.2231749693552653\n",
            "ITERATION_NO.: 855 LOSS_Generator: 3.996567726135254 LOSS_Discriminator: 0.1313739816347758\n",
            "ITERATION_NO.: 856 LOSS_Generator: 5.085363864898682 LOSS_Discriminator: 0.07763473192850749\n",
            "ITERATION_NO.: 857 LOSS_Generator: 5.364010334014893 LOSS_Discriminator: 0.3000810543696086\n",
            "ITERATION_NO.: 858 LOSS_Generator: 5.444703102111816 LOSS_Discriminator: 0.011861477047204971\n",
            "ITERATION_NO.: 859 LOSS_Generator: 5.777303695678711 LOSS_Discriminator: 0.19450662533442178\n",
            "ITERATION_NO.: 860 LOSS_Generator: 4.754499435424805 LOSS_Discriminator: 0.05149378379185995\n",
            "ITERATION_NO.: 861 LOSS_Generator: 5.375748157501221 LOSS_Discriminator: 0.014672711491584778\n",
            "ITERATION_NO.: 862 LOSS_Generator: 5.090092658996582 LOSS_Discriminator: 0.22464404503504434\n",
            "ITERATION_NO.: 863 LOSS_Generator: 4.406862258911133 LOSS_Discriminator: 0.030520334839820862\n",
            "ITERATION_NO.: 864 LOSS_Generator: 4.500155448913574 LOSS_Discriminator: 0.0633259117603302\n",
            "ITERATION_NO.: 865 LOSS_Generator: 4.891522407531738 LOSS_Discriminator: 0.038204473753770195\n",
            "ITERATION_NO.: 866 LOSS_Generator: 5.503532886505127 LOSS_Discriminator: 0.14461042483647665\n",
            "ITERATION_NO.: 867 LOSS_Generator: 5.245610237121582 LOSS_Discriminator: 0.12016923228899638\n",
            "ITERATION_NO.: 868 LOSS_Generator: 5.810049057006836 LOSS_Discriminator: 0.032105907797813416\n",
            "ITERATION_NO.: 869 LOSS_Generator: 5.915484428405762 LOSS_Discriminator: 0.008347834149996439\n",
            "ITERATION_NO.: 870 LOSS_Generator: 6.079545974731445 LOSS_Discriminator: 0.08270941178003947\n",
            "ITERATION_NO.: 871 LOSS_Generator: 5.567801475524902 LOSS_Discriminator: 0.3337620496749878\n",
            "ITERATION_NO.: 872 LOSS_Generator: 4.506888389587402 LOSS_Discriminator: 0.023436362544695537\n",
            "ITERATION_NO.: 873 LOSS_Generator: 4.939189910888672 LOSS_Discriminator: 0.16353962818781534\n",
            "ITERATION_NO.: 874 LOSS_Generator: 3.765434503555298 LOSS_Discriminator: 0.24168707927068075\n",
            "ITERATION_NO.: 875 LOSS_Generator: 4.475459575653076 LOSS_Discriminator: 0.13845142722129822\n",
            "ITERATION_NO.: 876 LOSS_Generator: 5.419732093811035 LOSS_Discriminator: 0.040100847681363426\n",
            "ITERATION_NO.: 877 LOSS_Generator: 6.130531311035156 LOSS_Discriminator: 0.1877582867940267\n",
            "ITERATION_NO.: 878 LOSS_Generator: 4.985486030578613 LOSS_Discriminator: 0.29446736971537274\n",
            "ITERATION_NO.: 879 LOSS_Generator: 4.688379287719727 LOSS_Discriminator: 0.021681147317091625\n",
            "ITERATION_NO.: 880 LOSS_Generator: 4.297913551330566 LOSS_Discriminator: 0.07726167639096577\n",
            "ITERATION_NO.: 881 LOSS_Generator: 4.804722785949707 LOSS_Discriminator: 0.03297693779071172\n",
            "ITERATION_NO.: 882 LOSS_Generator: 5.6277174949646 LOSS_Discriminator: 0.010722740242878595\n",
            "ITERATION_NO.: 883 LOSS_Generator: 6.0010576248168945 LOSS_Discriminator: 0.020394037167231243\n",
            "ITERATION_NO.: 884 LOSS_Generator: 6.021329879760742 LOSS_Discriminator: 0.014989577233791351\n",
            "ITERATION_NO.: 885 LOSS_Generator: 6.160597801208496 LOSS_Discriminator: 0.0518052875995636\n",
            "ITERATION_NO.: 886 LOSS_Generator: 5.568604469299316 LOSS_Discriminator: 0.23862590392430624\n",
            "ITERATION_NO.: 887 LOSS_Generator: 4.523530960083008 LOSS_Discriminator: 0.018958987047274906\n",
            "ITERATION_NO.: 888 LOSS_Generator: 4.487242698669434 LOSS_Discriminator: 0.03814697265625\n",
            "ITERATION_NO.: 889 LOSS_Generator: 4.26492166519165 LOSS_Discriminator: 0.06160608927408854\n",
            "ITERATION_NO.: 890 LOSS_Generator: 4.460954666137695 LOSS_Discriminator: 0.03838274876276652\n",
            "ITERATION_NO.: 891 LOSS_Generator: 5.440379619598389 LOSS_Discriminator: 0.12073771158854167\n",
            "ITERATION_NO.: 892 LOSS_Generator: 5.653191566467285 LOSS_Discriminator: 0.013639640063047409\n",
            "ITERATION_NO.: 893 LOSS_Generator: 5.651080131530762 LOSS_Discriminator: 0.132711132367452\n",
            "ITERATION_NO.: 894 LOSS_Generator: 5.310975074768066 LOSS_Discriminator: 0.03401596347490946\n",
            "ITERATION_NO.: 895 LOSS_Generator: 5.104527473449707 LOSS_Discriminator: 0.19814270734786987\n",
            "ITERATION_NO.: 896 LOSS_Generator: 4.487069606781006 LOSS_Discriminator: 0.03674543152252833\n",
            "ITERATION_NO.: 897 LOSS_Generator: 4.465832233428955 LOSS_Discriminator: 0.056687464316686\n",
            "ITERATION_NO.: 898 LOSS_Generator: 5.252354621887207 LOSS_Discriminator: 0.037129610776901245\n",
            "ITERATION_NO.: 899 LOSS_Generator: 5.678330421447754 LOSS_Discriminator: 0.0176093727350235\n",
            "ITERATION_NO.: 900 LOSS_Generator: 6.6910552978515625 LOSS_Discriminator: 0.08550433317820232\n",
            "ITERATION_NO.: 901 LOSS_Generator: 6.570191383361816 LOSS_Discriminator: 0.24959975481033325\n",
            "ITERATION_NO.: 902 LOSS_Generator: 5.617819786071777 LOSS_Discriminator: 0.004646347835659981\n",
            "ITERATION_NO.: 903 LOSS_Generator: 5.522500038146973 LOSS_Discriminator: 0.043611268202463783\n",
            "ITERATION_NO.: 904 LOSS_Generator: 4.685691833496094 LOSS_Discriminator: 0.1658535103003184\n",
            "ITERATION_NO.: 905 LOSS_Generator: 4.080164432525635 LOSS_Discriminator: 0.20083999633789062\n",
            "ITERATION_NO.: 906 LOSS_Generator: 3.602623462677002 LOSS_Discriminator: 0.15821534395217896\n",
            "ITERATION_NO.: 907 LOSS_Generator: 4.9810285568237305 LOSS_Discriminator: 0.10696276028951009\n",
            "ITERATION_NO.: 908 LOSS_Generator: 5.991142749786377 LOSS_Discriminator: 0.032044763366381325\n",
            "ITERATION_NO.: 909 LOSS_Generator: 6.745528221130371 LOSS_Discriminator: 0.18506069978078207\n",
            "ITERATION_NO.: 910 LOSS_Generator: 5.855344772338867 LOSS_Discriminator: 0.15807839234670004\n",
            "ITERATION_NO.: 911 LOSS_Generator: 6.045867919921875 LOSS_Discriminator: 0.004899378245075543\n",
            "ITERATION_NO.: 912 LOSS_Generator: 5.500779151916504 LOSS_Discriminator: 0.1633891761302948\n",
            "ITERATION_NO.: 913 LOSS_Generator: 4.18841028213501 LOSS_Discriminator: 0.11604636907577515\n",
            "ITERATION_NO.: 914 LOSS_Generator: 3.4008119106292725 LOSS_Discriminator: 0.08901480833689372\n",
            "ITERATION_NO.: 915 LOSS_Generator: 4.503389358520508 LOSS_Discriminator: 0.14517128467559814\n",
            "ITERATION_NO.: 916 LOSS_Generator: 5.380132675170898 LOSS_Discriminator: 0.06059673925240835\n",
            "ITERATION_NO.: 917 LOSS_Generator: 6.601941108703613 LOSS_Discriminator: 0.12810768683751425\n",
            "ITERATION_NO.: 918 LOSS_Generator: 6.451343536376953 LOSS_Discriminator: 0.06243970990180969\n",
            "ITERATION_NO.: 919 LOSS_Generator: 5.996228218078613 LOSS_Discriminator: 0.05312931537628174\n",
            "ITERATION_NO.: 920 LOSS_Generator: 4.955683708190918 LOSS_Discriminator: 0.1861971616744995\n",
            "ITERATION_NO.: 921 LOSS_Generator: 4.136592864990234 LOSS_Discriminator: 0.09365891416867574\n",
            "ITERATION_NO.: 922 LOSS_Generator: 3.7988858222961426 LOSS_Discriminator: 0.04207791884740194\n",
            "ITERATION_NO.: 923 LOSS_Generator: 4.294076919555664 LOSS_Discriminator: 0.12008582552274068\n",
            "ITERATION_NO.: 924 LOSS_Generator: 5.161003589630127 LOSS_Discriminator: 0.059842189153035484\n",
            "ITERATION_NO.: 925 LOSS_Generator: 5.551022529602051 LOSS_Discriminator: 0.0939942995707194\n",
            "ITERATION_NO.: 926 LOSS_Generator: 5.917027950286865 LOSS_Discriminator: 0.09607855478922527\n",
            "ITERATION_NO.: 927 LOSS_Generator: 5.2051825523376465 LOSS_Discriminator: 0.010587949305772781\n",
            "ITERATION_NO.: 928 LOSS_Generator: 5.152437210083008 LOSS_Discriminator: 0.02613149086634318\n",
            "ITERATION_NO.: 929 LOSS_Generator: 5.212322235107422 LOSS_Discriminator: 0.07546526193618774\n",
            "ITERATION_NO.: 930 LOSS_Generator: 5.452956199645996 LOSS_Discriminator: 0.037468296786149345\n",
            "ITERATION_NO.: 931 LOSS_Generator: 5.159270286560059 LOSS_Discriminator: 0.04472887019316355\n",
            "ITERATION_NO.: 932 LOSS_Generator: 5.258024215698242 LOSS_Discriminator: 0.02638891339302063\n",
            "ITERATION_NO.: 933 LOSS_Generator: 5.907750129699707 LOSS_Discriminator: 0.10729883114496867\n",
            "ITERATION_NO.: 934 LOSS_Generator: 5.444799900054932 LOSS_Discriminator: 0.12397238612174988\n",
            "ITERATION_NO.: 935 LOSS_Generator: 4.754059791564941 LOSS_Discriminator: 0.040369694431622825\n",
            "ITERATION_NO.: 936 LOSS_Generator: 4.7259087562561035 LOSS_Discriminator: 0.11590329806009929\n",
            "ITERATION_NO.: 937 LOSS_Generator: 4.1407270431518555 LOSS_Discriminator: 0.08798694610595703\n",
            "ITERATION_NO.: 938 LOSS_Generator: 5.492676734924316 LOSS_Discriminator: 0.1319749355316162\n",
            "ITERATION_NO.: 939 LOSS_Generator: 6.747145652770996 LOSS_Discriminator: 0.03554003685712814\n",
            "ITERATION_NO.: 940 LOSS_Generator: 7.020179748535156 LOSS_Discriminator: 0.014284523824850718\n",
            "ITERATION_NO.: 941 LOSS_Generator: 7.747142791748047 LOSS_Discriminator: 0.0016668629832565784\n",
            "ITERATION_NO.: 942 LOSS_Generator: 7.324934959411621 LOSS_Discriminator: 0.29665273427963257\n",
            "ITERATION_NO.: 943 LOSS_Generator: 6.596513748168945 LOSS_Discriminator: 0.02153987189133962\n",
            "ITERATION_NO.: 944 LOSS_Generator: 5.407827377319336 LOSS_Discriminator: 0.3337024450302124\n",
            "ITERATION_NO.: 945 LOSS_Generator: 3.94162917137146 LOSS_Discriminator: 0.03589942306280136\n",
            "ITERATION_NO.: 946 LOSS_Generator: 3.233931541442871 LOSS_Discriminator: 0.06364588936169942\n",
            "ITERATION_NO.: 947 LOSS_Generator: 4.557019233703613 LOSS_Discriminator: 0.2061858574549357\n",
            "ITERATION_NO.: 948 LOSS_Generator: 6.1272430419921875 LOSS_Discriminator: 0.0487199475367864\n",
            "ITERATION_NO.: 949 LOSS_Generator: 7.013230323791504 LOSS_Discriminator: 0.0392958919207255\n",
            "ITERATION_NO.: 950 LOSS_Generator: 6.796717643737793 LOSS_Discriminator: 0.04154728104670843\n",
            "ITERATION_NO.: 951 LOSS_Generator: 6.865150451660156 LOSS_Discriminator: 0.1613208850224813\n",
            "ITERATION_NO.: 952 LOSS_Generator: 6.811245918273926 LOSS_Discriminator: 0.011404864490032196\n",
            "ITERATION_NO.: 953 LOSS_Generator: 6.31852912902832 LOSS_Discriminator: 0.14173108339309692\n",
            "ITERATION_NO.: 954 LOSS_Generator: 5.807790756225586 LOSS_Discriminator: 0.017281436671813328\n",
            "ITERATION_NO.: 955 LOSS_Generator: 5.091856956481934 LOSS_Discriminator: 0.022077371676762898\n",
            "ITERATION_NO.: 956 LOSS_Generator: 5.305751800537109 LOSS_Discriminator: 0.020910354952017467\n",
            "ITERATION_NO.: 957 LOSS_Generator: 5.149903774261475 LOSS_Discriminator: 0.2355824907620748\n",
            "ITERATION_NO.: 958 LOSS_Generator: 4.6580810546875 LOSS_Discriminator: 0.02818232278029124\n",
            "ITERATION_NO.: 959 LOSS_Generator: 4.970793724060059 LOSS_Discriminator: 0.01721648747722308\n",
            "ITERATION_NO.: 960 LOSS_Generator: 5.385507583618164 LOSS_Discriminator: 0.13148963451385498\n",
            "ITERATION_NO.: 961 LOSS_Generator: 5.420654773712158 LOSS_Discriminator: 0.04106822609901428\n",
            "ITERATION_NO.: 962 LOSS_Generator: 5.565893173217773 LOSS_Discriminator: 0.02090114603439967\n",
            "ITERATION_NO.: 963 LOSS_Generator: 5.9697442054748535 LOSS_Discriminator: 0.011443431178728739\n",
            "ITERATION_NO.: 964 LOSS_Generator: 6.042203903198242 LOSS_Discriminator: 0.009017940610647202\n",
            "ITERATION_NO.: 965 LOSS_Generator: 6.040402412414551 LOSS_Discriminator: 0.00811000851293405\n",
            "ITERATION_NO.: 966 LOSS_Generator: 6.10987663269043 LOSS_Discriminator: 0.31418641408284503\n",
            "ITERATION_NO.: 967 LOSS_Generator: 5.145732879638672 LOSS_Discriminator: 0.03054187446832657\n",
            "ITERATION_NO.: 968 LOSS_Generator: 5.079257965087891 LOSS_Discriminator: 0.10143800576527913\n",
            "ITERATION_NO.: 969 LOSS_Generator: 4.591910362243652 LOSS_Discriminator: 0.16826045513153076\n",
            "ITERATION_NO.: 970 LOSS_Generator: 4.2144999504089355 LOSS_Discriminator: 0.22469305992126465\n",
            "ITERATION_NO.: 971 LOSS_Generator: 3.5420925617218018 LOSS_Discriminator: 0.038567982614040375\n",
            "ITERATION_NO.: 972 LOSS_Generator: 5.079619407653809 LOSS_Discriminator: 0.07019068797429402\n",
            "ITERATION_NO.: 973 LOSS_Generator: 5.452441215515137 LOSS_Discriminator: 0.19709142049153647\n",
            "ITERATION_NO.: 974 LOSS_Generator: 5.462587833404541 LOSS_Discriminator: 0.24194820721944174\n",
            "ITERATION_NO.: 975 LOSS_Generator: 5.021834373474121 LOSS_Discriminator: 0.11855411529541016\n",
            "ITERATION_NO.: 976 LOSS_Generator: 4.466183662414551 LOSS_Discriminator: 0.052538072069485985\n",
            "ITERATION_NO.: 977 LOSS_Generator: 4.6590576171875 LOSS_Discriminator: 0.05864688754081726\n",
            "ITERATION_NO.: 978 LOSS_Generator: 5.076661109924316 LOSS_Discriminator: 0.03625537951787313\n",
            "ITERATION_NO.: 979 LOSS_Generator: 5.681403160095215 LOSS_Discriminator: 0.15044701099395752\n",
            "ITERATION_NO.: 980 LOSS_Generator: 6.164139747619629 LOSS_Discriminator: 0.00857506754497687\n",
            "ITERATION_NO.: 981 LOSS_Generator: 5.916592121124268 LOSS_Discriminator: 0.049819424748420715\n",
            "ITERATION_NO.: 982 LOSS_Generator: 5.837474822998047 LOSS_Discriminator: 0.009967312837640444\n",
            "ITERATION_NO.: 983 LOSS_Generator: 5.897469997406006 LOSS_Discriminator: 0.23619933923085532\n",
            "ITERATION_NO.: 984 LOSS_Generator: 5.781512260437012 LOSS_Discriminator: 0.04180425405502319\n",
            "ITERATION_NO.: 985 LOSS_Generator: 5.701786994934082 LOSS_Discriminator: 0.01425584281484286\n",
            "ITERATION_NO.: 986 LOSS_Generator: 5.557977676391602 LOSS_Discriminator: 0.18413996696472168\n",
            "ITERATION_NO.: 987 LOSS_Generator: 4.5306267738342285 LOSS_Discriminator: 0.059488063057263695\n",
            "ITERATION_NO.: 988 LOSS_Generator: 4.558213233947754 LOSS_Discriminator: 0.16965254147847494\n",
            "ITERATION_NO.: 989 LOSS_Generator: 4.54421329498291 LOSS_Discriminator: 0.1161895493666331\n",
            "ITERATION_NO.: 990 LOSS_Generator: 4.844121932983398 LOSS_Discriminator: 0.09876911838849385\n",
            "ITERATION_NO.: 991 LOSS_Generator: 6.084980010986328 LOSS_Discriminator: 0.030766936639944713\n",
            "ITERATION_NO.: 992 LOSS_Generator: 6.038043975830078 LOSS_Discriminator: 0.20471556981404623\n",
            "ITERATION_NO.: 993 LOSS_Generator: 6.164642333984375 LOSS_Discriminator: 0.0030988783886035285\n",
            "ITERATION_NO.: 994 LOSS_Generator: 6.78189754486084 LOSS_Discriminator: 0.0788113276163737\n",
            "ITERATION_NO.: 995 LOSS_Generator: 6.237509727478027 LOSS_Discriminator: 0.18283905585606894\n",
            "ITERATION_NO.: 996 LOSS_Generator: 5.842272758483887 LOSS_Discriminator: 0.19562149047851562\n",
            "ITERATION_NO.: 997 LOSS_Generator: 5.291891098022461 LOSS_Discriminator: 0.008914745102326075\n",
            "ITERATION_NO.: 998 LOSS_Generator: 5.153746604919434 LOSS_Discriminator: 0.07102926572163899\n",
            "ITERATION_NO.: 999 LOSS_Generator: 4.709312438964844 LOSS_Discriminator: 0.28111449877421063\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 4.403887748718262 LOSS_Discriminator: 0.04905634621779124\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 4.665440082550049 LOSS_Discriminator: 0.09855649868647258\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 4.650627613067627 LOSS_Discriminator: 0.16944760084152222\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 4.043202877044678 LOSS_Discriminator: 0.13946841160456339\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 4.962014198303223 LOSS_Discriminator: 0.16798752546310425\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 4.973456859588623 LOSS_Discriminator: 0.037671307722727455\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 4.699944496154785 LOSS_Discriminator: 0.08277057607968648\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 5.10008430480957 LOSS_Discriminator: 0.05731343229611715\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 4.878283500671387 LOSS_Discriminator: 0.3521835406621297\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 4.1951799392700195 LOSS_Discriminator: 0.2314610481262207\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 4.167847633361816 LOSS_Discriminator: 0.1382510264714559\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 3.9051733016967773 LOSS_Discriminator: 0.09747836987177531\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 4.848170280456543 LOSS_Discriminator: 0.06222555538018545\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 5.477321147918701 LOSS_Discriminator: 0.01758602385719617\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 5.898045063018799 LOSS_Discriminator: 0.009573365251223246\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 6.036050796508789 LOSS_Discriminator: 0.004879954581459363\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 6.27662467956543 LOSS_Discriminator: 0.25473296642303467\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 5.99284553527832 LOSS_Discriminator: 0.10654014348983765\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 5.2507195472717285 LOSS_Discriminator: 0.006184566145141919\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 5.6139984130859375 LOSS_Discriminator: 0.02121041218439738\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 4.941727638244629 LOSS_Discriminator: 0.016020039717356365\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 5.333235263824463 LOSS_Discriminator: 0.028932360311349232\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 5.087015151977539 LOSS_Discriminator: 0.2221723993619283\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 4.82878303527832 LOSS_Discriminator: 0.019855571289857227\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 5.0184736251831055 LOSS_Discriminator: 0.07431936264038086\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 4.573008060455322 LOSS_Discriminator: 0.04523133238156637\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 4.649637222290039 LOSS_Discriminator: 0.03015364209810893\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 5.000857353210449 LOSS_Discriminator: 0.13636638720830283\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 5.045721054077148 LOSS_Discriminator: 0.028788278500239056\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 4.469604969024658 LOSS_Discriminator: 0.08106522758801778\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 4.570825099945068 LOSS_Discriminator: 0.04826285441716512\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 5.157364845275879 LOSS_Discriminator: 0.0719490647315979\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 5.315018653869629 LOSS_Discriminator: 0.0808100700378418\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 5.250983238220215 LOSS_Discriminator: 0.11898735165596008\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 4.767538070678711 LOSS_Discriminator: 0.10002857446670532\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 4.130827903747559 LOSS_Discriminator: 0.05463439226150513\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 3.3309683799743652 LOSS_Discriminator: 0.1748272975285848\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 3.6695117950439453 LOSS_Discriminator: 0.1398332118988037\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 5.066707611083984 LOSS_Discriminator: 0.1139017144838969\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 5.935548782348633 LOSS_Discriminator: 0.1739891767501831\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 5.89782190322876 LOSS_Discriminator: 0.003885250228146712\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 6.6109442710876465 LOSS_Discriminator: 0.0026672432820002237\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 6.576082229614258 LOSS_Discriminator: 0.06108607848485311\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 6.3309102058410645 LOSS_Discriminator: 0.0034952045728762946\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 6.188539981842041 LOSS_Discriminator: 0.16384893655776978\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 5.123296737670898 LOSS_Discriminator: 0.24632767836252847\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 4.264466762542725 LOSS_Discriminator: 0.030627578496932983\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 4.208786964416504 LOSS_Discriminator: 0.12071568767229716\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 3.593970537185669 LOSS_Discriminator: 0.08951371908187866\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 4.980518341064453 LOSS_Discriminator: 0.09521680076917012\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 5.582273483276367 LOSS_Discriminator: 0.0901179810365041\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 6.499878883361816 LOSS_Discriminator: 0.03375977774461111\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 6.474153995513916 LOSS_Discriminator: 0.11552706360816956\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 5.990292549133301 LOSS_Discriminator: 0.3630307912826538\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 4.591495513916016 LOSS_Discriminator: 0.02105286220709483\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 4.025555610656738 LOSS_Discriminator: 0.0708017349243164\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 3.090756893157959 LOSS_Discriminator: 0.07707415024439494\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 3.7388229370117188 LOSS_Discriminator: 0.0696793794631958\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 4.60944938659668 LOSS_Discriminator: 0.0728307416041692\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 5.108299255371094 LOSS_Discriminator: 0.07252870500087738\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 5.703662872314453 LOSS_Discriminator: 0.019412060578664143\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 6.011074066162109 LOSS_Discriminator: 0.05263075232505798\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 5.899504661560059 LOSS_Discriminator: 0.04112513115008672\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 5.223931312561035 LOSS_Discriminator: 0.04897906879583994\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 5.6122894287109375 LOSS_Discriminator: 0.016430336982011795\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 5.290187358856201 LOSS_Discriminator: 0.023127436637878418\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 5.183158874511719 LOSS_Discriminator: 0.06862226625283559\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 5.556388854980469 LOSS_Discriminator: 0.008739228049914042\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 5.6065216064453125 LOSS_Discriminator: 0.14248510201772055\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 4.83712100982666 LOSS_Discriminator: 0.14487538735071817\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.5905656814575195 LOSS_Discriminator: 0.045849944154421486\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 4.976471900939941 LOSS_Discriminator: 0.060801794131596885\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 4.910813808441162 LOSS_Discriminator: 0.019442114979028702\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 5.123701572418213 LOSS_Discriminator: 0.06692430873711903\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 5.276971340179443 LOSS_Discriminator: 0.08056601881980896\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 4.807667255401611 LOSS_Discriminator: 0.023711641629536945\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 4.617626667022705 LOSS_Discriminator: 0.1004085640112559\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 4.692947864532471 LOSS_Discriminator: 0.01914353296160698\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 4.95806884765625 LOSS_Discriminator: 0.08879069487253825\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 4.607479572296143 LOSS_Discriminator: 0.0446014404296875\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 4.91671085357666 LOSS_Discriminator: 0.115663876136144\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 4.996521949768066 LOSS_Discriminator: 0.023780956864356995\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 4.827676296234131 LOSS_Discriminator: 0.13479037086168924\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 5.026243209838867 LOSS_Discriminator: 0.05044975380102793\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 4.813399314880371 LOSS_Discriminator: 0.05247333645820618\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 5.141313076019287 LOSS_Discriminator: 0.02084333449602127\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 5.0463972091674805 LOSS_Discriminator: 0.17138230800628662\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 4.320446968078613 LOSS_Discriminator: 0.031370945274829865\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 4.788314342498779 LOSS_Discriminator: 0.06065597136815389\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 5.482943058013916 LOSS_Discriminator: 0.05007482568422953\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 4.516014099121094 LOSS_Discriminator: 0.1801348328590393\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 3.876542568206787 LOSS_Discriminator: 0.08508544166882832\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 4.136388778686523 LOSS_Discriminator: 0.10594309369723003\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 5.1883745193481445 LOSS_Discriminator: 0.17751910289128622\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 5.875134468078613 LOSS_Discriminator: 0.014154760787884394\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 5.748391151428223 LOSS_Discriminator: 0.07310907046000163\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 6.186687469482422 LOSS_Discriminator: 0.05371692279974619\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 6.031408786773682 LOSS_Discriminator: 0.009683920070528984\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 5.83617639541626 LOSS_Discriminator: 0.041677325963974\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 5.367271900177002 LOSS_Discriminator: 0.10800238450368245\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 5.00775671005249 LOSS_Discriminator: 0.03473751743634542\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 4.793073654174805 LOSS_Discriminator: 0.013427883386611938\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 5.3054022789001465 LOSS_Discriminator: 0.03774146238962809\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 5.585919380187988 LOSS_Discriminator: 0.04278759161631266\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 4.927127838134766 LOSS_Discriminator: 0.012033853679895401\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 6.020584583282471 LOSS_Discriminator: 0.037207151452700295\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 5.979745864868164 LOSS_Discriminator: 0.04785921176274618\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 5.715819358825684 LOSS_Discriminator: 0.04881710807482401\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 5.598757743835449 LOSS_Discriminator: 0.06633734206358592\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 5.547720432281494 LOSS_Discriminator: 0.02921821673711141\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 5.453205108642578 LOSS_Discriminator: 0.02117071549097697\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 5.3363752365112305 LOSS_Discriminator: 0.3011703888575236\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 4.126833915710449 LOSS_Discriminator: 0.11278973023096721\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 4.362242698669434 LOSS_Discriminator: 0.08308107654253642\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 4.991534233093262 LOSS_Discriminator: 0.08247692386309306\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 5.36894416809082 LOSS_Discriminator: 0.15148345629374185\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 5.500926494598389 LOSS_Discriminator: 0.17221049467722574\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.412960529327393 LOSS_Discriminator: 0.05282346407572428\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 5.432311058044434 LOSS_Discriminator: 0.03319220244884491\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 4.603872299194336 LOSS_Discriminator: 0.016717615226904552\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 5.269080638885498 LOSS_Discriminator: 0.02800757686297099\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 5.624892711639404 LOSS_Discriminator: 0.1816397507985433\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 4.941461563110352 LOSS_Discriminator: 0.03456549346446991\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 4.560478210449219 LOSS_Discriminator: 0.19775189956029257\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 4.5747880935668945 LOSS_Discriminator: 0.13456304868062338\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 5.388022422790527 LOSS_Discriminator: 0.0759067436059316\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 6.0832037925720215 LOSS_Discriminator: 0.012542850027481714\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 6.566573143005371 LOSS_Discriminator: 0.20284642775853476\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 5.678379058837891 LOSS_Discriminator: 0.05004812777042389\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 6.657680511474609 LOSS_Discriminator: 0.013463199138641357\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 6.505373954772949 LOSS_Discriminator: 0.09427639842033386\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 5.661163330078125 LOSS_Discriminator: 0.17089607318242392\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 5.106086730957031 LOSS_Discriminator: 0.19877413908640543\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 5.165904998779297 LOSS_Discriminator: 0.24779580036799112\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 4.442783355712891 LOSS_Discriminator: 0.08944226304690044\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 5.110777854919434 LOSS_Discriminator: 0.046944176157315574\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 5.538102149963379 LOSS_Discriminator: 0.025944848855336506\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 5.414891242980957 LOSS_Discriminator: 0.3435820738474528\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 4.598466873168945 LOSS_Discriminator: 0.045689533154169716\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 3.9333248138427734 LOSS_Discriminator: 0.0919945240020752\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 4.263118267059326 LOSS_Discriminator: 0.1045982837677002\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 5.152254581451416 LOSS_Discriminator: 0.07602295776208241\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 5.588791847229004 LOSS_Discriminator: 0.048199315865834556\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 5.86561918258667 LOSS_Discriminator: 0.008919411028424898\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 6.512746810913086 LOSS_Discriminator: 0.005704193686445554\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 7.000241756439209 LOSS_Discriminator: 0.11365320285161336\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 6.735851287841797 LOSS_Discriminator: 0.009150746588905653\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 6.464509963989258 LOSS_Discriminator: 0.007636964321136475\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 6.039060592651367 LOSS_Discriminator: 0.10278025269508362\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 5.89292049407959 LOSS_Discriminator: 0.08640070756276448\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 4.938765525817871 LOSS_Discriminator: 0.0342865064740181\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 4.302746772766113 LOSS_Discriminator: 0.026125426093737285\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 4.558601379394531 LOSS_Discriminator: 0.12560429175694784\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 4.30573844909668 LOSS_Discriminator: 0.09978961944580078\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 4.779439926147461 LOSS_Discriminator: 0.08299985527992249\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 5.446747779846191 LOSS_Discriminator: 0.02048341433207194\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 5.7277727127075195 LOSS_Discriminator: 0.0603037973244985\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 5.6776933670043945 LOSS_Discriminator: 0.1495309273401896\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 5.036218643188477 LOSS_Discriminator: 0.06271212299664815\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 5.350052833557129 LOSS_Discriminator: 0.011172267297903696\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 4.6871538162231445 LOSS_Discriminator: 0.14467538396517435\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 4.2790327072143555 LOSS_Discriminator: 0.038015288611253105\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 4.520166873931885 LOSS_Discriminator: 0.044199506441752114\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 4.747488498687744 LOSS_Discriminator: 0.021150459845860798\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 5.897373199462891 LOSS_Discriminator: 0.11928069591522217\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 5.452184677124023 LOSS_Discriminator: 0.07389530539512634\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 4.897851467132568 LOSS_Discriminator: 0.18968447049458823\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 4.273181438446045 LOSS_Discriminator: 0.0782413234313329\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 4.366656303405762 LOSS_Discriminator: 0.045058806737264\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 4.917385578155518 LOSS_Discriminator: 0.11489246288935344\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 4.932729721069336 LOSS_Discriminator: 0.11994139353434245\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 5.590206146240234 LOSS_Discriminator: 0.03340437759955724\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 5.64285945892334 LOSS_Discriminator: 0.05939655005931854\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 6.028256416320801 LOSS_Discriminator: 0.013707183301448822\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 6.278071880340576 LOSS_Discriminator: 0.007958146433035532\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 6.324138641357422 LOSS_Discriminator: 0.12510976195335388\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 5.894649505615234 LOSS_Discriminator: 0.03935815393924713\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 4.850180625915527 LOSS_Discriminator: 0.09541625777880351\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 4.544349193572998 LOSS_Discriminator: 0.07802136739095052\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 4.524242401123047 LOSS_Discriminator: 0.044719417889912925\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 5.1226396560668945 LOSS_Discriminator: 0.11133986711502075\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 4.963870048522949 LOSS_Discriminator: 0.12983042001724243\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 5.523929119110107 LOSS_Discriminator: 0.03675472239653269\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 5.757867813110352 LOSS_Discriminator: 0.12275637189547221\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 5.414201736450195 LOSS_Discriminator: 0.15308918555577597\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 4.970943450927734 LOSS_Discriminator: 0.014330392082532247\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 4.444038391113281 LOSS_Discriminator: 0.06105222304662069\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 4.028443336486816 LOSS_Discriminator: 0.20734639962514242\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 3.9422435760498047 LOSS_Discriminator: 0.13481887181599936\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 4.984490394592285 LOSS_Discriminator: 0.06398997207482655\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 5.918036460876465 LOSS_Discriminator: 0.01812918980916341\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 6.379183769226074 LOSS_Discriminator: 0.03320304056008657\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 6.848690032958984 LOSS_Discriminator: 0.2730027437210083\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 5.459883689880371 LOSS_Discriminator: 0.12245245774586995\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 4.441643714904785 LOSS_Discriminator: 0.12137478590011597\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 3.529318332672119 LOSS_Discriminator: 0.08881235122680664\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 4.198339939117432 LOSS_Discriminator: 0.16186965505282083\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 5.525154113769531 LOSS_Discriminator: 0.059899523854255676\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 6.302953720092773 LOSS_Discriminator: 0.12689709663391113\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 6.417762279510498 LOSS_Discriminator: 0.004942018849154313\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 6.627817153930664 LOSS_Discriminator: 0.0448934535185496\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 6.139903545379639 LOSS_Discriminator: 0.007924914360046387\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 5.923462867736816 LOSS_Discriminator: 0.18349548180898032\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 5.346292495727539 LOSS_Discriminator: 0.07064733902613322\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 3.607978343963623 LOSS_Discriminator: 0.13922404249509177\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 3.6124258041381836 LOSS_Discriminator: 0.22973936796188354\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 4.46585750579834 LOSS_Discriminator: 0.22737749417622885\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 5.200671195983887 LOSS_Discriminator: 0.018137171864509583\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 5.698220252990723 LOSS_Discriminator: 0.04003484050432841\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 5.5782670974731445 LOSS_Discriminator: 0.26361111799875897\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 4.605169296264648 LOSS_Discriminator: 0.1246643861134847\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 4.23076868057251 LOSS_Discriminator: 0.04633528490861257\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 4.443332672119141 LOSS_Discriminator: 0.13248095909754434\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 4.146181106567383 LOSS_Discriminator: 0.12769430875778198\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 5.1995038986206055 LOSS_Discriminator: 0.04530501365661621\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 5.788113594055176 LOSS_Discriminator: 0.015309243152538935\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 6.009762763977051 LOSS_Discriminator: 0.03795639177163442\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 6.435878276824951 LOSS_Discriminator: 0.12937636176745096\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 5.7279462814331055 LOSS_Discriminator: 0.08137994011243184\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 4.775947093963623 LOSS_Discriminator: 0.1565748155117035\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 4.1991119384765625 LOSS_Discriminator: 0.06044730544090271\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 3.6831212043762207 LOSS_Discriminator: 0.11233144005139668\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 4.238432884216309 LOSS_Discriminator: 0.09266673525174458\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 5.915390968322754 LOSS_Discriminator: 0.13385571042696634\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 6.6739349365234375 LOSS_Discriminator: 0.01802024369438489\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 6.958414077758789 LOSS_Discriminator: 0.14419316252072653\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 7.283975124359131 LOSS_Discriminator: 0.14024943113327026\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 6.891791343688965 LOSS_Discriminator: 0.04473714033762614\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 6.2728376388549805 LOSS_Discriminator: 0.06262083848317464\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 5.264675140380859 LOSS_Discriminator: 0.0880261758963267\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 4.2028608322143555 LOSS_Discriminator: 0.062435925006866455\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 3.3761377334594727 LOSS_Discriminator: 0.07734533150990804\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 4.0055251121521 LOSS_Discriminator: 0.19699587424596152\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 5.05216646194458 LOSS_Discriminator: 0.048780535658200584\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 5.541140556335449 LOSS_Discriminator: 0.13109113772710165\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 5.020381927490234 LOSS_Discriminator: 0.35006097952524823\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 4.8857316970825195 LOSS_Discriminator: 0.0550915797551473\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 3.8249716758728027 LOSS_Discriminator: 0.06283762057622273\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 4.798055648803711 LOSS_Discriminator: 0.1556085447470347\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 5.245234489440918 LOSS_Discriminator: 0.15536540746688843\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 5.744302749633789 LOSS_Discriminator: 0.2839975357055664\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 5.720005035400391 LOSS_Discriminator: 0.010725957651933035\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 5.634432792663574 LOSS_Discriminator: 0.057574848333994545\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 4.40207576751709 LOSS_Discriminator: 0.17927245299021402\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 3.4032585620880127 LOSS_Discriminator: 0.06318467855453491\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 4.2877655029296875 LOSS_Discriminator: 0.09638240933418274\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 4.891759872436523 LOSS_Discriminator: 0.058230623602867126\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 5.637495994567871 LOSS_Discriminator: 0.09747074047724406\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 5.31842565536499 LOSS_Discriminator: 0.24628718694051108\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 5.7443742752075195 LOSS_Discriminator: 0.2015624245007833\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 4.465919017791748 LOSS_Discriminator: 0.08100004990895589\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 4.090597152709961 LOSS_Discriminator: 0.035003269712130226\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 4.241048336029053 LOSS_Discriminator: 0.07028985023498535\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 4.509764671325684 LOSS_Discriminator: 0.13346774379412332\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 5.387587070465088 LOSS_Discriminator: 0.06207433342933655\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 5.829618453979492 LOSS_Discriminator: 0.09042547146479289\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 5.557810306549072 LOSS_Discriminator: 0.09294599294662476\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 5.840364933013916 LOSS_Discriminator: 0.011838709314664205\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 5.4372358322143555 LOSS_Discriminator: 0.07155255476633708\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.257795810699463 LOSS_Discriminator: 0.11098942160606384\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 4.162387847900391 LOSS_Discriminator: 0.12057096759478252\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 4.354077339172363 LOSS_Discriminator: 0.16146457195281982\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 4.859681606292725 LOSS_Discriminator: 0.044136504332224526\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 5.119235992431641 LOSS_Discriminator: 0.028789947430292766\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 5.598445415496826 LOSS_Discriminator: 0.15999348958333334\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 5.630255699157715 LOSS_Discriminator: 0.013889424502849579\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 5.568411827087402 LOSS_Discriminator: 0.013429385920365652\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 6.509696006774902 LOSS_Discriminator: 0.023335968454678852\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 5.623190402984619 LOSS_Discriminator: 0.045212830106417336\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 5.1975202560424805 LOSS_Discriminator: 0.1281216343243917\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 4.4223785400390625 LOSS_Discriminator: 0.09693099061648051\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 4.0674591064453125 LOSS_Discriminator: 0.09358382225036621\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 4.77840518951416 LOSS_Discriminator: 0.08361853162447612\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 6.303905487060547 LOSS_Discriminator: 0.11362993717193604\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 5.8546552658081055 LOSS_Discriminator: 0.06522262593110402\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 5.967220306396484 LOSS_Discriminator: 0.012895387907822927\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.765358924865723 LOSS_Discriminator: 0.0075268882016340894\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 5.252170085906982 LOSS_Discriminator: 0.17907321453094482\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 4.582099914550781 LOSS_Discriminator: 0.15240647395451865\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 4.043245315551758 LOSS_Discriminator: 0.25681473811467487\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 4.50924825668335 LOSS_Discriminator: 0.23301599423090616\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 5.587611675262451 LOSS_Discriminator: 0.03966640184322993\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 5.7133588790893555 LOSS_Discriminator: 0.10867011547088623\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 5.409801483154297 LOSS_Discriminator: 0.22761565446853638\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 4.884429931640625 LOSS_Discriminator: 0.017003475377957027\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 5.122049808502197 LOSS_Discriminator: 0.08947974443435669\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 4.887051582336426 LOSS_Discriminator: 0.08352097868919373\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 5.859121322631836 LOSS_Discriminator: 0.019781362265348434\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 5.530667781829834 LOSS_Discriminator: 0.24674471219380698\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 5.219930171966553 LOSS_Discriminator: 0.013069681823253632\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 5.3372273445129395 LOSS_Discriminator: 0.05333976944287618\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 5.290164947509766 LOSS_Discriminator: 0.028484488526980083\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 4.7048773765563965 LOSS_Discriminator: 0.13517379760742188\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 5.021460056304932 LOSS_Discriminator: 0.018371589481830597\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 5.179744243621826 LOSS_Discriminator: 0.020213909447193146\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 4.6404218673706055 LOSS_Discriminator: 0.17013380924860635\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 4.546808242797852 LOSS_Discriminator: 0.06203421950340271\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 4.4251885414123535 LOSS_Discriminator: 0.030139098564783733\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 4.7088541984558105 LOSS_Discriminator: 0.21721577644348145\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 4.896770477294922 LOSS_Discriminator: 0.04235084354877472\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 4.837414741516113 LOSS_Discriminator: 0.031058770914872486\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 5.434670448303223 LOSS_Discriminator: 0.019420549273490906\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 6.149933338165283 LOSS_Discriminator: 0.05687368909517924\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 6.383037567138672 LOSS_Discriminator: 0.007633055870731671\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 5.3102312088012695 LOSS_Discriminator: 0.11953214804331462\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 5.168644905090332 LOSS_Discriminator: 0.10557983318964641\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 3.80783748626709 LOSS_Discriminator: 0.19012451171875\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 4.20025634765625 LOSS_Discriminator: 0.0631868988275528\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 4.522565841674805 LOSS_Discriminator: 0.07004825274149577\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 4.906307220458984 LOSS_Discriminator: 0.08532696962356567\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 5.171990871429443 LOSS_Discriminator: 0.30253060658772785\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 4.446915626525879 LOSS_Discriminator: 0.16564900676409403\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 3.510049819946289 LOSS_Discriminator: 0.18853068351745605\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 4.044840335845947 LOSS_Discriminator: 0.18122732639312744\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 4.4166154861450195 LOSS_Discriminator: 0.05540864666302999\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 5.461180686950684 LOSS_Discriminator: 0.0411773274342219\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 6.169612884521484 LOSS_Discriminator: 0.014578203360239664\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 6.48000431060791 LOSS_Discriminator: 0.1303642193476359\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 6.836960792541504 LOSS_Discriminator: 0.0832793116569519\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 6.0397725105285645 LOSS_Discriminator: 0.01423241818944613\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 5.459496021270752 LOSS_Discriminator: 0.050596763690312706\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 4.999326229095459 LOSS_Discriminator: 0.01594778150320053\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 5.331463813781738 LOSS_Discriminator: 0.060437639554341636\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 4.529292106628418 LOSS_Discriminator: 0.10999232530593872\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 5.773516654968262 LOSS_Discriminator: 0.041005988915761314\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 5.0596418380737305 LOSS_Discriminator: 0.14451871315638223\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 5.2007246017456055 LOSS_Discriminator: 0.10618574420611064\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 4.633708953857422 LOSS_Discriminator: 0.13420657316843668\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 4.988945960998535 LOSS_Discriminator: 0.040565689404805504\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 5.373288154602051 LOSS_Discriminator: 0.11330140630404155\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 5.383884429931641 LOSS_Discriminator: 0.1058886448542277\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 5.181142807006836 LOSS_Discriminator: 0.10160791873931885\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 6.017409324645996 LOSS_Discriminator: 0.12113587061564128\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 4.897221565246582 LOSS_Discriminator: 0.06708964208761851\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 4.858114719390869 LOSS_Discriminator: 0.06111958622932434\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 4.896914482116699 LOSS_Discriminator: 0.08223551511764526\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 4.848231315612793 LOSS_Discriminator: 0.15713408589363098\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 5.505209445953369 LOSS_Discriminator: 0.1538624962170919\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 5.5032548904418945 LOSS_Discriminator: 0.02199197808901469\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 6.0089898109436035 LOSS_Discriminator: 0.04543294012546539\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 5.671085357666016 LOSS_Discriminator: 0.1378345489501953\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 5.872800827026367 LOSS_Discriminator: 0.02073702837030093\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 5.468334197998047 LOSS_Discriminator: 0.024080802996953327\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 5.82492733001709 LOSS_Discriminator: 0.023456203440825146\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 6.159671783447266 LOSS_Discriminator: 0.006927868972222011\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 6.69624137878418 LOSS_Discriminator: 0.14930258194605509\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 6.270955562591553 LOSS_Discriminator: 0.10836288332939148\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 5.639594554901123 LOSS_Discriminator: 0.030925194422403973\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 5.248941421508789 LOSS_Discriminator: 0.036617378393809\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 5.539193153381348 LOSS_Discriminator: 0.03057944526274999\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 6.037771224975586 LOSS_Discriminator: 0.06024608016014099\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 6.218881130218506 LOSS_Discriminator: 0.007541617999474208\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 5.843710422515869 LOSS_Discriminator: 0.20587694644927979\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 4.791989326477051 LOSS_Discriminator: 0.048079212506612144\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 5.157687187194824 LOSS_Discriminator: 0.03355829914410909\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 4.562837600708008 LOSS_Discriminator: 0.1097779671351115\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.732896327972412 LOSS_Discriminator: 0.17038065195083618\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 5.002558708190918 LOSS_Discriminator: 0.07364079852898915\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 5.405521392822266 LOSS_Discriminator: 0.11349473396937053\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 5.568140506744385 LOSS_Discriminator: 0.05860138932863871\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 6.020175457000732 LOSS_Discriminator: 0.08189623554547627\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 5.666457176208496 LOSS_Discriminator: 0.29448870817820233\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 4.793076038360596 LOSS_Discriminator: 0.2044303814570109\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 4.063066482543945 LOSS_Discriminator: 0.0994455615679423\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 3.6567654609680176 LOSS_Discriminator: 0.05924410621325175\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 4.725512504577637 LOSS_Discriminator: 0.05344236890474955\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 6.010493278503418 LOSS_Discriminator: 0.03763699779907862\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 6.182878494262695 LOSS_Discriminator: 0.11290005842844646\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 6.7917561531066895 LOSS_Discriminator: 0.05114126205444336\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 6.36408805847168 LOSS_Discriminator: 0.007343317071596782\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 6.858747482299805 LOSS_Discriminator: 0.05410618086655935\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 6.4403815269470215 LOSS_Discriminator: 0.014631867408752441\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 6.412297248840332 LOSS_Discriminator: 0.014473669230937958\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 6.594592094421387 LOSS_Discriminator: 0.08139985799789429\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 5.609724998474121 LOSS_Discriminator: 0.023997242252031963\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 5.977874755859375 LOSS_Discriminator: 0.15811322132746378\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 6.121150970458984 LOSS_Discriminator: 0.005244297906756401\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 5.884033203125 LOSS_Discriminator: 0.006418267885843913\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 6.090503692626953 LOSS_Discriminator: 0.1349731683731079\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 4.74713134765625 LOSS_Discriminator: 0.36237816015879315\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 4.135219097137451 LOSS_Discriminator: 0.09941958387692769\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 3.4598140716552734 LOSS_Discriminator: 0.24042457342147827\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 4.958596229553223 LOSS_Discriminator: 0.13135435183842978\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 6.056771278381348 LOSS_Discriminator: 0.23484639326731363\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 5.532033920288086 LOSS_Discriminator: 0.024356164038181305\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 5.779975891113281 LOSS_Discriminator: 0.15019547939300537\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 5.114502906799316 LOSS_Discriminator: 0.05518628656864166\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 4.8283843994140625 LOSS_Discriminator: 0.07337518036365509\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 4.813156604766846 LOSS_Discriminator: 0.1277891000111898\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 4.5991950035095215 LOSS_Discriminator: 0.07651076217492421\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 4.568676948547363 LOSS_Discriminator: 0.06508407990137736\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 5.057656764984131 LOSS_Discriminator: 0.0249824325243632\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 4.604901313781738 LOSS_Discriminator: 0.140191912651062\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 5.300189971923828 LOSS_Discriminator: 0.03576375047365824\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 5.853384494781494 LOSS_Discriminator: 0.025816403329372406\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 6.10617208480835 LOSS_Discriminator: 0.041451302667458854\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 6.0358781814575195 LOSS_Discriminator: 0.07376571993033092\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 5.963313102722168 LOSS_Discriminator: 0.057015697161356606\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 5.242805480957031 LOSS_Discriminator: 0.05894141892592112\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 4.754012584686279 LOSS_Discriminator: 0.18815354506174722\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 4.629865646362305 LOSS_Discriminator: 0.06984291474024455\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 4.7272210121154785 LOSS_Discriminator: 0.0867791473865509\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 5.341651916503906 LOSS_Discriminator: 0.03408052523930868\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 5.799099445343018 LOSS_Discriminator: 0.15636812647183737\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 5.116629123687744 LOSS_Discriminator: 0.12348034977912903\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 5.019459247589111 LOSS_Discriminator: 0.03876928240060806\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 5.057941436767578 LOSS_Discriminator: 0.03361679365237554\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 5.300715446472168 LOSS_Discriminator: 0.029212772846221924\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 6.0980424880981445 LOSS_Discriminator: 0.06059419612089793\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 5.7014055252075195 LOSS_Discriminator: 0.011557129522164663\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 5.437838554382324 LOSS_Discriminator: 0.0184882457057635\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 6.033933162689209 LOSS_Discriminator: 0.03403015931447347\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 5.440346717834473 LOSS_Discriminator: 0.11692550778388977\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 5.17527961730957 LOSS_Discriminator: 0.06297733883062999\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 4.369532585144043 LOSS_Discriminator: 0.1126743455727895\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 4.417969226837158 LOSS_Discriminator: 0.08953543504079182\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 4.763123035430908 LOSS_Discriminator: 0.14432636896769205\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 5.1585798263549805 LOSS_Discriminator: 0.12380009889602661\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 5.140417098999023 LOSS_Discriminator: 0.20285407702128092\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 4.944293975830078 LOSS_Discriminator: 0.014285353322823843\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 5.16353702545166 LOSS_Discriminator: 0.026574996610482533\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 5.181447505950928 LOSS_Discriminator: 0.08710971474647522\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 5.2599945068359375 LOSS_Discriminator: 0.1280189553896586\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 4.628461837768555 LOSS_Discriminator: 0.1022392213344574\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 3.846750259399414 LOSS_Discriminator: 0.07143991192181905\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 4.150661945343018 LOSS_Discriminator: 0.07177786529064178\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 5.03158712387085 LOSS_Discriminator: 0.054487774769465126\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 5.611630439758301 LOSS_Discriminator: 0.02699070672194163\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 6.2039289474487305 LOSS_Discriminator: 0.004872497792045276\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 6.398033142089844 LOSS_Discriminator: 0.12867252031962076\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 5.991648197174072 LOSS_Discriminator: 0.10912217696507771\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 5.075876712799072 LOSS_Discriminator: 0.1861821413040161\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 3.257417917251587 LOSS_Discriminator: 0.051983803510665894\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 3.973763942718506 LOSS_Discriminator: 0.09317833185195923\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 4.556145668029785 LOSS_Discriminator: 0.0949048896630605\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 5.655992031097412 LOSS_Discriminator: 0.08246342837810516\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 6.085958957672119 LOSS_Discriminator: 0.00716193454960982\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 5.998610496520996 LOSS_Discriminator: 0.008480977267026901\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 6.383833885192871 LOSS_Discriminator: 0.08514422178268433\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 5.763392925262451 LOSS_Discriminator: 0.03488224744796753\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 5.227564334869385 LOSS_Discriminator: 0.02687210589647293\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 5.176745414733887 LOSS_Discriminator: 0.05256597697734833\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 5.435771942138672 LOSS_Discriminator: 0.03012090673049291\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 4.6689043045043945 LOSS_Discriminator: 0.18344616889953613\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 3.813772201538086 LOSS_Discriminator: 0.057573278745015465\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 4.785656452178955 LOSS_Discriminator: 0.05189278721809387\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 4.840908050537109 LOSS_Discriminator: 0.1261473298072815\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 5.9425506591796875 LOSS_Discriminator: 0.012291637559731802\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 6.237628936767578 LOSS_Discriminator: 0.011189074565966925\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 6.119045257568359 LOSS_Discriminator: 0.09906863172849019\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 5.835882186889648 LOSS_Discriminator: 0.04242907961209615\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 5.739971160888672 LOSS_Discriminator: 0.03342069685459137\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 5.3619585037231445 LOSS_Discriminator: 0.04705743491649628\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 5.8992767333984375 LOSS_Discriminator: 0.02774863690137863\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 5.338791847229004 LOSS_Discriminator: 0.18331442276636759\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 5.671624660491943 LOSS_Discriminator: 0.049981956680615745\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 4.4735541343688965 LOSS_Discriminator: 0.09660443663597107\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 3.945791244506836 LOSS_Discriminator: 0.12001917759577434\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 2.674743175506592 LOSS_Discriminator: 0.3888229529062907\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 3.8831136226654053 LOSS_Discriminator: 0.31987518072128296\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 5.381343841552734 LOSS_Discriminator: 0.061227833231290184\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 6.728521823883057 LOSS_Discriminator: 0.010384290168682734\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 7.351195335388184 LOSS_Discriminator: 0.11365838845570882\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 6.809704303741455 LOSS_Discriminator: 0.23028026024500528\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 6.109295845031738 LOSS_Discriminator: 0.005444432298342387\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 5.593991279602051 LOSS_Discriminator: 0.014311586817105612\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 4.298397064208984 LOSS_Discriminator: 0.2869365413983663\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 3.748976230621338 LOSS_Discriminator: 0.09774923324584961\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 4.041260242462158 LOSS_Discriminator: 0.12728961308797201\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 5.721451759338379 LOSS_Discriminator: 0.06423510114351909\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 7.321591854095459 LOSS_Discriminator: 0.03673931956291199\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 7.479355812072754 LOSS_Discriminator: 0.128515362739563\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 8.246467590332031 LOSS_Discriminator: 0.0015406000117460887\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 8.371508598327637 LOSS_Discriminator: 0.19142049551010132\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 7.808993816375732 LOSS_Discriminator: 0.12447607517242432\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 6.820502281188965 LOSS_Discriminator: 0.001798321648190419\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 6.873167991638184 LOSS_Discriminator: 0.0021813463730116687\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 6.551126480102539 LOSS_Discriminator: 0.02579204986492793\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 5.4333977699279785 LOSS_Discriminator: 0.09804085890452068\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 5.212932109832764 LOSS_Discriminator: 0.028811603784561157\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 5.0078325271606445 LOSS_Discriminator: 0.07307125131289165\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 4.376517295837402 LOSS_Discriminator: 0.040489750603834786\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 4.929514408111572 LOSS_Discriminator: 0.262103796005249\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 5.107182025909424 LOSS_Discriminator: 0.09290205438931783\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 5.066558361053467 LOSS_Discriminator: 0.11422718564669292\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 5.299029350280762 LOSS_Discriminator: 0.02213706076145172\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 5.723710060119629 LOSS_Discriminator: 0.017022260775168736\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 5.7356061935424805 LOSS_Discriminator: 0.008527508626381556\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 5.575553894042969 LOSS_Discriminator: 0.22738748788833618\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 4.334721088409424 LOSS_Discriminator: 0.3507990837097168\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 3.8213040828704834 LOSS_Discriminator: 0.26188858350118\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 3.371399164199829 LOSS_Discriminator: 0.24936930338541666\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 4.95627498626709 LOSS_Discriminator: 0.18437703450520834\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 6.256530284881592 LOSS_Discriminator: 0.1246392826239268\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 6.170445442199707 LOSS_Discriminator: 0.24863940477371216\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 6.321280479431152 LOSS_Discriminator: 0.006353834023078282\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 5.702714920043945 LOSS_Discriminator: 0.1974265972773234\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 5.275247097015381 LOSS_Discriminator: 0.06680530309677124\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 4.585549831390381 LOSS_Discriminator: 0.03406272828578949\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 4.741567611694336 LOSS_Discriminator: 0.08157392342885335\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 4.934573173522949 LOSS_Discriminator: 0.035102926194667816\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 5.26408576965332 LOSS_Discriminator: 0.11912507812182109\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 5.4248576164245605 LOSS_Discriminator: 0.08934110403060913\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 4.944862365722656 LOSS_Discriminator: 0.06623819470405579\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 4.937668800354004 LOSS_Discriminator: 0.18967986106872559\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 4.524751663208008 LOSS_Discriminator: 0.027978405356407166\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 3.7357218265533447 LOSS_Discriminator: 0.1830936868985494\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 3.348127841949463 LOSS_Discriminator: 0.12549065550168356\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 4.153054237365723 LOSS_Discriminator: 0.07265778382619222\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 4.978057384490967 LOSS_Discriminator: 0.10632425546646118\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 5.426063537597656 LOSS_Discriminator: 0.012941882014274597\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 5.7498321533203125 LOSS_Discriminator: 0.0786994496981303\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 5.0908355712890625 LOSS_Discriminator: 0.2150689959526062\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 4.796611785888672 LOSS_Discriminator: 0.23120715220769247\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 3.295375108718872 LOSS_Discriminator: 0.06641995906829834\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 2.96475887298584 LOSS_Discriminator: 0.091024249792099\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 4.178086280822754 LOSS_Discriminator: 0.15671073397000632\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 5.422380447387695 LOSS_Discriminator: 0.07836780945460002\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 5.5114850997924805 LOSS_Discriminator: 0.010751833518346151\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 5.968804359436035 LOSS_Discriminator: 0.007459773371617\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 6.063511848449707 LOSS_Discriminator: 0.12556608517964682\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 5.862598419189453 LOSS_Discriminator: 0.004889892724653085\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 5.765817642211914 LOSS_Discriminator: 0.06917181611061096\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 4.798783779144287 LOSS_Discriminator: 0.14919675389925638\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 5.041891574859619 LOSS_Discriminator: 0.011213331172863642\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 3.952214002609253 LOSS_Discriminator: 0.1342939337094625\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 4.011379241943359 LOSS_Discriminator: 0.05668279528617859\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 3.3885927200317383 LOSS_Discriminator: 0.28663019339243573\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 4.039730548858643 LOSS_Discriminator: 0.09253059824307759\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 4.688776016235352 LOSS_Discriminator: 0.18337426582972208\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 5.267458915710449 LOSS_Discriminator: 0.020823508501052856\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 5.501185894012451 LOSS_Discriminator: 0.05301045378049215\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 5.721115589141846 LOSS_Discriminator: 0.005253671978910764\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 6.188598155975342 LOSS_Discriminator: 0.10178802410761516\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 5.856976509094238 LOSS_Discriminator: 0.12635843952496847\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 5.105135440826416 LOSS_Discriminator: 0.12098964055379231\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 4.186683654785156 LOSS_Discriminator: 0.13587416211764017\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 3.8874707221984863 LOSS_Discriminator: 0.24706673622131348\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 3.1918251514434814 LOSS_Discriminator: 0.18858514229456583\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 3.729617118835449 LOSS_Discriminator: 0.1508029898007711\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 4.95819616317749 LOSS_Discriminator: 0.09877818822860718\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 6.3400373458862305 LOSS_Discriminator: 0.014511164277791977\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 6.949160099029541 LOSS_Discriminator: 0.12996874252955118\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 6.819660186767578 LOSS_Discriminator: 0.28383872906366986\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 6.365382194519043 LOSS_Discriminator: 0.15196816126505533\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 6.142434597015381 LOSS_Discriminator: 0.1622331142425537\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 4.376518726348877 LOSS_Discriminator: 0.031037862102190655\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 4.311361312866211 LOSS_Discriminator: 0.16516618927319845\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 3.7322652339935303 LOSS_Discriminator: 0.08240298430124919\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 3.7438626289367676 LOSS_Discriminator: 0.1589264969031016\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 5.182027816772461 LOSS_Discriminator: 0.08038228750228882\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 5.466489791870117 LOSS_Discriminator: 0.12375309069951375\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 6.046896457672119 LOSS_Discriminator: 0.010000483443339666\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 6.605926513671875 LOSS_Discriminator: 0.28797511259714764\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 5.934298038482666 LOSS_Discriminator: 0.008957715705037117\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 5.913865089416504 LOSS_Discriminator: 0.0072023433943589526\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 5.788183689117432 LOSS_Discriminator: 0.006664969647924106\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 5.544682502746582 LOSS_Discriminator: 0.12633238236109415\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 5.071465492248535 LOSS_Discriminator: 0.19162813822428384\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 3.5014357566833496 LOSS_Discriminator: 0.19406344493230185\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 2.9841482639312744 LOSS_Discriminator: 0.24395620822906494\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 3.678783416748047 LOSS_Discriminator: 0.28287198146184284\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 5.294373989105225 LOSS_Discriminator: 0.05838204423586527\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 6.211745262145996 LOSS_Discriminator: 0.08183451493581136\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 6.465568542480469 LOSS_Discriminator: 0.11463140447934468\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 6.472055435180664 LOSS_Discriminator: 0.12173217535018921\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 6.22740364074707 LOSS_Discriminator: 0.16565808653831482\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.544974327087402 LOSS_Discriminator: 0.06100967526435852\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 4.928112983703613 LOSS_Discriminator: 0.05609526733557383\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 4.649350166320801 LOSS_Discriminator: 0.16318339109420776\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 3.8748488426208496 LOSS_Discriminator: 0.207969069480896\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 4.1780805587768555 LOSS_Discriminator: 0.191449244817098\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 3.8123302459716797 LOSS_Discriminator: 0.13231382767359415\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 4.991934776306152 LOSS_Discriminator: 0.137699693441391\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 4.8318986892700195 LOSS_Discriminator: 0.2996334632237752\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 5.071565628051758 LOSS_Discriminator: 0.08468950788180034\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 4.91233491897583 LOSS_Discriminator: 0.10027533769607544\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 4.017408847808838 LOSS_Discriminator: 0.13269488016764322\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 3.6816086769104004 LOSS_Discriminator: 0.0937018593152364\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 3.621001720428467 LOSS_Discriminator: 0.0522177517414093\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 4.146000862121582 LOSS_Discriminator: 0.06503115594387054\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 5.150724411010742 LOSS_Discriminator: 0.1600019931793213\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 5.461170196533203 LOSS_Discriminator: 0.05800643563270569\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 5.571027755737305 LOSS_Discriminator: 0.0890989104906718\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 5.4790143966674805 LOSS_Discriminator: 0.2404192090034485\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.456062316894531 LOSS_Discriminator: 0.04356121023495992\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 4.040510177612305 LOSS_Discriminator: 0.05473447839419047\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 4.221194267272949 LOSS_Discriminator: 0.08876262108484904\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 3.3779993057250977 LOSS_Discriminator: 0.29455409447352093\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 2.9718751907348633 LOSS_Discriminator: 0.11265809337298076\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 3.707190752029419 LOSS_Discriminator: 0.1519861419995626\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 5.242480754852295 LOSS_Discriminator: 0.03238984942436218\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 5.979064464569092 LOSS_Discriminator: 0.011501298596461615\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 6.8281965255737305 LOSS_Discriminator: 0.09658091266949971\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 6.558960437774658 LOSS_Discriminator: 0.12805277109146118\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 6.919193744659424 LOSS_Discriminator: 0.17866208155949911\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 6.10172176361084 LOSS_Discriminator: 0.0537238617738088\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 6.441902160644531 LOSS_Discriminator: 0.004193415554861228\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 5.940899848937988 LOSS_Discriminator: 0.01942882811029752\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 5.982699394226074 LOSS_Discriminator: 0.004268205414215724\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 5.638433933258057 LOSS_Discriminator: 0.13967193166414896\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 4.817276477813721 LOSS_Discriminator: 0.1638899544874827\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 4.015669822692871 LOSS_Discriminator: 0.14165990551312765\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 2.8824830055236816 LOSS_Discriminator: 0.19795175393422446\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 3.3603103160858154 LOSS_Discriminator: 0.18216307957967123\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 5.041131019592285 LOSS_Discriminator: 0.06121459106604258\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 5.65426778793335 LOSS_Discriminator: 0.1937094728151957\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 6.369853973388672 LOSS_Discriminator: 0.0811610867579778\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 6.217462062835693 LOSS_Discriminator: 0.07436605294545491\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 5.6493611335754395 LOSS_Discriminator: 0.19293320178985596\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 5.064211845397949 LOSS_Discriminator: 0.01649537185827891\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 5.037817001342773 LOSS_Discriminator: 0.03827144453922907\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 4.067037582397461 LOSS_Discriminator: 0.12688241402308145\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 4.427149295806885 LOSS_Discriminator: 0.04713000853856405\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 4.523662567138672 LOSS_Discriminator: 0.1847420334815979\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 4.3598809242248535 LOSS_Discriminator: 0.023777601619561512\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 4.51718807220459 LOSS_Discriminator: 0.11711039145787557\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 4.4326276779174805 LOSS_Discriminator: 0.030633941292762756\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 4.451931953430176 LOSS_Discriminator: 0.09619256854057312\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 4.5020551681518555 LOSS_Discriminator: 0.1631472110748291\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 4.932368278503418 LOSS_Discriminator: 0.02497341235478719\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 4.863318920135498 LOSS_Discriminator: 0.18226897716522217\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 4.638551235198975 LOSS_Discriminator: 0.07842723528544109\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 4.5153350830078125 LOSS_Discriminator: 0.02540867527325948\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 4.744430065155029 LOSS_Discriminator: 0.06677533686161041\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 4.967479228973389 LOSS_Discriminator: 0.01679321626822154\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 4.680858612060547 LOSS_Discriminator: 0.05569866796334585\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 4.943483352661133 LOSS_Discriminator: 0.016082579890886944\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 5.101526260375977 LOSS_Discriminator: 0.12110992272694905\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 5.001696586608887 LOSS_Discriminator: 0.023144836227099102\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 5.004693984985352 LOSS_Discriminator: 0.09164289633433025\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 5.155570030212402 LOSS_Discriminator: 0.023270231982072193\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 4.782699108123779 LOSS_Discriminator: 0.3250158627827962\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 4.461596488952637 LOSS_Discriminator: 0.07249403993288676\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 4.303674221038818 LOSS_Discriminator: 0.04842084149519602\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 4.204009056091309 LOSS_Discriminator: 0.3340800603230794\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 4.165168762207031 LOSS_Discriminator: 0.11217570304870605\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 4.324282646179199 LOSS_Discriminator: 0.19854454199473062\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 4.279833793640137 LOSS_Discriminator: 0.17906677722930908\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 3.9059653282165527 LOSS_Discriminator: 0.09808691342671712\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 4.357980728149414 LOSS_Discriminator: 0.08525372544924419\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 4.801106929779053 LOSS_Discriminator: 0.06118537485599518\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 5.067525863647461 LOSS_Discriminator: 0.07468116283416748\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 5.703595161437988 LOSS_Discriminator: 0.057938019434611\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 5.281201362609863 LOSS_Discriminator: 0.011983448018630346\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 5.386101245880127 LOSS_Discriminator: 0.09549704194068909\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 4.9871416091918945 LOSS_Discriminator: 0.02666134387254715\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 4.999720573425293 LOSS_Discriminator: 0.07550017535686493\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 4.38871431350708 LOSS_Discriminator: 0.025427572429180145\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 3.9856083393096924 LOSS_Discriminator: 0.17137720187505087\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 4.678683757781982 LOSS_Discriminator: 0.10766792297363281\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 4.095846176147461 LOSS_Discriminator: 0.1305259863535563\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 4.579258918762207 LOSS_Discriminator: 0.053422595063845314\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 5.197521686553955 LOSS_Discriminator: 0.10154968500137329\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 5.367164611816406 LOSS_Discriminator: 0.06061745683352152\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 4.614959716796875 LOSS_Discriminator: 0.08805366357167561\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 4.604620933532715 LOSS_Discriminator: 0.11683072646458943\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 4.185970306396484 LOSS_Discriminator: 0.24253018697102866\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 3.897003173828125 LOSS_Discriminator: 0.04078473895788193\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 4.797329425811768 LOSS_Discriminator: 0.08940343062082927\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 5.249755859375 LOSS_Discriminator: 0.03369698921839396\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 4.780398368835449 LOSS_Discriminator: 0.1434595783551534\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 4.899973392486572 LOSS_Discriminator: 0.036000835398832955\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 4.388397693634033 LOSS_Discriminator: 0.1462421715259552\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 3.911682605743408 LOSS_Discriminator: 0.10207209984461467\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 4.397515296936035 LOSS_Discriminator: 0.04758992294470469\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 5.031701564788818 LOSS_Discriminator: 0.04285078247388204\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 5.762269020080566 LOSS_Discriminator: 0.11637695630391438\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 4.592299938201904 LOSS_Discriminator: 0.14337597290674844\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 4.509059906005859 LOSS_Discriminator: 0.15090322494506836\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 4.562641143798828 LOSS_Discriminator: 0.04878840843836466\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 4.739490032196045 LOSS_Discriminator: 0.11982337633768718\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 5.413543701171875 LOSS_Discriminator: 0.0241754154364268\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 6.087614059448242 LOSS_Discriminator: 0.02886899809042613\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 6.878158092498779 LOSS_Discriminator: 0.17186047633488974\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 6.534667015075684 LOSS_Discriminator: 0.025449559092521667\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 6.902026653289795 LOSS_Discriminator: 0.0075723715126514435\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 6.516268253326416 LOSS_Discriminator: 0.21690907080968222\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 6.095746994018555 LOSS_Discriminator: 0.0086275947590669\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 5.815864562988281 LOSS_Discriminator: 0.016526552538077038\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 5.233336448669434 LOSS_Discriminator: 0.08753044406572978\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 4.98874568939209 LOSS_Discriminator: 0.031187628706296284\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 4.557972431182861 LOSS_Discriminator: 0.08087061842282613\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 4.604240417480469 LOSS_Discriminator: 0.08248788118362427\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 4.829960346221924 LOSS_Discriminator: 0.09519483645757039\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 5.157696723937988 LOSS_Discriminator: 0.16639192899068198\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 4.988984107971191 LOSS_Discriminator: 0.05620487034320831\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 5.242007732391357 LOSS_Discriminator: 0.2942545811335246\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 5.328409194946289 LOSS_Discriminator: 0.06969130039215088\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 4.502237319946289 LOSS_Discriminator: 0.08832470575968425\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 4.216108322143555 LOSS_Discriminator: 0.05514960487683614\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 4.890323638916016 LOSS_Discriminator: 0.032613267501195274\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 5.5334625244140625 LOSS_Discriminator: 0.021708746751149494\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 6.364306449890137 LOSS_Discriminator: 0.007188071186343829\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 6.256565093994141 LOSS_Discriminator: 0.19861278931299844\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 5.837427616119385 LOSS_Discriminator: 0.1393663783868154\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 4.810385227203369 LOSS_Discriminator: 0.06597068905830383\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 4.200796127319336 LOSS_Discriminator: 0.04391983151435852\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 3.9488587379455566 LOSS_Discriminator: 0.06907483438650767\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 4.931328773498535 LOSS_Discriminator: 0.07568216820557912\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 5.430253028869629 LOSS_Discriminator: 0.11743661761283875\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 5.506262302398682 LOSS_Discriminator: 0.3833554188410441\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 4.421306610107422 LOSS_Discriminator: 0.023593326409657795\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 3.794609546661377 LOSS_Discriminator: 0.043346524238586426\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 4.165832042694092 LOSS_Discriminator: 0.0396466056505839\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 4.60188102722168 LOSS_Discriminator: 0.03855728109677633\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 4.333319664001465 LOSS_Discriminator: 0.1193167765935262\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 5.016727447509766 LOSS_Discriminator: 0.09687540928522746\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 4.96781063079834 LOSS_Discriminator: 0.1300411820411682\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 5.278705596923828 LOSS_Discriminator: 0.03416018187999725\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 4.20850133895874 LOSS_Discriminator: 0.04240691661834717\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 5.076714515686035 LOSS_Discriminator: 0.06753031412760417\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 4.927498817443848 LOSS_Discriminator: 0.0829989065726598\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 4.878116607666016 LOSS_Discriminator: 0.06102823714415232\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 5.111051559448242 LOSS_Discriminator: 0.05080816646416982\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 5.378523349761963 LOSS_Discriminator: 0.012144789099693298\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 5.523229598999023 LOSS_Discriminator: 0.024581655859947205\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 5.439046382904053 LOSS_Discriminator: 0.011981905748446783\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 5.091566562652588 LOSS_Discriminator: 0.18719037373860678\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 5.057844161987305 LOSS_Discriminator: 0.05668797095616659\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 4.389749050140381 LOSS_Discriminator: 0.06233606735865275\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 4.606534004211426 LOSS_Discriminator: 0.12034539381663005\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 4.753950119018555 LOSS_Discriminator: 0.04608007272084554\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 5.833514213562012 LOSS_Discriminator: 0.013411539296309153\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.461716651916504 LOSS_Discriminator: 0.10706498225529988\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 4.698863983154297 LOSS_Discriminator: 0.20945831139882407\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 4.177197456359863 LOSS_Discriminator: 0.06376683712005615\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 4.644078254699707 LOSS_Discriminator: 0.07802131772041321\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 4.81156063079834 LOSS_Discriminator: 0.07149141530195872\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 5.502862930297852 LOSS_Discriminator: 0.10081433256467183\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 5.519626617431641 LOSS_Discriminator: 0.02153096596399943\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 5.268178939819336 LOSS_Discriminator: 0.018479914714892704\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 5.514484405517578 LOSS_Discriminator: 0.17820934454600015\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 4.521197319030762 LOSS_Discriminator: 0.2531882921854655\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 4.443748474121094 LOSS_Discriminator: 0.044998183846473694\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 4.772069931030273 LOSS_Discriminator: 0.037607659896214805\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 4.540682315826416 LOSS_Discriminator: 0.10531679789225261\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 4.829870700836182 LOSS_Discriminator: 0.08615231513977051\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 3.964702606201172 LOSS_Discriminator: 0.09869585434595744\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 4.1387834548950195 LOSS_Discriminator: 0.06932161748409271\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 4.951678276062012 LOSS_Discriminator: 0.11690469582875569\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 5.195950508117676 LOSS_Discriminator: 0.06757838030656178\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 6.015841960906982 LOSS_Discriminator: 0.026450509826342266\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 5.927390098571777 LOSS_Discriminator: 0.12767751018206278\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 5.074075698852539 LOSS_Discriminator: 0.12105954686800639\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 5.120196342468262 LOSS_Discriminator: 0.013895101845264435\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 5.137911796569824 LOSS_Discriminator: 0.031540912886460624\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 4.863572120666504 LOSS_Discriminator: 0.03463444113731384\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 5.70851993560791 LOSS_Discriminator: 0.024898745119571686\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 5.564062118530273 LOSS_Discriminator: 0.04533060888449351\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 5.298063278198242 LOSS_Discriminator: 0.1374245584011078\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 5.347352027893066 LOSS_Discriminator: 0.09359635909398396\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 5.159164905548096 LOSS_Discriminator: 0.29491980870564777\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 4.448221683502197 LOSS_Discriminator: 0.12561084826787314\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 4.917524337768555 LOSS_Discriminator: 0.11401981115341187\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 5.760663986206055 LOSS_Discriminator: 0.09874782959620158\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 6.449857711791992 LOSS_Discriminator: 0.04245213667551676\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 6.1512556076049805 LOSS_Discriminator: 0.04739158352216085\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 6.469236373901367 LOSS_Discriminator: 0.0041962483276923495\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 5.89919376373291 LOSS_Discriminator: 0.053380424777666725\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 6.523960113525391 LOSS_Discriminator: 0.006812786062558492\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 5.532151222229004 LOSS_Discriminator: 0.22476011514663696\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 5.55085563659668 LOSS_Discriminator: 0.1577071249485016\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 5.193577766418457 LOSS_Discriminator: 0.046056548754374184\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 5.1054840087890625 LOSS_Discriminator: 0.04582993189493815\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 4.055427551269531 LOSS_Discriminator: 0.059456770618756614\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 4.786201477050781 LOSS_Discriminator: 0.023674783607323963\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 4.608578681945801 LOSS_Discriminator: 0.19248958428700766\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 4.228793144226074 LOSS_Discriminator: 0.06343010564645131\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 5.609464645385742 LOSS_Discriminator: 0.03490698834260305\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 6.035343170166016 LOSS_Discriminator: 0.045480827490488686\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 6.264792442321777 LOSS_Discriminator: 0.10341899593671162\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 5.490193843841553 LOSS_Discriminator: 0.25728001197179157\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 4.5332136154174805 LOSS_Discriminator: 0.20004876454671225\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 4.012772560119629 LOSS_Discriminator: 0.09875768423080444\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 5.33015251159668 LOSS_Discriminator: 0.10799280802408855\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 5.180367469787598 LOSS_Discriminator: 0.1579139232635498\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 5.651188850402832 LOSS_Discriminator: 0.16206878423690796\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 5.560611248016357 LOSS_Discriminator: 0.024036313096682232\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 5.48443603515625 LOSS_Discriminator: 0.024499235053857166\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 4.905943870544434 LOSS_Discriminator: 0.19261942307154337\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 5.080883026123047 LOSS_Discriminator: 0.020566653460264206\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 4.863363742828369 LOSS_Discriminator: 0.07327140371004741\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 5.346375465393066 LOSS_Discriminator: 0.04790509740511576\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 4.8003621101379395 LOSS_Discriminator: 0.15271663665771484\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 4.5791425704956055 LOSS_Discriminator: 0.12310874462127686\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 5.083844184875488 LOSS_Discriminator: 0.049326409896214805\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 5.250616073608398 LOSS_Discriminator: 0.030120072265466053\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 5.260265350341797 LOSS_Discriminator: 0.02492970476547877\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 5.496872901916504 LOSS_Discriminator: 0.0389366348584493\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 5.58933162689209 LOSS_Discriminator: 0.01834916075070699\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 4.919944763183594 LOSS_Discriminator: 0.18336284160614014\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 4.353025913238525 LOSS_Discriminator: 0.13534043232599893\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 4.229290008544922 LOSS_Discriminator: 0.060750097036361694\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 5.326093673706055 LOSS_Discriminator: 0.16500409444173178\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 5.378583908081055 LOSS_Discriminator: 0.05236900349458059\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 6.003940582275391 LOSS_Discriminator: 0.008082468683520952\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 6.156939506530762 LOSS_Discriminator: 0.07022721072038014\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 6.126168251037598 LOSS_Discriminator: 0.04948374629020691\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 5.554407596588135 LOSS_Discriminator: 0.00970114270846049\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 5.150218963623047 LOSS_Discriminator: 0.10815285642941792\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 5.072174072265625 LOSS_Discriminator: 0.032308111588160195\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 3.528416156768799 LOSS_Discriminator: 0.22696536779403687\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 3.8177762031555176 LOSS_Discriminator: 0.2555321455001831\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 4.938073635101318 LOSS_Discriminator: 0.07912988464037578\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 5.214177131652832 LOSS_Discriminator: 0.1676280895868937\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 6.396468162536621 LOSS_Discriminator: 0.03478644539912542\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 6.402177810668945 LOSS_Discriminator: 0.19277199109395346\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 5.644818305969238 LOSS_Discriminator: 0.23070796330769858\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 5.892130374908447 LOSS_Discriminator: 0.06857097148895264\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 5.156073570251465 LOSS_Discriminator: 0.1121169924736023\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 4.31038761138916 LOSS_Discriminator: 0.05313131709893545\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 4.233945846557617 LOSS_Discriminator: 0.0633985052506129\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 4.379727363586426 LOSS_Discriminator: 0.05037479102611542\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 5.456789016723633 LOSS_Discriminator: 0.03697768102089564\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 5.843120574951172 LOSS_Discriminator: 0.18542039394378662\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 5.220403671264648 LOSS_Discriminator: 0.0951072374979655\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 4.048535346984863 LOSS_Discriminator: 0.0544495036204656\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 4.016841888427734 LOSS_Discriminator: 0.134822150071462\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 3.7766900062561035 LOSS_Discriminator: 0.15120933453241983\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 5.203245639801025 LOSS_Discriminator: 0.04649658501148224\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 5.649594306945801 LOSS_Discriminator: 0.1119230588277181\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 5.578405380249023 LOSS_Discriminator: 0.1839332381884257\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 4.894477367401123 LOSS_Discriminator: 0.11888694763183594\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 3.7272634506225586 LOSS_Discriminator: 0.058625047405560814\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 3.850600481033325 LOSS_Discriminator: 0.1024376352628072\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 5.055314064025879 LOSS_Discriminator: 0.07796470324198405\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 5.737737655639648 LOSS_Discriminator: 0.010686438530683517\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 6.8158087730407715 LOSS_Discriminator: 0.07092419763406117\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 6.696358680725098 LOSS_Discriminator: 0.08178194363911946\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 5.801789283752441 LOSS_Discriminator: 0.31567521890004474\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 4.917642593383789 LOSS_Discriminator: 0.016283072531223297\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 3.7049026489257812 LOSS_Discriminator: 0.02367124209801356\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 4.037240982055664 LOSS_Discriminator: 0.10228937864303589\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 4.712038040161133 LOSS_Discriminator: 0.09811260302861531\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 5.294949531555176 LOSS_Discriminator: 0.11882901191711426\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 6.094033241271973 LOSS_Discriminator: 0.04650864005088806\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 6.041922569274902 LOSS_Discriminator: 0.020048980911572773\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 5.852149486541748 LOSS_Discriminator: 0.09713457028071086\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 5.798784255981445 LOSS_Discriminator: 0.019018818934758503\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 5.549821853637695 LOSS_Discriminator: 0.10856052239735921\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 5.025210380554199 LOSS_Discriminator: 0.03947544594605764\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 5.209112167358398 LOSS_Discriminator: 0.017183153579632442\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 5.469291687011719 LOSS_Discriminator: 0.11688987414042155\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 4.483925819396973 LOSS_Discriminator: 0.051221405466397606\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 4.405757427215576 LOSS_Discriminator: 0.07651284337043762\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 5.098545551300049 LOSS_Discriminator: 0.11790592471758525\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 5.815091133117676 LOSS_Discriminator: 0.1282490293184916\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 6.013293743133545 LOSS_Discriminator: 0.14336535334587097\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 5.6785502433776855 LOSS_Discriminator: 0.1206592321395874\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 4.767988681793213 LOSS_Discriminator: 0.17732226848602295\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 4.212336540222168 LOSS_Discriminator: 0.027661378184954327\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 4.021803855895996 LOSS_Discriminator: 0.049312795201937355\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 4.740716934204102 LOSS_Discriminator: 0.12348071734110515\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 5.554434299468994 LOSS_Discriminator: 0.15909202893575033\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 6.1655354499816895 LOSS_Discriminator: 0.020917028188705444\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 5.909494400024414 LOSS_Discriminator: 0.24065476655960083\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 5.240931987762451 LOSS_Discriminator: 0.01358200858036677\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 4.723104476928711 LOSS_Discriminator: 0.13963844378789267\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 4.293677806854248 LOSS_Discriminator: 0.10988775889078777\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 4.212284564971924 LOSS_Discriminator: 0.05026707053184509\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 4.576089859008789 LOSS_Discriminator: 0.08053765694300334\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 5.455555438995361 LOSS_Discriminator: 0.07521699865659077\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 5.301042556762695 LOSS_Discriminator: 0.09718015789985657\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 4.476060390472412 LOSS_Discriminator: 0.1468168099721273\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 4.307357311248779 LOSS_Discriminator: 0.23074225584665933\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 3.2507810592651367 LOSS_Discriminator: 0.16596813996632895\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 4.008986473083496 LOSS_Discriminator: 0.239911158879598\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 5.162255764007568 LOSS_Discriminator: 0.09869832793871562\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 6.114514350891113 LOSS_Discriminator: 0.07334485650062561\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 6.407652854919434 LOSS_Discriminator: 0.07864414155483246\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 6.645134925842285 LOSS_Discriminator: 0.002288795541971922\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 6.329913139343262 LOSS_Discriminator: 0.10492939750353496\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 6.285794734954834 LOSS_Discriminator: 0.29242825508117676\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 5.557561874389648 LOSS_Discriminator: 0.026987512906392414\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 5.0767059326171875 LOSS_Discriminator: 0.02287711203098297\n",
            "EPOCH OVER: 27\n",
            "ITERATION_NO.: 1 LOSS_Generator: 4.878187656402588 LOSS_Discriminator: 0.016771030922730763\n",
            "ITERATION_NO.: 2 LOSS_Generator: 4.497100830078125 LOSS_Discriminator: 0.037896295388539634\n",
            "ITERATION_NO.: 3 LOSS_Generator: 4.792314052581787 LOSS_Discriminator: 0.13643338282903036\n",
            "ITERATION_NO.: 4 LOSS_Generator: 4.9175496101379395 LOSS_Discriminator: 0.053624093532562256\n",
            "ITERATION_NO.: 5 LOSS_Generator: 4.791502475738525 LOSS_Discriminator: 0.12045508623123169\n",
            "ITERATION_NO.: 6 LOSS_Generator: 4.893238544464111 LOSS_Discriminator: 0.1261194944381714\n",
            "ITERATION_NO.: 7 LOSS_Generator: 4.617177486419678 LOSS_Discriminator: 0.20360867182413736\n",
            "ITERATION_NO.: 8 LOSS_Generator: 4.636299133300781 LOSS_Discriminator: 0.04456426203250885\n",
            "ITERATION_NO.: 9 LOSS_Generator: 4.204482555389404 LOSS_Discriminator: 0.08280447125434875\n",
            "ITERATION_NO.: 10 LOSS_Generator: 4.070234298706055 LOSS_Discriminator: 0.042958284417788185\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.001620292663574 LOSS_Discriminator: 0.10964837670326233\n",
            "ITERATION_NO.: 12 LOSS_Generator: 4.516707420349121 LOSS_Discriminator: 0.20560391743977866\n",
            "ITERATION_NO.: 13 LOSS_Generator: 4.290729522705078 LOSS_Discriminator: 0.06561510761578877\n",
            "ITERATION_NO.: 14 LOSS_Generator: 4.815224647521973 LOSS_Discriminator: 0.0455461194117864\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.05517578125 LOSS_Discriminator: 0.09667026003201802\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.4283013343811035 LOSS_Discriminator: 0.017530875901381176\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.309730529785156 LOSS_Discriminator: 0.016680799424648285\n",
            "ITERATION_NO.: 18 LOSS_Generator: 6.123217582702637 LOSS_Discriminator: 0.010264693448940912\n",
            "ITERATION_NO.: 19 LOSS_Generator: 6.157296180725098 LOSS_Discriminator: 0.09551944335301717\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.422873497009277 LOSS_Discriminator: 0.12788854042689005\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.5196332931518555 LOSS_Discriminator: 0.03192408631245295\n",
            "ITERATION_NO.: 22 LOSS_Generator: 4.412632465362549 LOSS_Discriminator: 0.016523173699776333\n",
            "ITERATION_NO.: 23 LOSS_Generator: 4.694313049316406 LOSS_Discriminator: 0.018630705773830414\n",
            "ITERATION_NO.: 24 LOSS_Generator: 4.7247161865234375 LOSS_Discriminator: 0.0459895133972168\n",
            "ITERATION_NO.: 25 LOSS_Generator: 4.187283992767334 LOSS_Discriminator: 0.18197486797968546\n",
            "ITERATION_NO.: 26 LOSS_Generator: 3.6498875617980957 LOSS_Discriminator: 0.0372529203693072\n",
            "ITERATION_NO.: 27 LOSS_Generator: 4.177560806274414 LOSS_Discriminator: 0.15097459157307944\n",
            "ITERATION_NO.: 28 LOSS_Generator: 4.639106273651123 LOSS_Discriminator: 0.0607228030761083\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.45744514465332 LOSS_Discriminator: 0.04138539979855219\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.099491119384766 LOSS_Discriminator: 0.11568673451741536\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.072959899902344 LOSS_Discriminator: 0.08783896764119466\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.030387878417969 LOSS_Discriminator: 0.05502003928025564\n",
            "ITERATION_NO.: 33 LOSS_Generator: 4.977955341339111 LOSS_Discriminator: 0.0160109909872214\n",
            "ITERATION_NO.: 34 LOSS_Generator: 4.515174865722656 LOSS_Discriminator: 0.20332463582356772\n",
            "ITERATION_NO.: 35 LOSS_Generator: 4.008547306060791 LOSS_Discriminator: 0.06463176012039185\n",
            "ITERATION_NO.: 36 LOSS_Generator: 3.738097906112671 LOSS_Discriminator: 0.13625924785931906\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.286919593811035 LOSS_Discriminator: 0.05861361821492513\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.825995445251465 LOSS_Discriminator: 0.020271136114994686\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.833954811096191 LOSS_Discriminator: 0.0037715469176570573\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.209588050842285 LOSS_Discriminator: 0.0022450642039378486\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.201794147491455 LOSS_Discriminator: 0.2078624963760376\n",
            "ITERATION_NO.: 42 LOSS_Generator: 7.441510200500488 LOSS_Discriminator: 0.14402031898498535\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.796048641204834 LOSS_Discriminator: 0.17203164100646973\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.728270530700684 LOSS_Discriminator: 0.40643473466237384\n",
            "ITERATION_NO.: 45 LOSS_Generator: 4.618582725524902 LOSS_Discriminator: 0.021300385395685833\n",
            "ITERATION_NO.: 46 LOSS_Generator: 3.858325481414795 LOSS_Discriminator: 0.08901368578275044\n",
            "ITERATION_NO.: 47 LOSS_Generator: 3.5134124755859375 LOSS_Discriminator: 0.15606571237246195\n",
            "ITERATION_NO.: 48 LOSS_Generator: 3.9823875427246094 LOSS_Discriminator: 0.13385504484176636\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.317129135131836 LOSS_Discriminator: 0.06459167102972667\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.838179588317871 LOSS_Discriminator: 0.013755299150943756\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.9404730796813965 LOSS_Discriminator: 0.005268832668662071\n",
            "ITERATION_NO.: 52 LOSS_Generator: 6.829686164855957 LOSS_Discriminator: 0.23629115025202432\n",
            "ITERATION_NO.: 53 LOSS_Generator: 6.8311309814453125 LOSS_Discriminator: 0.0803297112385432\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.343959808349609 LOSS_Discriminator: 0.16963613033294678\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.755405426025391 LOSS_Discriminator: 0.03246419380108515\n",
            "ITERATION_NO.: 56 LOSS_Generator: 4.859103202819824 LOSS_Discriminator: 0.15158763527870178\n",
            "ITERATION_NO.: 57 LOSS_Generator: 4.302618503570557 LOSS_Discriminator: 0.15740748246510824\n",
            "ITERATION_NO.: 58 LOSS_Generator: 3.684551239013672 LOSS_Discriminator: 0.04934099813302358\n",
            "ITERATION_NO.: 59 LOSS_Generator: 3.07570743560791 LOSS_Discriminator: 0.24440016349156699\n",
            "ITERATION_NO.: 60 LOSS_Generator: 3.221336603164673 LOSS_Discriminator: 0.12260759870211284\n",
            "ITERATION_NO.: 61 LOSS_Generator: 4.929787635803223 LOSS_Discriminator: 0.24050970872243246\n",
            "ITERATION_NO.: 62 LOSS_Generator: 5.761992454528809 LOSS_Discriminator: 0.04093343516190847\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.121381759643555 LOSS_Discriminator: 0.09312014778455098\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.278651714324951 LOSS_Discriminator: 0.10931344827016194\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.011879920959473 LOSS_Discriminator: 0.03595663607120514\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.480856895446777 LOSS_Discriminator: 0.030241819719473522\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.019946098327637 LOSS_Discriminator: 0.20812265078226724\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.263399600982666 LOSS_Discriminator: 0.024255526562531788\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.076014995574951 LOSS_Discriminator: 0.08584316571553548\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.118074417114258 LOSS_Discriminator: 0.07955271005630493\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.342733383178711 LOSS_Discriminator: 0.015263151377439499\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.023642539978027 LOSS_Discriminator: 0.16595753033955893\n",
            "ITERATION_NO.: 73 LOSS_Generator: 4.787890434265137 LOSS_Discriminator: 0.0662867824236552\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.095885753631592 LOSS_Discriminator: 0.16877325375874838\n",
            "ITERATION_NO.: 75 LOSS_Generator: 4.587604522705078 LOSS_Discriminator: 0.12620234489440918\n",
            "ITERATION_NO.: 76 LOSS_Generator: 4.535202980041504 LOSS_Discriminator: 0.03647445638974508\n",
            "ITERATION_NO.: 77 LOSS_Generator: 4.476486682891846 LOSS_Discriminator: 0.13649614651997885\n",
            "ITERATION_NO.: 78 LOSS_Generator: 4.196747303009033 LOSS_Discriminator: 0.10753524303436279\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.158283710479736 LOSS_Discriminator: 0.03493553400039673\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.7007246017456055 LOSS_Discriminator: 0.01767696440219879\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.689984321594238 LOSS_Discriminator: 0.07287327945232391\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.965239524841309 LOSS_Discriminator: 0.007202596714099248\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.635968208312988 LOSS_Discriminator: 0.18920214970906576\n",
            "ITERATION_NO.: 84 LOSS_Generator: 4.557136535644531 LOSS_Discriminator: 0.05848469336827596\n",
            "ITERATION_NO.: 85 LOSS_Generator: 4.890743732452393 LOSS_Discriminator: 0.04230593144893646\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.163641929626465 LOSS_Discriminator: 0.07375795145829518\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.024782180786133 LOSS_Discriminator: 0.053218950827916466\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.090145111083984 LOSS_Discriminator: 0.14097845554351807\n",
            "ITERATION_NO.: 89 LOSS_Generator: 4.810340404510498 LOSS_Discriminator: 0.01806483417749405\n",
            "ITERATION_NO.: 90 LOSS_Generator: 4.892112731933594 LOSS_Discriminator: 0.05029182632764181\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.122916221618652 LOSS_Discriminator: 0.020382151007652283\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.956938743591309 LOSS_Discriminator: 0.016376961022615433\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.932554721832275 LOSS_Discriminator: 0.01647130896647771\n",
            "ITERATION_NO.: 94 LOSS_Generator: 6.515589714050293 LOSS_Discriminator: 0.0039198702822128935\n",
            "ITERATION_NO.: 95 LOSS_Generator: 6.358210563659668 LOSS_Discriminator: 0.1513105829556783\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.806418418884277 LOSS_Discriminator: 0.12133278449376424\n",
            "ITERATION_NO.: 97 LOSS_Generator: 4.572439193725586 LOSS_Discriminator: 0.1282514532407125\n",
            "ITERATION_NO.: 98 LOSS_Generator: 4.612639427185059 LOSS_Discriminator: 0.029933663705984753\n",
            "ITERATION_NO.: 99 LOSS_Generator: 4.090622901916504 LOSS_Discriminator: 0.02327500283718109\n",
            "ITERATION_NO.: 100 LOSS_Generator: 4.2378082275390625 LOSS_Discriminator: 0.08274216453234355\n",
            "ITERATION_NO.: 101 LOSS_Generator: 4.650511741638184 LOSS_Discriminator: 0.10734054446220398\n",
            "ITERATION_NO.: 102 LOSS_Generator: 4.981128692626953 LOSS_Discriminator: 0.04963591198126475\n",
            "ITERATION_NO.: 103 LOSS_Generator: 4.990589618682861 LOSS_Discriminator: 0.23189614216486612\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.138216972351074 LOSS_Discriminator: 0.10034671425819397\n",
            "ITERATION_NO.: 105 LOSS_Generator: 4.979288578033447 LOSS_Discriminator: 0.017637586841980617\n",
            "ITERATION_NO.: 106 LOSS_Generator: 4.9344072341918945 LOSS_Discriminator: 0.04791522026062012\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.5783371925354 LOSS_Discriminator: 0.06523712476094563\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.560768127441406 LOSS_Discriminator: 0.05456199745337168\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.72503662109375 LOSS_Discriminator: 0.08797586957613628\n",
            "ITERATION_NO.: 110 LOSS_Generator: 4.320070743560791 LOSS_Discriminator: 0.2565532128016154\n",
            "ITERATION_NO.: 111 LOSS_Generator: 3.760120391845703 LOSS_Discriminator: 0.08659583330154419\n",
            "ITERATION_NO.: 112 LOSS_Generator: 4.35867977142334 LOSS_Discriminator: 0.13731681307156882\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.27109432220459 LOSS_Discriminator: 0.06445469955603282\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.808464050292969 LOSS_Discriminator: 0.014474565784136454\n",
            "ITERATION_NO.: 115 LOSS_Generator: 6.54981803894043 LOSS_Discriminator: 0.1042544146378835\n",
            "ITERATION_NO.: 116 LOSS_Generator: 6.044015884399414 LOSS_Discriminator: 0.19956501324971518\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.433615684509277 LOSS_Discriminator: 0.1335375408331553\n",
            "ITERATION_NO.: 118 LOSS_Generator: 4.4493937492370605 LOSS_Discriminator: 0.08908286690711975\n",
            "ITERATION_NO.: 119 LOSS_Generator: 3.5343008041381836 LOSS_Discriminator: 0.09521052241325378\n",
            "ITERATION_NO.: 120 LOSS_Generator: 4.079900741577148 LOSS_Discriminator: 0.1603387494881948\n",
            "ITERATION_NO.: 121 LOSS_Generator: 4.163206577301025 LOSS_Discriminator: 0.1825480262438456\n",
            "ITERATION_NO.: 122 LOSS_Generator: 4.699941635131836 LOSS_Discriminator: 0.13136335213979086\n",
            "ITERATION_NO.: 123 LOSS_Generator: 4.417037010192871 LOSS_Discriminator: 0.07387601335843404\n",
            "ITERATION_NO.: 124 LOSS_Generator: 4.916674613952637 LOSS_Discriminator: 0.053175499041875206\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.330344200134277 LOSS_Discriminator: 0.08096774419148763\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.602578163146973 LOSS_Discriminator: 0.05672199030717214\n",
            "ITERATION_NO.: 127 LOSS_Generator: 6.137062072753906 LOSS_Discriminator: 0.025405908624331158\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.86586856842041 LOSS_Discriminator: 0.057623659571011863\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.592545986175537 LOSS_Discriminator: 0.09908233086268108\n",
            "ITERATION_NO.: 130 LOSS_Generator: 4.39455509185791 LOSS_Discriminator: 0.1058432658513387\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.136320114135742 LOSS_Discriminator: 0.08845406770706177\n",
            "ITERATION_NO.: 132 LOSS_Generator: 4.149692058563232 LOSS_Discriminator: 0.12603121995925903\n",
            "ITERATION_NO.: 133 LOSS_Generator: 4.750313758850098 LOSS_Discriminator: 0.058084964752197266\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.711752891540527 LOSS_Discriminator: 0.17674559354782104\n",
            "ITERATION_NO.: 135 LOSS_Generator: 6.034066200256348 LOSS_Discriminator: 0.016605183482170105\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.876862525939941 LOSS_Discriminator: 0.06658451755841573\n",
            "ITERATION_NO.: 137 LOSS_Generator: 6.216188430786133 LOSS_Discriminator: 0.0038896510377526283\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.112316131591797 LOSS_Discriminator: 0.003548537070552508\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.184998512268066 LOSS_Discriminator: 0.15363313754399618\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.709916114807129 LOSS_Discriminator: 0.007305546353260676\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.455988883972168 LOSS_Discriminator: 0.009932486340403557\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.687229633331299 LOSS_Discriminator: 0.01045664151509603\n",
            "ITERATION_NO.: 143 LOSS_Generator: 6.126164436340332 LOSS_Discriminator: 0.008663738146424294\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.433184623718262 LOSS_Discriminator: 0.07376668353875478\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.160156726837158 LOSS_Discriminator: 0.08471867442131042\n",
            "ITERATION_NO.: 146 LOSS_Generator: 4.702106475830078 LOSS_Discriminator: 0.1906848152478536\n",
            "ITERATION_NO.: 147 LOSS_Generator: 4.908031463623047 LOSS_Discriminator: 0.16441275676091513\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.11328649520874 LOSS_Discriminator: 0.02657535920540492\n",
            "ITERATION_NO.: 149 LOSS_Generator: 4.3463592529296875 LOSS_Discriminator: 0.22867457071940103\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.024710178375244 LOSS_Discriminator: 0.1251955529054006\n",
            "ITERATION_NO.: 151 LOSS_Generator: 4.625990390777588 LOSS_Discriminator: 0.19588669141133627\n",
            "ITERATION_NO.: 152 LOSS_Generator: 4.77138090133667 LOSS_Discriminator: 0.03503696868817011\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.744462013244629 LOSS_Discriminator: 0.02041852722565333\n",
            "ITERATION_NO.: 154 LOSS_Generator: 6.082863807678223 LOSS_Discriminator: 0.00601209079225858\n",
            "ITERATION_NO.: 155 LOSS_Generator: 6.654330253601074 LOSS_Discriminator: 0.16046535968780518\n",
            "ITERATION_NO.: 156 LOSS_Generator: 6.330567836761475 LOSS_Discriminator: 0.37911224365234375\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.173506736755371 LOSS_Discriminator: 0.009958332404494286\n",
            "ITERATION_NO.: 158 LOSS_Generator: 4.140809059143066 LOSS_Discriminator: 0.31044914325078327\n",
            "ITERATION_NO.: 159 LOSS_Generator: 2.979233741760254 LOSS_Discriminator: 0.2129610776901245\n",
            "ITERATION_NO.: 160 LOSS_Generator: 3.939438819885254 LOSS_Discriminator: 0.147417018810908\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.274343013763428 LOSS_Discriminator: 0.07220556338628133\n",
            "ITERATION_NO.: 162 LOSS_Generator: 6.238413333892822 LOSS_Discriminator: 0.008771097908417383\n",
            "ITERATION_NO.: 163 LOSS_Generator: 7.129100799560547 LOSS_Discriminator: 0.030862249433994293\n",
            "ITERATION_NO.: 164 LOSS_Generator: 7.647975444793701 LOSS_Discriminator: 0.091946542263031\n",
            "ITERATION_NO.: 165 LOSS_Generator: 7.037714004516602 LOSS_Discriminator: 0.09802813331286113\n",
            "ITERATION_NO.: 166 LOSS_Generator: 6.9209675788879395 LOSS_Discriminator: 0.002639520913362503\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.543456554412842 LOSS_Discriminator: 0.03276703258355459\n",
            "ITERATION_NO.: 168 LOSS_Generator: 6.389581680297852 LOSS_Discriminator: 0.011050386975208918\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.494539260864258 LOSS_Discriminator: 0.15873616933822632\n",
            "ITERATION_NO.: 170 LOSS_Generator: 4.017111778259277 LOSS_Discriminator: 0.262021541595459\n",
            "ITERATION_NO.: 171 LOSS_Generator: 2.9610538482666016 LOSS_Discriminator: 0.25335903962453205\n",
            "ITERATION_NO.: 172 LOSS_Generator: 4.049783706665039 LOSS_Discriminator: 0.21771782636642456\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.440656661987305 LOSS_Discriminator: 0.11267413695653279\n",
            "ITERATION_NO.: 174 LOSS_Generator: 6.314803123474121 LOSS_Discriminator: 0.05454187095165253\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.846260070800781 LOSS_Discriminator: 0.004785855300724506\n",
            "ITERATION_NO.: 176 LOSS_Generator: 7.165455341339111 LOSS_Discriminator: 0.21753843625386557\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.488739967346191 LOSS_Discriminator: 0.1377142369747162\n",
            "ITERATION_NO.: 178 LOSS_Generator: 6.104376316070557 LOSS_Discriminator: 0.1359664797782898\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.240169048309326 LOSS_Discriminator: 0.06746484835942586\n",
            "ITERATION_NO.: 180 LOSS_Generator: 4.311614036560059 LOSS_Discriminator: 0.0748028854529063\n",
            "ITERATION_NO.: 181 LOSS_Generator: 3.9589757919311523 LOSS_Discriminator: 0.07176081836223602\n",
            "ITERATION_NO.: 182 LOSS_Generator: 4.052718639373779 LOSS_Discriminator: 0.05034992595513662\n",
            "ITERATION_NO.: 183 LOSS_Generator: 4.919869422912598 LOSS_Discriminator: 0.06715081632137299\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.30035400390625 LOSS_Discriminator: 0.01934707909822464\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.75555419921875 LOSS_Discriminator: 0.008498882253964743\n",
            "ITERATION_NO.: 186 LOSS_Generator: 6.114648342132568 LOSS_Discriminator: 0.0682023564974467\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.855319023132324 LOSS_Discriminator: 0.006142850965261459\n",
            "ITERATION_NO.: 188 LOSS_Generator: 6.644213676452637 LOSS_Discriminator: 0.005296662449836731\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.868854522705078 LOSS_Discriminator: 0.24097285668055216\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.475211143493652 LOSS_Discriminator: 0.08832673231760661\n",
            "ITERATION_NO.: 191 LOSS_Generator: 4.833998203277588 LOSS_Discriminator: 0.18857892354329428\n",
            "ITERATION_NO.: 192 LOSS_Generator: 4.03340482711792 LOSS_Discriminator: 0.03834375490744909\n",
            "ITERATION_NO.: 193 LOSS_Generator: 4.118434906005859 LOSS_Discriminator: 0.0708077996969223\n",
            "ITERATION_NO.: 194 LOSS_Generator: 4.4486308097839355 LOSS_Discriminator: 0.03502664466698965\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.3097076416015625 LOSS_Discriminator: 0.04279508193333944\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.57767391204834 LOSS_Discriminator: 0.27641626199086505\n",
            "ITERATION_NO.: 197 LOSS_Generator: 4.952467918395996 LOSS_Discriminator: 0.014177519828081131\n",
            "ITERATION_NO.: 198 LOSS_Generator: 4.359605312347412 LOSS_Discriminator: 0.03528149425983429\n",
            "ITERATION_NO.: 199 LOSS_Generator: 4.359837532043457 LOSS_Discriminator: 0.048128644625345864\n",
            "ITERATION_NO.: 200 LOSS_Generator: 4.864793300628662 LOSS_Discriminator: 0.06192887822786967\n",
            "ITERATION_NO.: 201 LOSS_Generator: 4.807676315307617 LOSS_Discriminator: 0.08237116535504659\n",
            "ITERATION_NO.: 202 LOSS_Generator: 4.553259372711182 LOSS_Discriminator: 0.3209088246027629\n",
            "ITERATION_NO.: 203 LOSS_Generator: 4.023047924041748 LOSS_Discriminator: 0.14440828561782837\n",
            "ITERATION_NO.: 204 LOSS_Generator: 4.361264228820801 LOSS_Discriminator: 0.07716019948323567\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.10772705078125 LOSS_Discriminator: 0.0846852958202362\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.739072799682617 LOSS_Discriminator: 0.12256316343943278\n",
            "ITERATION_NO.: 207 LOSS_Generator: 6.381771087646484 LOSS_Discriminator: 0.026561712225278217\n",
            "ITERATION_NO.: 208 LOSS_Generator: 6.473089218139648 LOSS_Discriminator: 0.008181802307566008\n",
            "ITERATION_NO.: 209 LOSS_Generator: 6.966514587402344 LOSS_Discriminator: 0.003369602064291636\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.4851861000061035 LOSS_Discriminator: 0.5382879972457886\n",
            "ITERATION_NO.: 211 LOSS_Generator: 4.484906196594238 LOSS_Discriminator: 0.30337297916412354\n",
            "ITERATION_NO.: 212 LOSS_Generator: 3.100874185562134 LOSS_Discriminator: 0.16979146003723145\n",
            "ITERATION_NO.: 213 LOSS_Generator: 3.534116268157959 LOSS_Discriminator: 0.35184534390767414\n",
            "ITERATION_NO.: 214 LOSS_Generator: 4.326563835144043 LOSS_Discriminator: 0.12830479939778647\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.628970146179199 LOSS_Discriminator: 0.03741436700026194\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.21431303024292 LOSS_Discriminator: 0.10674373308817546\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.1895833015441895 LOSS_Discriminator: 0.2927786509195964\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.008736610412598 LOSS_Discriminator: 0.2588142156600952\n",
            "ITERATION_NO.: 219 LOSS_Generator: 4.879717826843262 LOSS_Discriminator: 0.19850683212280273\n",
            "ITERATION_NO.: 220 LOSS_Generator: 3.9109549522399902 LOSS_Discriminator: 0.30015259981155396\n",
            "ITERATION_NO.: 221 LOSS_Generator: 3.288451671600342 LOSS_Discriminator: 0.1759557525316874\n",
            "ITERATION_NO.: 222 LOSS_Generator: 3.532285690307617 LOSS_Discriminator: 0.20644938945770264\n",
            "ITERATION_NO.: 223 LOSS_Generator: 3.896115303039551 LOSS_Discriminator: 0.11873058478037517\n",
            "ITERATION_NO.: 224 LOSS_Generator: 5.474887371063232 LOSS_Discriminator: 0.055370792746543884\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.829739093780518 LOSS_Discriminator: 0.01698913425207138\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.235112190246582 LOSS_Discriminator: 0.17832501729329428\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.42911434173584 LOSS_Discriminator: 0.10970383882522583\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.282095432281494 LOSS_Discriminator: 0.12956050038337708\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.878813743591309 LOSS_Discriminator: 0.17061376571655273\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.272763252258301 LOSS_Discriminator: 0.05098748207092285\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.303874969482422 LOSS_Discriminator: 0.06354458630084991\n",
            "ITERATION_NO.: 232 LOSS_Generator: 4.597566604614258 LOSS_Discriminator: 0.02678978939851125\n",
            "ITERATION_NO.: 233 LOSS_Generator: 4.4538445472717285 LOSS_Discriminator: 0.1490913728872935\n",
            "ITERATION_NO.: 234 LOSS_Generator: 4.281421184539795 LOSS_Discriminator: 0.10247618953386943\n",
            "ITERATION_NO.: 235 LOSS_Generator: 4.18708610534668 LOSS_Discriminator: 0.09655864040056865\n",
            "ITERATION_NO.: 236 LOSS_Generator: 4.135965347290039 LOSS_Discriminator: 0.08242591718832652\n",
            "ITERATION_NO.: 237 LOSS_Generator: 4.061427116394043 LOSS_Discriminator: 0.09397108356157939\n",
            "ITERATION_NO.: 238 LOSS_Generator: 4.501431465148926 LOSS_Discriminator: 0.16610821088155112\n",
            "ITERATION_NO.: 239 LOSS_Generator: 4.1969313621521 LOSS_Discriminator: 0.13314620653788248\n",
            "ITERATION_NO.: 240 LOSS_Generator: 4.3202691078186035 LOSS_Discriminator: 0.02610787997643153\n",
            "ITERATION_NO.: 241 LOSS_Generator: 4.678339004516602 LOSS_Discriminator: 0.02955544243256251\n",
            "ITERATION_NO.: 242 LOSS_Generator: 4.7319440841674805 LOSS_Discriminator: 0.022259424130121868\n",
            "ITERATION_NO.: 243 LOSS_Generator: 4.693475246429443 LOSS_Discriminator: 0.11820581555366516\n",
            "ITERATION_NO.: 244 LOSS_Generator: 4.490299224853516 LOSS_Discriminator: 0.2111070156097412\n",
            "ITERATION_NO.: 245 LOSS_Generator: 4.232393264770508 LOSS_Discriminator: 0.17565184831619263\n",
            "ITERATION_NO.: 246 LOSS_Generator: 3.4290146827697754 LOSS_Discriminator: 0.20645177364349365\n",
            "ITERATION_NO.: 247 LOSS_Generator: 3.7841410636901855 LOSS_Discriminator: 0.10782949129740398\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.291202068328857 LOSS_Discriminator: 0.072600523630778\n",
            "ITERATION_NO.: 249 LOSS_Generator: 6.094518661499023 LOSS_Discriminator: 0.10839649041493733\n",
            "ITERATION_NO.: 250 LOSS_Generator: 6.477884292602539 LOSS_Discriminator: 0.14671730995178223\n",
            "ITERATION_NO.: 251 LOSS_Generator: 6.203810691833496 LOSS_Discriminator: 0.32088271776835126\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.1041717529296875 LOSS_Discriminator: 0.1672118902206421\n",
            "ITERATION_NO.: 253 LOSS_Generator: 4.7628397941589355 LOSS_Discriminator: 0.117916206518809\n",
            "ITERATION_NO.: 254 LOSS_Generator: 3.752796173095703 LOSS_Discriminator: 0.08176567653814952\n",
            "ITERATION_NO.: 255 LOSS_Generator: 3.5115678310394287 LOSS_Discriminator: 0.10143458843231201\n",
            "ITERATION_NO.: 256 LOSS_Generator: 4.236865043640137 LOSS_Discriminator: 0.06840806206067403\n",
            "ITERATION_NO.: 257 LOSS_Generator: 4.912520885467529 LOSS_Discriminator: 0.03743722289800644\n",
            "ITERATION_NO.: 258 LOSS_Generator: 6.138623237609863 LOSS_Discriminator: 0.07004083196322124\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.534256935119629 LOSS_Discriminator: 0.19178283214569092\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.166643142700195 LOSS_Discriminator: 0.010800226281086603\n",
            "ITERATION_NO.: 261 LOSS_Generator: 4.868159294128418 LOSS_Discriminator: 0.01434820145368576\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.145336151123047 LOSS_Discriminator: 0.013941814502080282\n",
            "ITERATION_NO.: 263 LOSS_Generator: 4.708932876586914 LOSS_Discriminator: 0.03030133992433548\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.029737949371338 LOSS_Discriminator: 0.11409060160319011\n",
            "ITERATION_NO.: 265 LOSS_Generator: 4.598011016845703 LOSS_Discriminator: 0.04349782566229502\n",
            "ITERATION_NO.: 266 LOSS_Generator: 4.461264610290527 LOSS_Discriminator: 0.19699764251708984\n",
            "ITERATION_NO.: 267 LOSS_Generator: 3.9875030517578125 LOSS_Discriminator: 0.09856438636779785\n",
            "ITERATION_NO.: 268 LOSS_Generator: 4.082753658294678 LOSS_Discriminator: 0.0665092021226883\n",
            "ITERATION_NO.: 269 LOSS_Generator: 4.363396644592285 LOSS_Discriminator: 0.08892881870269775\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.083094596862793 LOSS_Discriminator: 0.02498491108417511\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.094473361968994 LOSS_Discriminator: 0.1485002338886261\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.144166946411133 LOSS_Discriminator: 0.0405438169836998\n",
            "ITERATION_NO.: 273 LOSS_Generator: 4.857511520385742 LOSS_Discriminator: 0.2208628257115682\n",
            "ITERATION_NO.: 274 LOSS_Generator: 4.828815460205078 LOSS_Discriminator: 0.0446422795454661\n",
            "ITERATION_NO.: 275 LOSS_Generator: 4.979077339172363 LOSS_Discriminator: 0.026207442084948223\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.117864608764648 LOSS_Discriminator: 0.013547532260417938\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.119527339935303 LOSS_Discriminator: 0.01260920117298762\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.249856948852539 LOSS_Discriminator: 0.18425206343332926\n",
            "ITERATION_NO.: 279 LOSS_Generator: 4.875423431396484 LOSS_Discriminator: 0.01652677357196808\n",
            "ITERATION_NO.: 280 LOSS_Generator: 3.9702816009521484 LOSS_Discriminator: 0.457497239112854\n",
            "ITERATION_NO.: 281 LOSS_Generator: 3.1157400608062744 LOSS_Discriminator: 0.11588288346926372\n",
            "ITERATION_NO.: 282 LOSS_Generator: 4.287855625152588 LOSS_Discriminator: 0.16959917545318604\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.521055221557617 LOSS_Discriminator: 0.07687917351722717\n",
            "ITERATION_NO.: 284 LOSS_Generator: 6.524214744567871 LOSS_Discriminator: 0.1399348477522532\n",
            "ITERATION_NO.: 285 LOSS_Generator: 6.149674892425537 LOSS_Discriminator: 0.4914841254552205\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.944830894470215 LOSS_Discriminator: 0.011023027201493582\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.041604518890381 LOSS_Discriminator: 0.005603171264131864\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.908426761627197 LOSS_Discriminator: 0.09350309769312541\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.5694403648376465 LOSS_Discriminator: 0.11290319760640462\n",
            "ITERATION_NO.: 290 LOSS_Generator: 4.8691558837890625 LOSS_Discriminator: 0.15681427717208862\n",
            "ITERATION_NO.: 291 LOSS_Generator: 4.249926567077637 LOSS_Discriminator: 0.12339346607526143\n",
            "ITERATION_NO.: 292 LOSS_Generator: 3.724943161010742 LOSS_Discriminator: 0.08433886369069417\n",
            "ITERATION_NO.: 293 LOSS_Generator: 3.8038880825042725 LOSS_Discriminator: 0.1267033815383911\n",
            "ITERATION_NO.: 294 LOSS_Generator: 4.396790981292725 LOSS_Discriminator: 0.047730823357899986\n",
            "ITERATION_NO.: 295 LOSS_Generator: 4.924751281738281 LOSS_Discriminator: 0.11182833711306255\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.188736915588379 LOSS_Discriminator: 0.01576448107759158\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.341458797454834 LOSS_Discriminator: 0.21300220489501953\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.021451473236084 LOSS_Discriminator: 0.008723437165220579\n",
            "ITERATION_NO.: 299 LOSS_Generator: 4.615081310272217 LOSS_Discriminator: 0.034269715348879494\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.039261817932129 LOSS_Discriminator: 0.1622622013092041\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.215349197387695 LOSS_Discriminator: 0.08235130210717519\n",
            "ITERATION_NO.: 302 LOSS_Generator: 4.468997478485107 LOSS_Discriminator: 0.16708044211069742\n",
            "ITERATION_NO.: 303 LOSS_Generator: 4.487358093261719 LOSS_Discriminator: 0.053143719832102455\n",
            "ITERATION_NO.: 304 LOSS_Generator: 4.552916526794434 LOSS_Discriminator: 0.020384001235167187\n",
            "ITERATION_NO.: 305 LOSS_Generator: 4.902921676635742 LOSS_Discriminator: 0.052945807576179504\n",
            "ITERATION_NO.: 306 LOSS_Generator: 4.607008934020996 LOSS_Discriminator: 0.1325315237045288\n",
            "ITERATION_NO.: 307 LOSS_Generator: 4.406802177429199 LOSS_Discriminator: 0.03212358057498932\n",
            "ITERATION_NO.: 308 LOSS_Generator: 4.608348846435547 LOSS_Discriminator: 0.11294281482696533\n",
            "ITERATION_NO.: 309 LOSS_Generator: 3.7463881969451904 LOSS_Discriminator: 0.21419179439544678\n",
            "ITERATION_NO.: 310 LOSS_Generator: 3.8511083126068115 LOSS_Discriminator: 0.0789784590403239\n",
            "ITERATION_NO.: 311 LOSS_Generator: 4.544187545776367 LOSS_Discriminator: 0.08995385964711507\n",
            "ITERATION_NO.: 312 LOSS_Generator: 4.9558563232421875 LOSS_Discriminator: 0.02261567860841751\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.650709629058838 LOSS_Discriminator: 0.01155233383178711\n",
            "ITERATION_NO.: 314 LOSS_Generator: 6.2709431648254395 LOSS_Discriminator: 0.004572533071041107\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.525674343109131 LOSS_Discriminator: 0.10236570239067078\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.163197040557861 LOSS_Discriminator: 0.20188685258229574\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.8493757247924805 LOSS_Discriminator: 0.005519686887661616\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.208456993103027 LOSS_Discriminator: 0.1074294646581014\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.063211441040039 LOSS_Discriminator: 0.20336405436197916\n",
            "ITERATION_NO.: 320 LOSS_Generator: 3.7230143547058105 LOSS_Discriminator: 0.15259154637654623\n",
            "ITERATION_NO.: 321 LOSS_Generator: 3.517350196838379 LOSS_Discriminator: 0.12454528609911601\n",
            "ITERATION_NO.: 322 LOSS_Generator: 4.352025508880615 LOSS_Discriminator: 0.07352302471796672\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.38767671585083 LOSS_Discriminator: 0.07214640577634175\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.79641056060791 LOSS_Discriminator: 0.02368419865767161\n",
            "ITERATION_NO.: 325 LOSS_Generator: 6.262479782104492 LOSS_Discriminator: 0.003999959987898667\n",
            "ITERATION_NO.: 326 LOSS_Generator: 6.051944255828857 LOSS_Discriminator: 0.1605233152707418\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.838932514190674 LOSS_Discriminator: 0.3459092378616333\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.014910697937012 LOSS_Discriminator: 0.011911574751138687\n",
            "ITERATION_NO.: 329 LOSS_Generator: 4.267145156860352 LOSS_Discriminator: 0.08779885371526082\n",
            "ITERATION_NO.: 330 LOSS_Generator: 3.9410409927368164 LOSS_Discriminator: 0.09383634726206462\n",
            "ITERATION_NO.: 331 LOSS_Generator: 4.408307075500488 LOSS_Discriminator: 0.058729911843935646\n",
            "ITERATION_NO.: 332 LOSS_Generator: 4.178799152374268 LOSS_Discriminator: 0.140849898258845\n",
            "ITERATION_NO.: 333 LOSS_Generator: 4.475318908691406 LOSS_Discriminator: 0.03879545380671819\n",
            "ITERATION_NO.: 334 LOSS_Generator: 4.302301406860352 LOSS_Discriminator: 0.20588813225428262\n",
            "ITERATION_NO.: 335 LOSS_Generator: 4.152726173400879 LOSS_Discriminator: 0.24344823757807413\n",
            "ITERATION_NO.: 336 LOSS_Generator: 4.489020347595215 LOSS_Discriminator: 0.14586380124092102\n",
            "ITERATION_NO.: 337 LOSS_Generator: 4.057860374450684 LOSS_Discriminator: 0.08425162235895793\n",
            "ITERATION_NO.: 338 LOSS_Generator: 4.27447509765625 LOSS_Discriminator: 0.11011515061060588\n",
            "ITERATION_NO.: 339 LOSS_Generator: 4.939144134521484 LOSS_Discriminator: 0.12148090203603108\n",
            "ITERATION_NO.: 340 LOSS_Generator: 4.805354118347168 LOSS_Discriminator: 0.0868282417456309\n",
            "ITERATION_NO.: 341 LOSS_Generator: 4.8898162841796875 LOSS_Discriminator: 0.13385681311289468\n",
            "ITERATION_NO.: 342 LOSS_Generator: 4.061346530914307 LOSS_Discriminator: 0.12772415081659952\n",
            "ITERATION_NO.: 343 LOSS_Generator: 4.404375076293945 LOSS_Discriminator: 0.15259099006652832\n",
            "ITERATION_NO.: 344 LOSS_Generator: 4.767821311950684 LOSS_Discriminator: 0.11197559038798015\n",
            "ITERATION_NO.: 345 LOSS_Generator: 4.804569244384766 LOSS_Discriminator: 0.04363457361857096\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.274618148803711 LOSS_Discriminator: 0.030849461754163105\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.37979793548584 LOSS_Discriminator: 0.12363928556442261\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.290327072143555 LOSS_Discriminator: 0.23330791791280112\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.033306121826172 LOSS_Discriminator: 0.02532449613014857\n",
            "ITERATION_NO.: 350 LOSS_Generator: 4.820445537567139 LOSS_Discriminator: 0.03045033911863963\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.209637641906738 LOSS_Discriminator: 0.11979492505391438\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.470826625823975 LOSS_Discriminator: 0.06039509177207947\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.001340866088867 LOSS_Discriminator: 0.023181932667891186\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.014877796173096 LOSS_Discriminator: 0.08198111752669017\n",
            "ITERATION_NO.: 355 LOSS_Generator: 4.852067947387695 LOSS_Discriminator: 0.03284880518913269\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.361863136291504 LOSS_Discriminator: 0.1301728387673696\n",
            "ITERATION_NO.: 357 LOSS_Generator: 4.351287841796875 LOSS_Discriminator: 0.09179304043451945\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.1051025390625 LOSS_Discriminator: 0.04769585529963175\n",
            "ITERATION_NO.: 359 LOSS_Generator: 4.707498550415039 LOSS_Discriminator: 0.09522408246994019\n",
            "ITERATION_NO.: 360 LOSS_Generator: 4.81324577331543 LOSS_Discriminator: 0.05404404799143473\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.17926025390625 LOSS_Discriminator: 0.07416038711865743\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.372124671936035 LOSS_Discriminator: 0.0705499251683553\n",
            "ITERATION_NO.: 363 LOSS_Generator: 4.793946266174316 LOSS_Discriminator: 0.050151328245798744\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.4954376220703125 LOSS_Discriminator: 0.03198070079088211\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.2891435623168945 LOSS_Discriminator: 0.03375032295783361\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.42393684387207 LOSS_Discriminator: 0.032442741096019745\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.613193511962891 LOSS_Discriminator: 0.18094609181086221\n",
            "ITERATION_NO.: 368 LOSS_Generator: 4.913425445556641 LOSS_Discriminator: 0.23290028174718222\n",
            "ITERATION_NO.: 369 LOSS_Generator: 4.657042503356934 LOSS_Discriminator: 0.046153699358304344\n",
            "ITERATION_NO.: 370 LOSS_Generator: 4.6787109375 LOSS_Discriminator: 0.03714251766602198\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.422606468200684 LOSS_Discriminator: 0.05612175166606903\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.975380897521973 LOSS_Discriminator: 0.010381307452917099\n",
            "ITERATION_NO.: 373 LOSS_Generator: 4.744754314422607 LOSS_Discriminator: 0.23890572786331177\n",
            "ITERATION_NO.: 374 LOSS_Generator: 4.132576942443848 LOSS_Discriminator: 0.027436013023058575\n",
            "ITERATION_NO.: 375 LOSS_Generator: 4.281930923461914 LOSS_Discriminator: 0.08588546514511108\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.538724899291992 LOSS_Discriminator: 0.05080170432726542\n",
            "ITERATION_NO.: 377 LOSS_Generator: 6.092933177947998 LOSS_Discriminator: 0.009386055792371431\n",
            "ITERATION_NO.: 378 LOSS_Generator: 6.264636993408203 LOSS_Discriminator: 0.23693724473317465\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.9603753089904785 LOSS_Discriminator: 0.026842008034388225\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.3415021896362305 LOSS_Discriminator: 0.04055530329545339\n",
            "ITERATION_NO.: 381 LOSS_Generator: 4.963752746582031 LOSS_Discriminator: 0.035045976440111794\n",
            "ITERATION_NO.: 382 LOSS_Generator: 4.617766380310059 LOSS_Discriminator: 0.01893411825100581\n",
            "ITERATION_NO.: 383 LOSS_Generator: 4.932960510253906 LOSS_Discriminator: 0.04706131915251414\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.272204399108887 LOSS_Discriminator: 0.10369447867075603\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.312204360961914 LOSS_Discriminator: 0.028219883640607197\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.471202850341797 LOSS_Discriminator: 0.09010182817776997\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.76344108581543 LOSS_Discriminator: 0.07384517292181651\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.422757148742676 LOSS_Discriminator: 0.024028226733207703\n",
            "ITERATION_NO.: 389 LOSS_Generator: 4.789961814880371 LOSS_Discriminator: 0.01860673228899638\n",
            "ITERATION_NO.: 390 LOSS_Generator: 4.906268119812012 LOSS_Discriminator: 0.1611331800619761\n",
            "ITERATION_NO.: 391 LOSS_Generator: 4.704411506652832 LOSS_Discriminator: 0.05504268407821655\n",
            "ITERATION_NO.: 392 LOSS_Generator: 4.506175518035889 LOSS_Discriminator: 0.16891853014628092\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.45235013961792 LOSS_Discriminator: 0.02465846637884776\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.790236949920654 LOSS_Discriminator: 0.01227769503990809\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.462319850921631 LOSS_Discriminator: 0.1290533939997355\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.439483642578125 LOSS_Discriminator: 0.09337996443112691\n",
            "ITERATION_NO.: 397 LOSS_Generator: 4.1771345138549805 LOSS_Discriminator: 0.13143878181775412\n",
            "ITERATION_NO.: 398 LOSS_Generator: 4.199742317199707 LOSS_Discriminator: 0.08054779966672261\n",
            "ITERATION_NO.: 399 LOSS_Generator: 3.859372138977051 LOSS_Discriminator: 0.09610203901926677\n",
            "ITERATION_NO.: 400 LOSS_Generator: 4.712935447692871 LOSS_Discriminator: 0.06357590854167938\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.318756103515625 LOSS_Discriminator: 0.02376781900723775\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.866845607757568 LOSS_Discriminator: 0.11757151285807292\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.057277679443359 LOSS_Discriminator: 0.18897736072540283\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.313629150390625 LOSS_Discriminator: 0.019315538307030995\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.254115581512451 LOSS_Discriminator: 0.017058156430721283\n",
            "ITERATION_NO.: 406 LOSS_Generator: 4.856067180633545 LOSS_Discriminator: 0.14188583691914877\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.0144758224487305 LOSS_Discriminator: 0.06257802744706471\n",
            "ITERATION_NO.: 408 LOSS_Generator: 4.926920413970947 LOSS_Discriminator: 0.05775363743305206\n",
            "ITERATION_NO.: 409 LOSS_Generator: 5.562239646911621 LOSS_Discriminator: 0.013651420672734579\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.309914588928223 LOSS_Discriminator: 0.1533189813296\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.007555961608887 LOSS_Discriminator: 0.16955729325612387\n",
            "ITERATION_NO.: 412 LOSS_Generator: 4.259659767150879 LOSS_Discriminator: 0.10772790511449178\n",
            "ITERATION_NO.: 413 LOSS_Generator: 4.369039535522461 LOSS_Discriminator: 0.1630468269189199\n",
            "ITERATION_NO.: 414 LOSS_Generator: 4.737656116485596 LOSS_Discriminator: 0.07005803783734639\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.085887908935547 LOSS_Discriminator: 0.11242298285166423\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.532159328460693 LOSS_Discriminator: 0.19386831919352213\n",
            "ITERATION_NO.: 417 LOSS_Generator: 4.225780963897705 LOSS_Discriminator: 0.0616936186949412\n",
            "ITERATION_NO.: 418 LOSS_Generator: 3.5968680381774902 LOSS_Discriminator: 0.10287181536356609\n",
            "ITERATION_NO.: 419 LOSS_Generator: 4.022367000579834 LOSS_Discriminator: 0.15325095256169638\n",
            "ITERATION_NO.: 420 LOSS_Generator: 4.819915294647217 LOSS_Discriminator: 0.13753372430801392\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.746110439300537 LOSS_Discriminator: 0.18467235565185547\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.082945823669434 LOSS_Discriminator: 0.19929581880569458\n",
            "ITERATION_NO.: 423 LOSS_Generator: 4.2107720375061035 LOSS_Discriminator: 0.08596434195836385\n",
            "ITERATION_NO.: 424 LOSS_Generator: 4.248412132263184 LOSS_Discriminator: 0.04851473867893219\n",
            "ITERATION_NO.: 425 LOSS_Generator: 4.8508100509643555 LOSS_Discriminator: 0.040666709343592324\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.448507785797119 LOSS_Discriminator: 0.012199434141318003\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.607831954956055 LOSS_Discriminator: 0.16500356793403625\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.2139892578125 LOSS_Discriminator: 0.012142804761727652\n",
            "ITERATION_NO.: 429 LOSS_Generator: 6.147394180297852 LOSS_Discriminator: 0.014002196490764618\n",
            "ITERATION_NO.: 430 LOSS_Generator: 6.224180221557617 LOSS_Discriminator: 0.11035036047299702\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.782355308532715 LOSS_Discriminator: 0.010803364217281342\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.641773700714111 LOSS_Discriminator: 0.08777723709742229\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.118996620178223 LOSS_Discriminator: 0.13911434014638266\n",
            "ITERATION_NO.: 434 LOSS_Generator: 3.9424562454223633 LOSS_Discriminator: 0.07708998024463654\n",
            "ITERATION_NO.: 435 LOSS_Generator: 4.100043773651123 LOSS_Discriminator: 0.06339225172996521\n",
            "ITERATION_NO.: 436 LOSS_Generator: 4.561088562011719 LOSS_Discriminator: 0.09392840663592021\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.238544464111328 LOSS_Discriminator: 0.030188923080762226\n",
            "ITERATION_NO.: 438 LOSS_Generator: 4.716182708740234 LOSS_Discriminator: 0.18505388498306274\n",
            "ITERATION_NO.: 439 LOSS_Generator: 4.9455766677856445 LOSS_Discriminator: 0.06608805060386658\n",
            "ITERATION_NO.: 440 LOSS_Generator: 4.976535797119141 LOSS_Discriminator: 0.02362239609162013\n",
            "ITERATION_NO.: 441 LOSS_Generator: 4.927475929260254 LOSS_Discriminator: 0.02968970686197281\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.133756637573242 LOSS_Discriminator: 0.024736866354942322\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.798732757568359 LOSS_Discriminator: 0.09182684620221455\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.689726829528809 LOSS_Discriminator: 0.19476171334584555\n",
            "ITERATION_NO.: 445 LOSS_Generator: 3.917175769805908 LOSS_Discriminator: 0.1778657833735148\n",
            "ITERATION_NO.: 446 LOSS_Generator: 4.056707382202148 LOSS_Discriminator: 0.06733044981956482\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.257732391357422 LOSS_Discriminator: 0.1048981746037801\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.374823570251465 LOSS_Discriminator: 0.011915584405263266\n",
            "ITERATION_NO.: 449 LOSS_Generator: 7.010266304016113 LOSS_Discriminator: 0.11242514848709106\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.657108783721924 LOSS_Discriminator: 0.011585028221209845\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.534692764282227 LOSS_Discriminator: 0.22816840807596842\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.537524700164795 LOSS_Discriminator: 0.3811452388763428\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.860109329223633 LOSS_Discriminator: 0.00977088821431001\n",
            "ITERATION_NO.: 454 LOSS_Generator: 4.57061767578125 LOSS_Discriminator: 0.27423208951950073\n",
            "ITERATION_NO.: 455 LOSS_Generator: 4.0057806968688965 LOSS_Discriminator: 0.025959064563115437\n",
            "ITERATION_NO.: 456 LOSS_Generator: 4.166849136352539 LOSS_Discriminator: 0.20150315761566162\n",
            "ITERATION_NO.: 457 LOSS_Generator: 4.406339645385742 LOSS_Discriminator: 0.07354942957560222\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.140834808349609 LOSS_Discriminator: 0.23797547817230225\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.397576808929443 LOSS_Discriminator: 0.03308198849360148\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.029533386230469 LOSS_Discriminator: 0.19525432586669922\n",
            "ITERATION_NO.: 461 LOSS_Generator: 4.095415115356445 LOSS_Discriminator: 0.2059030532836914\n",
            "ITERATION_NO.: 462 LOSS_Generator: 3.6389079093933105 LOSS_Discriminator: 0.07693853974342346\n",
            "ITERATION_NO.: 463 LOSS_Generator: 3.6602940559387207 LOSS_Discriminator: 0.14410088459650675\n",
            "ITERATION_NO.: 464 LOSS_Generator: 4.052234172821045 LOSS_Discriminator: 0.08532855908075969\n",
            "ITERATION_NO.: 465 LOSS_Generator: 4.876443862915039 LOSS_Discriminator: 0.13952061533927917\n",
            "ITERATION_NO.: 466 LOSS_Generator: 4.790606498718262 LOSS_Discriminator: 0.17183488607406616\n",
            "ITERATION_NO.: 467 LOSS_Generator: 4.8441386222839355 LOSS_Discriminator: 0.03044745574394862\n",
            "ITERATION_NO.: 468 LOSS_Generator: 4.769965648651123 LOSS_Discriminator: 0.10698794325192769\n",
            "ITERATION_NO.: 469 LOSS_Generator: 4.923983573913574 LOSS_Discriminator: 0.06767258048057556\n",
            "ITERATION_NO.: 470 LOSS_Generator: 4.580715656280518 LOSS_Discriminator: 0.02184700717528661\n",
            "ITERATION_NO.: 471 LOSS_Generator: 4.860211372375488 LOSS_Discriminator: 0.015245531996091207\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.830843925476074 LOSS_Discriminator: 0.013833435873190561\n",
            "ITERATION_NO.: 473 LOSS_Generator: 4.926800727844238 LOSS_Discriminator: 0.13402012983957926\n",
            "ITERATION_NO.: 474 LOSS_Generator: 4.432246208190918 LOSS_Discriminator: 0.09902012348175049\n",
            "ITERATION_NO.: 475 LOSS_Generator: 4.39292049407959 LOSS_Discriminator: 0.08283433318138123\n",
            "ITERATION_NO.: 476 LOSS_Generator: 4.394032001495361 LOSS_Discriminator: 0.03797546525796255\n",
            "ITERATION_NO.: 477 LOSS_Generator: 4.405623435974121 LOSS_Discriminator: 0.057180553674697876\n",
            "ITERATION_NO.: 478 LOSS_Generator: 4.279460906982422 LOSS_Discriminator: 0.08689190944035848\n",
            "ITERATION_NO.: 479 LOSS_Generator: 4.763441562652588 LOSS_Discriminator: 0.16185269753138223\n",
            "ITERATION_NO.: 480 LOSS_Generator: 4.435640335083008 LOSS_Discriminator: 0.1315755844116211\n",
            "ITERATION_NO.: 481 LOSS_Generator: 4.5002970695495605 LOSS_Discriminator: 0.04252132773399353\n",
            "ITERATION_NO.: 482 LOSS_Generator: 4.814798355102539 LOSS_Discriminator: 0.03560266892115275\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.029264450073242 LOSS_Discriminator: 0.0769888162612915\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.250246047973633 LOSS_Discriminator: 0.0644459327061971\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.831113815307617 LOSS_Discriminator: 0.015421320994695028\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.559018135070801 LOSS_Discriminator: 0.2750736673672994\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.573696136474609 LOSS_Discriminator: 0.00600947563846906\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.374540328979492 LOSS_Discriminator: 0.01370951160788536\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.310354232788086 LOSS_Discriminator: 0.11694955825805664\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.078855037689209 LOSS_Discriminator: 0.12113482753435771\n",
            "ITERATION_NO.: 491 LOSS_Generator: 4.825638771057129 LOSS_Discriminator: 0.04559118549029032\n",
            "ITERATION_NO.: 492 LOSS_Generator: 4.530360221862793 LOSS_Discriminator: 0.1634661058584849\n",
            "ITERATION_NO.: 493 LOSS_Generator: 4.169271469116211 LOSS_Discriminator: 0.11446456114451091\n",
            "ITERATION_NO.: 494 LOSS_Generator: 3.982332229614258 LOSS_Discriminator: 0.0873953104019165\n",
            "ITERATION_NO.: 495 LOSS_Generator: 4.546558856964111 LOSS_Discriminator: 0.12088316679000854\n",
            "ITERATION_NO.: 496 LOSS_Generator: 4.855450630187988 LOSS_Discriminator: 0.03749606510003408\n",
            "ITERATION_NO.: 497 LOSS_Generator: 4.841829776763916 LOSS_Discriminator: 0.10039889812469482\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.0908002853393555 LOSS_Discriminator: 0.0708993673324585\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.065145492553711 LOSS_Discriminator: 0.18462804953257242\n",
            "ITERATION_NO.: 500 LOSS_Generator: 4.457859039306641 LOSS_Discriminator: 0.06220547358194987\n",
            "ITERATION_NO.: 501 LOSS_Generator: 4.434749603271484 LOSS_Discriminator: 0.059049238761266075\n",
            "ITERATION_NO.: 502 LOSS_Generator: 4.8295207023620605 LOSS_Discriminator: 0.11749462286631267\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.372095108032227 LOSS_Discriminator: 0.056804925203323364\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.102054595947266 LOSS_Discriminator: 0.03242156902949015\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.701210021972656 LOSS_Discriminator: 0.16937812169392905\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.1451921463012695 LOSS_Discriminator: 0.08278274536132812\n",
            "ITERATION_NO.: 507 LOSS_Generator: 4.759894371032715 LOSS_Discriminator: 0.040506343046824135\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.131134033203125 LOSS_Discriminator: 0.02890060345331828\n",
            "ITERATION_NO.: 509 LOSS_Generator: 4.978826522827148 LOSS_Discriminator: 0.07871192693710327\n",
            "ITERATION_NO.: 510 LOSS_Generator: 3.9744863510131836 LOSS_Discriminator: 0.0996734897295634\n",
            "ITERATION_NO.: 511 LOSS_Generator: 4.132760047912598 LOSS_Discriminator: 0.08469189206759135\n",
            "ITERATION_NO.: 512 LOSS_Generator: 4.417372703552246 LOSS_Discriminator: 0.11396327614784241\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.287998199462891 LOSS_Discriminator: 0.055088172356287636\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.7423996925354 LOSS_Discriminator: 0.14389156301816305\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.55196475982666 LOSS_Discriminator: 0.15231468280156454\n",
            "ITERATION_NO.: 516 LOSS_Generator: 3.948869466781616 LOSS_Discriminator: 0.1560786565144857\n",
            "ITERATION_NO.: 517 LOSS_Generator: 3.31057071685791 LOSS_Discriminator: 0.3066782553990682\n",
            "ITERATION_NO.: 518 LOSS_Generator: 4.414409160614014 LOSS_Discriminator: 0.166830042997996\n",
            "ITERATION_NO.: 519 LOSS_Generator: 6.319958686828613 LOSS_Discriminator: 0.035813999672730766\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.434659004211426 LOSS_Discriminator: 0.016768980771303177\n",
            "ITERATION_NO.: 521 LOSS_Generator: 7.682882308959961 LOSS_Discriminator: 0.11475830276807149\n",
            "ITERATION_NO.: 522 LOSS_Generator: 7.908567428588867 LOSS_Discriminator: 0.18435041109720865\n",
            "ITERATION_NO.: 523 LOSS_Generator: 7.872015476226807 LOSS_Discriminator: 0.08301851153373718\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.866358280181885 LOSS_Discriminator: 0.23509474595387778\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.142882347106934 LOSS_Discriminator: 0.08319183190663655\n",
            "ITERATION_NO.: 526 LOSS_Generator: 4.385404586791992 LOSS_Discriminator: 0.019204108665386837\n",
            "ITERATION_NO.: 527 LOSS_Generator: 3.7471697330474854 LOSS_Discriminator: 0.10894354184468587\n",
            "ITERATION_NO.: 528 LOSS_Generator: 4.027408599853516 LOSS_Discriminator: 0.13300933440526327\n",
            "ITERATION_NO.: 529 LOSS_Generator: 4.439125061035156 LOSS_Discriminator: 0.11528025070826213\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.120891571044922 LOSS_Discriminator: 0.0640330861012141\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.167057037353516 LOSS_Discriminator: 0.12350893020629883\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.249510765075684 LOSS_Discriminator: 0.10238828261693318\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.380210876464844 LOSS_Discriminator: 0.0116101602713267\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.821859836578369 LOSS_Discriminator: 0.01182759553194046\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.519021034240723 LOSS_Discriminator: 0.21475593249003092\n",
            "ITERATION_NO.: 536 LOSS_Generator: 4.624331951141357 LOSS_Discriminator: 0.05798581739266714\n",
            "ITERATION_NO.: 537 LOSS_Generator: 4.634096145629883 LOSS_Discriminator: 0.09851602713267009\n",
            "ITERATION_NO.: 538 LOSS_Generator: 3.967496156692505 LOSS_Discriminator: 0.16167563199996948\n",
            "ITERATION_NO.: 539 LOSS_Generator: 4.772762298583984 LOSS_Discriminator: 0.09546534220377605\n",
            "ITERATION_NO.: 540 LOSS_Generator: 4.886906623840332 LOSS_Discriminator: 0.040743122498194374\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.265061378479004 LOSS_Discriminator: 0.0219414879878362\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.366334915161133 LOSS_Discriminator: 0.08434017499287923\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.342279434204102 LOSS_Discriminator: 0.014449350535869598\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.071530342102051 LOSS_Discriminator: 0.14932242035865784\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.060065269470215 LOSS_Discriminator: 0.1356092890103658\n",
            "ITERATION_NO.: 546 LOSS_Generator: 4.738828659057617 LOSS_Discriminator: 0.05486858387788137\n",
            "ITERATION_NO.: 547 LOSS_Generator: 4.442437171936035 LOSS_Discriminator: 0.0778399407863617\n",
            "ITERATION_NO.: 548 LOSS_Generator: 4.429324150085449 LOSS_Discriminator: 0.11349326372146606\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.1905059814453125 LOSS_Discriminator: 0.015550905217727026\n",
            "ITERATION_NO.: 550 LOSS_Generator: 4.905723571777344 LOSS_Discriminator: 0.1222947637240092\n",
            "ITERATION_NO.: 551 LOSS_Generator: 4.975242614746094 LOSS_Discriminator: 0.05224125583966573\n",
            "ITERATION_NO.: 552 LOSS_Generator: 4.334628105163574 LOSS_Discriminator: 0.2603154182434082\n",
            "ITERATION_NO.: 553 LOSS_Generator: 4.3177080154418945 LOSS_Discriminator: 0.047364051143328346\n",
            "ITERATION_NO.: 554 LOSS_Generator: 4.92049503326416 LOSS_Discriminator: 0.11245946089426677\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.877094268798828 LOSS_Discriminator: 0.04069927831490835\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.911929607391357 LOSS_Discriminator: 0.08037303388118744\n",
            "ITERATION_NO.: 557 LOSS_Generator: 6.132762432098389 LOSS_Discriminator: 0.005475412433346112\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.352277755737305 LOSS_Discriminator: 0.003542815645535787\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.027807235717773 LOSS_Discriminator: 0.17224266131718954\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.539577484130859 LOSS_Discriminator: 0.011278295268615087\n",
            "ITERATION_NO.: 561 LOSS_Generator: 4.916534423828125 LOSS_Discriminator: 0.12540497382481894\n",
            "ITERATION_NO.: 562 LOSS_Generator: 3.9425621032714844 LOSS_Discriminator: 0.09528288245201111\n",
            "ITERATION_NO.: 563 LOSS_Generator: 4.200387001037598 LOSS_Discriminator: 0.05022822320461273\n",
            "ITERATION_NO.: 564 LOSS_Generator: 4.628948211669922 LOSS_Discriminator: 0.0385893831650416\n",
            "ITERATION_NO.: 565 LOSS_Generator: 4.881972312927246 LOSS_Discriminator: 0.10589921474456787\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.075629234313965 LOSS_Discriminator: 0.01297370841105779\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.74582576751709 LOSS_Discriminator: 0.03249200930198034\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.611758232116699 LOSS_Discriminator: 0.06352353096008301\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.898509979248047 LOSS_Discriminator: 0.010468263179063797\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.3533124923706055 LOSS_Discriminator: 0.033335370322068535\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.150774955749512 LOSS_Discriminator: 0.03734395653009415\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.1929216384887695 LOSS_Discriminator: 0.01653422663609187\n",
            "ITERATION_NO.: 573 LOSS_Generator: 4.920152187347412 LOSS_Discriminator: 0.025541578729947407\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.325405120849609 LOSS_Discriminator: 0.04384744167327881\n",
            "ITERATION_NO.: 575 LOSS_Generator: 4.980818748474121 LOSS_Discriminator: 0.015258365621169409\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.232110023498535 LOSS_Discriminator: 0.018089362730582554\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.55716609954834 LOSS_Discriminator: 0.09403595328330994\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.332499980926514 LOSS_Discriminator: 0.03239984313646952\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.418060779571533 LOSS_Discriminator: 0.08678936958312988\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.472787380218506 LOSS_Discriminator: 0.021596019466718037\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.452342987060547 LOSS_Discriminator: 0.07548084855079651\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.127475738525391 LOSS_Discriminator: 0.3179702361424764\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.254726409912109 LOSS_Discriminator: 0.036906776328881584\n",
            "ITERATION_NO.: 584 LOSS_Generator: 4.927867412567139 LOSS_Discriminator: 0.03741083045800527\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.189934730529785 LOSS_Discriminator: 0.018833037465810776\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.365134239196777 LOSS_Discriminator: 0.05991465846697489\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.155345916748047 LOSS_Discriminator: 0.2733728885650635\n",
            "ITERATION_NO.: 588 LOSS_Generator: 4.832664489746094 LOSS_Discriminator: 0.029929175972938538\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.202186584472656 LOSS_Discriminator: 0.043085177739461265\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.265221118927002 LOSS_Discriminator: 0.12642512718836466\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.506757736206055 LOSS_Discriminator: 0.2500062783559163\n",
            "ITERATION_NO.: 592 LOSS_Generator: 4.066190719604492 LOSS_Discriminator: 0.1947752038637797\n",
            "ITERATION_NO.: 593 LOSS_Generator: 4.5253214836120605 LOSS_Discriminator: 0.1630860964457194\n",
            "ITERATION_NO.: 594 LOSS_Generator: 4.603482246398926 LOSS_Discriminator: 0.25821133454640705\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.487163543701172 LOSS_Discriminator: 0.05033027629057566\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.853263854980469 LOSS_Discriminator: 0.03408713638782501\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.637608528137207 LOSS_Discriminator: 0.07275836666425069\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.91733455657959 LOSS_Discriminator: 0.04556013643741608\n",
            "ITERATION_NO.: 599 LOSS_Generator: 4.937252998352051 LOSS_Discriminator: 0.09507664044698079\n",
            "ITERATION_NO.: 600 LOSS_Generator: 4.900175094604492 LOSS_Discriminator: 0.08765673637390137\n",
            "ITERATION_NO.: 601 LOSS_Generator: 5.106680870056152 LOSS_Discriminator: 0.009604806701342264\n",
            "ITERATION_NO.: 602 LOSS_Generator: 5.50502872467041 LOSS_Discriminator: 0.11506378650665283\n",
            "ITERATION_NO.: 603 LOSS_Generator: 5.9004411697387695 LOSS_Discriminator: 0.019557848572731018\n",
            "ITERATION_NO.: 604 LOSS_Generator: 5.567724227905273 LOSS_Discriminator: 0.07049790024757385\n",
            "ITERATION_NO.: 605 LOSS_Generator: 5.158574104309082 LOSS_Discriminator: 0.13151378432909647\n",
            "ITERATION_NO.: 606 LOSS_Generator: 4.037827491760254 LOSS_Discriminator: 0.2543939749399821\n",
            "ITERATION_NO.: 607 LOSS_Generator: 3.8147854804992676 LOSS_Discriminator: 0.10276283820470174\n",
            "ITERATION_NO.: 608 LOSS_Generator: 5.209004878997803 LOSS_Discriminator: 0.12260748942693074\n",
            "ITERATION_NO.: 609 LOSS_Generator: 6.351448059082031 LOSS_Discriminator: 0.0629897912343343\n",
            "ITERATION_NO.: 610 LOSS_Generator: 7.847009658813477 LOSS_Discriminator: 0.0456793357928594\n",
            "ITERATION_NO.: 611 LOSS_Generator: 7.015451431274414 LOSS_Discriminator: 0.1784361203511556\n",
            "ITERATION_NO.: 612 LOSS_Generator: 6.318758964538574 LOSS_Discriminator: 0.13824661572774252\n",
            "ITERATION_NO.: 613 LOSS_Generator: 6.308795928955078 LOSS_Discriminator: 0.0063068872938553495\n",
            "ITERATION_NO.: 614 LOSS_Generator: 6.063206195831299 LOSS_Discriminator: 0.050982107718785606\n",
            "ITERATION_NO.: 615 LOSS_Generator: 5.723045349121094 LOSS_Discriminator: 0.14253233869870505\n",
            "ITERATION_NO.: 616 LOSS_Generator: 5.978052616119385 LOSS_Discriminator: 0.017284035682678223\n",
            "ITERATION_NO.: 617 LOSS_Generator: 5.54190731048584 LOSS_Discriminator: 0.02835256854693095\n",
            "ITERATION_NO.: 618 LOSS_Generator: 5.858719348907471 LOSS_Discriminator: 0.08376520872116089\n",
            "ITERATION_NO.: 619 LOSS_Generator: 5.718754768371582 LOSS_Discriminator: 0.0561457226673762\n",
            "ITERATION_NO.: 620 LOSS_Generator: 5.925343990325928 LOSS_Discriminator: 0.008093655109405518\n",
            "ITERATION_NO.: 621 LOSS_Generator: 5.099833011627197 LOSS_Discriminator: 0.1075386106967926\n",
            "ITERATION_NO.: 622 LOSS_Generator: 5.426135063171387 LOSS_Discriminator: 0.16368844111760458\n",
            "ITERATION_NO.: 623 LOSS_Generator: 4.8164496421813965 LOSS_Discriminator: 0.02221745252609253\n",
            "ITERATION_NO.: 624 LOSS_Generator: 5.143708229064941 LOSS_Discriminator: 0.03236318131287893\n",
            "ITERATION_NO.: 625 LOSS_Generator: 5.190614700317383 LOSS_Discriminator: 0.058235218127568565\n",
            "ITERATION_NO.: 626 LOSS_Generator: 5.7577691078186035 LOSS_Discriminator: 0.023662343621253967\n",
            "ITERATION_NO.: 627 LOSS_Generator: 5.453864097595215 LOSS_Discriminator: 0.28301185369491577\n",
            "ITERATION_NO.: 628 LOSS_Generator: 4.633942127227783 LOSS_Discriminator: 0.10333746671676636\n",
            "ITERATION_NO.: 629 LOSS_Generator: 4.323703765869141 LOSS_Discriminator: 0.037461839616298676\n",
            "ITERATION_NO.: 630 LOSS_Generator: 5.2404327392578125 LOSS_Discriminator: 0.01562663788596789\n",
            "ITERATION_NO.: 631 LOSS_Generator: 4.981133937835693 LOSS_Discriminator: 0.18844807147979736\n",
            "ITERATION_NO.: 632 LOSS_Generator: 4.601963996887207 LOSS_Discriminator: 0.07833121220270793\n",
            "ITERATION_NO.: 633 LOSS_Generator: 4.506009101867676 LOSS_Discriminator: 0.11725025375684102\n",
            "ITERATION_NO.: 634 LOSS_Generator: 3.996743679046631 LOSS_Discriminator: 0.08684299389521281\n",
            "ITERATION_NO.: 635 LOSS_Generator: 4.26807975769043 LOSS_Discriminator: 0.13124844431877136\n",
            "ITERATION_NO.: 636 LOSS_Generator: 4.305940628051758 LOSS_Discriminator: 0.21519593397776285\n",
            "ITERATION_NO.: 637 LOSS_Generator: 4.7878570556640625 LOSS_Discriminator: 0.12471048037211101\n",
            "ITERATION_NO.: 638 LOSS_Generator: 5.653127670288086 LOSS_Discriminator: 0.05264116823673248\n",
            "ITERATION_NO.: 639 LOSS_Generator: 6.727694034576416 LOSS_Discriminator: 0.05646491050720215\n",
            "ITERATION_NO.: 640 LOSS_Generator: 7.107697486877441 LOSS_Discriminator: 0.003335835412144661\n",
            "ITERATION_NO.: 641 LOSS_Generator: 7.0616631507873535 LOSS_Discriminator: 0.05893178780873617\n",
            "ITERATION_NO.: 642 LOSS_Generator: 6.843809127807617 LOSS_Discriminator: 0.1124163568019867\n",
            "ITERATION_NO.: 643 LOSS_Generator: 6.594050407409668 LOSS_Discriminator: 0.1294865608215332\n",
            "ITERATION_NO.: 644 LOSS_Generator: 5.7238545417785645 LOSS_Discriminator: 0.03869719058275223\n",
            "ITERATION_NO.: 645 LOSS_Generator: 4.870146751403809 LOSS_Discriminator: 0.20439509550730386\n",
            "ITERATION_NO.: 646 LOSS_Generator: 3.8290722370147705 LOSS_Discriminator: 0.0977997084458669\n",
            "ITERATION_NO.: 647 LOSS_Generator: 3.7959413528442383 LOSS_Discriminator: 0.07939463357130687\n",
            "ITERATION_NO.: 648 LOSS_Generator: 4.598880767822266 LOSS_Discriminator: 0.21485825379689535\n",
            "ITERATION_NO.: 649 LOSS_Generator: 6.131574630737305 LOSS_Discriminator: 0.058387180169423424\n",
            "ITERATION_NO.: 650 LOSS_Generator: 6.511414527893066 LOSS_Discriminator: 0.10882851481437683\n",
            "ITERATION_NO.: 651 LOSS_Generator: 6.939286231994629 LOSS_Discriminator: 0.3877420425415039\n",
            "ITERATION_NO.: 652 LOSS_Generator: 5.497839450836182 LOSS_Discriminator: 0.04055458058913549\n",
            "ITERATION_NO.: 653 LOSS_Generator: 5.087965488433838 LOSS_Discriminator: 0.029470354318618774\n",
            "ITERATION_NO.: 654 LOSS_Generator: 5.604637145996094 LOSS_Discriminator: 0.27399682998657227\n",
            "ITERATION_NO.: 655 LOSS_Generator: 4.445804119110107 LOSS_Discriminator: 0.27528611818949383\n",
            "ITERATION_NO.: 656 LOSS_Generator: 4.299530982971191 LOSS_Discriminator: 0.14021191000938416\n",
            "ITERATION_NO.: 657 LOSS_Generator: 4.196337699890137 LOSS_Discriminator: 0.058539122343063354\n",
            "ITERATION_NO.: 658 LOSS_Generator: 5.214552402496338 LOSS_Discriminator: 0.07133342822392781\n",
            "ITERATION_NO.: 659 LOSS_Generator: 5.494321346282959 LOSS_Discriminator: 0.04182331760724386\n",
            "ITERATION_NO.: 660 LOSS_Generator: 5.500985622406006 LOSS_Discriminator: 0.014745288838942846\n",
            "ITERATION_NO.: 661 LOSS_Generator: 5.276458740234375 LOSS_Discriminator: 0.019297254582246143\n",
            "ITERATION_NO.: 662 LOSS_Generator: 5.520635604858398 LOSS_Discriminator: 0.1038345495859782\n",
            "ITERATION_NO.: 663 LOSS_Generator: 5.4859700202941895 LOSS_Discriminator: 0.1369628111521403\n",
            "ITERATION_NO.: 664 LOSS_Generator: 4.820322513580322 LOSS_Discriminator: 0.011535361409187317\n",
            "ITERATION_NO.: 665 LOSS_Generator: 4.931882858276367 LOSS_Discriminator: 0.0806417167186737\n",
            "ITERATION_NO.: 666 LOSS_Generator: 4.8036041259765625 LOSS_Discriminator: 0.0898898442586263\n",
            "ITERATION_NO.: 667 LOSS_Generator: 4.238046169281006 LOSS_Discriminator: 0.1533557971318563\n",
            "ITERATION_NO.: 668 LOSS_Generator: 4.288695335388184 LOSS_Discriminator: 0.03783730914195379\n",
            "ITERATION_NO.: 669 LOSS_Generator: 4.9653167724609375 LOSS_Discriminator: 0.03281218806902567\n",
            "ITERATION_NO.: 670 LOSS_Generator: 5.125690460205078 LOSS_Discriminator: 0.04469812413056692\n",
            "ITERATION_NO.: 671 LOSS_Generator: 5.7994794845581055 LOSS_Discriminator: 0.012236503263314566\n",
            "ITERATION_NO.: 672 LOSS_Generator: 5.93575382232666 LOSS_Discriminator: 0.01693689078092575\n",
            "ITERATION_NO.: 673 LOSS_Generator: 5.786856651306152 LOSS_Discriminator: 0.18884710470835367\n",
            "ITERATION_NO.: 674 LOSS_Generator: 5.430849075317383 LOSS_Discriminator: 0.10679580767949422\n",
            "ITERATION_NO.: 675 LOSS_Generator: 4.612104415893555 LOSS_Discriminator: 0.028408274054527283\n",
            "ITERATION_NO.: 676 LOSS_Generator: 4.52044153213501 LOSS_Discriminator: 0.041634549697240196\n",
            "ITERATION_NO.: 677 LOSS_Generator: 3.975370168685913 LOSS_Discriminator: 0.13739357391993204\n",
            "ITERATION_NO.: 678 LOSS_Generator: 4.467580795288086 LOSS_Discriminator: 0.055958171685536705\n",
            "ITERATION_NO.: 679 LOSS_Generator: 5.344132900238037 LOSS_Discriminator: 0.06101733942826589\n",
            "ITERATION_NO.: 680 LOSS_Generator: 5.836230754852295 LOSS_Discriminator: 0.01650872454047203\n",
            "ITERATION_NO.: 681 LOSS_Generator: 6.163815498352051 LOSS_Discriminator: 0.0490130881468455\n",
            "ITERATION_NO.: 682 LOSS_Generator: 6.544063568115234 LOSS_Discriminator: 0.05904242396354675\n",
            "ITERATION_NO.: 683 LOSS_Generator: 6.39902400970459 LOSS_Discriminator: 0.0038535182053844133\n",
            "ITERATION_NO.: 684 LOSS_Generator: 6.062934875488281 LOSS_Discriminator: 0.16671693325042725\n",
            "ITERATION_NO.: 685 LOSS_Generator: 5.880249500274658 LOSS_Discriminator: 0.05099284648895264\n",
            "ITERATION_NO.: 686 LOSS_Generator: 5.219640731811523 LOSS_Discriminator: 0.11760857701301575\n",
            "ITERATION_NO.: 687 LOSS_Generator: 4.588934421539307 LOSS_Discriminator: 0.14742730061213175\n",
            "ITERATION_NO.: 688 LOSS_Generator: 4.773453712463379 LOSS_Discriminator: 0.08026861647764842\n",
            "ITERATION_NO.: 689 LOSS_Generator: 4.403919219970703 LOSS_Discriminator: 0.11321116487185161\n",
            "ITERATION_NO.: 690 LOSS_Generator: 5.130589485168457 LOSS_Discriminator: 0.03498652080694834\n",
            "ITERATION_NO.: 691 LOSS_Generator: 5.073175430297852 LOSS_Discriminator: 0.17216422160466513\n",
            "ITERATION_NO.: 692 LOSS_Generator: 5.224085807800293 LOSS_Discriminator: 0.03970754394928614\n",
            "ITERATION_NO.: 693 LOSS_Generator: 5.058423042297363 LOSS_Discriminator: 0.05861684183279673\n",
            "ITERATION_NO.: 694 LOSS_Generator: 5.264518737792969 LOSS_Discriminator: 0.07978707551956177\n",
            "ITERATION_NO.: 695 LOSS_Generator: 5.065709590911865 LOSS_Discriminator: 0.04171893000602722\n",
            "ITERATION_NO.: 696 LOSS_Generator: 5.267184257507324 LOSS_Discriminator: 0.12540805339813232\n",
            "ITERATION_NO.: 697 LOSS_Generator: 5.0896525382995605 LOSS_Discriminator: 0.15785115957260132\n",
            "ITERATION_NO.: 698 LOSS_Generator: 4.644778251647949 LOSS_Discriminator: 0.05580014983812968\n",
            "ITERATION_NO.: 699 LOSS_Generator: 5.501282691955566 LOSS_Discriminator: 0.041896561781565346\n",
            "ITERATION_NO.: 700 LOSS_Generator: 6.677587509155273 LOSS_Discriminator: 0.007623366390665372\n",
            "ITERATION_NO.: 701 LOSS_Generator: 6.308449745178223 LOSS_Discriminator: 0.06868886450926463\n",
            "ITERATION_NO.: 702 LOSS_Generator: 6.086252212524414 LOSS_Discriminator: 0.03557613988717397\n",
            "ITERATION_NO.: 703 LOSS_Generator: 6.1069793701171875 LOSS_Discriminator: 0.012957376738389334\n",
            "ITERATION_NO.: 704 LOSS_Generator: 5.720264911651611 LOSS_Discriminator: 0.14802460869153342\n",
            "ITERATION_NO.: 705 LOSS_Generator: 5.85373067855835 LOSS_Discriminator: 0.032232960065205894\n",
            "ITERATION_NO.: 706 LOSS_Generator: 4.97914981842041 LOSS_Discriminator: 0.2827189763387044\n",
            "ITERATION_NO.: 707 LOSS_Generator: 3.9558725357055664 LOSS_Discriminator: 0.05453893542289734\n",
            "ITERATION_NO.: 708 LOSS_Generator: 4.222817420959473 LOSS_Discriminator: 0.12406252821286519\n",
            "ITERATION_NO.: 709 LOSS_Generator: 4.802455902099609 LOSS_Discriminator: 0.06333313385645549\n",
            "ITERATION_NO.: 710 LOSS_Generator: 5.076504230499268 LOSS_Discriminator: 0.027279429137706757\n",
            "ITERATION_NO.: 711 LOSS_Generator: 5.779841423034668 LOSS_Discriminator: 0.23282047112782797\n",
            "ITERATION_NO.: 712 LOSS_Generator: 4.960679531097412 LOSS_Discriminator: 0.015290343513091406\n",
            "ITERATION_NO.: 713 LOSS_Generator: 5.25114631652832 LOSS_Discriminator: 0.19816625118255615\n",
            "ITERATION_NO.: 714 LOSS_Generator: 4.611547470092773 LOSS_Discriminator: 0.06204097469647726\n",
            "ITERATION_NO.: 715 LOSS_Generator: 4.256594657897949 LOSS_Discriminator: 0.0309382105867068\n",
            "ITERATION_NO.: 716 LOSS_Generator: 4.555599212646484 LOSS_Discriminator: 0.033533637722333275\n",
            "ITERATION_NO.: 717 LOSS_Generator: 4.78009033203125 LOSS_Discriminator: 0.15005025267601013\n",
            "ITERATION_NO.: 718 LOSS_Generator: 5.075829029083252 LOSS_Discriminator: 0.03892733405033747\n",
            "ITERATION_NO.: 719 LOSS_Generator: 5.257147312164307 LOSS_Discriminator: 0.04681834081808726\n",
            "ITERATION_NO.: 720 LOSS_Generator: 5.751887798309326 LOSS_Discriminator: 0.10138308008511861\n",
            "ITERATION_NO.: 721 LOSS_Generator: 4.652685165405273 LOSS_Discriminator: 0.25099581480026245\n",
            "ITERATION_NO.: 722 LOSS_Generator: 3.5774731636047363 LOSS_Discriminator: 0.20142861207326254\n",
            "ITERATION_NO.: 723 LOSS_Generator: 3.7668697834014893 LOSS_Discriminator: 0.14876964688301086\n",
            "ITERATION_NO.: 724 LOSS_Generator: 4.207779884338379 LOSS_Discriminator: 0.1616986890633901\n",
            "ITERATION_NO.: 725 LOSS_Generator: 4.835813522338867 LOSS_Discriminator: 0.12148571014404297\n",
            "ITERATION_NO.: 726 LOSS_Generator: 5.108240604400635 LOSS_Discriminator: 0.11168831586837769\n",
            "ITERATION_NO.: 727 LOSS_Generator: 4.55039119720459 LOSS_Discriminator: 0.1860867738723755\n",
            "ITERATION_NO.: 728 LOSS_Generator: 3.9419097900390625 LOSS_Discriminator: 0.16263272364934286\n",
            "ITERATION_NO.: 729 LOSS_Generator: 3.5979390144348145 LOSS_Discriminator: 0.2090020775794983\n",
            "ITERATION_NO.: 730 LOSS_Generator: 4.658209800720215 LOSS_Discriminator: 0.11343165238698323\n",
            "ITERATION_NO.: 731 LOSS_Generator: 6.124382972717285 LOSS_Discriminator: 0.01438545435667038\n",
            "ITERATION_NO.: 732 LOSS_Generator: 6.922939300537109 LOSS_Discriminator: 0.13714179396629333\n",
            "ITERATION_NO.: 733 LOSS_Generator: 7.021115303039551 LOSS_Discriminator: 0.13034047683080038\n",
            "ITERATION_NO.: 734 LOSS_Generator: 7.093160629272461 LOSS_Discriminator: 0.07526727517445882\n",
            "ITERATION_NO.: 735 LOSS_Generator: 5.956722259521484 LOSS_Discriminator: 0.07856763899326324\n",
            "ITERATION_NO.: 736 LOSS_Generator: 5.4352521896362305 LOSS_Discriminator: 0.04792134960492452\n",
            "ITERATION_NO.: 737 LOSS_Generator: 4.918128967285156 LOSS_Discriminator: 0.009079581747452417\n",
            "ITERATION_NO.: 738 LOSS_Generator: 4.470016002655029 LOSS_Discriminator: 0.02812485893567403\n",
            "ITERATION_NO.: 739 LOSS_Generator: 4.919434547424316 LOSS_Discriminator: 0.033246733248233795\n",
            "ITERATION_NO.: 740 LOSS_Generator: 5.022541046142578 LOSS_Discriminator: 0.07131009300549825\n",
            "ITERATION_NO.: 741 LOSS_Generator: 4.894272804260254 LOSS_Discriminator: 0.16145939628283182\n",
            "ITERATION_NO.: 742 LOSS_Generator: 4.9139509201049805 LOSS_Discriminator: 0.050198718905448914\n",
            "ITERATION_NO.: 743 LOSS_Generator: 4.675563812255859 LOSS_Discriminator: 0.030534009138743084\n",
            "ITERATION_NO.: 744 LOSS_Generator: 4.7788286209106445 LOSS_Discriminator: 0.03926262011130651\n",
            "ITERATION_NO.: 745 LOSS_Generator: 5.042327880859375 LOSS_Discriminator: 0.027612422903378803\n",
            "ITERATION_NO.: 746 LOSS_Generator: 5.543067932128906 LOSS_Discriminator: 0.16591840982437134\n",
            "ITERATION_NO.: 747 LOSS_Generator: 4.959057807922363 LOSS_Discriminator: 0.1095299522082011\n",
            "ITERATION_NO.: 748 LOSS_Generator: 4.685959815979004 LOSS_Discriminator: 0.019024233023325603\n",
            "ITERATION_NO.: 749 LOSS_Generator: 4.8349480628967285 LOSS_Discriminator: 0.06074802577495575\n",
            "ITERATION_NO.: 750 LOSS_Generator: 4.817988395690918 LOSS_Discriminator: 0.09565453728040059\n",
            "ITERATION_NO.: 751 LOSS_Generator: 4.70222806930542 LOSS_Discriminator: 0.17322017749150595\n",
            "ITERATION_NO.: 752 LOSS_Generator: 4.078315734863281 LOSS_Discriminator: 0.14632792274157205\n",
            "ITERATION_NO.: 753 LOSS_Generator: 3.1488003730773926 LOSS_Discriminator: 0.22276131312052408\n",
            "ITERATION_NO.: 754 LOSS_Generator: 3.89005708694458 LOSS_Discriminator: 0.10151662429173787\n",
            "ITERATION_NO.: 755 LOSS_Generator: 4.718693256378174 LOSS_Discriminator: 0.12679576873779297\n",
            "ITERATION_NO.: 756 LOSS_Generator: 5.1795878410339355 LOSS_Discriminator: 0.04224692781766256\n",
            "ITERATION_NO.: 757 LOSS_Generator: 5.439981460571289 LOSS_Discriminator: 0.058431784311930336\n",
            "ITERATION_NO.: 758 LOSS_Generator: 5.548492431640625 LOSS_Discriminator: 0.09245288372039795\n",
            "ITERATION_NO.: 759 LOSS_Generator: 5.158721446990967 LOSS_Discriminator: 0.03231498599052429\n",
            "ITERATION_NO.: 760 LOSS_Generator: 5.255463123321533 LOSS_Discriminator: 0.0869705577691396\n",
            "ITERATION_NO.: 761 LOSS_Generator: 5.381847381591797 LOSS_Discriminator: 0.020894162356853485\n",
            "ITERATION_NO.: 762 LOSS_Generator: 5.622422218322754 LOSS_Discriminator: 0.014886679748694101\n",
            "ITERATION_NO.: 763 LOSS_Generator: 4.9787492752075195 LOSS_Discriminator: 0.0381620724995931\n",
            "ITERATION_NO.: 764 LOSS_Generator: 5.7118635177612305 LOSS_Discriminator: 0.017812445759773254\n",
            "ITERATION_NO.: 765 LOSS_Generator: 5.431033134460449 LOSS_Discriminator: 0.0249156653881073\n",
            "ITERATION_NO.: 766 LOSS_Generator: 5.806997299194336 LOSS_Discriminator: 0.06679726143678029\n",
            "ITERATION_NO.: 767 LOSS_Generator: 5.508231163024902 LOSS_Discriminator: 0.010651481648286184\n",
            "ITERATION_NO.: 768 LOSS_Generator: 5.386900901794434 LOSS_Discriminator: 0.19447922706604004\n",
            "ITERATION_NO.: 769 LOSS_Generator: 4.472809791564941 LOSS_Discriminator: 0.13675441344579062\n",
            "ITERATION_NO.: 770 LOSS_Generator: 4.2014851570129395 LOSS_Discriminator: 0.03554502626260122\n",
            "ITERATION_NO.: 771 LOSS_Generator: 4.70528507232666 LOSS_Discriminator: 0.16697307427724203\n",
            "ITERATION_NO.: 772 LOSS_Generator: 5.448812007904053 LOSS_Discriminator: 0.06122945745786031\n",
            "ITERATION_NO.: 773 LOSS_Generator: 6.140913009643555 LOSS_Discriminator: 0.011027529835700989\n",
            "ITERATION_NO.: 774 LOSS_Generator: 6.522815704345703 LOSS_Discriminator: 0.12407811482747395\n",
            "ITERATION_NO.: 775 LOSS_Generator: 5.934670925140381 LOSS_Discriminator: 0.11305115620295207\n",
            "ITERATION_NO.: 776 LOSS_Generator: 5.334314823150635 LOSS_Discriminator: 0.025040999054908752\n",
            "ITERATION_NO.: 777 LOSS_Generator: 4.758219242095947 LOSS_Discriminator: 0.21478474140167236\n",
            "ITERATION_NO.: 778 LOSS_Generator: 4.769592761993408 LOSS_Discriminator: 0.07058680057525635\n",
            "ITERATION_NO.: 779 LOSS_Generator: 4.320699691772461 LOSS_Discriminator: 0.165169358253479\n",
            "ITERATION_NO.: 780 LOSS_Generator: 4.217379570007324 LOSS_Discriminator: 0.11433927218119304\n",
            "ITERATION_NO.: 781 LOSS_Generator: 4.286107063293457 LOSS_Discriminator: 0.05128428339958191\n",
            "ITERATION_NO.: 782 LOSS_Generator: 4.981997489929199 LOSS_Discriminator: 0.08237895866235097\n",
            "ITERATION_NO.: 783 LOSS_Generator: 5.286523342132568 LOSS_Discriminator: 0.05193643271923065\n",
            "ITERATION_NO.: 784 LOSS_Generator: 5.781248092651367 LOSS_Discriminator: 0.014245999356110891\n",
            "ITERATION_NO.: 785 LOSS_Generator: 5.851163864135742 LOSS_Discriminator: 0.01063777506351471\n",
            "ITERATION_NO.: 786 LOSS_Generator: 5.887207984924316 LOSS_Discriminator: 0.02937258283297221\n",
            "ITERATION_NO.: 787 LOSS_Generator: 6.207815647125244 LOSS_Discriminator: 0.22087164719899496\n",
            "ITERATION_NO.: 788 LOSS_Generator: 5.99868106842041 LOSS_Discriminator: 0.1043320894241333\n",
            "ITERATION_NO.: 789 LOSS_Generator: 5.319732189178467 LOSS_Discriminator: 0.006890265271067619\n",
            "ITERATION_NO.: 790 LOSS_Generator: 4.835216522216797 LOSS_Discriminator: 0.20765737692515054\n",
            "ITERATION_NO.: 791 LOSS_Generator: 3.844883918762207 LOSS_Discriminator: 0.060115029414494835\n",
            "ITERATION_NO.: 792 LOSS_Generator: 3.5730929374694824 LOSS_Discriminator: 0.08185192942619324\n",
            "ITERATION_NO.: 793 LOSS_Generator: 4.255577564239502 LOSS_Discriminator: 0.08502863844235738\n",
            "ITERATION_NO.: 794 LOSS_Generator: 4.470735549926758 LOSS_Discriminator: 0.16974002122879028\n",
            "ITERATION_NO.: 795 LOSS_Generator: 5.002026557922363 LOSS_Discriminator: 0.061663741866747536\n",
            "ITERATION_NO.: 796 LOSS_Generator: 6.093955039978027 LOSS_Discriminator: 0.01347443088889122\n",
            "ITERATION_NO.: 797 LOSS_Generator: 6.880107879638672 LOSS_Discriminator: 0.011877524356047312\n",
            "ITERATION_NO.: 798 LOSS_Generator: 6.334051609039307 LOSS_Discriminator: 0.5124492645263672\n",
            "ITERATION_NO.: 799 LOSS_Generator: 5.439395904541016 LOSS_Discriminator: 0.06670974691708882\n",
            "ITERATION_NO.: 800 LOSS_Generator: 4.326964378356934 LOSS_Discriminator: 0.17736361424128214\n",
            "ITERATION_NO.: 801 LOSS_Generator: 2.571171760559082 LOSS_Discriminator: 0.13022547960281372\n",
            "ITERATION_NO.: 802 LOSS_Generator: 3.8676371574401855 LOSS_Discriminator: 0.21609920263290405\n",
            "ITERATION_NO.: 803 LOSS_Generator: 5.309561729431152 LOSS_Discriminator: 0.20496058464050293\n",
            "ITERATION_NO.: 804 LOSS_Generator: 5.6187543869018555 LOSS_Discriminator: 0.0313208152850469\n",
            "ITERATION_NO.: 805 LOSS_Generator: 6.24846076965332 LOSS_Discriminator: 0.006093389044205348\n",
            "ITERATION_NO.: 806 LOSS_Generator: 6.199198246002197 LOSS_Discriminator: 0.037202782928943634\n",
            "ITERATION_NO.: 807 LOSS_Generator: 5.436788082122803 LOSS_Discriminator: 0.2900712490081787\n",
            "ITERATION_NO.: 808 LOSS_Generator: 5.348235607147217 LOSS_Discriminator: 0.03344938904047012\n",
            "ITERATION_NO.: 809 LOSS_Generator: 5.156373977661133 LOSS_Discriminator: 0.05537190039952596\n",
            "ITERATION_NO.: 810 LOSS_Generator: 4.604673385620117 LOSS_Discriminator: 0.029318441947301228\n",
            "ITERATION_NO.: 811 LOSS_Generator: 4.944733619689941 LOSS_Discriminator: 0.04020266979932785\n",
            "ITERATION_NO.: 812 LOSS_Generator: 4.680418014526367 LOSS_Discriminator: 0.15338470538457236\n",
            "ITERATION_NO.: 813 LOSS_Generator: 5.7383832931518555 LOSS_Discriminator: 0.021367462972799938\n",
            "ITERATION_NO.: 814 LOSS_Generator: 5.658878326416016 LOSS_Discriminator: 0.23089444637298584\n",
            "ITERATION_NO.: 815 LOSS_Generator: 5.095903396606445 LOSS_Discriminator: 0.01979583501815796\n",
            "ITERATION_NO.: 816 LOSS_Generator: 5.083291053771973 LOSS_Discriminator: 0.015897660205761593\n",
            "ITERATION_NO.: 817 LOSS_Generator: 5.044914722442627 LOSS_Discriminator: 0.07032712300618489\n",
            "ITERATION_NO.: 818 LOSS_Generator: 4.996403694152832 LOSS_Discriminator: 0.12293869256973267\n",
            "ITERATION_NO.: 819 LOSS_Generator: 4.8496270179748535 LOSS_Discriminator: 0.05141475796699524\n",
            "ITERATION_NO.: 820 LOSS_Generator: 4.821709156036377 LOSS_Discriminator: 0.03890694926182429\n",
            "ITERATION_NO.: 821 LOSS_Generator: 5.122308731079102 LOSS_Discriminator: 0.10339728991190593\n",
            "ITERATION_NO.: 822 LOSS_Generator: 4.279593467712402 LOSS_Discriminator: 0.14527530471483865\n",
            "ITERATION_NO.: 823 LOSS_Generator: 4.995096683502197 LOSS_Discriminator: 0.07827876508235931\n",
            "ITERATION_NO.: 824 LOSS_Generator: 5.363341331481934 LOSS_Discriminator: 0.08482282360394795\n",
            "ITERATION_NO.: 825 LOSS_Generator: 5.135448455810547 LOSS_Discriminator: 0.07060532768567403\n",
            "ITERATION_NO.: 826 LOSS_Generator: 5.199894905090332 LOSS_Discriminator: 0.021948119004567463\n",
            "ITERATION_NO.: 827 LOSS_Generator: 4.94571590423584 LOSS_Discriminator: 0.10128151377042134\n",
            "ITERATION_NO.: 828 LOSS_Generator: 4.669477462768555 LOSS_Discriminator: 0.01873620351155599\n",
            "ITERATION_NO.: 829 LOSS_Generator: 4.9415106773376465 LOSS_Discriminator: 0.13318843642870584\n",
            "ITERATION_NO.: 830 LOSS_Generator: 5.300493240356445 LOSS_Discriminator: 0.036176356176535286\n",
            "ITERATION_NO.: 831 LOSS_Generator: 4.931267738342285 LOSS_Discriminator: 0.09201866388320923\n",
            "ITERATION_NO.: 832 LOSS_Generator: 4.909053325653076 LOSS_Discriminator: 0.05665351947148641\n",
            "ITERATION_NO.: 833 LOSS_Generator: 4.857019424438477 LOSS_Discriminator: 0.02767208715279897\n",
            "ITERATION_NO.: 834 LOSS_Generator: 5.437386512756348 LOSS_Discriminator: 0.036316826939582825\n",
            "ITERATION_NO.: 835 LOSS_Generator: 5.729310035705566 LOSS_Discriminator: 0.06654598315556844\n",
            "ITERATION_NO.: 836 LOSS_Generator: 5.763794898986816 LOSS_Discriminator: 0.015522407988707224\n",
            "ITERATION_NO.: 837 LOSS_Generator: 6.3007636070251465 LOSS_Discriminator: 0.12122041980425517\n",
            "ITERATION_NO.: 838 LOSS_Generator: 5.488595008850098 LOSS_Discriminator: 0.19357657432556152\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.52092981338501 LOSS_Discriminator: 0.026244252920150757\n",
            "ITERATION_NO.: 840 LOSS_Generator: 4.7262773513793945 LOSS_Discriminator: 0.12906437118848166\n",
            "ITERATION_NO.: 841 LOSS_Generator: 5.067596435546875 LOSS_Discriminator: 0.10023070375124614\n",
            "ITERATION_NO.: 842 LOSS_Generator: 5.458438873291016 LOSS_Discriminator: 0.06802692015965779\n",
            "ITERATION_NO.: 843 LOSS_Generator: 6.143671035766602 LOSS_Discriminator: 0.024773473540941875\n",
            "ITERATION_NO.: 844 LOSS_Generator: 6.5240983963012695 LOSS_Discriminator: 0.11173882087071736\n",
            "ITERATION_NO.: 845 LOSS_Generator: 5.982351303100586 LOSS_Discriminator: 0.0721895694732666\n",
            "ITERATION_NO.: 846 LOSS_Generator: 5.407250881195068 LOSS_Discriminator: 0.23855586846669516\n",
            "ITERATION_NO.: 847 LOSS_Generator: 3.868619918823242 LOSS_Discriminator: 0.03973161429166794\n",
            "ITERATION_NO.: 848 LOSS_Generator: 4.029568195343018 LOSS_Discriminator: 0.09107699990272522\n",
            "ITERATION_NO.: 849 LOSS_Generator: 4.87369441986084 LOSS_Discriminator: 0.07122163474559784\n",
            "ITERATION_NO.: 850 LOSS_Generator: 5.598381042480469 LOSS_Discriminator: 0.03844560186068217\n",
            "ITERATION_NO.: 851 LOSS_Generator: 6.555470943450928 LOSS_Discriminator: 0.15084965030352274\n",
            "ITERATION_NO.: 852 LOSS_Generator: 6.509922504425049 LOSS_Discriminator: 0.33400686581929523\n",
            "ITERATION_NO.: 853 LOSS_Generator: 5.231671333312988 LOSS_Discriminator: 0.0872008999188741\n",
            "ITERATION_NO.: 854 LOSS_Generator: 5.156261444091797 LOSS_Discriminator: 0.008593720694382986\n",
            "ITERATION_NO.: 855 LOSS_Generator: 4.56649112701416 LOSS_Discriminator: 0.14918490250905356\n",
            "ITERATION_NO.: 856 LOSS_Generator: 3.5240771770477295 LOSS_Discriminator: 0.24190755685170492\n",
            "ITERATION_NO.: 857 LOSS_Generator: 3.7595105171203613 LOSS_Discriminator: 0.1318162480990092\n",
            "ITERATION_NO.: 858 LOSS_Generator: 4.783021450042725 LOSS_Discriminator: 0.09618193904558818\n",
            "ITERATION_NO.: 859 LOSS_Generator: 5.830038070678711 LOSS_Discriminator: 0.13721144199371338\n",
            "ITERATION_NO.: 860 LOSS_Generator: 6.306829452514648 LOSS_Discriminator: 0.06921979784965515\n",
            "ITERATION_NO.: 861 LOSS_Generator: 6.678011894226074 LOSS_Discriminator: 0.002958131954073906\n",
            "ITERATION_NO.: 862 LOSS_Generator: 6.6748576164245605 LOSS_Discriminator: 0.003474685363471508\n",
            "ITERATION_NO.: 863 LOSS_Generator: 6.755993843078613 LOSS_Discriminator: 0.0646875649690628\n",
            "ITERATION_NO.: 864 LOSS_Generator: 6.385605335235596 LOSS_Discriminator: 0.13589662313461304\n",
            "ITERATION_NO.: 865 LOSS_Generator: 5.312454700469971 LOSS_Discriminator: 0.2077725132306417\n",
            "ITERATION_NO.: 866 LOSS_Generator: 4.5133280754089355 LOSS_Discriminator: 0.10658760865529378\n",
            "ITERATION_NO.: 867 LOSS_Generator: 4.256992340087891 LOSS_Discriminator: 0.11099104086558025\n",
            "ITERATION_NO.: 868 LOSS_Generator: 4.957365036010742 LOSS_Discriminator: 0.060283660888671875\n",
            "ITERATION_NO.: 869 LOSS_Generator: 5.096344947814941 LOSS_Discriminator: 0.04349514842033386\n",
            "ITERATION_NO.: 870 LOSS_Generator: 5.519791603088379 LOSS_Discriminator: 0.02569427341222763\n",
            "ITERATION_NO.: 871 LOSS_Generator: 6.35030460357666 LOSS_Discriminator: 0.011144198477268219\n",
            "ITERATION_NO.: 872 LOSS_Generator: 5.951045989990234 LOSS_Discriminator: 0.15930048624674478\n",
            "ITERATION_NO.: 873 LOSS_Generator: 5.459255695343018 LOSS_Discriminator: 0.05736982822418213\n",
            "ITERATION_NO.: 874 LOSS_Generator: 4.882765769958496 LOSS_Discriminator: 0.030404945214589436\n",
            "ITERATION_NO.: 875 LOSS_Generator: 4.465712547302246 LOSS_Discriminator: 0.10096188386281331\n",
            "ITERATION_NO.: 876 LOSS_Generator: 4.042025566101074 LOSS_Discriminator: 0.05436927080154419\n",
            "ITERATION_NO.: 877 LOSS_Generator: 4.19180965423584 LOSS_Discriminator: 0.13334540526072183\n",
            "ITERATION_NO.: 878 LOSS_Generator: 4.995334625244141 LOSS_Discriminator: 0.17446386814117432\n",
            "ITERATION_NO.: 879 LOSS_Generator: 5.683610439300537 LOSS_Discriminator: 0.01828020562728246\n",
            "ITERATION_NO.: 880 LOSS_Generator: 6.490200042724609 LOSS_Discriminator: 0.09934725364049275\n",
            "ITERATION_NO.: 881 LOSS_Generator: 6.001413822174072 LOSS_Discriminator: 0.06645986437797546\n",
            "ITERATION_NO.: 882 LOSS_Generator: 5.722086429595947 LOSS_Discriminator: 0.012752356628576914\n",
            "ITERATION_NO.: 883 LOSS_Generator: 5.222479820251465 LOSS_Discriminator: 0.09512469172477722\n",
            "ITERATION_NO.: 884 LOSS_Generator: 5.04551887512207 LOSS_Discriminator: 0.15168630083402\n",
            "ITERATION_NO.: 885 LOSS_Generator: 4.453757286071777 LOSS_Discriminator: 0.08740413188934326\n",
            "ITERATION_NO.: 886 LOSS_Generator: 4.106679916381836 LOSS_Discriminator: 0.05086036026477814\n",
            "ITERATION_NO.: 887 LOSS_Generator: 5.042191505432129 LOSS_Discriminator: 0.07898626724878947\n",
            "ITERATION_NO.: 888 LOSS_Generator: 5.627335548400879 LOSS_Discriminator: 0.035316646099090576\n",
            "ITERATION_NO.: 889 LOSS_Generator: 6.438547134399414 LOSS_Discriminator: 0.0552130788564682\n",
            "ITERATION_NO.: 890 LOSS_Generator: 5.505725383758545 LOSS_Discriminator: 0.2261449694633484\n",
            "ITERATION_NO.: 891 LOSS_Generator: 4.877115249633789 LOSS_Discriminator: 0.06381671627362569\n",
            "ITERATION_NO.: 892 LOSS_Generator: 4.199889659881592 LOSS_Discriminator: 0.03630232314268748\n",
            "ITERATION_NO.: 893 LOSS_Generator: 4.3861870765686035 LOSS_Discriminator: 0.02883545309305191\n",
            "ITERATION_NO.: 894 LOSS_Generator: 4.023745536804199 LOSS_Discriminator: 0.254752238591512\n",
            "ITERATION_NO.: 895 LOSS_Generator: 4.092794895172119 LOSS_Discriminator: 0.08946795264879863\n",
            "ITERATION_NO.: 896 LOSS_Generator: 4.737166404724121 LOSS_Discriminator: 0.07044088840484619\n",
            "ITERATION_NO.: 897 LOSS_Generator: 5.539173603057861 LOSS_Discriminator: 0.11094874143600464\n",
            "ITERATION_NO.: 898 LOSS_Generator: 5.568231582641602 LOSS_Discriminator: 0.14380290110905966\n",
            "ITERATION_NO.: 899 LOSS_Generator: 5.089535713195801 LOSS_Discriminator: 0.09204105536142985\n",
            "ITERATION_NO.: 900 LOSS_Generator: 4.496461868286133 LOSS_Discriminator: 0.10538472731908162\n",
            "ITERATION_NO.: 901 LOSS_Generator: 5.0157575607299805 LOSS_Discriminator: 0.08810664216677348\n",
            "ITERATION_NO.: 902 LOSS_Generator: 5.285042762756348 LOSS_Discriminator: 0.03033822774887085\n",
            "ITERATION_NO.: 903 LOSS_Generator: 6.188648223876953 LOSS_Discriminator: 0.007194483031829198\n",
            "ITERATION_NO.: 904 LOSS_Generator: 6.89729118347168 LOSS_Discriminator: 0.003546169027686119\n",
            "ITERATION_NO.: 905 LOSS_Generator: 7.080483436584473 LOSS_Discriminator: 0.0017003586205343406\n",
            "ITERATION_NO.: 906 LOSS_Generator: 7.641995429992676 LOSS_Discriminator: 0.0013394810569783051\n",
            "ITERATION_NO.: 907 LOSS_Generator: 7.6827311515808105 LOSS_Discriminator: 0.07869697113831838\n",
            "ITERATION_NO.: 908 LOSS_Generator: 7.393071174621582 LOSS_Discriminator: 0.17275025447209677\n",
            "ITERATION_NO.: 909 LOSS_Generator: 7.306214809417725 LOSS_Discriminator: 0.27230513095855713\n",
            "ITERATION_NO.: 910 LOSS_Generator: 5.92708683013916 LOSS_Discriminator: 0.25959571202596027\n",
            "ITERATION_NO.: 911 LOSS_Generator: 4.512759685516357 LOSS_Discriminator: 0.22939825057983398\n",
            "ITERATION_NO.: 912 LOSS_Generator: 3.7606089115142822 LOSS_Discriminator: 0.13855117559432983\n",
            "ITERATION_NO.: 913 LOSS_Generator: 3.9783377647399902 LOSS_Discriminator: 0.09420526027679443\n",
            "ITERATION_NO.: 914 LOSS_Generator: 5.23144006729126 LOSS_Discriminator: 0.06769959131876628\n",
            "ITERATION_NO.: 915 LOSS_Generator: 5.5619306564331055 LOSS_Discriminator: 0.07085449993610382\n",
            "ITERATION_NO.: 916 LOSS_Generator: 6.112173080444336 LOSS_Discriminator: 0.026648811995983124\n",
            "ITERATION_NO.: 917 LOSS_Generator: 5.383411884307861 LOSS_Discriminator: 0.12210435668627422\n",
            "ITERATION_NO.: 918 LOSS_Generator: 5.402626991271973 LOSS_Discriminator: 0.007310242702563603\n",
            "ITERATION_NO.: 919 LOSS_Generator: 5.12930965423584 LOSS_Discriminator: 0.1976479490598043\n",
            "ITERATION_NO.: 920 LOSS_Generator: 4.905116081237793 LOSS_Discriminator: 0.13309184710184732\n",
            "ITERATION_NO.: 921 LOSS_Generator: 4.382628440856934 LOSS_Discriminator: 0.029210716485977173\n",
            "ITERATION_NO.: 922 LOSS_Generator: 4.503800392150879 LOSS_Discriminator: 0.055223157008488975\n",
            "ITERATION_NO.: 923 LOSS_Generator: 3.919207811355591 LOSS_Discriminator: 0.09389472007751465\n",
            "ITERATION_NO.: 924 LOSS_Generator: 4.333258152008057 LOSS_Discriminator: 0.07269641260306041\n",
            "ITERATION_NO.: 925 LOSS_Generator: 4.799923896789551 LOSS_Discriminator: 0.04361573358376821\n",
            "ITERATION_NO.: 926 LOSS_Generator: 5.64556884765625 LOSS_Discriminator: 0.03003249814112981\n",
            "ITERATION_NO.: 927 LOSS_Generator: 6.2622528076171875 LOSS_Discriminator: 0.005440594007571538\n",
            "ITERATION_NO.: 928 LOSS_Generator: 6.282198429107666 LOSS_Discriminator: 0.005364656448364258\n",
            "ITERATION_NO.: 929 LOSS_Generator: 6.4850358963012695 LOSS_Discriminator: 0.11828091740608215\n",
            "ITERATION_NO.: 930 LOSS_Generator: 5.647254943847656 LOSS_Discriminator: 0.43749646345774335\n",
            "ITERATION_NO.: 931 LOSS_Generator: 4.263124942779541 LOSS_Discriminator: 0.017359785735607147\n",
            "ITERATION_NO.: 932 LOSS_Generator: 4.279094696044922 LOSS_Discriminator: 0.04996385176976522\n",
            "ITERATION_NO.: 933 LOSS_Generator: 3.9244284629821777 LOSS_Discriminator: 0.20477718114852905\n",
            "ITERATION_NO.: 934 LOSS_Generator: 4.096033096313477 LOSS_Discriminator: 0.09102706114451091\n",
            "ITERATION_NO.: 935 LOSS_Generator: 4.481578350067139 LOSS_Discriminator: 0.11080377300580342\n",
            "ITERATION_NO.: 936 LOSS_Generator: 4.751058578491211 LOSS_Discriminator: 0.05653572082519531\n",
            "ITERATION_NO.: 937 LOSS_Generator: 4.894380569458008 LOSS_Discriminator: 0.19997290770212808\n",
            "ITERATION_NO.: 938 LOSS_Generator: 5.235662937164307 LOSS_Discriminator: 0.08553181091944377\n",
            "ITERATION_NO.: 939 LOSS_Generator: 4.816305637359619 LOSS_Discriminator: 0.027987947066624958\n",
            "ITERATION_NO.: 940 LOSS_Generator: 4.400051593780518 LOSS_Discriminator: 0.1099872887134552\n",
            "ITERATION_NO.: 941 LOSS_Generator: 4.465825080871582 LOSS_Discriminator: 0.10318197806676228\n",
            "ITERATION_NO.: 942 LOSS_Generator: 4.561112403869629 LOSS_Discriminator: 0.061924164493878685\n",
            "ITERATION_NO.: 943 LOSS_Generator: 4.196665287017822 LOSS_Discriminator: 0.16882864634195963\n",
            "ITERATION_NO.: 944 LOSS_Generator: 4.658339500427246 LOSS_Discriminator: 0.07670266429583232\n",
            "ITERATION_NO.: 945 LOSS_Generator: 4.8598127365112305 LOSS_Discriminator: 0.0897911787033081\n",
            "ITERATION_NO.: 946 LOSS_Generator: 5.287283897399902 LOSS_Discriminator: 0.1290017863114675\n",
            "ITERATION_NO.: 947 LOSS_Generator: 5.650246620178223 LOSS_Discriminator: 0.012978008637825647\n",
            "ITERATION_NO.: 948 LOSS_Generator: 5.626735687255859 LOSS_Discriminator: 0.13523711760838827\n",
            "ITERATION_NO.: 949 LOSS_Generator: 5.2792558670043945 LOSS_Discriminator: 0.19256705045700073\n",
            "ITERATION_NO.: 950 LOSS_Generator: 4.730590343475342 LOSS_Discriminator: 0.09462461868921916\n",
            "ITERATION_NO.: 951 LOSS_Generator: 4.1642913818359375 LOSS_Discriminator: 0.021096423268318176\n",
            "ITERATION_NO.: 952 LOSS_Generator: 4.391630172729492 LOSS_Discriminator: 0.1440127690633138\n",
            "ITERATION_NO.: 953 LOSS_Generator: 4.984299182891846 LOSS_Discriminator: 0.137294073899587\n",
            "ITERATION_NO.: 954 LOSS_Generator: 4.91718864440918 LOSS_Discriminator: 0.14801992972691855\n",
            "ITERATION_NO.: 955 LOSS_Generator: 5.452609062194824 LOSS_Discriminator: 0.13159271081288657\n",
            "ITERATION_NO.: 956 LOSS_Generator: 4.563266754150391 LOSS_Discriminator: 0.12197083234786987\n",
            "ITERATION_NO.: 957 LOSS_Generator: 3.8175086975097656 LOSS_Discriminator: 0.07085207601388295\n",
            "ITERATION_NO.: 958 LOSS_Generator: 5.944105625152588 LOSS_Discriminator: 0.10804366072018941\n",
            "ITERATION_NO.: 959 LOSS_Generator: 5.966781139373779 LOSS_Discriminator: 0.0629710207382838\n",
            "ITERATION_NO.: 960 LOSS_Generator: 6.239008903503418 LOSS_Discriminator: 0.007299484685063362\n",
            "ITERATION_NO.: 961 LOSS_Generator: 6.915071487426758 LOSS_Discriminator: 0.010741380353768667\n",
            "ITERATION_NO.: 962 LOSS_Generator: 7.139008522033691 LOSS_Discriminator: 0.09592122832934062\n",
            "ITERATION_NO.: 963 LOSS_Generator: 6.423288822174072 LOSS_Discriminator: 0.3008468945821126\n",
            "ITERATION_NO.: 964 LOSS_Generator: 5.436646938323975 LOSS_Discriminator: 0.10974828402201335\n",
            "ITERATION_NO.: 965 LOSS_Generator: 4.256361484527588 LOSS_Discriminator: 0.019890256226062775\n",
            "ITERATION_NO.: 966 LOSS_Generator: 3.64149808883667 LOSS_Discriminator: 0.09674723943074544\n",
            "ITERATION_NO.: 967 LOSS_Generator: 4.514065742492676 LOSS_Discriminator: 0.1708350976308187\n",
            "ITERATION_NO.: 968 LOSS_Generator: 4.977236747741699 LOSS_Discriminator: 0.04818833867708842\n",
            "ITERATION_NO.: 969 LOSS_Generator: 6.230245590209961 LOSS_Discriminator: 0.0069186923404534655\n",
            "ITERATION_NO.: 970 LOSS_Generator: 6.063760757446289 LOSS_Discriminator: 0.09925254185994466\n",
            "ITERATION_NO.: 971 LOSS_Generator: 6.372804164886475 LOSS_Discriminator: 0.06804547707239787\n",
            "ITERATION_NO.: 972 LOSS_Generator: 6.265111923217773 LOSS_Discriminator: 0.0830990473429362\n",
            "ITERATION_NO.: 973 LOSS_Generator: 5.906146049499512 LOSS_Discriminator: 0.010979942977428436\n",
            "ITERATION_NO.: 974 LOSS_Generator: 5.082481861114502 LOSS_Discriminator: 0.2695816159248352\n",
            "ITERATION_NO.: 975 LOSS_Generator: 4.481257438659668 LOSS_Discriminator: 0.18490556875864664\n",
            "ITERATION_NO.: 976 LOSS_Generator: 3.346201181411743 LOSS_Discriminator: 0.09676913420359294\n",
            "ITERATION_NO.: 977 LOSS_Generator: 4.25 LOSS_Discriminator: 0.13917471965154013\n",
            "ITERATION_NO.: 978 LOSS_Generator: 5.742273330688477 LOSS_Discriminator: 0.19775919119517008\n",
            "ITERATION_NO.: 979 LOSS_Generator: 6.206433296203613 LOSS_Discriminator: 0.02647546927134196\n",
            "ITERATION_NO.: 980 LOSS_Generator: 7.054903984069824 LOSS_Discriminator: 0.043321579694747925\n",
            "ITERATION_NO.: 981 LOSS_Generator: 7.5494537353515625 LOSS_Discriminator: 0.18934915463129678\n",
            "ITERATION_NO.: 982 LOSS_Generator: 7.250670909881592 LOSS_Discriminator: 0.0283297598361969\n",
            "ITERATION_NO.: 983 LOSS_Generator: 7.439745903015137 LOSS_Discriminator: 0.007871548334757486\n",
            "ITERATION_NO.: 984 LOSS_Generator: 6.798774719238281 LOSS_Discriminator: 0.21766569217046103\n",
            "ITERATION_NO.: 985 LOSS_Generator: 5.872619152069092 LOSS_Discriminator: 0.00417693518102169\n",
            "ITERATION_NO.: 986 LOSS_Generator: 4.912501335144043 LOSS_Discriminator: 0.036305785179138184\n",
            "ITERATION_NO.: 987 LOSS_Generator: 5.213642120361328 LOSS_Discriminator: 0.013699733962615332\n",
            "ITERATION_NO.: 988 LOSS_Generator: 4.708758354187012 LOSS_Discriminator: 0.19458605845769247\n",
            "ITERATION_NO.: 989 LOSS_Generator: 4.640239715576172 LOSS_Discriminator: 0.06912219027678172\n",
            "ITERATION_NO.: 990 LOSS_Generator: 4.080376148223877 LOSS_Discriminator: 0.07977428038914998\n",
            "ITERATION_NO.: 991 LOSS_Generator: 5.0951080322265625 LOSS_Discriminator: 0.03286714106798172\n",
            "ITERATION_NO.: 992 LOSS_Generator: 4.941267490386963 LOSS_Discriminator: 0.13649539152781168\n",
            "ITERATION_NO.: 993 LOSS_Generator: 4.854262828826904 LOSS_Discriminator: 0.23821922143300375\n",
            "ITERATION_NO.: 994 LOSS_Generator: 3.646005868911743 LOSS_Discriminator: 0.13573011755943298\n",
            "ITERATION_NO.: 995 LOSS_Generator: 3.1709771156311035 LOSS_Discriminator: 0.20236676931381226\n",
            "ITERATION_NO.: 996 LOSS_Generator: 3.9786014556884766 LOSS_Discriminator: 0.2033263842264811\n",
            "ITERATION_NO.: 997 LOSS_Generator: 4.9761271476745605 LOSS_Discriminator: 0.05589189132054647\n",
            "ITERATION_NO.: 998 LOSS_Generator: 6.1207499504089355 LOSS_Discriminator: 0.0310445378224055\n",
            "ITERATION_NO.: 999 LOSS_Generator: 6.809577941894531 LOSS_Discriminator: 0.002943575382232666\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 7.219135284423828 LOSS_Discriminator: 0.12149651845296223\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 6.841958999633789 LOSS_Discriminator: 0.18293813864390054\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 7.036500930786133 LOSS_Discriminator: 0.009789414082964262\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 6.560786247253418 LOSS_Discriminator: 0.0025250998636086783\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 6.911266803741455 LOSS_Discriminator: 0.06508096059163411\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 5.822683334350586 LOSS_Discriminator: 0.04356898367404938\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 5.661716461181641 LOSS_Discriminator: 0.13942411541938782\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 4.896415710449219 LOSS_Discriminator: 0.08849242329597473\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 4.2694196701049805 LOSS_Discriminator: 0.033016701539357506\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 4.648443698883057 LOSS_Discriminator: 0.08620947599411011\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 4.6425580978393555 LOSS_Discriminator: 0.03053949773311615\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 5.19757080078125 LOSS_Discriminator: 0.08210537830988567\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 4.966130256652832 LOSS_Discriminator: 0.27927476167678833\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 4.407546520233154 LOSS_Discriminator: 0.1474307676156362\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 3.803941011428833 LOSS_Discriminator: 0.12440296014149983\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 4.186728000640869 LOSS_Discriminator: 0.18139813343683878\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 4.583338737487793 LOSS_Discriminator: 0.0741482824087143\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 5.783823013305664 LOSS_Discriminator: 0.04265520473321279\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 6.185546398162842 LOSS_Discriminator: 0.20490115880966187\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 5.974360466003418 LOSS_Discriminator: 0.17472505569458008\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 4.524054527282715 LOSS_Discriminator: 0.15287299950917563\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 4.238805294036865 LOSS_Discriminator: 0.02528422325849533\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 4.421268463134766 LOSS_Discriminator: 0.13645600279172262\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 4.921525478363037 LOSS_Discriminator: 0.05336766938368479\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 4.713362216949463 LOSS_Discriminator: 0.24016414086023966\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 4.74177360534668 LOSS_Discriminator: 0.0629491110642751\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 4.379290580749512 LOSS_Discriminator: 0.05694316824277242\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 4.182890892028809 LOSS_Discriminator: 0.06927739083766937\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 4.756608009338379 LOSS_Discriminator: 0.07820678253968556\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 5.16408634185791 LOSS_Discriminator: 0.07011054456233978\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 4.912700653076172 LOSS_Discriminator: 0.08364787697792053\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 5.26961612701416 LOSS_Discriminator: 0.028267748653888702\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 4.668698310852051 LOSS_Discriminator: 0.11103467146555583\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 4.243692874908447 LOSS_Discriminator: 0.24336894353230795\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 3.894742488861084 LOSS_Discriminator: 0.09106011192003886\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 4.156036853790283 LOSS_Discriminator: 0.10759801665941875\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 5.11940860748291 LOSS_Discriminator: 0.05181850492954254\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 6.0258917808532715 LOSS_Discriminator: 0.02108932038148244\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 5.6762847900390625 LOSS_Discriminator: 0.47914767265319824\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 4.886323928833008 LOSS_Discriminator: 0.02362951636314392\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 3.9776506423950195 LOSS_Discriminator: 0.0689491331577301\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 4.0694403648376465 LOSS_Discriminator: 0.09288103381792705\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 4.2616286277771 LOSS_Discriminator: 0.05302014946937561\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 5.1792097091674805 LOSS_Discriminator: 0.04152273635069529\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 5.334639549255371 LOSS_Discriminator: 0.08530162771542867\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 4.915772914886475 LOSS_Discriminator: 0.1096370021502177\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 5.060741901397705 LOSS_Discriminator: 0.03750849018494288\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 5.083116054534912 LOSS_Discriminator: 0.05673192938168844\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 4.977291107177734 LOSS_Discriminator: 0.19034186999003092\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 5.36013650894165 LOSS_Discriminator: 0.022432779272397358\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 5.14208984375 LOSS_Discriminator: 0.035581208765506744\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 5.411752223968506 LOSS_Discriminator: 0.11272361874580383\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 4.689642429351807 LOSS_Discriminator: 0.019419608016808827\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 4.6755828857421875 LOSS_Discriminator: 0.04261446992556254\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 4.865168571472168 LOSS_Discriminator: 0.044688100616137184\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 4.874780654907227 LOSS_Discriminator: 0.08434890707333882\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 4.964095115661621 LOSS_Discriminator: 0.023325125376383465\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 5.4626312255859375 LOSS_Discriminator: 0.029142352441946667\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 5.525811195373535 LOSS_Discriminator: 0.1605161428451538\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 4.775602340698242 LOSS_Discriminator: 0.1894956429799398\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 3.4602060317993164 LOSS_Discriminator: 0.0843286116917928\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 4.0642242431640625 LOSS_Discriminator: 0.10264219840367635\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 5.065329551696777 LOSS_Discriminator: 0.09439613421758015\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 5.733462333679199 LOSS_Discriminator: 0.011620699117581049\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 6.402948379516602 LOSS_Discriminator: 0.03410756587982178\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 6.0036797523498535 LOSS_Discriminator: 0.1136096715927124\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 5.611413955688477 LOSS_Discriminator: 0.019291200985511143\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 5.7983198165893555 LOSS_Discriminator: 0.02074140061934789\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 5.949831962585449 LOSS_Discriminator: 0.015219974021116892\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 5.136966705322266 LOSS_Discriminator: 0.1301005780696869\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.690377235412598 LOSS_Discriminator: 0.027610597511132557\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 4.367059707641602 LOSS_Discriminator: 0.03263622522354126\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 5.652547359466553 LOSS_Discriminator: 0.04612221817175547\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 5.473923206329346 LOSS_Discriminator: 0.024078937868277233\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 5.073879241943359 LOSS_Discriminator: 0.16016729672749838\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 4.786052703857422 LOSS_Discriminator: 0.23684364557266235\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 4.053718090057373 LOSS_Discriminator: 0.1004822850227356\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 3.717535972595215 LOSS_Discriminator: 0.1887915333112081\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 3.6477363109588623 LOSS_Discriminator: 0.10038135449091594\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 4.7958598136901855 LOSS_Discriminator: 0.056993226210276283\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 5.7824296951293945 LOSS_Discriminator: 0.016882029672463734\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 6.062883377075195 LOSS_Discriminator: 0.04067037502924601\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 6.260125160217285 LOSS_Discriminator: 0.08594292402267456\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 6.005449295043945 LOSS_Discriminator: 0.25835923353830975\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 4.563974380493164 LOSS_Discriminator: 0.1544913649559021\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 3.2806901931762695 LOSS_Discriminator: 0.17530667781829834\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 3.4602465629577637 LOSS_Discriminator: 0.1854567527770996\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 4.922154426574707 LOSS_Discriminator: 0.1401028037071228\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 5.961760997772217 LOSS_Discriminator: 0.03797177970409393\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 6.802515506744385 LOSS_Discriminator: 0.13515700896581015\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 6.832124710083008 LOSS_Discriminator: 0.032820788522561394\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 6.940615177154541 LOSS_Discriminator: 0.004541452353199323\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 7.081334114074707 LOSS_Discriminator: 0.09902310371398926\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 6.485500335693359 LOSS_Discriminator: 0.1981248656908671\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 4.832712173461914 LOSS_Discriminator: 0.15693145990371704\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 3.9861769676208496 LOSS_Discriminator: 0.05597081780433655\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 4.199272632598877 LOSS_Discriminator: 0.07500653465588887\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 4.4508056640625 LOSS_Discriminator: 0.12123737732569377\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 5.477419376373291 LOSS_Discriminator: 0.04296886424223582\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 5.81361198425293 LOSS_Discriminator: 0.1222988764444987\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 5.305664539337158 LOSS_Discriminator: 0.030035994946956635\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 6.199882507324219 LOSS_Discriminator: 0.008444559449950853\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 5.865227699279785 LOSS_Discriminator: 0.08692767222722371\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 5.387233257293701 LOSS_Discriminator: 0.09575782219568889\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 4.661026954650879 LOSS_Discriminator: 0.19224663575490317\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 3.3637447357177734 LOSS_Discriminator: 0.08050450682640076\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 4.582695007324219 LOSS_Discriminator: 0.1382620930671692\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 5.776586055755615 LOSS_Discriminator: 0.03351961076259613\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 7.046411514282227 LOSS_Discriminator: 0.02247798442840576\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 7.248507022857666 LOSS_Discriminator: 0.11188383897145589\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 6.851940155029297 LOSS_Discriminator: 0.16507976253827414\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 6.160730838775635 LOSS_Discriminator: 0.08013999462127686\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 5.725820064544678 LOSS_Discriminator: 0.1835229992866516\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 3.802938461303711 LOSS_Discriminator: 0.04183406631151835\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 3.6316652297973633 LOSS_Discriminator: 0.13903703292210898\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 4.050573348999023 LOSS_Discriminator: 0.19134169816970825\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 4.908186912536621 LOSS_Discriminator: 0.03777831047773361\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.992805480957031 LOSS_Discriminator: 0.022519121567408245\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 6.481795310974121 LOSS_Discriminator: 0.006016020352641742\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 6.644657135009766 LOSS_Discriminator: 0.245834747950236\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 5.899452209472656 LOSS_Discriminator: 0.06173628568649292\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 5.76940393447876 LOSS_Discriminator: 0.03269427518049876\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 4.819613456726074 LOSS_Discriminator: 0.1501425306002299\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 4.565033912658691 LOSS_Discriminator: 0.1833172639211019\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 3.889737367630005 LOSS_Discriminator: 0.08114650348822276\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 4.885366439819336 LOSS_Discriminator: 0.06924071411291759\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 5.4759392738342285 LOSS_Discriminator: 0.10597962141036987\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 5.7400383949279785 LOSS_Discriminator: 0.009573937704165777\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 5.8491902351379395 LOSS_Discriminator: 0.00612840677301089\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 6.374858856201172 LOSS_Discriminator: 0.0852288802464803\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 5.624665260314941 LOSS_Discriminator: 0.39111121495564777\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 4.759301662445068 LOSS_Discriminator: 0.02261638641357422\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 4.434512138366699 LOSS_Discriminator: 0.07192854086558025\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 4.122442245483398 LOSS_Discriminator: 0.12469939390818278\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 4.391489028930664 LOSS_Discriminator: 0.12977537512779236\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 4.822030067443848 LOSS_Discriminator: 0.12944100300470987\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 5.289797782897949 LOSS_Discriminator: 0.12015625834465027\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 5.149641513824463 LOSS_Discriminator: 0.08798887332280476\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 5.3530683517456055 LOSS_Discriminator: 0.013467833399772644\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 4.700651168823242 LOSS_Discriminator: 0.028042204678058624\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 5.072319030761719 LOSS_Discriminator: 0.16568902134895325\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 4.3720855712890625 LOSS_Discriminator: 0.08837754527727763\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 3.9732396602630615 LOSS_Discriminator: 0.03894957403341929\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 4.15824031829834 LOSS_Discriminator: 0.12474177281061809\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 5.12257719039917 LOSS_Discriminator: 0.038871015111605324\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 5.734078407287598 LOSS_Discriminator: 0.030261451999346416\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 6.148220062255859 LOSS_Discriminator: 0.011101594815651575\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 6.959443092346191 LOSS_Discriminator: 0.043361147244771324\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 6.723092079162598 LOSS_Discriminator: 0.23536574840545654\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 6.026880264282227 LOSS_Discriminator: 0.20115633805592856\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 5.08066463470459 LOSS_Discriminator: 0.13691345850626627\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 3.823835849761963 LOSS_Discriminator: 0.1441778540611267\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 3.9994313716888428 LOSS_Discriminator: 0.14937312404314676\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 4.48328971862793 LOSS_Discriminator: 0.04991123080253601\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 5.031756401062012 LOSS_Discriminator: 0.0786249190568924\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 5.613858222961426 LOSS_Discriminator: 0.12527849276860556\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 5.760885238647461 LOSS_Discriminator: 0.03193571170171102\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 6.532876014709473 LOSS_Discriminator: 0.008262996872266134\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 5.720266819000244 LOSS_Discriminator: 0.38874971866607666\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 4.633655071258545 LOSS_Discriminator: 0.019570837418238323\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 4.539775371551514 LOSS_Discriminator: 0.12516375382741293\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 3.8282530307769775 LOSS_Discriminator: 0.11336059371630351\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 4.828701019287109 LOSS_Discriminator: 0.055100137988726296\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 5.064187526702881 LOSS_Discriminator: 0.08330787221590678\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 5.768383979797363 LOSS_Discriminator: 0.15240788459777832\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 5.644820213317871 LOSS_Discriminator: 0.11947200695673625\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 5.676110744476318 LOSS_Discriminator: 0.2003034750620524\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 5.103160858154297 LOSS_Discriminator: 0.01619024947285652\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 3.599640130996704 LOSS_Discriminator: 0.1344725489616394\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 3.943667411804199 LOSS_Discriminator: 0.0865878164768219\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 4.979374885559082 LOSS_Discriminator: 0.17902952432632446\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 4.3163886070251465 LOSS_Discriminator: 0.2231273651123047\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 4.635061264038086 LOSS_Discriminator: 0.09912566343943278\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 4.077209949493408 LOSS_Discriminator: 0.12634718418121338\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 4.936601638793945 LOSS_Discriminator: 0.08448838194211324\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 5.157962799072266 LOSS_Discriminator: 0.032223972181479134\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 6.006537437438965 LOSS_Discriminator: 0.01327938586473465\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 6.003044128417969 LOSS_Discriminator: 0.12390233079592387\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 6.0084662437438965 LOSS_Discriminator: 0.00785628023246924\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 5.500073432922363 LOSS_Discriminator: 0.0961991548538208\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 5.797525405883789 LOSS_Discriminator: 0.02189602454503377\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 5.296316146850586 LOSS_Discriminator: 0.10199503103892009\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 5.078582763671875 LOSS_Discriminator: 0.049168785413106285\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 5.094818592071533 LOSS_Discriminator: 0.03564651062091192\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 5.5879011154174805 LOSS_Discriminator: 0.08046005169550578\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 5.104233264923096 LOSS_Discriminator: 0.01618011047442754\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 5.111105918884277 LOSS_Discriminator: 0.26188095410664874\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 4.851439476013184 LOSS_Discriminator: 0.06212122241655985\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 4.6921515464782715 LOSS_Discriminator: 0.12659618258476257\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 4.858449935913086 LOSS_Discriminator: 0.14581241210301718\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 4.930249214172363 LOSS_Discriminator: 0.07534346481164296\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 4.868889808654785 LOSS_Discriminator: 0.022394801179567974\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 5.426273345947266 LOSS_Discriminator: 0.018483119706312817\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 5.435385704040527 LOSS_Discriminator: 0.07890202601750691\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 4.883965969085693 LOSS_Discriminator: 0.3182976047197978\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 4.278172016143799 LOSS_Discriminator: 0.029591098427772522\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 4.601949691772461 LOSS_Discriminator: 0.06056564052899679\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 4.623896598815918 LOSS_Discriminator: 0.0401503269871076\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 5.25065803527832 LOSS_Discriminator: 0.07906435926755269\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 5.753914833068848 LOSS_Discriminator: 0.027264773845672607\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 6.629230976104736 LOSS_Discriminator: 0.04405441880226135\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 6.076671600341797 LOSS_Discriminator: 0.13909884293874106\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 5.533944129943848 LOSS_Discriminator: 0.0916936198870341\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 5.318955421447754 LOSS_Discriminator: 0.008642416447401047\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 5.101499080657959 LOSS_Discriminator: 0.24838882684707642\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 3.780116081237793 LOSS_Discriminator: 0.33904147148132324\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 3.2851672172546387 LOSS_Discriminator: 0.1947587529818217\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 4.3580732345581055 LOSS_Discriminator: 0.09033524990081787\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 6.523796081542969 LOSS_Discriminator: 0.027200989425182343\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 7.073675155639648 LOSS_Discriminator: 0.01406678557395935\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 7.2889628410339355 LOSS_Discriminator: 0.013948957125345865\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 7.540671348571777 LOSS_Discriminator: 0.2819805145263672\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 7.369399070739746 LOSS_Discriminator: 0.11711715658505757\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 6.739701271057129 LOSS_Discriminator: 0.012610025703907013\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 6.881771564483643 LOSS_Discriminator: 0.041423959036668144\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 6.524696350097656 LOSS_Discriminator: 0.016513541340827942\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 6.558320999145508 LOSS_Discriminator: 0.12654539942741394\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 6.054848670959473 LOSS_Discriminator: 0.14124731222788492\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 6.113219261169434 LOSS_Discriminator: 0.1561645269393921\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 5.234072685241699 LOSS_Discriminator: 0.1099449594815572\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 4.265997886657715 LOSS_Discriminator: 0.04103027284145355\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 3.985952138900757 LOSS_Discriminator: 0.14018680651982626\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 4.536416053771973 LOSS_Discriminator: 0.06721419095993042\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 4.70548152923584 LOSS_Discriminator: 0.09619845946629842\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 4.885599136352539 LOSS_Discriminator: 0.17482171456019083\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 4.545225620269775 LOSS_Discriminator: 0.04141992082198461\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 4.327055931091309 LOSS_Discriminator: 0.14619497458140054\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 4.068701267242432 LOSS_Discriminator: 0.05601418515046438\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 4.535683631896973 LOSS_Discriminator: 0.11686212817827861\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 4.895510673522949 LOSS_Discriminator: 0.06645660102367401\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 5.208662509918213 LOSS_Discriminator: 0.04794709881146749\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 5.723589897155762 LOSS_Discriminator: 0.02061511327823003\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 5.3252410888671875 LOSS_Discriminator: 0.052214935421943665\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 5.29017972946167 LOSS_Discriminator: 0.049342721700668335\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 5.4093170166015625 LOSS_Discriminator: 0.05952456593513489\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 4.818597793579102 LOSS_Discriminator: 0.2707282304763794\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 4.264179229736328 LOSS_Discriminator: 0.06737197438875835\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 3.945659637451172 LOSS_Discriminator: 0.0990377167860667\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 4.615354537963867 LOSS_Discriminator: 0.1729394793510437\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 5.588845729827881 LOSS_Discriminator: 0.01774267852306366\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 5.696949481964111 LOSS_Discriminator: 0.12865854303042093\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 5.828171730041504 LOSS_Discriminator: 0.1198175847530365\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 5.6993560791015625 LOSS_Discriminator: 0.15336286028226218\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 4.897880554199219 LOSS_Discriminator: 0.29112263520558673\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 4.306281089782715 LOSS_Discriminator: 0.014775354415178299\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 4.2090911865234375 LOSS_Discriminator: 0.16167029738426208\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 4.123233318328857 LOSS_Discriminator: 0.12227980295817058\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 4.328854560852051 LOSS_Discriminator: 0.0723695456981659\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 4.5394768714904785 LOSS_Discriminator: 0.04140552878379822\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 4.565991401672363 LOSS_Discriminator: 0.1489008069038391\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 4.871639251708984 LOSS_Discriminator: 0.05931588510672251\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 5.2737627029418945 LOSS_Discriminator: 0.01625105490287145\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 5.177550792694092 LOSS_Discriminator: 0.11376033226648967\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 5.2425537109375 LOSS_Discriminator: 0.15769695242245993\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 4.974184036254883 LOSS_Discriminator: 0.2089785933494568\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 4.278505325317383 LOSS_Discriminator: 0.10267210006713867\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 3.659036159515381 LOSS_Discriminator: 0.10732760032018025\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 4.019355773925781 LOSS_Discriminator: 0.0623087485631307\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 4.93422794342041 LOSS_Discriminator: 0.02140444020430247\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.516098976135254 LOSS_Discriminator: 0.10136590401331584\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 5.203527450561523 LOSS_Discriminator: 0.0928926666577657\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 4.879572868347168 LOSS_Discriminator: 0.08294953405857086\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 4.420137405395508 LOSS_Discriminator: 0.030359089374542236\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 4.228608131408691 LOSS_Discriminator: 0.033998534083366394\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 4.714813709259033 LOSS_Discriminator: 0.1010316510995229\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 5.426662445068359 LOSS_Discriminator: 0.0455317497253418\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 5.394134521484375 LOSS_Discriminator: 0.13498416543006897\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 4.717850208282471 LOSS_Discriminator: 0.069752370317777\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 4.664690971374512 LOSS_Discriminator: 0.0796860009431839\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 4.728561878204346 LOSS_Discriminator: 0.060230329632759094\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 5.190046787261963 LOSS_Discriminator: 0.032802547017733254\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 5.313590049743652 LOSS_Discriminator: 0.04080548137426376\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 5.804678916931152 LOSS_Discriminator: 0.06852865715821584\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 5.4087395668029785 LOSS_Discriminator: 0.05787390470504761\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 5.338080406188965 LOSS_Discriminator: 0.04690588514010111\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 5.346713066101074 LOSS_Discriminator: 0.026570071776707966\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.080673694610596 LOSS_Discriminator: 0.15815595785776773\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 4.5641350746154785 LOSS_Discriminator: 0.041814327239990234\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 4.9965667724609375 LOSS_Discriminator: 0.1068535049756368\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 4.688538074493408 LOSS_Discriminator: 0.02510608732700348\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 5.336002826690674 LOSS_Discriminator: 0.03251568228006363\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 5.938234329223633 LOSS_Discriminator: 0.01643158992131551\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 6.078137397766113 LOSS_Discriminator: 0.0392474631468455\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 5.883358478546143 LOSS_Discriminator: 0.08923870325088501\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 5.298818588256836 LOSS_Discriminator: 0.17137948671976724\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 4.841851234436035 LOSS_Discriminator: 0.07035101453463237\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 3.929185152053833 LOSS_Discriminator: 0.059908429781595864\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 4.253989219665527 LOSS_Discriminator: 0.11750912666320801\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 5.271230697631836 LOSS_Discriminator: 0.025460099180539448\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 4.9989447593688965 LOSS_Discriminator: 0.05956617991129557\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 5.346982955932617 LOSS_Discriminator: 0.13968974351882935\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 6.139035701751709 LOSS_Discriminator: 0.00835538220902284\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 5.4190850257873535 LOSS_Discriminator: 0.17092307408650717\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 4.901052474975586 LOSS_Discriminator: 0.05007659395535787\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 4.7014617919921875 LOSS_Discriminator: 0.0634591281414032\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 4.480925559997559 LOSS_Discriminator: 0.22231662273406982\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 4.585353851318359 LOSS_Discriminator: 0.1321126421292623\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 4.448115348815918 LOSS_Discriminator: 0.0913249949614207\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 4.910306930541992 LOSS_Discriminator: 0.05394678314526876\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 5.397101402282715 LOSS_Discriminator: 0.08533534407615662\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 5.420754432678223 LOSS_Discriminator: 0.03953643639882406\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 5.56424617767334 LOSS_Discriminator: 0.10639525453249614\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 5.2440996170043945 LOSS_Discriminator: 0.02581515411535899\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 5.409604549407959 LOSS_Discriminator: 0.12024688720703125\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 4.928613662719727 LOSS_Discriminator: 0.09484841426213582\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 4.417717456817627 LOSS_Discriminator: 0.18993105491002402\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 4.02119779586792 LOSS_Discriminator: 0.08199057479699452\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 3.91166353225708 LOSS_Discriminator: 0.05137651662031809\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 4.626561641693115 LOSS_Discriminator: 0.08395833770434062\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 5.6542158126831055 LOSS_Discriminator: 0.015159795681635538\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 6.509943962097168 LOSS_Discriminator: 0.007321386287609736\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 6.365398406982422 LOSS_Discriminator: 0.17054406801859537\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 5.9290666580200195 LOSS_Discriminator: 0.14025980234146118\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 5.17897367477417 LOSS_Discriminator: 0.007713105529546738\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 4.690587043762207 LOSS_Discriminator: 0.13147896528244019\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 3.369222640991211 LOSS_Discriminator: 0.06594143311182658\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 4.057236671447754 LOSS_Discriminator: 0.09865703185399373\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 4.7220458984375 LOSS_Discriminator: 0.08369292815526326\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 6.379822254180908 LOSS_Discriminator: 0.029455396036307018\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 6.468902587890625 LOSS_Discriminator: 0.08485161264737447\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 7.0609354972839355 LOSS_Discriminator: 0.033507260183493294\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 6.913018226623535 LOSS_Discriminator: 0.005835569153229396\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 6.569916725158691 LOSS_Discriminator: 0.012852532168229422\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 6.178025722503662 LOSS_Discriminator: 0.17195109526316324\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 6.161376476287842 LOSS_Discriminator: 0.1361332635084788\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 4.580785274505615 LOSS_Discriminator: 0.20267003774642944\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 3.6149330139160156 LOSS_Discriminator: 0.21386166413625082\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 3.292310953140259 LOSS_Discriminator: 0.11156249046325684\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 4.228548526763916 LOSS_Discriminator: 0.13754706581433615\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 5.49415397644043 LOSS_Discriminator: 0.02629664291938146\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 6.08436393737793 LOSS_Discriminator: 0.07685371239980061\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 6.641283988952637 LOSS_Discriminator: 0.156399796406428\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 7.658012866973877 LOSS_Discriminator: 0.006852280348539352\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 7.294666290283203 LOSS_Discriminator: 0.084617813428243\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 6.400031089782715 LOSS_Discriminator: 0.06029433012008667\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 6.454672813415527 LOSS_Discriminator: 0.06774454812208812\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 6.474630355834961 LOSS_Discriminator: 0.16920755306879678\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 5.690820693969727 LOSS_Discriminator: 0.029907872279485066\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 4.786413192749023 LOSS_Discriminator: 0.22916698455810547\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 4.497100830078125 LOSS_Discriminator: 0.04580599069595337\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 3.779470443725586 LOSS_Discriminator: 0.2510552406311035\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 3.921171188354492 LOSS_Discriminator: 0.14555946985880533\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 4.573802947998047 LOSS_Discriminator: 0.060295119881629944\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 5.538325309753418 LOSS_Discriminator: 0.02889160066843033\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 5.683981895446777 LOSS_Discriminator: 0.1920612851778666\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 5.853355407714844 LOSS_Discriminator: 0.005320598681767781\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 5.926641464233398 LOSS_Discriminator: 0.006846862534681956\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 6.0043439865112305 LOSS_Discriminator: 0.03920810421307882\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 5.406363487243652 LOSS_Discriminator: 0.1868953307469686\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 5.069153785705566 LOSS_Discriminator: 0.01885208487510681\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 4.599639415740967 LOSS_Discriminator: 0.03607793649037679\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 4.190190315246582 LOSS_Discriminator: 0.058424984415372215\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 4.650064468383789 LOSS_Discriminator: 0.05016053716341654\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 3.9307146072387695 LOSS_Discriminator: 0.08001736799875896\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 4.116655349731445 LOSS_Discriminator: 0.08180602888266246\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 4.02248477935791 LOSS_Discriminator: 0.10486060380935669\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.782292366027832 LOSS_Discriminator: 0.04116427401701609\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 5.072531700134277 LOSS_Discriminator: 0.25995393594106037\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 4.288116931915283 LOSS_Discriminator: 0.06347488363583882\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 3.9329452514648438 LOSS_Discriminator: 0.23755224545796713\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 3.22662091255188 LOSS_Discriminator: 0.37372978528340656\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 3.416053533554077 LOSS_Discriminator: 0.14522637923558554\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 4.82806396484375 LOSS_Discriminator: 0.12679521242777506\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 5.50499963760376 LOSS_Discriminator: 0.14667105674743652\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 5.72251558303833 LOSS_Discriminator: 0.09150602420171101\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 5.153176307678223 LOSS_Discriminator: 0.09665451447168986\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 4.645660877227783 LOSS_Discriminator: 0.2244208256403605\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 3.735873222351074 LOSS_Discriminator: 0.0936338206132253\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 4.123175621032715 LOSS_Discriminator: 0.12731422980626425\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 5.0756120681762695 LOSS_Discriminator: 0.017509599526723225\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 5.9213714599609375 LOSS_Discriminator: 0.007508857796589534\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 6.678879737854004 LOSS_Discriminator: 0.004676501887540023\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 6.623693943023682 LOSS_Discriminator: 0.06180807948112488\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 5.9973039627075195 LOSS_Discriminator: 0.07357212404410045\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 6.0297040939331055 LOSS_Discriminator: 0.037641783555348717\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 5.299785137176514 LOSS_Discriminator: 0.02538973093032837\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 5.067427158355713 LOSS_Discriminator: 0.05520059665044149\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 5.025233268737793 LOSS_Discriminator: 0.026046201586723328\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 4.6457014083862305 LOSS_Discriminator: 0.01582994560400645\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 5.175899982452393 LOSS_Discriminator: 0.06020836532115936\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 4.7115478515625 LOSS_Discriminator: 0.10191417733828227\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 4.706614971160889 LOSS_Discriminator: 0.04871632158756256\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 5.140225410461426 LOSS_Discriminator: 0.041293077170848846\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 5.562641143798828 LOSS_Discriminator: 0.11690737803777058\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 5.5181989669799805 LOSS_Discriminator: 0.10587213436762492\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 5.028526306152344 LOSS_Discriminator: 0.11095656951268514\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 4.750936985015869 LOSS_Discriminator: 0.07395256062348683\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 4.454405784606934 LOSS_Discriminator: 0.2108070453008016\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 4.230062961578369 LOSS_Discriminator: 0.20816763242085776\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 4.294534206390381 LOSS_Discriminator: 0.08523691693941753\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 4.096467018127441 LOSS_Discriminator: 0.13390042384465536\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 5.2346086502075195 LOSS_Discriminator: 0.06817460060119629\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 5.614465236663818 LOSS_Discriminator: 0.010935332626104355\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 5.627935409545898 LOSS_Discriminator: 0.054652437567710876\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 5.573515892028809 LOSS_Discriminator: 0.1592305302619934\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 5.211276531219482 LOSS_Discriminator: 0.018871837606032688\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 5.143903732299805 LOSS_Discriminator: 0.013329721987247467\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 5.024932861328125 LOSS_Discriminator: 0.0361910214026769\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 5.420668601989746 LOSS_Discriminator: 0.14668182531992593\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 5.034538269042969 LOSS_Discriminator: 0.015516931811968485\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 4.441341876983643 LOSS_Discriminator: 0.3044869105021159\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 4.274152755737305 LOSS_Discriminator: 0.12538965543111166\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 4.055510520935059 LOSS_Discriminator: 0.20639665921529135\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 5.113340377807617 LOSS_Discriminator: 0.05858999490737915\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 6.401875019073486 LOSS_Discriminator: 0.17126633723576865\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 6.159454822540283 LOSS_Discriminator: 0.008384426434834799\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 6.383896350860596 LOSS_Discriminator: 0.12981140613555908\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 5.994424343109131 LOSS_Discriminator: 0.2543481985727946\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 4.670548439025879 LOSS_Discriminator: 0.29574765761693317\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 3.580044746398926 LOSS_Discriminator: 0.16631479064623514\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 3.6358296871185303 LOSS_Discriminator: 0.195450226465861\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 5.621056079864502 LOSS_Discriminator: 0.052520702282587685\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 5.964452743530273 LOSS_Discriminator: 0.011952961484591166\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 6.840009689331055 LOSS_Discriminator: 0.2700393597284953\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 6.807133674621582 LOSS_Discriminator: 0.047801713148752846\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 6.077701568603516 LOSS_Discriminator: 0.006082285816470782\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 6.151515960693359 LOSS_Discriminator: 0.01901014894247055\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 5.712105751037598 LOSS_Discriminator: 0.36493217945098877\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 4.479759216308594 LOSS_Discriminator: 0.24417511622111002\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 3.704765796661377 LOSS_Discriminator: 0.09051306049029033\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 3.7700724601745605 LOSS_Discriminator: 0.1416628360748291\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 3.8671634197235107 LOSS_Discriminator: 0.09596455097198486\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 4.49832820892334 LOSS_Discriminator: 0.06100855767726898\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 4.905240058898926 LOSS_Discriminator: 0.06836311022440593\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 5.6254167556762695 LOSS_Discriminator: 0.16440494855244955\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 5.979853630065918 LOSS_Discriminator: 0.10934941967328389\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 4.746010780334473 LOSS_Discriminator: 0.02301009992758433\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 5.148138046264648 LOSS_Discriminator: 0.01435326412320137\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 4.921568870544434 LOSS_Discriminator: 0.0780293842156728\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 5.137258529663086 LOSS_Discriminator: 0.10289124647776286\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 4.621685981750488 LOSS_Discriminator: 0.10803140203158061\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 3.806731700897217 LOSS_Discriminator: 0.0849992036819458\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 3.554874897003174 LOSS_Discriminator: 0.07335403561592102\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 4.2383832931518555 LOSS_Discriminator: 0.09970265626907349\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 4.259896755218506 LOSS_Discriminator: 0.10585514704386394\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 5.734308242797852 LOSS_Discriminator: 0.023763726154963177\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 5.4950432777404785 LOSS_Discriminator: 0.10087897380193074\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 5.465210437774658 LOSS_Discriminator: 0.10290907820065816\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 5.448779582977295 LOSS_Discriminator: 0.08297017216682434\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 5.171726703643799 LOSS_Discriminator: 0.021409325301647186\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 5.013569355010986 LOSS_Discriminator: 0.02017038439710935\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 4.8646392822265625 LOSS_Discriminator: 0.01649532839655876\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 4.570200443267822 LOSS_Discriminator: 0.22979414463043213\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 4.456482887268066 LOSS_Discriminator: 0.06784217556317647\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 3.867105007171631 LOSS_Discriminator: 0.09480255842208862\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 4.320622444152832 LOSS_Discriminator: 0.0493545929590861\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 5.257942199707031 LOSS_Discriminator: 0.022047221660614014\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 4.981481552124023 LOSS_Discriminator: 0.07287626465161641\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 5.015755653381348 LOSS_Discriminator: 0.05517542362213135\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 4.577282905578613 LOSS_Discriminator: 0.02067013333241145\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 4.857636451721191 LOSS_Discriminator: 0.08343762159347534\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 4.464256286621094 LOSS_Discriminator: 0.03001229465007782\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 4.818316459655762 LOSS_Discriminator: 0.04612261553605398\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 5.3117218017578125 LOSS_Discriminator: 0.052401185035705566\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 5.7570600509643555 LOSS_Discriminator: 0.02830575903256734\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 5.767056465148926 LOSS_Discriminator: 0.13549067576726279\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 5.201999664306641 LOSS_Discriminator: 0.014675517876942953\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 5.063262939453125 LOSS_Discriminator: 0.10995620489120483\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 5.153921127319336 LOSS_Discriminator: 0.0090460404753685\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 5.04540491104126 LOSS_Discriminator: 0.04813775420188904\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 4.749963760375977 LOSS_Discriminator: 0.2698867718378703\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 4.3179731369018555 LOSS_Discriminator: 0.03222805758317312\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 4.052189826965332 LOSS_Discriminator: 0.11657606561978658\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 4.5904083251953125 LOSS_Discriminator: 0.020762766400973003\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 4.400212287902832 LOSS_Discriminator: 0.19791382551193237\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 4.088947296142578 LOSS_Discriminator: 0.08160724739233653\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 4.237482070922852 LOSS_Discriminator: 0.10718335707982381\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 4.568381309509277 LOSS_Discriminator: 0.028883472084999084\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 5.284667015075684 LOSS_Discriminator: 0.12606515487035116\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 5.199385166168213 LOSS_Discriminator: 0.09549599885940552\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 5.377633094787598 LOSS_Discriminator: 0.14420209328333536\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 4.836676597595215 LOSS_Discriminator: 0.030660122632980347\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 4.819195747375488 LOSS_Discriminator: 0.019308503717184067\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 5.170921325683594 LOSS_Discriminator: 0.036707840859889984\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 5.41521692276001 LOSS_Discriminator: 0.06860911846160889\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 5.018030643463135 LOSS_Discriminator: 0.053710718949635826\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 5.014560699462891 LOSS_Discriminator: 0.039522031943003334\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 4.834250450134277 LOSS_Discriminator: 0.12602266669273376\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 4.348687171936035 LOSS_Discriminator: 0.03561332325140635\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 4.5217604637146 LOSS_Discriminator: 0.06168277064959208\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 4.8631367683410645 LOSS_Discriminator: 0.038888643185297646\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 5.424765586853027 LOSS_Discriminator: 0.0302650382121404\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 5.452427387237549 LOSS_Discriminator: 0.06127520898977915\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 5.183905124664307 LOSS_Discriminator: 0.24813361962636313\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 4.045444488525391 LOSS_Discriminator: 0.03191747764746348\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 4.411273002624512 LOSS_Discriminator: 0.12965216239293417\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 4.466268062591553 LOSS_Discriminator: 0.16125641266504923\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 5.043557167053223 LOSS_Discriminator: 0.08381346861521403\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 6.037522315979004 LOSS_Discriminator: 0.030426484843095142\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 6.010922431945801 LOSS_Discriminator: 0.10776962836583455\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 6.035346031188965 LOSS_Discriminator: 0.005236332615216573\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 5.590872287750244 LOSS_Discriminator: 0.2276384433110555\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 4.423943996429443 LOSS_Discriminator: 0.025969865421454113\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 4.373470306396484 LOSS_Discriminator: 0.051486119627952576\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 4.2628021240234375 LOSS_Discriminator: 0.1608867049217224\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 4.142331123352051 LOSS_Discriminator: 0.11615732312202454\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 4.252894878387451 LOSS_Discriminator: 0.1218949556350708\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 4.424657821655273 LOSS_Discriminator: 0.11684990922609965\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 5.019890785217285 LOSS_Discriminator: 0.08018888036410014\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 5.0611772537231445 LOSS_Discriminator: 0.08849517504374187\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 5.035073280334473 LOSS_Discriminator: 0.018625348806381226\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 4.7683000564575195 LOSS_Discriminator: 0.2257253328959147\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 4.776057243347168 LOSS_Discriminator: 0.1473862131436666\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 5.05403995513916 LOSS_Discriminator: 0.06835379699865977\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 5.530883312225342 LOSS_Discriminator: 0.03832692404588064\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 5.852425575256348 LOSS_Discriminator: 0.03352151314417521\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 5.824915885925293 LOSS_Discriminator: 0.07592132687568665\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 5.766355037689209 LOSS_Discriminator: 0.01073940098285675\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 5.806269645690918 LOSS_Discriminator: 0.012578155845403671\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 5.3088531494140625 LOSS_Discriminator: 0.1485977073510488\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 4.376776695251465 LOSS_Discriminator: 0.11151576042175293\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 4.7503533363342285 LOSS_Discriminator: 0.06225035587946574\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 5.373618125915527 LOSS_Discriminator: 0.03966563940048218\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 5.520279884338379 LOSS_Discriminator: 0.2175577481587728\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 5.988062381744385 LOSS_Discriminator: 0.022344072659810383\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 5.913616180419922 LOSS_Discriminator: 0.032763779163360596\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 5.892051696777344 LOSS_Discriminator: 0.09182868401209514\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 5.937634468078613 LOSS_Discriminator: 0.06016175945599874\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 5.675102233886719 LOSS_Discriminator: 0.014122302333513895\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 5.945635795593262 LOSS_Discriminator: 0.01953742653131485\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 5.4267401695251465 LOSS_Discriminator: 0.01622172196706136\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 5.9295973777771 LOSS_Discriminator: 0.0289240429798762\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 6.066354751586914 LOSS_Discriminator: 0.16283953189849854\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 5.371293067932129 LOSS_Discriminator: 0.19059904416402182\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 4.698291778564453 LOSS_Discriminator: 0.019657003382841747\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 4.855618000030518 LOSS_Discriminator: 0.03244320054848989\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 5.425898551940918 LOSS_Discriminator: 0.03372653325398763\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 4.611166000366211 LOSS_Discriminator: 0.17895587285359701\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 4.233680248260498 LOSS_Discriminator: 0.059926504890124\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 4.865096092224121 LOSS_Discriminator: 0.08448618650436401\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 5.922067165374756 LOSS_Discriminator: 0.06554582218329112\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 6.735345840454102 LOSS_Discriminator: 0.0038091220582524934\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 7.250853538513184 LOSS_Discriminator: 0.042367676893870033\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 7.743778705596924 LOSS_Discriminator: 0.19826396306355795\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 7.3426408767700195 LOSS_Discriminator: 0.31537961959838867\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 6.740133762359619 LOSS_Discriminator: 0.19998514652252197\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 6.105515003204346 LOSS_Discriminator: 0.005143751700719197\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 4.948627471923828 LOSS_Discriminator: 0.20524744192759195\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 4.604679584503174 LOSS_Discriminator: 0.06437380115191142\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 4.227874755859375 LOSS_Discriminator: 0.12653936942418417\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 4.4826436042785645 LOSS_Discriminator: 0.09284255901972453\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 4.622154235839844 LOSS_Discriminator: 0.13006016612052917\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 4.779582977294922 LOSS_Discriminator: 0.04804496467113495\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 5.551939010620117 LOSS_Discriminator: 0.06849934160709381\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 5.585704803466797 LOSS_Discriminator: 0.13084203004837036\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 5.797123908996582 LOSS_Discriminator: 0.03562351564566294\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 5.773399353027344 LOSS_Discriminator: 0.007858321070671082\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 5.186861991882324 LOSS_Discriminator: 0.030887633562088013\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 5.253767967224121 LOSS_Discriminator: 0.018395613878965378\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 5.437330722808838 LOSS_Discriminator: 0.215579887231191\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 5.075474739074707 LOSS_Discriminator: 0.01876673474907875\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 4.71955680847168 LOSS_Discriminator: 0.17273078362147012\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 4.572134971618652 LOSS_Discriminator: 0.021373411019643147\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 4.426669597625732 LOSS_Discriminator: 0.05185281236966451\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 5.359537124633789 LOSS_Discriminator: 0.07657658060391744\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 5.594543933868408 LOSS_Discriminator: 0.20300344626108804\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 4.987725257873535 LOSS_Discriminator: 0.021104757984479267\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 5.112361431121826 LOSS_Discriminator: 0.13909655809402466\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 5.164141654968262 LOSS_Discriminator: 0.021891583998998005\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 4.368247032165527 LOSS_Discriminator: 0.16861255963643393\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 4.784212112426758 LOSS_Discriminator: 0.10505868991216023\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 5.047695159912109 LOSS_Discriminator: 0.06783716380596161\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 4.711694717407227 LOSS_Discriminator: 0.12386652827262878\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 4.674203872680664 LOSS_Discriminator: 0.08763827880223592\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 5.345310211181641 LOSS_Discriminator: 0.03488058348496755\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 5.115436553955078 LOSS_Discriminator: 0.03619352728128433\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 5.477359771728516 LOSS_Discriminator: 0.2198634147644043\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.53961706161499 LOSS_Discriminator: 0.045863439639409385\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 5.250946044921875 LOSS_Discriminator: 0.12337265412012736\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 4.827956199645996 LOSS_Discriminator: 0.03609737505515417\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 4.409234046936035 LOSS_Discriminator: 0.16403434673945108\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 4.231592178344727 LOSS_Discriminator: 0.08226530253887177\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 5.142075061798096 LOSS_Discriminator: 0.16504298647244772\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 5.141678810119629 LOSS_Discriminator: 0.023335278034210205\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 5.16326379776001 LOSS_Discriminator: 0.19119731585184732\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 4.273036479949951 LOSS_Discriminator: 0.12991726398468018\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 3.967454433441162 LOSS_Discriminator: 0.07172722617785136\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 4.9192352294921875 LOSS_Discriminator: 0.054526363809903465\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 5.52781867980957 LOSS_Discriminator: 0.08830525477727254\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 4.967123031616211 LOSS_Discriminator: 0.09077618519465129\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 5.110775947570801 LOSS_Discriminator: 0.1826190948486328\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 5.1424713134765625 LOSS_Discriminator: 0.014430330445369085\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 4.97133731842041 LOSS_Discriminator: 0.09567068020502727\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 5.113407611846924 LOSS_Discriminator: 0.08420474330584209\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 4.489407539367676 LOSS_Discriminator: 0.029608127971490223\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.961648464202881 LOSS_Discriminator: 0.028128892183303833\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 5.8445634841918945 LOSS_Discriminator: 0.03448242445786794\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 5.47278356552124 LOSS_Discriminator: 0.014718187352021536\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 5.4719953536987305 LOSS_Discriminator: 0.17879740397135416\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 5.269303321838379 LOSS_Discriminator: 0.05220464368661245\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 5.472074508666992 LOSS_Discriminator: 0.09741099675496419\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 4.296815395355225 LOSS_Discriminator: 0.1901691754659017\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 3.9822351932525635 LOSS_Discriminator: 0.08376499017079671\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 5.2854814529418945 LOSS_Discriminator: 0.05660546819368998\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 5.882699966430664 LOSS_Discriminator: 0.1921991507212321\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 5.700339317321777 LOSS_Discriminator: 0.14411616325378418\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 5.9130353927612305 LOSS_Discriminator: 0.10751819610595703\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 4.663525104522705 LOSS_Discriminator: 0.011501399179299673\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 4.702267646789551 LOSS_Discriminator: 0.051212782661120095\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 4.8811540603637695 LOSS_Discriminator: 0.08174980183442433\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 5.133901119232178 LOSS_Discriminator: 0.02490571141242981\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 4.682445526123047 LOSS_Discriminator: 0.23175668716430664\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 3.6463027000427246 LOSS_Discriminator: 0.07829831540584564\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 3.894684314727783 LOSS_Discriminator: 0.11946193377176921\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 5.468111038208008 LOSS_Discriminator: 0.08276594678560893\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 6.358834266662598 LOSS_Discriminator: 0.165048877398173\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 6.684762001037598 LOSS_Discriminator: 0.10431917508443196\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 5.870785713195801 LOSS_Discriminator: 0.12543992201487222\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 5.986851692199707 LOSS_Discriminator: 0.005118311382830143\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 5.4459147453308105 LOSS_Discriminator: 0.2435619036356608\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 4.206446647644043 LOSS_Discriminator: 0.11312763889630635\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 2.8037147521972656 LOSS_Discriminator: 0.17388004064559937\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 4.143282890319824 LOSS_Discriminator: 0.14642434318860373\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 5.75081729888916 LOSS_Discriminator: 0.07754650712013245\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 6.85103702545166 LOSS_Discriminator: 0.009764509896437326\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 7.498746871948242 LOSS_Discriminator: 0.200538436571757\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 7.774742126464844 LOSS_Discriminator: 0.0015283605704704921\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 7.472677230834961 LOSS_Discriminator: 0.18412462870279947\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 7.126973628997803 LOSS_Discriminator: 0.11713320016860962\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 6.332219123840332 LOSS_Discriminator: 0.003785727545619011\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 5.190524101257324 LOSS_Discriminator: 0.09253422419230144\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 5.227287292480469 LOSS_Discriminator: 0.126234233379364\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 4.5627641677856445 LOSS_Discriminator: 0.01997990409533183\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 4.1252546310424805 LOSS_Discriminator: 0.059478312730789185\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 4.310868263244629 LOSS_Discriminator: 0.10242336988449097\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 5.312364101409912 LOSS_Discriminator: 0.093476802110672\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 5.311184883117676 LOSS_Discriminator: 0.05058476825555166\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 5.449652194976807 LOSS_Discriminator: 0.017907779663801193\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 5.032166481018066 LOSS_Discriminator: 0.15706825256347656\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 5.385265350341797 LOSS_Discriminator: 0.011096807817618052\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 5.3745269775390625 LOSS_Discriminator: 0.02576057364543279\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 5.070112228393555 LOSS_Discriminator: 0.08673038085301717\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 5.229949474334717 LOSS_Discriminator: 0.031061490376790363\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 4.073057174682617 LOSS_Discriminator: 0.1249392032623291\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 3.7776942253112793 LOSS_Discriminator: 0.02975691854953766\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 4.416869163513184 LOSS_Discriminator: 0.0736224502325058\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 5.315263748168945 LOSS_Discriminator: 0.10676248868306477\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 5.579629898071289 LOSS_Discriminator: 0.027165375649929047\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 5.833798885345459 LOSS_Discriminator: 0.024046972393989563\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 5.772411823272705 LOSS_Discriminator: 0.17836600542068481\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 5.6254987716674805 LOSS_Discriminator: 0.008056119705239931\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 5.608129501342773 LOSS_Discriminator: 0.10548705856005351\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 4.930807113647461 LOSS_Discriminator: 0.01535673439502716\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 4.607484340667725 LOSS_Discriminator: 0.16608611742655435\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 3.9031550884246826 LOSS_Discriminator: 0.09851384162902832\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 4.211120128631592 LOSS_Discriminator: 0.24862202008565268\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 4.511298179626465 LOSS_Discriminator: 0.08710099260012309\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 4.846436023712158 LOSS_Discriminator: 0.13623850544293722\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 5.0218424797058105 LOSS_Discriminator: 0.1377927859624227\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 5.225361347198486 LOSS_Discriminator: 0.2770893375078837\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 4.813183784484863 LOSS_Discriminator: 0.06164306402206421\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 4.322091102600098 LOSS_Discriminator: 0.16504226128260294\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 3.7475948333740234 LOSS_Discriminator: 0.14993063608805338\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 4.10725736618042 LOSS_Discriminator: 0.09821385145187378\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 4.793071746826172 LOSS_Discriminator: 0.08083561062812805\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 5.919172286987305 LOSS_Discriminator: 0.015035908669233322\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 6.640326499938965 LOSS_Discriminator: 0.003981974286337693\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 6.2730793952941895 LOSS_Discriminator: 0.2442315419514974\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 6.197968006134033 LOSS_Discriminator: 0.18784149487813315\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 5.4905500411987305 LOSS_Discriminator: 0.25078686078389484\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 5.141397476196289 LOSS_Discriminator: 0.04600008328755697\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 4.351079940795898 LOSS_Discriminator: 0.026253486673037212\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 4.2426300048828125 LOSS_Discriminator: 0.07547580202420552\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 3.9809927940368652 LOSS_Discriminator: 0.05455549558003744\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 4.5391845703125 LOSS_Discriminator: 0.032085721691449486\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 4.397398948669434 LOSS_Discriminator: 0.20247324307759604\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 3.8740153312683105 LOSS_Discriminator: 0.13206527630488077\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 4.278027534484863 LOSS_Discriminator: 0.141273965438207\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 5.26243257522583 LOSS_Discriminator: 0.033994351824124656\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 5.763062000274658 LOSS_Discriminator: 0.1437364121278127\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 5.945725440979004 LOSS_Discriminator: 0.15456165870030722\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 5.470727920532227 LOSS_Discriminator: 0.06739125152428944\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 4.925850868225098 LOSS_Discriminator: 0.019770470758279163\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 5.049640655517578 LOSS_Discriminator: 0.15085838238398233\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 4.090213775634766 LOSS_Discriminator: 0.0906054178873698\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 3.974928617477417 LOSS_Discriminator: 0.20815306901931763\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 3.6900248527526855 LOSS_Discriminator: 0.05116725961367289\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 3.9980628490448 LOSS_Discriminator: 0.12145530184110005\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 4.4902238845825195 LOSS_Discriminator: 0.028725363314151764\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 5.257715702056885 LOSS_Discriminator: 0.08211549123128255\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 4.973535537719727 LOSS_Discriminator: 0.16030937433242798\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 4.652371883392334 LOSS_Discriminator: 0.03621178368727366\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 4.234678268432617 LOSS_Discriminator: 0.2152027686436971\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 4.184146881103516 LOSS_Discriminator: 0.03786400457223257\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 3.736283302307129 LOSS_Discriminator: 0.12966295083363852\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 4.328060626983643 LOSS_Discriminator: 0.10288832585016887\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 4.769883155822754 LOSS_Discriminator: 0.09893296162287395\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 4.695652961730957 LOSS_Discriminator: 0.0394886831442515\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 4.434758186340332 LOSS_Discriminator: 0.10034650564193726\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 4.908408164978027 LOSS_Discriminator: 0.027582906186580658\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 4.822615623474121 LOSS_Discriminator: 0.16504804293314615\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 4.235584259033203 LOSS_Discriminator: 0.0713175634543101\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 4.0372843742370605 LOSS_Discriminator: 0.10523513952891032\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 4.842771053314209 LOSS_Discriminator: 0.05065663158893585\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 5.181873321533203 LOSS_Discriminator: 0.024485026796658833\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 5.371220588684082 LOSS_Discriminator: 0.03456610937913259\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 5.047610282897949 LOSS_Discriminator: 0.10468826691309611\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 5.128973960876465 LOSS_Discriminator: 0.1540855666001638\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 3.815035343170166 LOSS_Discriminator: 0.08046821753184001\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 4.277602195739746 LOSS_Discriminator: 0.06945466001828511\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 4.982593536376953 LOSS_Discriminator: 0.12587222456932068\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 5.453179359436035 LOSS_Discriminator: 0.014837315926949183\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 5.825723648071289 LOSS_Discriminator: 0.12863453229268393\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 5.611331462860107 LOSS_Discriminator: 0.01033757378657659\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 5.535043716430664 LOSS_Discriminator: 0.07637547453244527\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 4.754106044769287 LOSS_Discriminator: 0.17211214701334634\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 3.894101619720459 LOSS_Discriminator: 0.07116747895876567\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 3.827440023422241 LOSS_Discriminator: 0.14293696482976279\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 5.413064479827881 LOSS_Discriminator: 0.07535964250564575\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 6.554586887359619 LOSS_Discriminator: 0.1594292422135671\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 6.723900318145752 LOSS_Discriminator: 0.16313007473945618\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 6.278148651123047 LOSS_Discriminator: 0.004128282889723778\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 5.769042015075684 LOSS_Discriminator: 0.3656279643376668\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 4.936666488647461 LOSS_Discriminator: 0.14161191383997598\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 4.35910701751709 LOSS_Discriminator: 0.03912949562072754\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 4.687862873077393 LOSS_Discriminator: 0.09916152556737264\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 4.670352935791016 LOSS_Discriminator: 0.09888880451520284\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 5.131466865539551 LOSS_Discriminator: 0.09779858589172363\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 5.232133865356445 LOSS_Discriminator: 0.03237701952457428\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 5.263005256652832 LOSS_Discriminator: 0.10363027453422546\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 5.022754669189453 LOSS_Discriminator: 0.040931070844332375\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 4.711019515991211 LOSS_Discriminator: 0.13353355725606283\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 5.386246681213379 LOSS_Discriminator: 0.039335732658704124\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 4.876527786254883 LOSS_Discriminator: 0.023021981120109558\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.38136100769043 LOSS_Discriminator: 0.03472472230593363\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 5.720216751098633 LOSS_Discriminator: 0.047020941972732544\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 5.2843427658081055 LOSS_Discriminator: 0.10314871867497762\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 5.566926002502441 LOSS_Discriminator: 0.010034686575333277\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 4.482429027557373 LOSS_Discriminator: 0.06383008758227031\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 4.690935134887695 LOSS_Discriminator: 0.04652698338031769\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 4.681279182434082 LOSS_Discriminator: 0.18512511253356934\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 4.070863246917725 LOSS_Discriminator: 0.13447211186091104\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 4.213464736938477 LOSS_Discriminator: 0.05151561896006266\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 5.05605411529541 LOSS_Discriminator: 0.04744383692741394\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 5.908419609069824 LOSS_Discriminator: 0.053681751092274986\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 6.523401260375977 LOSS_Discriminator: 0.02889864643414815\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 6.399405002593994 LOSS_Discriminator: 0.04377813140551249\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 6.544735908508301 LOSS_Discriminator: 0.11891324321428935\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 5.762609958648682 LOSS_Discriminator: 0.2776229977607727\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 4.384516716003418 LOSS_Discriminator: 0.0360526368021965\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 3.630248785018921 LOSS_Discriminator: 0.20146965980529785\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 3.873473644256592 LOSS_Discriminator: 0.15828126668930054\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 5.479065895080566 LOSS_Discriminator: 0.0890774925549825\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 6.434186935424805 LOSS_Discriminator: 0.07387304306030273\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 6.983043670654297 LOSS_Discriminator: 0.1162863274415334\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 6.469839096069336 LOSS_Discriminator: 0.340692679087321\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 5.9023261070251465 LOSS_Discriminator: 0.12573721011479697\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 4.401885032653809 LOSS_Discriminator: 0.16242124636967978\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 4.038430690765381 LOSS_Discriminator: 0.20995833476384482\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 3.6884117126464844 LOSS_Discriminator: 0.23099199930826822\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 4.618281364440918 LOSS_Discriminator: 0.058134754498799644\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 5.421639442443848 LOSS_Discriminator: 0.10240668058395386\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 6.452862739562988 LOSS_Discriminator: 0.020652477939923603\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 6.65207576751709 LOSS_Discriminator: 0.18098453680674234\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 5.9077863693237305 LOSS_Discriminator: 0.08920650680859883\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 5.974059104919434 LOSS_Discriminator: 0.01119854673743248\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 5.471583366394043 LOSS_Discriminator: 0.013820774853229523\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 5.636880397796631 LOSS_Discriminator: 0.03102644036213557\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 5.362029075622559 LOSS_Discriminator: 0.028186726073424023\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 5.762585639953613 LOSS_Discriminator: 0.16833579540252686\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 5.575878143310547 LOSS_Discriminator: 0.014456300685803095\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 5.309193134307861 LOSS_Discriminator: 0.08648364742596944\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 5.1404876708984375 LOSS_Discriminator: 0.15571395556131998\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 5.092308044433594 LOSS_Discriminator: 0.019620998452107113\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 4.947785377502441 LOSS_Discriminator: 0.13429676493008932\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 5.394067764282227 LOSS_Discriminator: 0.06948018570741017\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 5.631670951843262 LOSS_Discriminator: 0.03767079611619314\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 5.2862982749938965 LOSS_Discriminator: 0.20096755027770996\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 4.752050399780273 LOSS_Discriminator: 0.03871172914902369\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 4.492280006408691 LOSS_Discriminator: 0.04889507591724396\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 4.949833869934082 LOSS_Discriminator: 0.1420670449733734\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 4.804331302642822 LOSS_Discriminator: 0.09465992450714111\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 4.290545463562012 LOSS_Discriminator: 0.23964214324951172\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 3.552476406097412 LOSS_Discriminator: 0.1594829559326172\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 3.493797540664673 LOSS_Discriminator: 0.25738227367401123\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 5.288049697875977 LOSS_Discriminator: 0.06678573290506999\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 6.330065727233887 LOSS_Discriminator: 0.10823047161102295\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 6.639689922332764 LOSS_Discriminator: 0.013422577331463495\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 6.994057655334473 LOSS_Discriminator: 0.08796694874763489\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 6.910303115844727 LOSS_Discriminator: 0.001692564847568671\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 7.220012664794922 LOSS_Discriminator: 0.0018244682190318902\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 7.153848648071289 LOSS_Discriminator: 0.03700374315182368\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 6.90938663482666 LOSS_Discriminator: 0.013610350588957468\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 6.716801643371582 LOSS_Discriminator: 0.0148806298772494\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 6.762443542480469 LOSS_Discriminator: 0.16787678003311157\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 5.7035675048828125 LOSS_Discriminator: 0.21613585948944092\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 5.193807601928711 LOSS_Discriminator: 0.01632365584373474\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 4.420657157897949 LOSS_Discriminator: 0.16310691833496094\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 4.060673713684082 LOSS_Discriminator: 0.11770604054133098\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 4.875458717346191 LOSS_Discriminator: 0.08672616879145305\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 5.575862407684326 LOSS_Discriminator: 0.027966074645519257\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 6.154176712036133 LOSS_Discriminator: 0.16766677300135294\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 6.344832897186279 LOSS_Discriminator: 0.11971845229466756\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 6.076988220214844 LOSS_Discriminator: 0.11677900950113933\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 5.54822301864624 LOSS_Discriminator: 0.2584232489267985\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 4.2403411865234375 LOSS_Discriminator: 0.2190169095993042\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 3.391923427581787 LOSS_Discriminator: 0.09436780214309692\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 4.602409362792969 LOSS_Discriminator: 0.08563344677289327\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 5.1529388427734375 LOSS_Discriminator: 0.17589332660039267\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 5.390160083770752 LOSS_Discriminator: 0.17993927001953125\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 5.551182746887207 LOSS_Discriminator: 0.1613917350769043\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 4.90140438079834 LOSS_Discriminator: 0.01873632272084554\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 5.044642448425293 LOSS_Discriminator: 0.05299307902654012\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 4.670549392700195 LOSS_Discriminator: 0.1471263567606608\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 4.418941020965576 LOSS_Discriminator: 0.1686453421910604\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 4.008930683135986 LOSS_Discriminator: 0.0702342689037323\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 3.9895360469818115 LOSS_Discriminator: 0.26642600695292157\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 4.127790927886963 LOSS_Discriminator: 0.1435249149799347\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 4.232789993286133 LOSS_Discriminator: 0.15014179547627768\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 4.97465705871582 LOSS_Discriminator: 0.12392844756444295\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 5.567825794219971 LOSS_Discriminator: 0.026171498000621796\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 5.893002510070801 LOSS_Discriminator: 0.008213511357704798\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 5.924120903015137 LOSS_Discriminator: 0.042205169796943665\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 5.750856876373291 LOSS_Discriminator: 0.2779871424039205\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 4.823581695556641 LOSS_Discriminator: 0.21159271399180093\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 4.020791053771973 LOSS_Discriminator: 0.07881938914457957\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 4.386049270629883 LOSS_Discriminator: 0.1963417927424113\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 4.458956718444824 LOSS_Discriminator: 0.03989894191424052\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 5.0731520652771 LOSS_Discriminator: 0.016940706719954807\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 5.466861724853516 LOSS_Discriminator: 0.131168395280838\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 5.552227973937988 LOSS_Discriminator: 0.24219719568888345\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 4.867183208465576 LOSS_Discriminator: 0.16072489817937216\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 4.279621124267578 LOSS_Discriminator: 0.06782633562882741\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 4.1601409912109375 LOSS_Discriminator: 0.06034615139166514\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 4.8978590965271 LOSS_Discriminator: 0.06037294864654541\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 5.457958698272705 LOSS_Discriminator: 0.09870592753092448\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 5.831930160522461 LOSS_Discriminator: 0.06460998455683391\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 5.842299938201904 LOSS_Discriminator: 0.025003160039583843\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 5.78051233291626 LOSS_Discriminator: 0.03587063898642858\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 5.583529472351074 LOSS_Discriminator: 0.3128541906674703\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 4.682308673858643 LOSS_Discriminator: 0.10075612862904866\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 3.9679412841796875 LOSS_Discriminator: 0.03389536837736765\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 4.405059337615967 LOSS_Discriminator: 0.055511285861333214\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 4.631916046142578 LOSS_Discriminator: 0.13583234945933023\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 4.655697822570801 LOSS_Discriminator: 0.16278135776519775\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 5.001596450805664 LOSS_Discriminator: 0.02752439429362615\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 5.041923522949219 LOSS_Discriminator: 0.046516746282577515\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 4.887119293212891 LOSS_Discriminator: 0.06710431973139445\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 4.801382541656494 LOSS_Discriminator: 0.1212495466073354\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 4.873470783233643 LOSS_Discriminator: 0.043985992670059204\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 5.250478744506836 LOSS_Discriminator: 0.04717579980691274\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 5.630237579345703 LOSS_Discriminator: 0.0974839727083842\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 5.601655960083008 LOSS_Discriminator: 0.036712855100631714\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 5.250995635986328 LOSS_Discriminator: 0.00984201394021511\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 5.387307643890381 LOSS_Discriminator: 0.12212645014127095\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 3.992875337600708 LOSS_Discriminator: 0.17859357595443726\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 2.707101821899414 LOSS_Discriminator: 0.15970329443613687\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 3.9558656215667725 LOSS_Discriminator: 0.22936201095581055\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 5.5600457191467285 LOSS_Discriminator: 0.10479881366093953\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 6.376252174377441 LOSS_Discriminator: 0.08004943033059438\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 6.812370777130127 LOSS_Discriminator: 0.0037385386725266776\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 7.185082912445068 LOSS_Discriminator: 0.1766714652379354\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 6.635627746582031 LOSS_Discriminator: 0.0013786822867890198\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 6.916061878204346 LOSS_Discriminator: 0.23620525995890299\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 6.496002197265625 LOSS_Discriminator: 0.19471458594004312\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 5.716330051422119 LOSS_Discriminator: 0.004898932452003161\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 5.448185920715332 LOSS_Discriminator: 0.01117946207523346\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 5.114376544952393 LOSS_Discriminator: 0.19061487913131714\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 4.5862531661987305 LOSS_Discriminator: 0.028810851275920868\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 4.047057151794434 LOSS_Discriminator: 0.03918276230494181\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 4.527749538421631 LOSS_Discriminator: 0.07490143179893494\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 5.31041145324707 LOSS_Discriminator: 0.04168510437011719\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 6.1525468826293945 LOSS_Discriminator: 0.2552766402562459\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 6.037351608276367 LOSS_Discriminator: 0.03362812350193659\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 5.759610652923584 LOSS_Discriminator: 0.1020756463209788\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 5.153769493103027 LOSS_Discriminator: 0.17472986380259195\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 4.804555892944336 LOSS_Discriminator: 0.14927456776301065\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 4.363157272338867 LOSS_Discriminator: 0.06299164891242981\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 4.0618696212768555 LOSS_Discriminator: 0.06331167121728261\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 4.121158599853516 LOSS_Discriminator: 0.21266818046569824\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 5.9231061935424805 LOSS_Discriminator: 0.09107246001561482\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 5.914794921875 LOSS_Discriminator: 0.06630885601043701\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 5.807058811187744 LOSS_Discriminator: 0.1433504025141398\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 5.5688676834106445 LOSS_Discriminator: 0.10357066988945007\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 4.740750789642334 LOSS_Discriminator: 0.19679278135299683\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 4.032886028289795 LOSS_Discriminator: 0.08871881167093913\n",
            "EPOCH OVER: 28\n",
            "ITERATION_NO.: 1 LOSS_Generator: 4.129324913024902 LOSS_Discriminator: 0.10842982927958171\n",
            "ITERATION_NO.: 2 LOSS_Generator: 4.94564962387085 LOSS_Discriminator: 0.11311371127764384\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.329140663146973 LOSS_Discriminator: 0.02397991716861725\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.839138031005859 LOSS_Discriminator: 0.009512842322389284\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.668879985809326 LOSS_Discriminator: 0.2166736125946045\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.151958465576172 LOSS_Discriminator: 0.08295497794946034\n",
            "ITERATION_NO.: 7 LOSS_Generator: 4.428081512451172 LOSS_Discriminator: 0.14507835110028586\n",
            "ITERATION_NO.: 8 LOSS_Generator: 4.555675506591797 LOSS_Discriminator: 0.06050158540407816\n",
            "ITERATION_NO.: 9 LOSS_Generator: 4.556729793548584 LOSS_Discriminator: 0.16889095306396484\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.128724575042725 LOSS_Discriminator: 0.07120398183663686\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.778796195983887 LOSS_Discriminator: 0.08926369746526082\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.72812557220459 LOSS_Discriminator: 0.021882491807142895\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.928274631500244 LOSS_Discriminator: 0.18380419413248697\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.463173866271973 LOSS_Discriminator: 0.07977147897084554\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.135819435119629 LOSS_Discriminator: 0.1333280106385549\n",
            "ITERATION_NO.: 16 LOSS_Generator: 4.69622802734375 LOSS_Discriminator: 0.09987123807271321\n",
            "ITERATION_NO.: 17 LOSS_Generator: 4.787306785583496 LOSS_Discriminator: 0.05463096499443054\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.778357982635498 LOSS_Discriminator: 0.07620449364185333\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.988327980041504 LOSS_Discriminator: 0.045303910970687866\n",
            "ITERATION_NO.: 20 LOSS_Generator: 6.093608856201172 LOSS_Discriminator: 0.05417133371035258\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.373517990112305 LOSS_Discriminator: 0.005454787363608678\n",
            "ITERATION_NO.: 22 LOSS_Generator: 6.610068321228027 LOSS_Discriminator: 0.0588520218928655\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.848358154296875 LOSS_Discriminator: 0.07538857062657674\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.919834613800049 LOSS_Discriminator: 0.038364884754021965\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.727171897888184 LOSS_Discriminator: 0.059926887353261314\n",
            "ITERATION_NO.: 26 LOSS_Generator: 4.888171195983887 LOSS_Discriminator: 0.016677599400281906\n",
            "ITERATION_NO.: 27 LOSS_Generator: 4.647452354431152 LOSS_Discriminator: 0.19559588034947714\n",
            "ITERATION_NO.: 28 LOSS_Generator: 4.579521179199219 LOSS_Discriminator: 0.06394435962041219\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.5462541580200195 LOSS_Discriminator: 0.07729545732339223\n",
            "ITERATION_NO.: 30 LOSS_Generator: 4.669084072113037 LOSS_Discriminator: 0.15647226572036743\n",
            "ITERATION_NO.: 31 LOSS_Generator: 4.897977352142334 LOSS_Discriminator: 0.04416319727897644\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.431004524230957 LOSS_Discriminator: 0.04018078247706095\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.162903785705566 LOSS_Discriminator: 0.05922112365563711\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.578240871429443 LOSS_Discriminator: 0.03095049907763799\n",
            "ITERATION_NO.: 35 LOSS_Generator: 6.186349868774414 LOSS_Discriminator: 0.009168831010659536\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.138082981109619 LOSS_Discriminator: 0.007549019530415535\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.326318740844727 LOSS_Discriminator: 0.26782437165578205\n",
            "ITERATION_NO.: 38 LOSS_Generator: 4.710740089416504 LOSS_Discriminator: 0.40445903937021893\n",
            "ITERATION_NO.: 39 LOSS_Generator: 2.7215750217437744 LOSS_Discriminator: 0.20845377445220947\n",
            "ITERATION_NO.: 40 LOSS_Generator: 3.2144484519958496 LOSS_Discriminator: 0.2552509307861328\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.590095520019531 LOSS_Discriminator: 0.18027790387471518\n",
            "ITERATION_NO.: 42 LOSS_Generator: 6.326847076416016 LOSS_Discriminator: 0.04363854229450226\n",
            "ITERATION_NO.: 43 LOSS_Generator: 7.266206741333008 LOSS_Discriminator: 0.01902836188673973\n",
            "ITERATION_NO.: 44 LOSS_Generator: 7.461251258850098 LOSS_Discriminator: 0.3399934768676758\n",
            "ITERATION_NO.: 45 LOSS_Generator: 7.338160514831543 LOSS_Discriminator: 0.017573426167170208\n",
            "ITERATION_NO.: 46 LOSS_Generator: 7.020635604858398 LOSS_Discriminator: 0.001710055706401666\n",
            "ITERATION_NO.: 47 LOSS_Generator: 7.319479465484619 LOSS_Discriminator: 0.14579635858535767\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.897574424743652 LOSS_Discriminator: 0.227708101272583\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.021040916442871 LOSS_Discriminator: 0.006791739414135615\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.06559944152832 LOSS_Discriminator: 0.17086664835611978\n",
            "ITERATION_NO.: 51 LOSS_Generator: 4.868996620178223 LOSS_Discriminator: 0.031567456821600594\n",
            "ITERATION_NO.: 52 LOSS_Generator: 4.087095260620117 LOSS_Discriminator: 0.10645824670791626\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.016231060028076 LOSS_Discriminator: 0.11838631828625996\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.04572868347168 LOSS_Discriminator: 0.021924637258052826\n",
            "ITERATION_NO.: 55 LOSS_Generator: 6.327197551727295 LOSS_Discriminator: 0.12413134177525838\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.896096229553223 LOSS_Discriminator: 0.005448365584015846\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.997413635253906 LOSS_Discriminator: 0.03812414159377416\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.145120143890381 LOSS_Discriminator: 0.13321329156557718\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.023667335510254 LOSS_Discriminator: 0.1502460241317749\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.208819389343262 LOSS_Discriminator: 0.0737154483795166\n",
            "ITERATION_NO.: 61 LOSS_Generator: 4.825886249542236 LOSS_Discriminator: 0.16861106952031454\n",
            "ITERATION_NO.: 62 LOSS_Generator: 4.64763069152832 LOSS_Discriminator: 0.1646841069062551\n",
            "ITERATION_NO.: 63 LOSS_Generator: 4.636332988739014 LOSS_Discriminator: 0.08372233311335246\n",
            "ITERATION_NO.: 64 LOSS_Generator: 4.418688774108887 LOSS_Discriminator: 0.05167647202809652\n",
            "ITERATION_NO.: 65 LOSS_Generator: 4.6876935958862305 LOSS_Discriminator: 0.020338510473569233\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.029610633850098 LOSS_Discriminator: 0.07289284964402516\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.37841796875 LOSS_Discriminator: 0.01968979959686597\n",
            "ITERATION_NO.: 68 LOSS_Generator: 4.987703323364258 LOSS_Discriminator: 0.08204392592112224\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.183191299438477 LOSS_Discriminator: 0.14410855372746786\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.087494850158691 LOSS_Discriminator: 0.07186000049114227\n",
            "ITERATION_NO.: 71 LOSS_Generator: 4.988681316375732 LOSS_Discriminator: 0.04072689016660055\n",
            "ITERATION_NO.: 72 LOSS_Generator: 4.483975410461426 LOSS_Discriminator: 0.03053155541419983\n",
            "ITERATION_NO.: 73 LOSS_Generator: 4.803703308105469 LOSS_Discriminator: 0.03232106566429138\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.485585689544678 LOSS_Discriminator: 0.060034687320391335\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.617766380310059 LOSS_Discriminator: 0.02666849394639333\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.603783130645752 LOSS_Discriminator: 0.03076518326997757\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.5074968338012695 LOSS_Discriminator: 0.31983379522959393\n",
            "ITERATION_NO.: 78 LOSS_Generator: 4.638111114501953 LOSS_Discriminator: 0.14064122239748636\n",
            "ITERATION_NO.: 79 LOSS_Generator: 4.4517669677734375 LOSS_Discriminator: 0.14183988173802695\n",
            "ITERATION_NO.: 80 LOSS_Generator: 3.9821486473083496 LOSS_Discriminator: 0.20531153678894043\n",
            "ITERATION_NO.: 81 LOSS_Generator: 4.230652809143066 LOSS_Discriminator: 0.1727909247080485\n",
            "ITERATION_NO.: 82 LOSS_Generator: 4.600478172302246 LOSS_Discriminator: 0.027097543080647785\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.430166244506836 LOSS_Discriminator: 0.3839558760325114\n",
            "ITERATION_NO.: 84 LOSS_Generator: 4.786811828613281 LOSS_Discriminator: 0.11526018381118774\n",
            "ITERATION_NO.: 85 LOSS_Generator: 4.839918613433838 LOSS_Discriminator: 0.04151817907889684\n",
            "ITERATION_NO.: 86 LOSS_Generator: 4.4112229347229 LOSS_Discriminator: 0.0507896343866984\n",
            "ITERATION_NO.: 87 LOSS_Generator: 4.693177223205566 LOSS_Discriminator: 0.08116128544012706\n",
            "ITERATION_NO.: 88 LOSS_Generator: 4.986852645874023 LOSS_Discriminator: 0.019936208923657734\n",
            "ITERATION_NO.: 89 LOSS_Generator: 4.88486385345459 LOSS_Discriminator: 0.02378682792186737\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.797952651977539 LOSS_Discriminator: 0.10575005412101746\n",
            "ITERATION_NO.: 91 LOSS_Generator: 4.539278984069824 LOSS_Discriminator: 0.30796051025390625\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.240015983581543 LOSS_Discriminator: 0.02223395307858785\n",
            "ITERATION_NO.: 93 LOSS_Generator: 4.223191261291504 LOSS_Discriminator: 0.02523656189441681\n",
            "ITERATION_NO.: 94 LOSS_Generator: 4.546427249908447 LOSS_Discriminator: 0.05663511653741201\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.083071708679199 LOSS_Discriminator: 0.03070070594549179\n",
            "ITERATION_NO.: 96 LOSS_Generator: 4.867264747619629 LOSS_Discriminator: 0.05251826345920563\n",
            "ITERATION_NO.: 97 LOSS_Generator: 3.9242706298828125 LOSS_Discriminator: 0.31868743896484375\n",
            "ITERATION_NO.: 98 LOSS_Generator: 3.328101634979248 LOSS_Discriminator: 0.07800008356571198\n",
            "ITERATION_NO.: 99 LOSS_Generator: 4.02256965637207 LOSS_Discriminator: 0.1120637059211731\n",
            "ITERATION_NO.: 100 LOSS_Generator: 4.802683353424072 LOSS_Discriminator: 0.17236379782358804\n",
            "ITERATION_NO.: 101 LOSS_Generator: 6.18251895904541 LOSS_Discriminator: 0.11737893025080363\n",
            "ITERATION_NO.: 102 LOSS_Generator: 6.389708518981934 LOSS_Discriminator: 0.32621198892593384\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.431399345397949 LOSS_Discriminator: 0.00509452261030674\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.546677112579346 LOSS_Discriminator: 0.006449516241749127\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.034862518310547 LOSS_Discriminator: 0.3527652819951375\n",
            "ITERATION_NO.: 106 LOSS_Generator: 3.71097993850708 LOSS_Discriminator: 0.16682104269663492\n",
            "ITERATION_NO.: 107 LOSS_Generator: 3.2011096477508545 LOSS_Discriminator: 0.12426383296648662\n",
            "ITERATION_NO.: 108 LOSS_Generator: 4.53845739364624 LOSS_Discriminator: 0.10059853394826253\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.468881607055664 LOSS_Discriminator: 0.1023000677426656\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.519316673278809 LOSS_Discriminator: 0.22879954179128012\n",
            "ITERATION_NO.: 111 LOSS_Generator: 4.986204147338867 LOSS_Discriminator: 0.13897316654523215\n",
            "ITERATION_NO.: 112 LOSS_Generator: 4.481526851654053 LOSS_Discriminator: 0.052351286013921104\n",
            "ITERATION_NO.: 113 LOSS_Generator: 4.530435562133789 LOSS_Discriminator: 0.033850640058517456\n",
            "ITERATION_NO.: 114 LOSS_Generator: 4.195630073547363 LOSS_Discriminator: 0.1442048947016398\n",
            "ITERATION_NO.: 115 LOSS_Generator: 4.526423454284668 LOSS_Discriminator: 0.1145816445350647\n",
            "ITERATION_NO.: 116 LOSS_Generator: 4.871435165405273 LOSS_Discriminator: 0.1398156483968099\n",
            "ITERATION_NO.: 117 LOSS_Generator: 4.865517616271973 LOSS_Discriminator: 0.023302083214124043\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.189480781555176 LOSS_Discriminator: 0.013557871182759603\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.5289225578308105 LOSS_Discriminator: 0.012117963284254074\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.6760759353637695 LOSS_Discriminator: 0.08545170227686565\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.164412021636963 LOSS_Discriminator: 0.10160293181737264\n",
            "ITERATION_NO.: 122 LOSS_Generator: 4.792112827301025 LOSS_Discriminator: 0.061203062534332275\n",
            "ITERATION_NO.: 123 LOSS_Generator: 4.352076530456543 LOSS_Discriminator: 0.14365942279497781\n",
            "ITERATION_NO.: 124 LOSS_Generator: 4.332864761352539 LOSS_Discriminator: 0.11025087038675944\n",
            "ITERATION_NO.: 125 LOSS_Generator: 4.173129558563232 LOSS_Discriminator: 0.23181339104970297\n",
            "ITERATION_NO.: 126 LOSS_Generator: 4.779409408569336 LOSS_Discriminator: 0.08074210087458293\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.111052513122559 LOSS_Discriminator: 0.02001712719599406\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.803388595581055 LOSS_Discriminator: 0.017567093173662823\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.54248046875 LOSS_Discriminator: 0.13860967755317688\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.06893253326416 LOSS_Discriminator: 0.16490977009137472\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.884112358093262 LOSS_Discriminator: 0.02515435963869095\n",
            "ITERATION_NO.: 132 LOSS_Generator: 4.344631195068359 LOSS_Discriminator: 0.1601519783337911\n",
            "ITERATION_NO.: 133 LOSS_Generator: 3.5739502906799316 LOSS_Discriminator: 0.12422077854474385\n",
            "ITERATION_NO.: 134 LOSS_Generator: 2.979948043823242 LOSS_Discriminator: 0.2254629929860433\n",
            "ITERATION_NO.: 135 LOSS_Generator: 3.862286329269409 LOSS_Discriminator: 0.19113781054814658\n",
            "ITERATION_NO.: 136 LOSS_Generator: 4.892267227172852 LOSS_Discriminator: 0.05051714678605398\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.121349334716797 LOSS_Discriminator: 0.13393634557724\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.68977689743042 LOSS_Discriminator: 0.0946218768755595\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.943158149719238 LOSS_Discriminator: 0.01830540969967842\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.565586566925049 LOSS_Discriminator: 0.09815709789594014\n",
            "ITERATION_NO.: 141 LOSS_Generator: 4.724861145019531 LOSS_Discriminator: 0.12403382857640584\n",
            "ITERATION_NO.: 142 LOSS_Generator: 4.767274856567383 LOSS_Discriminator: 0.04723511139551798\n",
            "ITERATION_NO.: 143 LOSS_Generator: 4.15184211730957 LOSS_Discriminator: 0.05214521288871765\n",
            "ITERATION_NO.: 144 LOSS_Generator: 4.140303611755371 LOSS_Discriminator: 0.06411468982696533\n",
            "ITERATION_NO.: 145 LOSS_Generator: 4.705370903015137 LOSS_Discriminator: 0.04922215143839518\n",
            "ITERATION_NO.: 146 LOSS_Generator: 4.973649501800537 LOSS_Discriminator: 0.07038097580273946\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.245635986328125 LOSS_Discriminator: 0.06585462391376495\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.515447616577148 LOSS_Discriminator: 0.015808509041865666\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.340245246887207 LOSS_Discriminator: 0.028939828276634216\n",
            "ITERATION_NO.: 150 LOSS_Generator: 6.063427925109863 LOSS_Discriminator: 0.03464516500631968\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.272841930389404 LOSS_Discriminator: 0.042817811171213783\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.925931453704834 LOSS_Discriminator: 0.01670312136411667\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.383090972900391 LOSS_Discriminator: 0.5512929757436117\n",
            "ITERATION_NO.: 154 LOSS_Generator: 4.416690826416016 LOSS_Discriminator: 0.331140140692393\n",
            "ITERATION_NO.: 155 LOSS_Generator: 3.1913671493530273 LOSS_Discriminator: 0.09942426284154256\n",
            "ITERATION_NO.: 156 LOSS_Generator: 3.8524086475372314 LOSS_Discriminator: 0.2220225731531779\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.37343692779541 LOSS_Discriminator: 0.07242532571156819\n",
            "ITERATION_NO.: 158 LOSS_Generator: 6.175765037536621 LOSS_Discriminator: 0.0067983753979206085\n",
            "ITERATION_NO.: 159 LOSS_Generator: 6.784536361694336 LOSS_Discriminator: 0.0028889055053393045\n",
            "ITERATION_NO.: 160 LOSS_Generator: 7.112391471862793 LOSS_Discriminator: 0.039730424682299294\n",
            "ITERATION_NO.: 161 LOSS_Generator: 7.315434455871582 LOSS_Discriminator: 0.23743915557861328\n",
            "ITERATION_NO.: 162 LOSS_Generator: 7.249401569366455 LOSS_Discriminator: 0.07330943644046783\n",
            "ITERATION_NO.: 163 LOSS_Generator: 6.501825332641602 LOSS_Discriminator: 0.13537966211636862\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.653051376342773 LOSS_Discriminator: 0.41391468048095703\n",
            "ITERATION_NO.: 165 LOSS_Generator: 4.49250602722168 LOSS_Discriminator: 0.3169203797976176\n",
            "ITERATION_NO.: 166 LOSS_Generator: 3.202456474304199 LOSS_Discriminator: 0.0740229884783427\n",
            "ITERATION_NO.: 167 LOSS_Generator: 3.15867280960083 LOSS_Discriminator: 0.2706569830576579\n",
            "ITERATION_NO.: 168 LOSS_Generator: 3.560981273651123 LOSS_Discriminator: 0.199475367863973\n",
            "ITERATION_NO.: 169 LOSS_Generator: 4.821406364440918 LOSS_Discriminator: 0.09320986270904541\n",
            "ITERATION_NO.: 170 LOSS_Generator: 4.890744686126709 LOSS_Discriminator: 0.23329277833302817\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.3133134841918945 LOSS_Discriminator: 0.013397092620531717\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.514651775360107 LOSS_Discriminator: 0.008982311313350996\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.310060977935791 LOSS_Discriminator: 0.010371974358956019\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.593698024749756 LOSS_Discriminator: 0.057695756355921425\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.513856887817383 LOSS_Discriminator: 0.007947524388631185\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.599384307861328 LOSS_Discriminator: 0.010073016708095869\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.795069694519043 LOSS_Discriminator: 0.006870203341046969\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.493121147155762 LOSS_Discriminator: 0.10675090551376343\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.186923980712891 LOSS_Discriminator: 0.12859774629275003\n",
            "ITERATION_NO.: 180 LOSS_Generator: 4.811979293823242 LOSS_Discriminator: 0.013065396497646967\n",
            "ITERATION_NO.: 181 LOSS_Generator: 3.9359652996063232 LOSS_Discriminator: 0.06651859482129414\n",
            "ITERATION_NO.: 182 LOSS_Generator: 4.137636184692383 LOSS_Discriminator: 0.15925034880638123\n",
            "ITERATION_NO.: 183 LOSS_Generator: 4.836236000061035 LOSS_Discriminator: 0.07607115805149078\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.098336219787598 LOSS_Discriminator: 0.14507966240247092\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.2508015632629395 LOSS_Discriminator: 0.00985224669178327\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.816667079925537 LOSS_Discriminator: 0.10007651646931966\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.671164512634277 LOSS_Discriminator: 0.00797021264831225\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.093135356903076 LOSS_Discriminator: 0.11310413479804993\n",
            "ITERATION_NO.: 189 LOSS_Generator: 4.939483642578125 LOSS_Discriminator: 0.12451525529225667\n",
            "ITERATION_NO.: 190 LOSS_Generator: 4.1372880935668945 LOSS_Discriminator: 0.22320876518885294\n",
            "ITERATION_NO.: 191 LOSS_Generator: 4.0239787101745605 LOSS_Discriminator: 0.21209263801574707\n",
            "ITERATION_NO.: 192 LOSS_Generator: 3.3972511291503906 LOSS_Discriminator: 0.11183082063992818\n",
            "ITERATION_NO.: 193 LOSS_Generator: 4.402454376220703 LOSS_Discriminator: 0.14016338189442953\n",
            "ITERATION_NO.: 194 LOSS_Generator: 4.340864658355713 LOSS_Discriminator: 0.15349078178405762\n",
            "ITERATION_NO.: 195 LOSS_Generator: 4.954755783081055 LOSS_Discriminator: 0.053707813223203026\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.292459011077881 LOSS_Discriminator: 0.11581820249557495\n",
            "ITERATION_NO.: 197 LOSS_Generator: 4.792913913726807 LOSS_Discriminator: 0.19882955153783163\n",
            "ITERATION_NO.: 198 LOSS_Generator: 4.3922295570373535 LOSS_Discriminator: 0.17172745863596597\n",
            "ITERATION_NO.: 199 LOSS_Generator: 4.111446380615234 LOSS_Discriminator: 0.0868947704633077\n",
            "ITERATION_NO.: 200 LOSS_Generator: 4.185059070587158 LOSS_Discriminator: 0.09354258577028911\n",
            "ITERATION_NO.: 201 LOSS_Generator: 4.982605934143066 LOSS_Discriminator: 0.03513317803541819\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.146359443664551 LOSS_Discriminator: 0.18316193421681723\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.39381742477417 LOSS_Discriminator: 0.08190698424975078\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.017828941345215 LOSS_Discriminator: 0.051089540123939514\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.147590637207031 LOSS_Discriminator: 0.021754880746205647\n",
            "ITERATION_NO.: 206 LOSS_Generator: 4.557677268981934 LOSS_Discriminator: 0.10923797885576884\n",
            "ITERATION_NO.: 207 LOSS_Generator: 4.789880752563477 LOSS_Discriminator: 0.10151076316833496\n",
            "ITERATION_NO.: 208 LOSS_Generator: 4.541263580322266 LOSS_Discriminator: 0.0398192952076594\n",
            "ITERATION_NO.: 209 LOSS_Generator: 4.4808759689331055 LOSS_Discriminator: 0.07210823893547058\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.013522148132324 LOSS_Discriminator: 0.03353161116441091\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.138432025909424 LOSS_Discriminator: 0.07016592721144359\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.098344802856445 LOSS_Discriminator: 0.08028713862101237\n",
            "ITERATION_NO.: 213 LOSS_Generator: 4.817314147949219 LOSS_Discriminator: 0.017548431952794392\n",
            "ITERATION_NO.: 214 LOSS_Generator: 4.881566047668457 LOSS_Discriminator: 0.09993306795756023\n",
            "ITERATION_NO.: 215 LOSS_Generator: 4.684435844421387 LOSS_Discriminator: 0.01867356275518735\n",
            "ITERATION_NO.: 216 LOSS_Generator: 4.623793601989746 LOSS_Discriminator: 0.06299465894699097\n",
            "ITERATION_NO.: 217 LOSS_Generator: 4.827816963195801 LOSS_Discriminator: 0.12502169609069824\n",
            "ITERATION_NO.: 218 LOSS_Generator: 4.387971878051758 LOSS_Discriminator: 0.13634355862935385\n",
            "ITERATION_NO.: 219 LOSS_Generator: 3.960625648498535 LOSS_Discriminator: 0.04391611615816752\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.014988422393799 LOSS_Discriminator: 0.06566964089870453\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.240272521972656 LOSS_Discriminator: 0.017505576213200886\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.133755683898926 LOSS_Discriminator: 0.12750234206517538\n",
            "ITERATION_NO.: 223 LOSS_Generator: 6.062591075897217 LOSS_Discriminator: 0.01213438684741656\n",
            "ITERATION_NO.: 224 LOSS_Generator: 6.136703968048096 LOSS_Discriminator: 0.03348998228708903\n",
            "ITERATION_NO.: 225 LOSS_Generator: 6.107725143432617 LOSS_Discriminator: 0.18694154421488443\n",
            "ITERATION_NO.: 226 LOSS_Generator: 4.996156692504883 LOSS_Discriminator: 0.4561714331309001\n",
            "ITERATION_NO.: 227 LOSS_Generator: 3.328130006790161 LOSS_Discriminator: 0.14996426304181418\n",
            "ITERATION_NO.: 228 LOSS_Generator: 3.008439540863037 LOSS_Discriminator: 0.16248220205307007\n",
            "ITERATION_NO.: 229 LOSS_Generator: 4.199103355407715 LOSS_Discriminator: 0.14964860677719116\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.6443867683410645 LOSS_Discriminator: 0.03924394647280375\n",
            "ITERATION_NO.: 231 LOSS_Generator: 6.315328121185303 LOSS_Discriminator: 0.0952576994895935\n",
            "ITERATION_NO.: 232 LOSS_Generator: 6.861522674560547 LOSS_Discriminator: 0.06358055273691814\n",
            "ITERATION_NO.: 233 LOSS_Generator: 6.3451433181762695 LOSS_Discriminator: 0.1537946859995524\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.801084518432617 LOSS_Discriminator: 0.4921569029490153\n",
            "ITERATION_NO.: 235 LOSS_Generator: 4.937685012817383 LOSS_Discriminator: 0.09270995855331421\n",
            "ITERATION_NO.: 236 LOSS_Generator: 3.977627754211426 LOSS_Discriminator: 0.04611319303512573\n",
            "ITERATION_NO.: 237 LOSS_Generator: 3.664860248565674 LOSS_Discriminator: 0.21098701159159342\n",
            "ITERATION_NO.: 238 LOSS_Generator: 4.042113304138184 LOSS_Discriminator: 0.1421379049619039\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.372632026672363 LOSS_Discriminator: 0.026950786511103313\n",
            "ITERATION_NO.: 240 LOSS_Generator: 6.180448532104492 LOSS_Discriminator: 0.013890755673249563\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.710084438323975 LOSS_Discriminator: 0.003293412116666635\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.645575523376465 LOSS_Discriminator: 0.6401108900705973\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.136386394500732 LOSS_Discriminator: 0.1473710834980011\n",
            "ITERATION_NO.: 244 LOSS_Generator: 4.896303176879883 LOSS_Discriminator: 0.09693615635236104\n",
            "ITERATION_NO.: 245 LOSS_Generator: 3.967538356781006 LOSS_Discriminator: 0.04185721774895986\n",
            "ITERATION_NO.: 246 LOSS_Generator: 3.8098490238189697 LOSS_Discriminator: 0.07397609949111938\n",
            "ITERATION_NO.: 247 LOSS_Generator: 3.722844123840332 LOSS_Discriminator: 0.0996817946434021\n",
            "ITERATION_NO.: 248 LOSS_Generator: 4.836498737335205 LOSS_Discriminator: 0.042444199323654175\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.175568580627441 LOSS_Discriminator: 0.1867223580678304\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.29151725769043 LOSS_Discriminator: 0.019663066913684208\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.474799156188965 LOSS_Discriminator: 0.14529598752657572\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.585610389709473 LOSS_Discriminator: 0.011132664978504181\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.455519676208496 LOSS_Discriminator: 0.007255338753263156\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.608885765075684 LOSS_Discriminator: 0.04998784760634104\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.8055524826049805 LOSS_Discriminator: 0.04237145185470581\n",
            "ITERATION_NO.: 256 LOSS_Generator: 4.312041759490967 LOSS_Discriminator: 0.2639448642730713\n",
            "ITERATION_NO.: 257 LOSS_Generator: 3.8264503479003906 LOSS_Discriminator: 0.037938142816225685\n",
            "ITERATION_NO.: 258 LOSS_Generator: 3.522024154663086 LOSS_Discriminator: 0.10620134075482686\n",
            "ITERATION_NO.: 259 LOSS_Generator: 4.524252891540527 LOSS_Discriminator: 0.06314677993456523\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.738029956817627 LOSS_Discriminator: 0.022165951629479725\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.2981719970703125 LOSS_Discriminator: 0.006722442805767059\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.417984962463379 LOSS_Discriminator: 0.15527806679407755\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.402927398681641 LOSS_Discriminator: 0.09270877639452617\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.746374130249023 LOSS_Discriminator: 0.1892599662144979\n",
            "ITERATION_NO.: 265 LOSS_Generator: 5.303999900817871 LOSS_Discriminator: 0.0651695728302002\n",
            "ITERATION_NO.: 266 LOSS_Generator: 4.397701740264893 LOSS_Discriminator: 0.07469683885574341\n",
            "ITERATION_NO.: 267 LOSS_Generator: 3.9313836097717285 LOSS_Discriminator: 0.15818317731221518\n",
            "ITERATION_NO.: 268 LOSS_Generator: 3.7683987617492676 LOSS_Discriminator: 0.20963823795318604\n",
            "ITERATION_NO.: 269 LOSS_Generator: 4.137327194213867 LOSS_Discriminator: 0.16749163468678793\n",
            "ITERATION_NO.: 270 LOSS_Generator: 3.967681884765625 LOSS_Discriminator: 0.06644252936045329\n",
            "ITERATION_NO.: 271 LOSS_Generator: 4.92968225479126 LOSS_Discriminator: 0.1515111823876699\n",
            "ITERATION_NO.: 272 LOSS_Generator: 4.711416244506836 LOSS_Discriminator: 0.373746395111084\n",
            "ITERATION_NO.: 273 LOSS_Generator: 3.6547327041625977 LOSS_Discriminator: 0.10422455271085103\n",
            "ITERATION_NO.: 274 LOSS_Generator: 3.65779447555542 LOSS_Discriminator: 0.09164244929949443\n",
            "ITERATION_NO.: 275 LOSS_Generator: 3.6740450859069824 LOSS_Discriminator: 0.06879737973213196\n",
            "ITERATION_NO.: 276 LOSS_Generator: 4.497320652008057 LOSS_Discriminator: 0.10432267189025879\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.375676155090332 LOSS_Discriminator: 0.11769968271255493\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.230629920959473 LOSS_Discriminator: 0.010239319254954657\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.25184440612793 LOSS_Discriminator: 0.3075409730275472\n",
            "ITERATION_NO.: 280 LOSS_Generator: 4.912199020385742 LOSS_Discriminator: 0.04309822122255961\n",
            "ITERATION_NO.: 281 LOSS_Generator: 4.869078159332275 LOSS_Discriminator: 0.010653849691152573\n",
            "ITERATION_NO.: 282 LOSS_Generator: 4.589983940124512 LOSS_Discriminator: 0.016080377002557118\n",
            "ITERATION_NO.: 283 LOSS_Generator: 4.562959671020508 LOSS_Discriminator: 0.1851634979248047\n",
            "ITERATION_NO.: 284 LOSS_Generator: 3.820063829421997 LOSS_Discriminator: 0.08968069156010945\n",
            "ITERATION_NO.: 285 LOSS_Generator: 3.6249022483825684 LOSS_Discriminator: 0.12767519553502402\n",
            "ITERATION_NO.: 286 LOSS_Generator: 4.053914546966553 LOSS_Discriminator: 0.06755017240842183\n",
            "ITERATION_NO.: 287 LOSS_Generator: 4.4524946212768555 LOSS_Discriminator: 0.07352734605471294\n",
            "ITERATION_NO.: 288 LOSS_Generator: 4.79994010925293 LOSS_Discriminator: 0.2072367270787557\n",
            "ITERATION_NO.: 289 LOSS_Generator: 4.833497047424316 LOSS_Discriminator: 0.03410871575276057\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.164497375488281 LOSS_Discriminator: 0.03625381489594778\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.215143203735352 LOSS_Discriminator: 0.041483283042907715\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.490786552429199 LOSS_Discriminator: 0.04596478740374247\n",
            "ITERATION_NO.: 293 LOSS_Generator: 4.82075309753418 LOSS_Discriminator: 0.11078583200772603\n",
            "ITERATION_NO.: 294 LOSS_Generator: 4.2236409187316895 LOSS_Discriminator: 0.08314994970957439\n",
            "ITERATION_NO.: 295 LOSS_Generator: 4.082098007202148 LOSS_Discriminator: 0.07434280713399251\n",
            "ITERATION_NO.: 296 LOSS_Generator: 4.008986949920654 LOSS_Discriminator: 0.059760610262552895\n",
            "ITERATION_NO.: 297 LOSS_Generator: 4.085825443267822 LOSS_Discriminator: 0.09234660863876343\n",
            "ITERATION_NO.: 298 LOSS_Generator: 4.661281585693359 LOSS_Discriminator: 0.038009703159332275\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.1660380363464355 LOSS_Discriminator: 0.013942512373129526\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.562288284301758 LOSS_Discriminator: 0.13881168762842813\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.288257598876953 LOSS_Discriminator: 0.11911399165789287\n",
            "ITERATION_NO.: 302 LOSS_Generator: 4.394780158996582 LOSS_Discriminator: 0.0734251042207082\n",
            "ITERATION_NO.: 303 LOSS_Generator: 3.9770355224609375 LOSS_Discriminator: 0.15315654873847961\n",
            "ITERATION_NO.: 304 LOSS_Generator: 3.4299025535583496 LOSS_Discriminator: 0.06505727767944336\n",
            "ITERATION_NO.: 305 LOSS_Generator: 4.52618408203125 LOSS_Discriminator: 0.08391340573628743\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.196313858032227 LOSS_Discriminator: 0.11228010058403015\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.948313236236572 LOSS_Discriminator: 0.02241550137599309\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.113491058349609 LOSS_Discriminator: 0.10350556174914043\n",
            "ITERATION_NO.: 309 LOSS_Generator: 6.060672283172607 LOSS_Discriminator: 0.005795914679765701\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.19624662399292 LOSS_Discriminator: 0.08635310331980388\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.951178550720215 LOSS_Discriminator: 0.014157183468341827\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.68146276473999 LOSS_Discriminator: 0.012078836560249329\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.716858863830566 LOSS_Discriminator: 0.09649904568990071\n",
            "ITERATION_NO.: 314 LOSS_Generator: 4.525272369384766 LOSS_Discriminator: 0.18827283382415771\n",
            "ITERATION_NO.: 315 LOSS_Generator: 4.400376796722412 LOSS_Discriminator: 0.08061712980270386\n",
            "ITERATION_NO.: 316 LOSS_Generator: 4.504827499389648 LOSS_Discriminator: 0.06434068083763123\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.180068492889404 LOSS_Discriminator: 0.036984180410703026\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.449802398681641 LOSS_Discriminator: 0.09842907389005025\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.453888416290283 LOSS_Discriminator: 0.20992066462834677\n",
            "ITERATION_NO.: 320 LOSS_Generator: 4.943224906921387 LOSS_Discriminator: 0.012594804167747498\n",
            "ITERATION_NO.: 321 LOSS_Generator: 4.992093086242676 LOSS_Discriminator: 0.17629092931747437\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.152947425842285 LOSS_Discriminator: 0.09663317600886027\n",
            "ITERATION_NO.: 323 LOSS_Generator: 4.42672061920166 LOSS_Discriminator: 0.35399556159973145\n",
            "ITERATION_NO.: 324 LOSS_Generator: 3.7129621505737305 LOSS_Discriminator: 0.08092217147350311\n",
            "ITERATION_NO.: 325 LOSS_Generator: 3.5007989406585693 LOSS_Discriminator: 0.06004680196444193\n",
            "ITERATION_NO.: 326 LOSS_Generator: 4.400367736816406 LOSS_Discriminator: 0.0596547524134318\n",
            "ITERATION_NO.: 327 LOSS_Generator: 4.6858415603637695 LOSS_Discriminator: 0.15777179598808289\n",
            "ITERATION_NO.: 328 LOSS_Generator: 4.549291610717773 LOSS_Discriminator: 0.06085886061191559\n",
            "ITERATION_NO.: 329 LOSS_Generator: 4.193049907684326 LOSS_Discriminator: 0.3062091072400411\n",
            "ITERATION_NO.: 330 LOSS_Generator: 3.945502996444702 LOSS_Discriminator: 0.15522815783818564\n",
            "ITERATION_NO.: 331 LOSS_Generator: 3.8266239166259766 LOSS_Discriminator: 0.06964141130447388\n",
            "ITERATION_NO.: 332 LOSS_Generator: 4.695068359375 LOSS_Discriminator: 0.035995326936244965\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.266901016235352 LOSS_Discriminator: 0.017143957316875458\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.320711135864258 LOSS_Discriminator: 0.019933012624581654\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.564723491668701 LOSS_Discriminator: 0.12652909755706787\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.446209907531738 LOSS_Discriminator: 0.0992447833220164\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.273408889770508 LOSS_Discriminator: 0.15034348766009012\n",
            "ITERATION_NO.: 338 LOSS_Generator: 4.715916633605957 LOSS_Discriminator: 0.10409583648045857\n",
            "ITERATION_NO.: 339 LOSS_Generator: 4.454927444458008 LOSS_Discriminator: 0.06797640522321065\n",
            "ITERATION_NO.: 340 LOSS_Generator: 4.254326343536377 LOSS_Discriminator: 0.04243559638659159\n",
            "ITERATION_NO.: 341 LOSS_Generator: 4.107133388519287 LOSS_Discriminator: 0.06395216286182404\n",
            "ITERATION_NO.: 342 LOSS_Generator: 4.989744186401367 LOSS_Discriminator: 0.022548864285151165\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.413148880004883 LOSS_Discriminator: 0.09439238905906677\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.864005088806152 LOSS_Discriminator: 0.00843981901804606\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.636708736419678 LOSS_Discriminator: 0.18186149994532266\n",
            "ITERATION_NO.: 346 LOSS_Generator: 4.643838882446289 LOSS_Discriminator: 0.20102045933405557\n",
            "ITERATION_NO.: 347 LOSS_Generator: 3.9183874130249023 LOSS_Discriminator: 0.06959402561187744\n",
            "ITERATION_NO.: 348 LOSS_Generator: 3.9265294075012207 LOSS_Discriminator: 0.05384773015975952\n",
            "ITERATION_NO.: 349 LOSS_Generator: 4.829038143157959 LOSS_Discriminator: 0.08751368522644043\n",
            "ITERATION_NO.: 350 LOSS_Generator: 4.86646842956543 LOSS_Discriminator: 0.01815275102853775\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.227878570556641 LOSS_Discriminator: 0.03701319297154745\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.328783988952637 LOSS_Discriminator: 0.07465798656145732\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.2220988273620605 LOSS_Discriminator: 0.19329055150349936\n",
            "ITERATION_NO.: 354 LOSS_Generator: 4.171125888824463 LOSS_Discriminator: 0.237950066725413\n",
            "ITERATION_NO.: 355 LOSS_Generator: 3.548724889755249 LOSS_Discriminator: 0.11434160669644673\n",
            "ITERATION_NO.: 356 LOSS_Generator: 3.5651276111602783 LOSS_Discriminator: 0.09745546181996663\n",
            "ITERATION_NO.: 357 LOSS_Generator: 4.715363502502441 LOSS_Discriminator: 0.07227951288223267\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.399020195007324 LOSS_Discriminator: 0.07859245439370473\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.367635726928711 LOSS_Discriminator: 0.20182031393051147\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.350093364715576 LOSS_Discriminator: 0.06044130523999532\n",
            "ITERATION_NO.: 361 LOSS_Generator: 4.979592323303223 LOSS_Discriminator: 0.14683568477630615\n",
            "ITERATION_NO.: 362 LOSS_Generator: 4.601415157318115 LOSS_Discriminator: 0.01867303003867467\n",
            "ITERATION_NO.: 363 LOSS_Generator: 4.85551118850708 LOSS_Discriminator: 0.025017311175664265\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.2009992599487305 LOSS_Discriminator: 0.06108689308166504\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.473198890686035 LOSS_Discriminator: 0.10284037391344707\n",
            "ITERATION_NO.: 366 LOSS_Generator: 4.983554363250732 LOSS_Discriminator: 0.03133494406938553\n",
            "ITERATION_NO.: 367 LOSS_Generator: 4.796958923339844 LOSS_Discriminator: 0.011796172708272934\n",
            "ITERATION_NO.: 368 LOSS_Generator: 4.834897994995117 LOSS_Discriminator: 0.14702850580215454\n",
            "ITERATION_NO.: 369 LOSS_Generator: 4.500360488891602 LOSS_Discriminator: 0.11406206091245015\n",
            "ITERATION_NO.: 370 LOSS_Generator: 4.157135963439941 LOSS_Discriminator: 0.099455326795578\n",
            "ITERATION_NO.: 371 LOSS_Generator: 3.8604393005371094 LOSS_Discriminator: 0.12628614902496338\n",
            "ITERATION_NO.: 372 LOSS_Generator: 4.864831924438477 LOSS_Discriminator: 0.03702525794506073\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.744023323059082 LOSS_Discriminator: 0.011682109286387762\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.2846784591674805 LOSS_Discriminator: 0.2937493324279785\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.09610652923584 LOSS_Discriminator: 0.09389164050420125\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.141151428222656 LOSS_Discriminator: 0.11232826113700867\n",
            "ITERATION_NO.: 377 LOSS_Generator: 4.974824905395508 LOSS_Discriminator: 0.053495521346728005\n",
            "ITERATION_NO.: 378 LOSS_Generator: 4.265407085418701 LOSS_Discriminator: 0.044635613759358726\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.008012771606445 LOSS_Discriminator: 0.051784699161847435\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.126876354217529 LOSS_Discriminator: 0.02729040135939916\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.438241004943848 LOSS_Discriminator: 0.30956172943115234\n",
            "ITERATION_NO.: 382 LOSS_Generator: 4.959169387817383 LOSS_Discriminator: 0.1105000873406728\n",
            "ITERATION_NO.: 383 LOSS_Generator: 4.5098876953125 LOSS_Discriminator: 0.09476303060849507\n",
            "ITERATION_NO.: 384 LOSS_Generator: 4.06477165222168 LOSS_Discriminator: 0.14349140723546347\n",
            "ITERATION_NO.: 385 LOSS_Generator: 3.701817750930786 LOSS_Discriminator: 0.09619457523028056\n",
            "ITERATION_NO.: 386 LOSS_Generator: 4.0602898597717285 LOSS_Discriminator: 0.06503016750017802\n",
            "ITERATION_NO.: 387 LOSS_Generator: 4.987363815307617 LOSS_Discriminator: 0.0484249492486318\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.601353168487549 LOSS_Discriminator: 0.09441418449083964\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.390867233276367 LOSS_Discriminator: 0.13807934522628784\n",
            "ITERATION_NO.: 390 LOSS_Generator: 4.405187129974365 LOSS_Discriminator: 0.1707993745803833\n",
            "ITERATION_NO.: 391 LOSS_Generator: 4.253257751464844 LOSS_Discriminator: 0.0529467910528183\n",
            "ITERATION_NO.: 392 LOSS_Generator: 3.593615770339966 LOSS_Discriminator: 0.20146673917770386\n",
            "ITERATION_NO.: 393 LOSS_Generator: 4.104445934295654 LOSS_Discriminator: 0.13939669728279114\n",
            "ITERATION_NO.: 394 LOSS_Generator: 4.547586917877197 LOSS_Discriminator: 0.18034990628560385\n",
            "ITERATION_NO.: 395 LOSS_Generator: 4.778226375579834 LOSS_Discriminator: 0.01821657766898473\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.246103286743164 LOSS_Discriminator: 0.047906135519345604\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.46491813659668 LOSS_Discriminator: 0.024981580674648285\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.893923759460449 LOSS_Discriminator: 0.007424431542555491\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.755525588989258 LOSS_Discriminator: 0.045978580911954246\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.31058931350708 LOSS_Discriminator: 0.006668231760462125\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.731236457824707 LOSS_Discriminator: 0.11101582646369934\n",
            "ITERATION_NO.: 402 LOSS_Generator: 4.424685478210449 LOSS_Discriminator: 0.20761418342590332\n",
            "ITERATION_NO.: 403 LOSS_Generator: 3.4878430366516113 LOSS_Discriminator: 0.12159266074498494\n",
            "ITERATION_NO.: 404 LOSS_Generator: 3.4062559604644775 LOSS_Discriminator: 0.16855275630950928\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.3259687423706055 LOSS_Discriminator: 0.06029685835043589\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.622361183166504 LOSS_Discriminator: 0.014987088739871979\n",
            "ITERATION_NO.: 407 LOSS_Generator: 7.149534702301025 LOSS_Discriminator: 0.003640640527009964\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.622332572937012 LOSS_Discriminator: 0.0032593272626399994\n",
            "ITERATION_NO.: 409 LOSS_Generator: 7.468043327331543 LOSS_Discriminator: 0.27003451188405353\n",
            "ITERATION_NO.: 410 LOSS_Generator: 7.566389083862305 LOSS_Discriminator: 0.22455376386642456\n",
            "ITERATION_NO.: 411 LOSS_Generator: 6.692789077758789 LOSS_Discriminator: 0.17345734437306723\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.745233058929443 LOSS_Discriminator: 0.1172449787457784\n",
            "ITERATION_NO.: 413 LOSS_Generator: 4.368587493896484 LOSS_Discriminator: 0.1921990712483724\n",
            "ITERATION_NO.: 414 LOSS_Generator: 3.117724895477295 LOSS_Discriminator: 0.11529368162155151\n",
            "ITERATION_NO.: 415 LOSS_Generator: 3.3001551628112793 LOSS_Discriminator: 0.20189185937245688\n",
            "ITERATION_NO.: 416 LOSS_Generator: 4.9565839767456055 LOSS_Discriminator: 0.1649511456489563\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.6437225341796875 LOSS_Discriminator: 0.04981542627016703\n",
            "ITERATION_NO.: 418 LOSS_Generator: 6.603667259216309 LOSS_Discriminator: 0.10637461145718892\n",
            "ITERATION_NO.: 419 LOSS_Generator: 6.623642444610596 LOSS_Discriminator: 0.003252785342435042\n",
            "ITERATION_NO.: 420 LOSS_Generator: 6.622467994689941 LOSS_Discriminator: 0.1962334712346395\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.574194431304932 LOSS_Discriminator: 0.13363728920618692\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.3405303955078125 LOSS_Discriminator: 0.1508802572886149\n",
            "ITERATION_NO.: 423 LOSS_Generator: 4.825803756713867 LOSS_Discriminator: 0.014480774601300558\n",
            "ITERATION_NO.: 424 LOSS_Generator: 4.616204261779785 LOSS_Discriminator: 0.03729925553003947\n",
            "ITERATION_NO.: 425 LOSS_Generator: 4.441233158111572 LOSS_Discriminator: 0.11997824907302856\n",
            "ITERATION_NO.: 426 LOSS_Generator: 4.656583309173584 LOSS_Discriminator: 0.07600366075833638\n",
            "ITERATION_NO.: 427 LOSS_Generator: 4.474215030670166 LOSS_Discriminator: 0.07297607759634654\n",
            "ITERATION_NO.: 428 LOSS_Generator: 4.369912147521973 LOSS_Discriminator: 0.14984065294265747\n",
            "ITERATION_NO.: 429 LOSS_Generator: 4.5562028884887695 LOSS_Discriminator: 0.04427403211593628\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.0552778244018555 LOSS_Discriminator: 0.0585606594880422\n",
            "ITERATION_NO.: 431 LOSS_Generator: 4.873044967651367 LOSS_Discriminator: 0.016724001616239548\n",
            "ITERATION_NO.: 432 LOSS_Generator: 4.860999584197998 LOSS_Discriminator: 0.024163931608200073\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.6287407875061035 LOSS_Discriminator: 0.03719321141640345\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.976171016693115 LOSS_Discriminator: 0.18940665324529013\n",
            "ITERATION_NO.: 435 LOSS_Generator: 4.705124378204346 LOSS_Discriminator: 0.030086974302927654\n",
            "ITERATION_NO.: 436 LOSS_Generator: 4.500946044921875 LOSS_Discriminator: 0.15033447742462158\n",
            "ITERATION_NO.: 437 LOSS_Generator: 4.56242561340332 LOSS_Discriminator: 0.06651372214158376\n",
            "ITERATION_NO.: 438 LOSS_Generator: 4.632270812988281 LOSS_Discriminator: 0.02476958930492401\n",
            "ITERATION_NO.: 439 LOSS_Generator: 4.805543899536133 LOSS_Discriminator: 0.10603498419125874\n",
            "ITERATION_NO.: 440 LOSS_Generator: 5.137418270111084 LOSS_Discriminator: 0.01432195802529653\n",
            "ITERATION_NO.: 441 LOSS_Generator: 5.6005635261535645 LOSS_Discriminator: 0.01267652710278829\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.699857711791992 LOSS_Discriminator: 0.24158692359924316\n",
            "ITERATION_NO.: 443 LOSS_Generator: 4.9317121505737305 LOSS_Discriminator: 0.0956016480922699\n",
            "ITERATION_NO.: 444 LOSS_Generator: 4.479909896850586 LOSS_Discriminator: 0.07502067585786183\n",
            "ITERATION_NO.: 445 LOSS_Generator: 4.085339546203613 LOSS_Discriminator: 0.16627806425094604\n",
            "ITERATION_NO.: 446 LOSS_Generator: 4.315583229064941 LOSS_Discriminator: 0.049679060777028404\n",
            "ITERATION_NO.: 447 LOSS_Generator: 4.635387897491455 LOSS_Discriminator: 0.0709552268187205\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.497499942779541 LOSS_Discriminator: 0.10245288411776225\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.197471618652344 LOSS_Discriminator: 0.13385102152824402\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.326887130737305 LOSS_Discriminator: 0.1764118274052938\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.1215739250183105 LOSS_Discriminator: 0.011611121396223703\n",
            "ITERATION_NO.: 452 LOSS_Generator: 4.756752014160156 LOSS_Discriminator: 0.09739102919896443\n",
            "ITERATION_NO.: 453 LOSS_Generator: 4.562221527099609 LOSS_Discriminator: 0.02822209894657135\n",
            "ITERATION_NO.: 454 LOSS_Generator: 3.933457136154175 LOSS_Discriminator: 0.05811327199141184\n",
            "ITERATION_NO.: 455 LOSS_Generator: 4.674761772155762 LOSS_Discriminator: 0.0695544183254242\n",
            "ITERATION_NO.: 456 LOSS_Generator: 4.6517205238342285 LOSS_Discriminator: 0.036403216421604156\n",
            "ITERATION_NO.: 457 LOSS_Generator: 4.742631912231445 LOSS_Discriminator: 0.2065209150314331\n",
            "ITERATION_NO.: 458 LOSS_Generator: 4.097665786743164 LOSS_Discriminator: 0.04164352019627889\n",
            "ITERATION_NO.: 459 LOSS_Generator: 4.191555023193359 LOSS_Discriminator: 0.19564561049143472\n",
            "ITERATION_NO.: 460 LOSS_Generator: 3.905348300933838 LOSS_Discriminator: 0.24531217416127524\n",
            "ITERATION_NO.: 461 LOSS_Generator: 4.380781650543213 LOSS_Discriminator: 0.08982595801353455\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.155728340148926 LOSS_Discriminator: 0.030457233389218647\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.918543815612793 LOSS_Discriminator: 0.024624735116958618\n",
            "ITERATION_NO.: 464 LOSS_Generator: 6.029948711395264 LOSS_Discriminator: 0.006092523535092671\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.9962077140808105 LOSS_Discriminator: 0.01975112532575925\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.11811637878418 LOSS_Discriminator: 0.09565337498982747\n",
            "ITERATION_NO.: 467 LOSS_Generator: 6.295670509338379 LOSS_Discriminator: 0.017352746178706486\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.4678802490234375 LOSS_Discriminator: 0.1900422771771749\n",
            "ITERATION_NO.: 469 LOSS_Generator: 4.557701587677002 LOSS_Discriminator: 0.09833012024561565\n",
            "ITERATION_NO.: 470 LOSS_Generator: 3.837397813796997 LOSS_Discriminator: 0.09506585200627644\n",
            "ITERATION_NO.: 471 LOSS_Generator: 3.9500131607055664 LOSS_Discriminator: 0.11849920948346455\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.879602432250977 LOSS_Discriminator: 0.08575111627578735\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.665537357330322 LOSS_Discriminator: 0.11864192287127177\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.571880340576172 LOSS_Discriminator: 0.14908953507741293\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.375907897949219 LOSS_Discriminator: 0.09316208958625793\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.797426223754883 LOSS_Discriminator: 0.1691205898920695\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.312376022338867 LOSS_Discriminator: 0.02304137994845708\n",
            "ITERATION_NO.: 478 LOSS_Generator: 4.649385452270508 LOSS_Discriminator: 0.02666365106900533\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.326021194458008 LOSS_Discriminator: 0.02091667801141739\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.5376129150390625 LOSS_Discriminator: 0.2014735738436381\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.453185081481934 LOSS_Discriminator: 0.05746335287888845\n",
            "ITERATION_NO.: 482 LOSS_Generator: 4.80649471282959 LOSS_Discriminator: 0.11211416125297546\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.406423091888428 LOSS_Discriminator: 0.020228850344816845\n",
            "ITERATION_NO.: 484 LOSS_Generator: 4.4494404792785645 LOSS_Discriminator: 0.07682959238688152\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.065938472747803 LOSS_Discriminator: 0.02533290535211563\n",
            "ITERATION_NO.: 486 LOSS_Generator: 4.9927144050598145 LOSS_Discriminator: 0.1388692855834961\n",
            "ITERATION_NO.: 487 LOSS_Generator: 4.8413848876953125 LOSS_Discriminator: 0.03442021210988363\n",
            "ITERATION_NO.: 488 LOSS_Generator: 4.974490165710449 LOSS_Discriminator: 0.01867682735125224\n",
            "ITERATION_NO.: 489 LOSS_Generator: 4.7618865966796875 LOSS_Discriminator: 0.06639484564463298\n",
            "ITERATION_NO.: 490 LOSS_Generator: 4.556869029998779 LOSS_Discriminator: 0.2324139674504598\n",
            "ITERATION_NO.: 491 LOSS_Generator: 4.175538063049316 LOSS_Discriminator: 0.09431356191635132\n",
            "ITERATION_NO.: 492 LOSS_Generator: 4.392068862915039 LOSS_Discriminator: 0.14010139306386313\n",
            "ITERATION_NO.: 493 LOSS_Generator: 4.43105411529541 LOSS_Discriminator: 0.05394625663757324\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.4925456047058105 LOSS_Discriminator: 0.11064540346463521\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.2280192375183105 LOSS_Discriminator: 0.2729678948720296\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.285382270812988 LOSS_Discriminator: 0.011822397510210672\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.212389945983887 LOSS_Discriminator: 0.17465287446975708\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.073925018310547 LOSS_Discriminator: 0.30064960320790607\n",
            "ITERATION_NO.: 499 LOSS_Generator: 4.331748962402344 LOSS_Discriminator: 0.02315545330444972\n",
            "ITERATION_NO.: 500 LOSS_Generator: 3.732050895690918 LOSS_Discriminator: 0.16622443993886313\n",
            "ITERATION_NO.: 501 LOSS_Generator: 4.011457443237305 LOSS_Discriminator: 0.09294195969899495\n",
            "ITERATION_NO.: 502 LOSS_Generator: 4.490325450897217 LOSS_Discriminator: 0.08459057410558064\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.649148941040039 LOSS_Discriminator: 0.22173869609832764\n",
            "ITERATION_NO.: 504 LOSS_Generator: 4.658740997314453 LOSS_Discriminator: 0.07434943318367004\n",
            "ITERATION_NO.: 505 LOSS_Generator: 4.814582824707031 LOSS_Discriminator: 0.03651519864797592\n",
            "ITERATION_NO.: 506 LOSS_Generator: 4.632828712463379 LOSS_Discriminator: 0.07200554013252258\n",
            "ITERATION_NO.: 507 LOSS_Generator: 4.665584564208984 LOSS_Discriminator: 0.08947444955507915\n",
            "ITERATION_NO.: 508 LOSS_Generator: 3.6612448692321777 LOSS_Discriminator: 0.0960703194141388\n",
            "ITERATION_NO.: 509 LOSS_Generator: 4.041196346282959 LOSS_Discriminator: 0.10865792632102966\n",
            "ITERATION_NO.: 510 LOSS_Generator: 4.352245330810547 LOSS_Discriminator: 0.08027361333370209\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.523928642272949 LOSS_Discriminator: 0.05740005771319071\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.69683837890625 LOSS_Discriminator: 0.05767300228277842\n",
            "ITERATION_NO.: 513 LOSS_Generator: 6.021487712860107 LOSS_Discriminator: 0.15002808968226114\n",
            "ITERATION_NO.: 514 LOSS_Generator: 4.708171844482422 LOSS_Discriminator: 0.16345679759979248\n",
            "ITERATION_NO.: 515 LOSS_Generator: 4.1579976081848145 LOSS_Discriminator: 0.03424030542373657\n",
            "ITERATION_NO.: 516 LOSS_Generator: 4.278225898742676 LOSS_Discriminator: 0.1058688759803772\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.316399574279785 LOSS_Discriminator: 0.06314742068449657\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.42569637298584 LOSS_Discriminator: 0.01177376757065455\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.205493927001953 LOSS_Discriminator: 0.3090662161509196\n",
            "ITERATION_NO.: 520 LOSS_Generator: 4.047375202178955 LOSS_Discriminator: 0.14072789748509726\n",
            "ITERATION_NO.: 521 LOSS_Generator: 3.9649362564086914 LOSS_Discriminator: 0.1617246468861898\n",
            "ITERATION_NO.: 522 LOSS_Generator: 4.635566711425781 LOSS_Discriminator: 0.05991315344969431\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.754790306091309 LOSS_Discriminator: 0.07614019016424815\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.100882053375244 LOSS_Discriminator: 0.08519011735916138\n",
            "ITERATION_NO.: 525 LOSS_Generator: 6.40800142288208 LOSS_Discriminator: 0.039641983807086945\n",
            "ITERATION_NO.: 526 LOSS_Generator: 6.603847026824951 LOSS_Discriminator: 0.015172451734542847\n",
            "ITERATION_NO.: 527 LOSS_Generator: 6.6269378662109375 LOSS_Discriminator: 0.002821594476699829\n",
            "ITERATION_NO.: 528 LOSS_Generator: 6.088689804077148 LOSS_Discriminator: 0.11641323566436768\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.645394802093506 LOSS_Discriminator: 0.10805538296699524\n",
            "ITERATION_NO.: 530 LOSS_Generator: 4.4053730964660645 LOSS_Discriminator: 0.15647085507710776\n",
            "ITERATION_NO.: 531 LOSS_Generator: 3.3025975227355957 LOSS_Discriminator: 0.1546406944592794\n",
            "ITERATION_NO.: 532 LOSS_Generator: 3.6233294010162354 LOSS_Discriminator: 0.1569305956363678\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.571226119995117 LOSS_Discriminator: 0.035124555230140686\n",
            "ITERATION_NO.: 534 LOSS_Generator: 7.035355567932129 LOSS_Discriminator: 0.09839447339375813\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.028127670288086 LOSS_Discriminator: 0.07156524062156677\n",
            "ITERATION_NO.: 536 LOSS_Generator: 7.03322696685791 LOSS_Discriminator: 0.224860946337382\n",
            "ITERATION_NO.: 537 LOSS_Generator: 6.046384334564209 LOSS_Discriminator: 0.062537282705307\n",
            "ITERATION_NO.: 538 LOSS_Generator: 6.15516471862793 LOSS_Discriminator: 0.1674437920252482\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.695751667022705 LOSS_Discriminator: 0.08193819224834442\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.257106781005859 LOSS_Discriminator: 0.032051036755243935\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.494462966918945 LOSS_Discriminator: 0.06736823916435242\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.521198272705078 LOSS_Discriminator: 0.26173583666483563\n",
            "ITERATION_NO.: 543 LOSS_Generator: 4.595205307006836 LOSS_Discriminator: 0.0979065199693044\n",
            "ITERATION_NO.: 544 LOSS_Generator: 4.7744903564453125 LOSS_Discriminator: 0.0411302795012792\n",
            "ITERATION_NO.: 545 LOSS_Generator: 4.458364486694336 LOSS_Discriminator: 0.1051638126373291\n",
            "ITERATION_NO.: 546 LOSS_Generator: 4.474650859832764 LOSS_Discriminator: 0.05315819879372915\n",
            "ITERATION_NO.: 547 LOSS_Generator: 4.6628007888793945 LOSS_Discriminator: 0.08753598729769389\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.578028678894043 LOSS_Discriminator: 0.026206413904825848\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.510470867156982 LOSS_Discriminator: 0.07018047571182251\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.1038618087768555 LOSS_Discriminator: 0.2813827594121297\n",
            "ITERATION_NO.: 551 LOSS_Generator: 4.229076862335205 LOSS_Discriminator: 0.03589686999718348\n",
            "ITERATION_NO.: 552 LOSS_Generator: 4.441217422485352 LOSS_Discriminator: 0.05919426679611206\n",
            "ITERATION_NO.: 553 LOSS_Generator: 4.599996566772461 LOSS_Discriminator: 0.04427338639895121\n",
            "ITERATION_NO.: 554 LOSS_Generator: 4.530096054077148 LOSS_Discriminator: 0.04695138335227966\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.0275726318359375 LOSS_Discriminator: 0.017267315338055294\n",
            "ITERATION_NO.: 556 LOSS_Generator: 4.880098342895508 LOSS_Discriminator: 0.1604059338569641\n",
            "ITERATION_NO.: 557 LOSS_Generator: 4.754089832305908 LOSS_Discriminator: 0.07576805849870046\n",
            "ITERATION_NO.: 558 LOSS_Generator: 4.318556308746338 LOSS_Discriminator: 0.136225163936615\n",
            "ITERATION_NO.: 559 LOSS_Generator: 4.146247863769531 LOSS_Discriminator: 0.11010680596033733\n",
            "ITERATION_NO.: 560 LOSS_Generator: 4.760400772094727 LOSS_Discriminator: 0.038788254062334694\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.145229339599609 LOSS_Discriminator: 0.04336548844973246\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.631211757659912 LOSS_Discriminator: 0.04574421048164368\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.913159370422363 LOSS_Discriminator: 0.023392011721928913\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.7475409507751465 LOSS_Discriminator: 0.23426592350006104\n",
            "ITERATION_NO.: 565 LOSS_Generator: 4.86198616027832 LOSS_Discriminator: 0.19632367293039957\n",
            "ITERATION_NO.: 566 LOSS_Generator: 4.178588390350342 LOSS_Discriminator: 0.06093311309814453\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.0225419998168945 LOSS_Discriminator: 0.09853672981262207\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.152531147003174 LOSS_Discriminator: 0.0396722083290418\n",
            "ITERATION_NO.: 569 LOSS_Generator: 4.707756042480469 LOSS_Discriminator: 0.16925654808680216\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.125637054443359 LOSS_Discriminator: 0.1461288034915924\n",
            "ITERATION_NO.: 571 LOSS_Generator: 4.330399990081787 LOSS_Discriminator: 0.12271857261657715\n",
            "ITERATION_NO.: 572 LOSS_Generator: 4.361741065979004 LOSS_Discriminator: 0.17618378003438315\n",
            "ITERATION_NO.: 573 LOSS_Generator: 4.018642902374268 LOSS_Discriminator: 0.19924604892730713\n",
            "ITERATION_NO.: 574 LOSS_Generator: 4.391836166381836 LOSS_Discriminator: 0.12194627523422241\n",
            "ITERATION_NO.: 575 LOSS_Generator: 4.222110748291016 LOSS_Discriminator: 0.06501193841298421\n",
            "ITERATION_NO.: 576 LOSS_Generator: 4.348827362060547 LOSS_Discriminator: 0.05189510186513265\n",
            "ITERATION_NO.: 577 LOSS_Generator: 4.896948337554932 LOSS_Discriminator: 0.054513707756996155\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.018712520599365 LOSS_Discriminator: 0.036672125260035195\n",
            "ITERATION_NO.: 579 LOSS_Generator: 4.345462799072266 LOSS_Discriminator: 0.05998259782791138\n",
            "ITERATION_NO.: 580 LOSS_Generator: 4.7859392166137695 LOSS_Discriminator: 0.08934259414672852\n",
            "ITERATION_NO.: 581 LOSS_Generator: 4.555636405944824 LOSS_Discriminator: 0.061344062288602196\n",
            "ITERATION_NO.: 582 LOSS_Generator: 4.381806373596191 LOSS_Discriminator: 0.1912318468093872\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.18388557434082 LOSS_Discriminator: 0.07949698964754741\n",
            "ITERATION_NO.: 584 LOSS_Generator: 4.712240695953369 LOSS_Discriminator: 0.1462556521097819\n",
            "ITERATION_NO.: 585 LOSS_Generator: 4.090791702270508 LOSS_Discriminator: 0.12608359257380167\n",
            "ITERATION_NO.: 586 LOSS_Generator: 4.668616771697998 LOSS_Discriminator: 0.07122686505317688\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.437231540679932 LOSS_Discriminator: 0.0320262610912323\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.983410835266113 LOSS_Discriminator: 0.011066983143488566\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.303691864013672 LOSS_Discriminator: 0.14116670687993368\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.339858055114746 LOSS_Discriminator: 0.004708052612841129\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.304989814758301 LOSS_Discriminator: 0.14069571097691855\n",
            "ITERATION_NO.: 592 LOSS_Generator: 4.695136070251465 LOSS_Discriminator: 0.017263832191626232\n",
            "ITERATION_NO.: 593 LOSS_Generator: 4.915609359741211 LOSS_Discriminator: 0.08407791455586751\n",
            "ITERATION_NO.: 594 LOSS_Generator: 4.420881271362305 LOSS_Discriminator: 0.06307665506998698\n",
            "ITERATION_NO.: 595 LOSS_Generator: 4.921693801879883 LOSS_Discriminator: 0.27457167704900104\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.633940696716309 LOSS_Discriminator: 0.07363540430863698\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.880792617797852 LOSS_Discriminator: 0.006647087633609772\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.07551383972168 LOSS_Discriminator: 0.004345063741008441\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.156253337860107 LOSS_Discriminator: 0.07525758445262909\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.715185165405273 LOSS_Discriminator: 0.005529789254069328\n",
            "ITERATION_NO.: 601 LOSS_Generator: 5.349015712738037 LOSS_Discriminator: 0.08435470859209697\n",
            "ITERATION_NO.: 602 LOSS_Generator: 5.038043022155762 LOSS_Discriminator: 0.09233023722966512\n",
            "ITERATION_NO.: 603 LOSS_Generator: 4.036170959472656 LOSS_Discriminator: 0.10446032881736755\n",
            "ITERATION_NO.: 604 LOSS_Generator: 3.4718940258026123 LOSS_Discriminator: 0.20856710275014242\n",
            "ITERATION_NO.: 605 LOSS_Generator: 3.9713294506073 LOSS_Discriminator: 0.08047413329283397\n",
            "ITERATION_NO.: 606 LOSS_Generator: 5.140995502471924 LOSS_Discriminator: 0.08915741244951884\n",
            "ITERATION_NO.: 607 LOSS_Generator: 5.272393226623535 LOSS_Discriminator: 0.09009885787963867\n",
            "ITERATION_NO.: 608 LOSS_Generator: 6.382256031036377 LOSS_Discriminator: 0.03559760500987371\n",
            "ITERATION_NO.: 609 LOSS_Generator: 6.028104782104492 LOSS_Discriminator: 0.16165340940157572\n",
            "ITERATION_NO.: 610 LOSS_Generator: 5.760525703430176 LOSS_Discriminator: 0.13072737058003744\n",
            "ITERATION_NO.: 611 LOSS_Generator: 4.897302627563477 LOSS_Discriminator: 0.37460915247599286\n",
            "ITERATION_NO.: 612 LOSS_Generator: 3.9555773735046387 LOSS_Discriminator: 0.06629177431265514\n",
            "ITERATION_NO.: 613 LOSS_Generator: 3.8558411598205566 LOSS_Discriminator: 0.07101863622665405\n",
            "ITERATION_NO.: 614 LOSS_Generator: 5.021162033081055 LOSS_Discriminator: 0.1075751781463623\n",
            "ITERATION_NO.: 615 LOSS_Generator: 5.865152359008789 LOSS_Discriminator: 0.01680183286468188\n",
            "ITERATION_NO.: 616 LOSS_Generator: 6.2550249099731445 LOSS_Discriminator: 0.009505607187747955\n",
            "ITERATION_NO.: 617 LOSS_Generator: 6.429187774658203 LOSS_Discriminator: 0.3129396041234334\n",
            "ITERATION_NO.: 618 LOSS_Generator: 6.3746232986450195 LOSS_Discriminator: 0.004054759629070759\n",
            "ITERATION_NO.: 619 LOSS_Generator: 6.228696823120117 LOSS_Discriminator: 0.1973398526509603\n",
            "ITERATION_NO.: 620 LOSS_Generator: 5.600743293762207 LOSS_Discriminator: 0.009121774385372797\n",
            "ITERATION_NO.: 621 LOSS_Generator: 5.197445869445801 LOSS_Discriminator: 0.07498654226462047\n",
            "ITERATION_NO.: 622 LOSS_Generator: 4.900310516357422 LOSS_Discriminator: 0.29836519559224445\n",
            "ITERATION_NO.: 623 LOSS_Generator: 3.5117008686065674 LOSS_Discriminator: 0.14465457201004028\n",
            "ITERATION_NO.: 624 LOSS_Generator: 3.7438805103302 LOSS_Discriminator: 0.13775316874186197\n",
            "ITERATION_NO.: 625 LOSS_Generator: 4.216911792755127 LOSS_Discriminator: 0.05212135116259257\n",
            "ITERATION_NO.: 626 LOSS_Generator: 5.484562873840332 LOSS_Discriminator: 0.07849417130152385\n",
            "ITERATION_NO.: 627 LOSS_Generator: 5.458285331726074 LOSS_Discriminator: 0.026461149255434673\n",
            "ITERATION_NO.: 628 LOSS_Generator: 6.232000350952148 LOSS_Discriminator: 0.012148623665173849\n",
            "ITERATION_NO.: 629 LOSS_Generator: 6.266024589538574 LOSS_Discriminator: 0.14731448888778687\n",
            "ITERATION_NO.: 630 LOSS_Generator: 6.127783298492432 LOSS_Discriminator: 0.00306083665539821\n",
            "ITERATION_NO.: 631 LOSS_Generator: 6.449419021606445 LOSS_Discriminator: 0.1491187115510305\n",
            "ITERATION_NO.: 632 LOSS_Generator: 5.0415754318237305 LOSS_Discriminator: 0.30802865823109943\n",
            "ITERATION_NO.: 633 LOSS_Generator: 4.876237869262695 LOSS_Discriminator: 0.059713939825693764\n",
            "ITERATION_NO.: 634 LOSS_Generator: 4.460963249206543 LOSS_Discriminator: 0.03744776050249735\n",
            "ITERATION_NO.: 635 LOSS_Generator: 3.841343879699707 LOSS_Discriminator: 0.13391629854838052\n",
            "ITERATION_NO.: 636 LOSS_Generator: 3.47322154045105 LOSS_Discriminator: 0.09532497326533\n",
            "ITERATION_NO.: 637 LOSS_Generator: 3.718759059906006 LOSS_Discriminator: 0.05159283677736918\n",
            "ITERATION_NO.: 638 LOSS_Generator: 4.826385974884033 LOSS_Discriminator: 0.0416500469048818\n",
            "ITERATION_NO.: 639 LOSS_Generator: 5.056277751922607 LOSS_Discriminator: 0.20750210682551065\n",
            "ITERATION_NO.: 640 LOSS_Generator: 5.50490665435791 LOSS_Discriminator: 0.06012274821599325\n",
            "ITERATION_NO.: 641 LOSS_Generator: 5.348156929016113 LOSS_Discriminator: 0.008115760361154875\n",
            "ITERATION_NO.: 642 LOSS_Generator: 5.552326202392578 LOSS_Discriminator: 0.13164297739664713\n",
            "ITERATION_NO.: 643 LOSS_Generator: 5.398253917694092 LOSS_Discriminator: 0.1945632497469584\n",
            "ITERATION_NO.: 644 LOSS_Generator: 4.7945404052734375 LOSS_Discriminator: 0.08859775463740031\n",
            "ITERATION_NO.: 645 LOSS_Generator: 4.356314659118652 LOSS_Discriminator: 0.09056683381398518\n",
            "ITERATION_NO.: 646 LOSS_Generator: 4.486700057983398 LOSS_Discriminator: 0.08928946654001872\n",
            "ITERATION_NO.: 647 LOSS_Generator: 4.129795074462891 LOSS_Discriminator: 0.27190325657526654\n",
            "ITERATION_NO.: 648 LOSS_Generator: 3.816854953765869 LOSS_Discriminator: 0.0960954229036967\n",
            "ITERATION_NO.: 649 LOSS_Generator: 4.223649024963379 LOSS_Discriminator: 0.11789705355962117\n",
            "ITERATION_NO.: 650 LOSS_Generator: 4.004358291625977 LOSS_Discriminator: 0.23497754335403442\n",
            "ITERATION_NO.: 651 LOSS_Generator: 4.399146556854248 LOSS_Discriminator: 0.058948323130607605\n",
            "ITERATION_NO.: 652 LOSS_Generator: 4.619002819061279 LOSS_Discriminator: 0.045519510904947914\n",
            "ITERATION_NO.: 653 LOSS_Generator: 4.777228832244873 LOSS_Discriminator: 0.17445802688598633\n",
            "ITERATION_NO.: 654 LOSS_Generator: 5.048224449157715 LOSS_Discriminator: 0.05582855145136515\n",
            "ITERATION_NO.: 655 LOSS_Generator: 4.682872772216797 LOSS_Discriminator: 0.18773365020751953\n",
            "ITERATION_NO.: 656 LOSS_Generator: 4.88863468170166 LOSS_Discriminator: 0.033069307605425514\n",
            "ITERATION_NO.: 657 LOSS_Generator: 4.805680274963379 LOSS_Discriminator: 0.027839464445908863\n",
            "ITERATION_NO.: 658 LOSS_Generator: 5.169920921325684 LOSS_Discriminator: 0.31772615512212116\n",
            "ITERATION_NO.: 659 LOSS_Generator: 5.154046058654785 LOSS_Discriminator: 0.11839182178179423\n",
            "ITERATION_NO.: 660 LOSS_Generator: 5.132384300231934 LOSS_Discriminator: 0.11673847834269206\n",
            "ITERATION_NO.: 661 LOSS_Generator: 4.663184642791748 LOSS_Discriminator: 0.15650148193041483\n",
            "ITERATION_NO.: 662 LOSS_Generator: 3.472691297531128 LOSS_Discriminator: 0.0878896713256836\n",
            "ITERATION_NO.: 663 LOSS_Generator: 4.069426536560059 LOSS_Discriminator: 0.05190227429072062\n",
            "ITERATION_NO.: 664 LOSS_Generator: 4.251593589782715 LOSS_Discriminator: 0.05294308066368103\n",
            "ITERATION_NO.: 665 LOSS_Generator: 4.31170654296875 LOSS_Discriminator: 0.28815678755442303\n",
            "ITERATION_NO.: 666 LOSS_Generator: 3.9692914485931396 LOSS_Discriminator: 0.07418955365816753\n",
            "ITERATION_NO.: 667 LOSS_Generator: 4.176868438720703 LOSS_Discriminator: 0.10309912761052449\n",
            "ITERATION_NO.: 668 LOSS_Generator: 4.621573448181152 LOSS_Discriminator: 0.1275085210800171\n",
            "ITERATION_NO.: 669 LOSS_Generator: 4.804999351501465 LOSS_Discriminator: 0.027930515507857006\n",
            "ITERATION_NO.: 670 LOSS_Generator: 5.155605316162109 LOSS_Discriminator: 0.07374809682369232\n",
            "ITERATION_NO.: 671 LOSS_Generator: 5.236140727996826 LOSS_Discriminator: 0.01712913066148758\n",
            "ITERATION_NO.: 672 LOSS_Generator: 5.2082905769348145 LOSS_Discriminator: 0.10888550678888957\n",
            "ITERATION_NO.: 673 LOSS_Generator: 4.631285667419434 LOSS_Discriminator: 0.08835259079933167\n",
            "ITERATION_NO.: 674 LOSS_Generator: 4.0460686683654785 LOSS_Discriminator: 0.14836261669794717\n",
            "ITERATION_NO.: 675 LOSS_Generator: 3.4739878177642822 LOSS_Discriminator: 0.05730412403742472\n",
            "ITERATION_NO.: 676 LOSS_Generator: 4.7053728103637695 LOSS_Discriminator: 0.12521315614382425\n",
            "ITERATION_NO.: 677 LOSS_Generator: 5.805963039398193 LOSS_Discriminator: 0.023652921120325725\n",
            "ITERATION_NO.: 678 LOSS_Generator: 6.276310920715332 LOSS_Discriminator: 0.01667290801803271\n",
            "ITERATION_NO.: 679 LOSS_Generator: 7.20732307434082 LOSS_Discriminator: 0.06300491094589233\n",
            "ITERATION_NO.: 680 LOSS_Generator: 6.945165634155273 LOSS_Discriminator: 0.24264810482660928\n",
            "ITERATION_NO.: 681 LOSS_Generator: 6.358108997344971 LOSS_Discriminator: 0.002426541099945704\n",
            "ITERATION_NO.: 682 LOSS_Generator: 6.202919006347656 LOSS_Discriminator: 0.004601188314457734\n",
            "ITERATION_NO.: 683 LOSS_Generator: 6.547089576721191 LOSS_Discriminator: 0.004255031545956929\n",
            "ITERATION_NO.: 684 LOSS_Generator: 5.6293206214904785 LOSS_Discriminator: 0.3003787199656169\n",
            "ITERATION_NO.: 685 LOSS_Generator: 4.5666961669921875 LOSS_Discriminator: 0.13214062650998434\n",
            "ITERATION_NO.: 686 LOSS_Generator: 2.8004918098449707 LOSS_Discriminator: 0.10720408956209819\n",
            "ITERATION_NO.: 687 LOSS_Generator: 3.0956220626831055 LOSS_Discriminator: 0.16668895880381265\n",
            "ITERATION_NO.: 688 LOSS_Generator: 4.649704456329346 LOSS_Discriminator: 0.07756704092025757\n",
            "ITERATION_NO.: 689 LOSS_Generator: 5.483285903930664 LOSS_Discriminator: 0.037596640487511955\n",
            "ITERATION_NO.: 690 LOSS_Generator: 6.572544574737549 LOSS_Discriminator: 0.006169895951946576\n",
            "ITERATION_NO.: 691 LOSS_Generator: 6.862010478973389 LOSS_Discriminator: 0.10331001877784729\n",
            "ITERATION_NO.: 692 LOSS_Generator: 6.830862045288086 LOSS_Discriminator: 0.03410264104604721\n",
            "ITERATION_NO.: 693 LOSS_Generator: 6.41549015045166 LOSS_Discriminator: 0.20228564739227295\n",
            "ITERATION_NO.: 694 LOSS_Generator: 5.784200668334961 LOSS_Discriminator: 0.16070237755775452\n",
            "ITERATION_NO.: 695 LOSS_Generator: 5.184911727905273 LOSS_Discriminator: 0.00982702523469925\n",
            "ITERATION_NO.: 696 LOSS_Generator: 3.539301872253418 LOSS_Discriminator: 0.13883677124977112\n",
            "ITERATION_NO.: 697 LOSS_Generator: 2.6104185581207275 LOSS_Discriminator: 0.17514506975809732\n",
            "ITERATION_NO.: 698 LOSS_Generator: 4.092386245727539 LOSS_Discriminator: 0.13504841923713684\n",
            "ITERATION_NO.: 699 LOSS_Generator: 5.180702209472656 LOSS_Discriminator: 0.05337709188461304\n",
            "ITERATION_NO.: 700 LOSS_Generator: 5.715590953826904 LOSS_Discriminator: 0.077253391345342\n",
            "ITERATION_NO.: 701 LOSS_Generator: 6.216146469116211 LOSS_Discriminator: 0.22808027267456055\n",
            "ITERATION_NO.: 702 LOSS_Generator: 6.396782875061035 LOSS_Discriminator: 0.013951254387696585\n",
            "ITERATION_NO.: 703 LOSS_Generator: 6.061657428741455 LOSS_Discriminator: 0.10533231496810913\n",
            "ITERATION_NO.: 704 LOSS_Generator: 5.6124067306518555 LOSS_Discriminator: 0.05383768677711487\n",
            "ITERATION_NO.: 705 LOSS_Generator: 5.777534484863281 LOSS_Discriminator: 0.14292763670285544\n",
            "ITERATION_NO.: 706 LOSS_Generator: 4.565990924835205 LOSS_Discriminator: 0.2892775932947795\n",
            "ITERATION_NO.: 707 LOSS_Generator: 3.856350898742676 LOSS_Discriminator: 0.2714298168818156\n",
            "ITERATION_NO.: 708 LOSS_Generator: 3.7385153770446777 LOSS_Discriminator: 0.13622334599494934\n",
            "ITERATION_NO.: 709 LOSS_Generator: 4.438529968261719 LOSS_Discriminator: 0.073640376329422\n",
            "ITERATION_NO.: 710 LOSS_Generator: 5.356471538543701 LOSS_Discriminator: 0.02249641219774882\n",
            "ITERATION_NO.: 711 LOSS_Generator: 6.201069355010986 LOSS_Discriminator: 0.023044933875401814\n",
            "ITERATION_NO.: 712 LOSS_Generator: 6.0226287841796875 LOSS_Discriminator: 0.07475139697392781\n",
            "ITERATION_NO.: 713 LOSS_Generator: 6.281146049499512 LOSS_Discriminator: 0.05668764313062032\n",
            "ITERATION_NO.: 714 LOSS_Generator: 5.8580145835876465 LOSS_Discriminator: 0.1278121074040731\n",
            "ITERATION_NO.: 715 LOSS_Generator: 5.2738189697265625 LOSS_Discriminator: 0.07348040739695232\n",
            "ITERATION_NO.: 716 LOSS_Generator: 4.475306987762451 LOSS_Discriminator: 0.08840519189834595\n",
            "ITERATION_NO.: 717 LOSS_Generator: 3.9215526580810547 LOSS_Discriminator: 0.09958880146344502\n",
            "ITERATION_NO.: 718 LOSS_Generator: 3.8101963996887207 LOSS_Discriminator: 0.05856230854988098\n",
            "ITERATION_NO.: 719 LOSS_Generator: 4.513443470001221 LOSS_Discriminator: 0.0791593889395396\n",
            "ITERATION_NO.: 720 LOSS_Generator: 4.765809059143066 LOSS_Discriminator: 0.0771792729695638\n",
            "ITERATION_NO.: 721 LOSS_Generator: 4.694454193115234 LOSS_Discriminator: 0.14480783541997275\n",
            "ITERATION_NO.: 722 LOSS_Generator: 4.876303195953369 LOSS_Discriminator: 0.013665806502103806\n",
            "ITERATION_NO.: 723 LOSS_Generator: 4.880259037017822 LOSS_Discriminator: 0.08283212284247081\n",
            "ITERATION_NO.: 724 LOSS_Generator: 4.92048978805542 LOSS_Discriminator: 0.12612001101175943\n",
            "ITERATION_NO.: 725 LOSS_Generator: 4.8795671463012695 LOSS_Discriminator: 0.02995009223620097\n",
            "ITERATION_NO.: 726 LOSS_Generator: 4.967348098754883 LOSS_Discriminator: 0.026102912922700245\n",
            "ITERATION_NO.: 727 LOSS_Generator: 5.711438179016113 LOSS_Discriminator: 0.05815780162811279\n",
            "ITERATION_NO.: 728 LOSS_Generator: 4.956001281738281 LOSS_Discriminator: 0.08422191937764485\n",
            "ITERATION_NO.: 729 LOSS_Generator: 4.495600700378418 LOSS_Discriminator: 0.022362567484378815\n",
            "ITERATION_NO.: 730 LOSS_Generator: 4.7416582107543945 LOSS_Discriminator: 0.12289029359817505\n",
            "ITERATION_NO.: 731 LOSS_Generator: 3.8310580253601074 LOSS_Discriminator: 0.039416901767253876\n",
            "ITERATION_NO.: 732 LOSS_Generator: 3.969099521636963 LOSS_Discriminator: 0.1549715797106425\n",
            "ITERATION_NO.: 733 LOSS_Generator: 4.475347995758057 LOSS_Discriminator: 0.09978660941123962\n",
            "ITERATION_NO.: 734 LOSS_Generator: 5.549769401550293 LOSS_Discriminator: 0.09646629293759663\n",
            "ITERATION_NO.: 735 LOSS_Generator: 5.812497615814209 LOSS_Discriminator: 0.18897823492685953\n",
            "ITERATION_NO.: 736 LOSS_Generator: 5.547171592712402 LOSS_Discriminator: 0.1631494164466858\n",
            "ITERATION_NO.: 737 LOSS_Generator: 4.6750168800354 LOSS_Discriminator: 0.16488856077194214\n",
            "ITERATION_NO.: 738 LOSS_Generator: 4.3098907470703125 LOSS_Discriminator: 0.024461266895135243\n",
            "ITERATION_NO.: 739 LOSS_Generator: 4.179834842681885 LOSS_Discriminator: 0.13883688052495322\n",
            "ITERATION_NO.: 740 LOSS_Generator: 3.701005220413208 LOSS_Discriminator: 0.26248639822006226\n",
            "ITERATION_NO.: 741 LOSS_Generator: 3.8342323303222656 LOSS_Discriminator: 0.07612688342730205\n",
            "ITERATION_NO.: 742 LOSS_Generator: 4.263326168060303 LOSS_Discriminator: 0.07320626080036163\n",
            "ITERATION_NO.: 743 LOSS_Generator: 4.697301864624023 LOSS_Discriminator: 0.041940838098526\n",
            "ITERATION_NO.: 744 LOSS_Generator: 5.591591835021973 LOSS_Discriminator: 0.020387149105469387\n",
            "ITERATION_NO.: 745 LOSS_Generator: 5.780590057373047 LOSS_Discriminator: 0.010383724545439085\n",
            "ITERATION_NO.: 746 LOSS_Generator: 6.538179397583008 LOSS_Discriminator: 0.022857507069905598\n",
            "ITERATION_NO.: 747 LOSS_Generator: 6.41061544418335 LOSS_Discriminator: 0.09136553605397542\n",
            "ITERATION_NO.: 748 LOSS_Generator: 6.2395339012146 LOSS_Discriminator: 0.005055677766601245\n",
            "ITERATION_NO.: 749 LOSS_Generator: 5.492997169494629 LOSS_Discriminator: 0.24733750025431314\n",
            "ITERATION_NO.: 750 LOSS_Generator: 4.475766181945801 LOSS_Discriminator: 0.1803243358929952\n",
            "ITERATION_NO.: 751 LOSS_Generator: 3.7109014987945557 LOSS_Discriminator: 0.10457913080851237\n",
            "ITERATION_NO.: 752 LOSS_Generator: 3.6068594455718994 LOSS_Discriminator: 0.2730022072792053\n",
            "ITERATION_NO.: 753 LOSS_Generator: 4.231741428375244 LOSS_Discriminator: 0.11086833477020264\n",
            "ITERATION_NO.: 754 LOSS_Generator: 5.339104652404785 LOSS_Discriminator: 0.018799512336651485\n",
            "ITERATION_NO.: 755 LOSS_Generator: 6.085902214050293 LOSS_Discriminator: 0.051731218894322716\n",
            "ITERATION_NO.: 756 LOSS_Generator: 5.987325668334961 LOSS_Discriminator: 0.13328222433725992\n",
            "ITERATION_NO.: 757 LOSS_Generator: 6.413119792938232 LOSS_Discriminator: 0.24444282054901123\n",
            "ITERATION_NO.: 758 LOSS_Generator: 5.946351051330566 LOSS_Discriminator: 0.008263394857446352\n",
            "ITERATION_NO.: 759 LOSS_Generator: 5.17854118347168 LOSS_Discriminator: 0.037806848684946694\n",
            "ITERATION_NO.: 760 LOSS_Generator: 4.962584495544434 LOSS_Discriminator: 0.055103654662768044\n",
            "ITERATION_NO.: 761 LOSS_Generator: 4.501887321472168 LOSS_Discriminator: 0.0441512664159139\n",
            "ITERATION_NO.: 762 LOSS_Generator: 4.2173871994018555 LOSS_Discriminator: 0.10799062252044678\n",
            "ITERATION_NO.: 763 LOSS_Generator: 4.571981430053711 LOSS_Discriminator: 0.11085229118665059\n",
            "ITERATION_NO.: 764 LOSS_Generator: 4.801640510559082 LOSS_Discriminator: 0.0584509422381719\n",
            "ITERATION_NO.: 765 LOSS_Generator: 5.282115936279297 LOSS_Discriminator: 0.043055971463521324\n",
            "ITERATION_NO.: 766 LOSS_Generator: 5.344729900360107 LOSS_Discriminator: 0.06928929189840953\n",
            "ITERATION_NO.: 767 LOSS_Generator: 5.849126815795898 LOSS_Discriminator: 0.005934872974952062\n",
            "ITERATION_NO.: 768 LOSS_Generator: 5.366772174835205 LOSS_Discriminator: 0.16596495111783346\n",
            "ITERATION_NO.: 769 LOSS_Generator: 5.185540676116943 LOSS_Discriminator: 0.019422467797994614\n",
            "ITERATION_NO.: 770 LOSS_Generator: 5.551912307739258 LOSS_Discriminator: 0.09754480918248494\n",
            "ITERATION_NO.: 771 LOSS_Generator: 5.3422393798828125 LOSS_Discriminator: 0.022842881580193836\n",
            "ITERATION_NO.: 772 LOSS_Generator: 5.285268783569336 LOSS_Discriminator: 0.012207041184107462\n",
            "ITERATION_NO.: 773 LOSS_Generator: 4.845282554626465 LOSS_Discriminator: 0.1438627541065216\n",
            "ITERATION_NO.: 774 LOSS_Generator: 4.838568687438965 LOSS_Discriminator: 0.15265682339668274\n",
            "ITERATION_NO.: 775 LOSS_Generator: 5.318387985229492 LOSS_Discriminator: 0.015709480891625088\n",
            "ITERATION_NO.: 776 LOSS_Generator: 5.3355255126953125 LOSS_Discriminator: 0.028471911946932476\n",
            "ITERATION_NO.: 777 LOSS_Generator: 4.753802299499512 LOSS_Discriminator: 0.3024287819862366\n",
            "ITERATION_NO.: 778 LOSS_Generator: 4.430137634277344 LOSS_Discriminator: 0.04782102505366007\n",
            "ITERATION_NO.: 779 LOSS_Generator: 4.531480312347412 LOSS_Discriminator: 0.05022907257080078\n",
            "ITERATION_NO.: 780 LOSS_Generator: 4.092081069946289 LOSS_Discriminator: 0.1142508586247762\n",
            "ITERATION_NO.: 781 LOSS_Generator: 5.100383758544922 LOSS_Discriminator: 0.10740309953689575\n",
            "ITERATION_NO.: 782 LOSS_Generator: 5.354430198669434 LOSS_Discriminator: 0.07305005192756653\n",
            "ITERATION_NO.: 783 LOSS_Generator: 4.848593235015869 LOSS_Discriminator: 0.16312666734059653\n",
            "ITERATION_NO.: 784 LOSS_Generator: 4.956716537475586 LOSS_Discriminator: 0.13719922304153442\n",
            "ITERATION_NO.: 785 LOSS_Generator: 4.612618923187256 LOSS_Discriminator: 0.0806100070476532\n",
            "ITERATION_NO.: 786 LOSS_Generator: 4.333297252655029 LOSS_Discriminator: 0.10192306836446126\n",
            "ITERATION_NO.: 787 LOSS_Generator: 3.9163565635681152 LOSS_Discriminator: 0.21991614500681558\n",
            "ITERATION_NO.: 788 LOSS_Generator: 3.9866724014282227 LOSS_Discriminator: 0.09058616558710735\n",
            "ITERATION_NO.: 789 LOSS_Generator: 4.485871315002441 LOSS_Discriminator: 0.07888715465863545\n",
            "ITERATION_NO.: 790 LOSS_Generator: 4.933162689208984 LOSS_Discriminator: 0.051221479972203575\n",
            "ITERATION_NO.: 791 LOSS_Generator: 4.5287885665893555 LOSS_Discriminator: 0.12081378698348999\n",
            "ITERATION_NO.: 792 LOSS_Generator: 4.6040215492248535 LOSS_Discriminator: 0.10214022795359294\n",
            "ITERATION_NO.: 793 LOSS_Generator: 4.438194274902344 LOSS_Discriminator: 0.09047455588976543\n",
            "ITERATION_NO.: 794 LOSS_Generator: 4.956277847290039 LOSS_Discriminator: 0.020663637667894363\n",
            "ITERATION_NO.: 795 LOSS_Generator: 5.393754005432129 LOSS_Discriminator: 0.025964096188545227\n",
            "ITERATION_NO.: 796 LOSS_Generator: 5.348901748657227 LOSS_Discriminator: 0.06564926107724507\n",
            "ITERATION_NO.: 797 LOSS_Generator: 5.215236663818359 LOSS_Discriminator: 0.017934833963712055\n",
            "ITERATION_NO.: 798 LOSS_Generator: 5.377043724060059 LOSS_Discriminator: 0.12146530548731486\n",
            "ITERATION_NO.: 799 LOSS_Generator: 5.195014953613281 LOSS_Discriminator: 0.028790153563022614\n",
            "ITERATION_NO.: 800 LOSS_Generator: 5.035632610321045 LOSS_Discriminator: 0.06573101381460826\n",
            "ITERATION_NO.: 801 LOSS_Generator: 4.29965353012085 LOSS_Discriminator: 0.1862244208653768\n",
            "ITERATION_NO.: 802 LOSS_Generator: 4.960474967956543 LOSS_Discriminator: 0.042058964570363365\n",
            "ITERATION_NO.: 803 LOSS_Generator: 5.2425384521484375 LOSS_Discriminator: 0.13654444615046182\n",
            "ITERATION_NO.: 804 LOSS_Generator: 4.971886157989502 LOSS_Discriminator: 0.13549619913101196\n",
            "ITERATION_NO.: 805 LOSS_Generator: 5.075066566467285 LOSS_Discriminator: 0.018241527179876964\n",
            "ITERATION_NO.: 806 LOSS_Generator: 4.827814102172852 LOSS_Discriminator: 0.019816031058629353\n",
            "ITERATION_NO.: 807 LOSS_Generator: 4.879491806030273 LOSS_Discriminator: 0.10724854469299316\n",
            "ITERATION_NO.: 808 LOSS_Generator: 4.935565948486328 LOSS_Discriminator: 0.025526878734429676\n",
            "ITERATION_NO.: 809 LOSS_Generator: 4.626773834228516 LOSS_Discriminator: 0.05259833733240763\n",
            "ITERATION_NO.: 810 LOSS_Generator: 4.726792335510254 LOSS_Discriminator: 0.13050113121668497\n",
            "ITERATION_NO.: 811 LOSS_Generator: 4.475876808166504 LOSS_Discriminator: 0.04498341182867686\n",
            "ITERATION_NO.: 812 LOSS_Generator: 4.1364593505859375 LOSS_Discriminator: 0.1908060908317566\n",
            "ITERATION_NO.: 813 LOSS_Generator: 4.397307395935059 LOSS_Discriminator: 0.0416296124458313\n",
            "ITERATION_NO.: 814 LOSS_Generator: 4.584029197692871 LOSS_Discriminator: 0.060653348763783775\n",
            "ITERATION_NO.: 815 LOSS_Generator: 5.316366672515869 LOSS_Discriminator: 0.022953535119692486\n",
            "ITERATION_NO.: 816 LOSS_Generator: 5.653441429138184 LOSS_Discriminator: 0.012467198073863983\n",
            "ITERATION_NO.: 817 LOSS_Generator: 6.125983238220215 LOSS_Discriminator: 0.17848742008209229\n",
            "ITERATION_NO.: 818 LOSS_Generator: 5.373778343200684 LOSS_Discriminator: 0.03608371069033941\n",
            "ITERATION_NO.: 819 LOSS_Generator: 4.870260238647461 LOSS_Discriminator: 0.12360689043998718\n",
            "ITERATION_NO.: 820 LOSS_Generator: 4.008391380310059 LOSS_Discriminator: 0.08244704206784566\n",
            "ITERATION_NO.: 821 LOSS_Generator: 4.068526744842529 LOSS_Discriminator: 0.07332107424736023\n",
            "ITERATION_NO.: 822 LOSS_Generator: 4.506163597106934 LOSS_Discriminator: 0.044897258281707764\n",
            "ITERATION_NO.: 823 LOSS_Generator: 5.123598098754883 LOSS_Discriminator: 0.07698312898476918\n",
            "ITERATION_NO.: 824 LOSS_Generator: 5.534914016723633 LOSS_Discriminator: 0.07130759954452515\n",
            "ITERATION_NO.: 825 LOSS_Generator: 4.9965057373046875 LOSS_Discriminator: 0.25187329451243085\n",
            "ITERATION_NO.: 826 LOSS_Generator: 4.214794158935547 LOSS_Discriminator: 0.2453256050745646\n",
            "ITERATION_NO.: 827 LOSS_Generator: 3.046077251434326 LOSS_Discriminator: 0.19422763586044312\n",
            "ITERATION_NO.: 828 LOSS_Generator: 4.148507118225098 LOSS_Discriminator: 0.3358965317408244\n",
            "ITERATION_NO.: 829 LOSS_Generator: 5.944962501525879 LOSS_Discriminator: 0.08662562568982442\n",
            "ITERATION_NO.: 830 LOSS_Generator: 7.177707672119141 LOSS_Discriminator: 0.08592770497004192\n",
            "ITERATION_NO.: 831 LOSS_Generator: 7.787230491638184 LOSS_Discriminator: 0.028026379644870758\n",
            "ITERATION_NO.: 832 LOSS_Generator: 8.001606941223145 LOSS_Discriminator: 0.001067382050678134\n",
            "ITERATION_NO.: 833 LOSS_Generator: 7.995981693267822 LOSS_Discriminator: 0.19894889990488687\n",
            "ITERATION_NO.: 834 LOSS_Generator: 8.038675308227539 LOSS_Discriminator: 0.18531366189320883\n",
            "ITERATION_NO.: 835 LOSS_Generator: 7.800015926361084 LOSS_Discriminator: 0.026475978394349415\n",
            "ITERATION_NO.: 836 LOSS_Generator: 7.158263206481934 LOSS_Discriminator: 0.011198909332354864\n",
            "ITERATION_NO.: 837 LOSS_Generator: 6.405333042144775 LOSS_Discriminator: 0.6373085180918375\n",
            "ITERATION_NO.: 838 LOSS_Generator: 5.657829284667969 LOSS_Discriminator: 0.17428316672643027\n",
            "ITERATION_NO.: 839 LOSS_Generator: 4.591919898986816 LOSS_Discriminator: 0.016666367650032043\n",
            "ITERATION_NO.: 840 LOSS_Generator: 3.7408056259155273 LOSS_Discriminator: 0.24767951170603433\n",
            "ITERATION_NO.: 841 LOSS_Generator: 2.5733909606933594 LOSS_Discriminator: 0.23753511905670166\n",
            "ITERATION_NO.: 842 LOSS_Generator: 3.3043432235717773 LOSS_Discriminator: 0.18284799655278525\n",
            "ITERATION_NO.: 843 LOSS_Generator: 4.403843879699707 LOSS_Discriminator: 0.04891026020050049\n",
            "ITERATION_NO.: 844 LOSS_Generator: 5.255066394805908 LOSS_Discriminator: 0.17015655835469565\n",
            "ITERATION_NO.: 845 LOSS_Generator: 5.182758331298828 LOSS_Discriminator: 0.20468608538309732\n",
            "ITERATION_NO.: 846 LOSS_Generator: 4.888254165649414 LOSS_Discriminator: 0.10685928662618001\n",
            "ITERATION_NO.: 847 LOSS_Generator: 4.4620585441589355 LOSS_Discriminator: 0.02156090239683787\n",
            "ITERATION_NO.: 848 LOSS_Generator: 4.214786529541016 LOSS_Discriminator: 0.0468535323937734\n",
            "ITERATION_NO.: 849 LOSS_Generator: 4.5587921142578125 LOSS_Discriminator: 0.026198168595631916\n",
            "ITERATION_NO.: 850 LOSS_Generator: 4.330978870391846 LOSS_Discriminator: 0.10120634237925212\n",
            "ITERATION_NO.: 851 LOSS_Generator: 4.717069625854492 LOSS_Discriminator: 0.08830433090527852\n",
            "ITERATION_NO.: 852 LOSS_Generator: 4.256532669067383 LOSS_Discriminator: 0.1230428417523702\n",
            "ITERATION_NO.: 853 LOSS_Generator: 4.307803153991699 LOSS_Discriminator: 0.06902191042900085\n",
            "ITERATION_NO.: 854 LOSS_Generator: 4.269745826721191 LOSS_Discriminator: 0.2524327039718628\n",
            "ITERATION_NO.: 855 LOSS_Generator: 4.543278694152832 LOSS_Discriminator: 0.04454850157101949\n",
            "ITERATION_NO.: 856 LOSS_Generator: 4.4180908203125 LOSS_Discriminator: 0.08504369854927063\n",
            "ITERATION_NO.: 857 LOSS_Generator: 4.505488395690918 LOSS_Discriminator: 0.0744501103957494\n",
            "ITERATION_NO.: 858 LOSS_Generator: 4.522494792938232 LOSS_Discriminator: 0.023377612233161926\n",
            "ITERATION_NO.: 859 LOSS_Generator: 4.899065971374512 LOSS_Discriminator: 0.07060998678207397\n",
            "ITERATION_NO.: 860 LOSS_Generator: 4.910613059997559 LOSS_Discriminator: 0.021933143337567646\n",
            "ITERATION_NO.: 861 LOSS_Generator: 5.040408134460449 LOSS_Discriminator: 0.034654331703980766\n",
            "ITERATION_NO.: 862 LOSS_Generator: 5.235476493835449 LOSS_Discriminator: 0.030403623978296917\n",
            "ITERATION_NO.: 863 LOSS_Generator: 5.153343200683594 LOSS_Discriminator: 0.04999156296253204\n",
            "ITERATION_NO.: 864 LOSS_Generator: 4.708979606628418 LOSS_Discriminator: 0.3642265796661377\n",
            "ITERATION_NO.: 865 LOSS_Generator: 4.365102291107178 LOSS_Discriminator: 0.06269871691862743\n",
            "ITERATION_NO.: 866 LOSS_Generator: 4.081758499145508 LOSS_Discriminator: 0.056970298290252686\n",
            "ITERATION_NO.: 867 LOSS_Generator: 4.037858963012695 LOSS_Discriminator: 0.08615994453430176\n",
            "ITERATION_NO.: 868 LOSS_Generator: 4.713430404663086 LOSS_Discriminator: 0.06388404965400696\n",
            "ITERATION_NO.: 869 LOSS_Generator: 5.3845062255859375 LOSS_Discriminator: 0.07113633056481679\n",
            "ITERATION_NO.: 870 LOSS_Generator: 5.20881462097168 LOSS_Discriminator: 0.128350963195165\n",
            "ITERATION_NO.: 871 LOSS_Generator: 4.739386558532715 LOSS_Discriminator: 0.04907513658205668\n",
            "ITERATION_NO.: 872 LOSS_Generator: 5.061694145202637 LOSS_Discriminator: 0.012790652612845102\n",
            "ITERATION_NO.: 873 LOSS_Generator: 5.193707466125488 LOSS_Discriminator: 0.02144940197467804\n",
            "ITERATION_NO.: 874 LOSS_Generator: 5.09178352355957 LOSS_Discriminator: 0.011608590682347616\n",
            "ITERATION_NO.: 875 LOSS_Generator: 5.2807841300964355 LOSS_Discriminator: 0.029525846242904663\n",
            "ITERATION_NO.: 876 LOSS_Generator: 5.245375156402588 LOSS_Discriminator: 0.05958542227745056\n",
            "ITERATION_NO.: 877 LOSS_Generator: 4.9703688621521 LOSS_Discriminator: 0.15711853901545206\n",
            "ITERATION_NO.: 878 LOSS_Generator: 4.489864826202393 LOSS_Discriminator: 0.08011567095915477\n",
            "ITERATION_NO.: 879 LOSS_Generator: 4.0282392501831055 LOSS_Discriminator: 0.11517801880836487\n",
            "ITERATION_NO.: 880 LOSS_Generator: 3.308769702911377 LOSS_Discriminator: 0.07490760087966919\n",
            "ITERATION_NO.: 881 LOSS_Generator: 4.221743106842041 LOSS_Discriminator: 0.07003446420033772\n",
            "ITERATION_NO.: 882 LOSS_Generator: 5.0495500564575195 LOSS_Discriminator: 0.0853768785794576\n",
            "ITERATION_NO.: 883 LOSS_Generator: 5.532379150390625 LOSS_Discriminator: 0.13207212090492249\n",
            "ITERATION_NO.: 884 LOSS_Generator: 5.809065818786621 LOSS_Discriminator: 0.005524275824427605\n",
            "ITERATION_NO.: 885 LOSS_Generator: 5.8202643394470215 LOSS_Discriminator: 0.280539592107137\n",
            "ITERATION_NO.: 886 LOSS_Generator: 5.255317211151123 LOSS_Discriminator: 0.07315377394358318\n",
            "ITERATION_NO.: 887 LOSS_Generator: 4.4275641441345215 LOSS_Discriminator: 0.022781347235043842\n",
            "ITERATION_NO.: 888 LOSS_Generator: 3.687108278274536 LOSS_Discriminator: 0.09432309865951538\n",
            "ITERATION_NO.: 889 LOSS_Generator: 3.5741333961486816 LOSS_Discriminator: 0.05739101767539978\n",
            "ITERATION_NO.: 890 LOSS_Generator: 4.337769508361816 LOSS_Discriminator: 0.06852100292841594\n",
            "ITERATION_NO.: 891 LOSS_Generator: 5.059716701507568 LOSS_Discriminator: 0.040388815104961395\n",
            "ITERATION_NO.: 892 LOSS_Generator: 5.460778713226318 LOSS_Discriminator: 0.0224596510330836\n",
            "ITERATION_NO.: 893 LOSS_Generator: 6.063979625701904 LOSS_Discriminator: 0.02183808634678523\n",
            "ITERATION_NO.: 894 LOSS_Generator: 6.401214599609375 LOSS_Discriminator: 0.0029758702342708907\n",
            "ITERATION_NO.: 895 LOSS_Generator: 6.331621170043945 LOSS_Discriminator: 0.057528942823410034\n",
            "ITERATION_NO.: 896 LOSS_Generator: 6.325501918792725 LOSS_Discriminator: 0.09289817015329997\n",
            "ITERATION_NO.: 897 LOSS_Generator: 5.907660484313965 LOSS_Discriminator: 0.17296218872070312\n",
            "ITERATION_NO.: 898 LOSS_Generator: 5.016269683837891 LOSS_Discriminator: 0.21652195851008096\n",
            "ITERATION_NO.: 899 LOSS_Generator: 3.7119140625 LOSS_Discriminator: 0.03897612045208613\n",
            "ITERATION_NO.: 900 LOSS_Generator: 4.093783378601074 LOSS_Discriminator: 0.08593231439590454\n",
            "ITERATION_NO.: 901 LOSS_Generator: 4.229680061340332 LOSS_Discriminator: 0.060725962122281395\n",
            "ITERATION_NO.: 902 LOSS_Generator: 5.349056243896484 LOSS_Discriminator: 0.10300563772519429\n",
            "ITERATION_NO.: 903 LOSS_Generator: 6.305948734283447 LOSS_Discriminator: 0.06895128885904948\n",
            "ITERATION_NO.: 904 LOSS_Generator: 7.052495956420898 LOSS_Discriminator: 0.00633695162832737\n",
            "ITERATION_NO.: 905 LOSS_Generator: 7.027021408081055 LOSS_Discriminator: 0.011376156161228815\n",
            "ITERATION_NO.: 906 LOSS_Generator: 7.457420349121094 LOSS_Discriminator: 0.04342138270537058\n",
            "ITERATION_NO.: 907 LOSS_Generator: 6.64278507232666 LOSS_Discriminator: 0.002037346207847198\n",
            "ITERATION_NO.: 908 LOSS_Generator: 6.621306419372559 LOSS_Discriminator: 0.21977357069651285\n",
            "ITERATION_NO.: 909 LOSS_Generator: 5.794561386108398 LOSS_Discriminator: 0.12730946143468222\n",
            "ITERATION_NO.: 910 LOSS_Generator: 5.746861457824707 LOSS_Discriminator: 0.0038351897771159806\n",
            "ITERATION_NO.: 911 LOSS_Generator: 5.2243242263793945 LOSS_Discriminator: 0.142575204372406\n",
            "ITERATION_NO.: 912 LOSS_Generator: 4.117037773132324 LOSS_Discriminator: 0.09054846564928691\n",
            "ITERATION_NO.: 913 LOSS_Generator: 4.614425182342529 LOSS_Discriminator: 0.13730119665463766\n",
            "ITERATION_NO.: 914 LOSS_Generator: 4.011241912841797 LOSS_Discriminator: 0.20457571744918823\n",
            "ITERATION_NO.: 915 LOSS_Generator: 4.5667033195495605 LOSS_Discriminator: 0.1244728962580363\n",
            "ITERATION_NO.: 916 LOSS_Generator: 4.662764549255371 LOSS_Discriminator: 0.06624697645505269\n",
            "ITERATION_NO.: 917 LOSS_Generator: 5.154526710510254 LOSS_Discriminator: 0.12106335163116455\n",
            "ITERATION_NO.: 918 LOSS_Generator: 4.989988803863525 LOSS_Discriminator: 0.09553634126981099\n",
            "ITERATION_NO.: 919 LOSS_Generator: 5.038779258728027 LOSS_Discriminator: 0.020027093589305878\n",
            "ITERATION_NO.: 920 LOSS_Generator: 5.395169258117676 LOSS_Discriminator: 0.013972145815690359\n",
            "ITERATION_NO.: 921 LOSS_Generator: 4.664271354675293 LOSS_Discriminator: 0.15254477659861246\n",
            "ITERATION_NO.: 922 LOSS_Generator: 5.038384437561035 LOSS_Discriminator: 0.29997336864471436\n",
            "ITERATION_NO.: 923 LOSS_Generator: 4.099163055419922 LOSS_Discriminator: 0.223675807317098\n",
            "ITERATION_NO.: 924 LOSS_Generator: 4.249120712280273 LOSS_Discriminator: 0.06835857033729553\n",
            "ITERATION_NO.: 925 LOSS_Generator: 4.472387313842773 LOSS_Discriminator: 0.05904137094815572\n",
            "ITERATION_NO.: 926 LOSS_Generator: 5.372654914855957 LOSS_Discriminator: 0.04332576195398966\n",
            "ITERATION_NO.: 927 LOSS_Generator: 5.991285800933838 LOSS_Discriminator: 0.04722171028455099\n",
            "ITERATION_NO.: 928 LOSS_Generator: 6.347995758056641 LOSS_Discriminator: 0.1636139154434204\n",
            "ITERATION_NO.: 929 LOSS_Generator: 5.832535743713379 LOSS_Discriminator: 0.11774914463361104\n",
            "ITERATION_NO.: 930 LOSS_Generator: 5.373453140258789 LOSS_Discriminator: 0.15374870101610819\n",
            "ITERATION_NO.: 931 LOSS_Generator: 4.832460403442383 LOSS_Discriminator: 0.12208330631256104\n",
            "ITERATION_NO.: 932 LOSS_Generator: 4.097106456756592 LOSS_Discriminator: 0.06108735998471578\n",
            "ITERATION_NO.: 933 LOSS_Generator: 4.064821243286133 LOSS_Discriminator: 0.08183873196442921\n",
            "ITERATION_NO.: 934 LOSS_Generator: 4.252337455749512 LOSS_Discriminator: 0.22638042767842612\n",
            "ITERATION_NO.: 935 LOSS_Generator: 4.410855293273926 LOSS_Discriminator: 0.08119372526804607\n",
            "ITERATION_NO.: 936 LOSS_Generator: 4.385517120361328 LOSS_Discriminator: 0.1957105795542399\n",
            "ITERATION_NO.: 937 LOSS_Generator: 5.004101753234863 LOSS_Discriminator: 0.03100024660428365\n",
            "ITERATION_NO.: 938 LOSS_Generator: 5.761604309082031 LOSS_Discriminator: 0.19442562262217203\n",
            "ITERATION_NO.: 939 LOSS_Generator: 5.287635803222656 LOSS_Discriminator: 0.011507472644249598\n",
            "ITERATION_NO.: 940 LOSS_Generator: 4.871186256408691 LOSS_Discriminator: 0.10561451315879822\n",
            "ITERATION_NO.: 941 LOSS_Generator: 4.93990421295166 LOSS_Discriminator: 0.04240374267101288\n",
            "ITERATION_NO.: 942 LOSS_Generator: 4.2704925537109375 LOSS_Discriminator: 0.05460607508818308\n",
            "ITERATION_NO.: 943 LOSS_Generator: 4.525643825531006 LOSS_Discriminator: 0.030860185623168945\n",
            "ITERATION_NO.: 944 LOSS_Generator: 4.948397636413574 LOSS_Discriminator: 0.1070253054300944\n",
            "ITERATION_NO.: 945 LOSS_Generator: 4.729215145111084 LOSS_Discriminator: 0.11676194270451863\n",
            "ITERATION_NO.: 946 LOSS_Generator: 4.538511753082275 LOSS_Discriminator: 0.2098544438680013\n",
            "ITERATION_NO.: 947 LOSS_Generator: 4.810865879058838 LOSS_Discriminator: 0.03419811278581619\n",
            "ITERATION_NO.: 948 LOSS_Generator: 5.0612030029296875 LOSS_Discriminator: 0.13269281387329102\n",
            "ITERATION_NO.: 949 LOSS_Generator: 5.455948352813721 LOSS_Discriminator: 0.01957621177037557\n",
            "ITERATION_NO.: 950 LOSS_Generator: 5.54195499420166 LOSS_Discriminator: 0.021592281758785248\n",
            "ITERATION_NO.: 951 LOSS_Generator: 5.852341651916504 LOSS_Discriminator: 0.038683717449506126\n",
            "ITERATION_NO.: 952 LOSS_Generator: 6.0306291580200195 LOSS_Discriminator: 0.1318689783414205\n",
            "ITERATION_NO.: 953 LOSS_Generator: 5.852808952331543 LOSS_Discriminator: 0.012311037629842758\n",
            "ITERATION_NO.: 954 LOSS_Generator: 5.453342437744141 LOSS_Discriminator: 0.13855252663294473\n",
            "ITERATION_NO.: 955 LOSS_Generator: 4.73928165435791 LOSS_Discriminator: 0.35126550992329914\n",
            "ITERATION_NO.: 956 LOSS_Generator: 4.154153823852539 LOSS_Discriminator: 0.03132879237333933\n",
            "ITERATION_NO.: 957 LOSS_Generator: 4.2809343338012695 LOSS_Discriminator: 0.05804008742173513\n",
            "ITERATION_NO.: 958 LOSS_Generator: 4.684706687927246 LOSS_Discriminator: 0.044877469539642334\n",
            "ITERATION_NO.: 959 LOSS_Generator: 5.324938774108887 LOSS_Discriminator: 0.0440014104048411\n",
            "ITERATION_NO.: 960 LOSS_Generator: 5.642443656921387 LOSS_Discriminator: 0.015008853127559027\n",
            "ITERATION_NO.: 961 LOSS_Generator: 5.51797342300415 LOSS_Discriminator: 0.020517372836669285\n",
            "ITERATION_NO.: 962 LOSS_Generator: 5.682651042938232 LOSS_Discriminator: 0.006269983947277069\n",
            "ITERATION_NO.: 963 LOSS_Generator: 6.344221591949463 LOSS_Discriminator: 0.08145546913146973\n",
            "ITERATION_NO.: 964 LOSS_Generator: 5.884452819824219 LOSS_Discriminator: 0.40289350350697833\n",
            "ITERATION_NO.: 965 LOSS_Generator: 4.863723278045654 LOSS_Discriminator: 0.019722638030846912\n",
            "ITERATION_NO.: 966 LOSS_Generator: 4.290446758270264 LOSS_Discriminator: 0.14977940917015076\n",
            "ITERATION_NO.: 967 LOSS_Generator: 3.7550454139709473 LOSS_Discriminator: 0.14119903246561685\n",
            "ITERATION_NO.: 968 LOSS_Generator: 3.331902503967285 LOSS_Discriminator: 0.15312238534291586\n",
            "ITERATION_NO.: 969 LOSS_Generator: 4.255972385406494 LOSS_Discriminator: 0.1783531109491984\n",
            "ITERATION_NO.: 970 LOSS_Generator: 4.818435192108154 LOSS_Discriminator: 0.05319016178448995\n",
            "ITERATION_NO.: 971 LOSS_Generator: 5.4144744873046875 LOSS_Discriminator: 0.0514048288265864\n",
            "ITERATION_NO.: 972 LOSS_Generator: 5.676644802093506 LOSS_Discriminator: 0.1066777507464091\n",
            "ITERATION_NO.: 973 LOSS_Generator: 5.58367919921875 LOSS_Discriminator: 0.19340741634368896\n",
            "ITERATION_NO.: 974 LOSS_Generator: 5.026968955993652 LOSS_Discriminator: 0.13739226261774698\n",
            "ITERATION_NO.: 975 LOSS_Generator: 4.114178657531738 LOSS_Discriminator: 0.021193285783131916\n",
            "ITERATION_NO.: 976 LOSS_Generator: 4.159976959228516 LOSS_Discriminator: 0.10222480694452922\n",
            "ITERATION_NO.: 977 LOSS_Generator: 4.352883338928223 LOSS_Discriminator: 0.08851288755734761\n",
            "ITERATION_NO.: 978 LOSS_Generator: 4.811239242553711 LOSS_Discriminator: 0.02550622820854187\n",
            "ITERATION_NO.: 979 LOSS_Generator: 5.312424659729004 LOSS_Discriminator: 0.1394323209921519\n",
            "ITERATION_NO.: 980 LOSS_Generator: 5.399176597595215 LOSS_Discriminator: 0.016366661836703617\n",
            "ITERATION_NO.: 981 LOSS_Generator: 5.024956703186035 LOSS_Discriminator: 0.09409025311470032\n",
            "ITERATION_NO.: 982 LOSS_Generator: 4.878918647766113 LOSS_Discriminator: 0.1535972754160563\n",
            "ITERATION_NO.: 983 LOSS_Generator: 4.5803070068359375 LOSS_Discriminator: 0.044106642405192055\n",
            "ITERATION_NO.: 984 LOSS_Generator: 4.36664342880249 LOSS_Discriminator: 0.06308006743590037\n",
            "ITERATION_NO.: 985 LOSS_Generator: 5.0100555419921875 LOSS_Discriminator: 0.033723038931687675\n",
            "ITERATION_NO.: 986 LOSS_Generator: 5.557249546051025 LOSS_Discriminator: 0.02905461937189102\n",
            "ITERATION_NO.: 987 LOSS_Generator: 5.910678863525391 LOSS_Discriminator: 0.2973695397377014\n",
            "ITERATION_NO.: 988 LOSS_Generator: 5.113407611846924 LOSS_Discriminator: 0.04792670408884684\n",
            "ITERATION_NO.: 989 LOSS_Generator: 4.947498321533203 LOSS_Discriminator: 0.015941860775152843\n",
            "ITERATION_NO.: 990 LOSS_Generator: 4.701847076416016 LOSS_Discriminator: 0.11460824807484944\n",
            "ITERATION_NO.: 991 LOSS_Generator: 4.349112033843994 LOSS_Discriminator: 0.11614388227462769\n",
            "ITERATION_NO.: 992 LOSS_Generator: 3.938389301300049 LOSS_Discriminator: 0.25019421180089313\n",
            "ITERATION_NO.: 993 LOSS_Generator: 3.686774730682373 LOSS_Discriminator: 0.0916180709997813\n",
            "ITERATION_NO.: 994 LOSS_Generator: 4.476814270019531 LOSS_Discriminator: 0.0752989649772644\n",
            "ITERATION_NO.: 995 LOSS_Generator: 5.023931980133057 LOSS_Discriminator: 0.04618437588214874\n",
            "ITERATION_NO.: 996 LOSS_Generator: 6.1548051834106445 LOSS_Discriminator: 0.2011341651280721\n",
            "ITERATION_NO.: 997 LOSS_Generator: 5.629678726196289 LOSS_Discriminator: 0.3309508164723714\n",
            "ITERATION_NO.: 998 LOSS_Generator: 4.227270126342773 LOSS_Discriminator: 0.04454415043195089\n",
            "ITERATION_NO.: 999 LOSS_Generator: 4.0890212059021 LOSS_Discriminator: 0.0906251072883606\n",
            "ITERATION_NO.: 1000 LOSS_Generator: 4.640380859375 LOSS_Discriminator: 0.08252632121245067\n",
            "ITERATION_NO.: 1001 LOSS_Generator: 5.314223289489746 LOSS_Discriminator: 0.07801932096481323\n",
            "ITERATION_NO.: 1002 LOSS_Generator: 5.58432674407959 LOSS_Discriminator: 0.060629566510518394\n",
            "ITERATION_NO.: 1003 LOSS_Generator: 5.389737129211426 LOSS_Discriminator: 0.30676156282424927\n",
            "ITERATION_NO.: 1004 LOSS_Generator: 5.18636417388916 LOSS_Discriminator: 0.03752098480860392\n",
            "ITERATION_NO.: 1005 LOSS_Generator: 4.437017440795898 LOSS_Discriminator: 0.09387014309565227\n",
            "ITERATION_NO.: 1006 LOSS_Generator: 4.322842597961426 LOSS_Discriminator: 0.07200466593106587\n",
            "ITERATION_NO.: 1007 LOSS_Generator: 3.83130145072937 LOSS_Discriminator: 0.1962484916051229\n",
            "ITERATION_NO.: 1008 LOSS_Generator: 4.24424934387207 LOSS_Discriminator: 0.11051587263743083\n",
            "ITERATION_NO.: 1009 LOSS_Generator: 4.684996128082275 LOSS_Discriminator: 0.031113942464192707\n",
            "ITERATION_NO.: 1010 LOSS_Generator: 5.043358325958252 LOSS_Discriminator: 0.0911208987236023\n",
            "ITERATION_NO.: 1011 LOSS_Generator: 5.098339080810547 LOSS_Discriminator: 0.11765215794245402\n",
            "ITERATION_NO.: 1012 LOSS_Generator: 4.0473737716674805 LOSS_Discriminator: 0.09000007311503093\n",
            "ITERATION_NO.: 1013 LOSS_Generator: 3.61918306350708 LOSS_Discriminator: 0.04734182357788086\n",
            "ITERATION_NO.: 1014 LOSS_Generator: 4.316061496734619 LOSS_Discriminator: 0.05325108766555786\n",
            "ITERATION_NO.: 1015 LOSS_Generator: 5.093627452850342 LOSS_Discriminator: 0.03305753072102865\n",
            "ITERATION_NO.: 1016 LOSS_Generator: 5.559386253356934 LOSS_Discriminator: 0.18881386518478394\n",
            "ITERATION_NO.: 1017 LOSS_Generator: 5.642213344573975 LOSS_Discriminator: 0.027878584961096447\n",
            "ITERATION_NO.: 1018 LOSS_Generator: 5.184061527252197 LOSS_Discriminator: 0.18643033504486084\n",
            "ITERATION_NO.: 1019 LOSS_Generator: 4.689079761505127 LOSS_Discriminator: 0.015077680349349976\n",
            "ITERATION_NO.: 1020 LOSS_Generator: 4.227566719055176 LOSS_Discriminator: 0.033313244581222534\n",
            "ITERATION_NO.: 1021 LOSS_Generator: 5.107036113739014 LOSS_Discriminator: 0.1286045511563619\n",
            "ITERATION_NO.: 1022 LOSS_Generator: 4.542881965637207 LOSS_Discriminator: 0.13482869664827982\n",
            "ITERATION_NO.: 1023 LOSS_Generator: 5.019299507141113 LOSS_Discriminator: 0.016141049563884735\n",
            "ITERATION_NO.: 1024 LOSS_Generator: 5.284918308258057 LOSS_Discriminator: 0.0939826766649882\n",
            "ITERATION_NO.: 1025 LOSS_Generator: 4.936330795288086 LOSS_Discriminator: 0.07456191380818684\n",
            "ITERATION_NO.: 1026 LOSS_Generator: 3.566560745239258 LOSS_Discriminator: 0.20498871803283691\n",
            "ITERATION_NO.: 1027 LOSS_Generator: 3.1975221633911133 LOSS_Discriminator: 0.10354546705881755\n",
            "ITERATION_NO.: 1028 LOSS_Generator: 4.249083518981934 LOSS_Discriminator: 0.08147207895914714\n",
            "ITERATION_NO.: 1029 LOSS_Generator: 5.44562292098999 LOSS_Discriminator: 0.027945123612880707\n",
            "ITERATION_NO.: 1030 LOSS_Generator: 6.386735916137695 LOSS_Discriminator: 0.04228290915489197\n",
            "ITERATION_NO.: 1031 LOSS_Generator: 6.698626518249512 LOSS_Discriminator: 0.00913848727941513\n",
            "ITERATION_NO.: 1032 LOSS_Generator: 7.316617965698242 LOSS_Discriminator: 0.15062002340952554\n",
            "ITERATION_NO.: 1033 LOSS_Generator: 6.981300354003906 LOSS_Discriminator: 0.1441639761130015\n",
            "ITERATION_NO.: 1034 LOSS_Generator: 6.300431251525879 LOSS_Discriminator: 0.17842902739842734\n",
            "ITERATION_NO.: 1035 LOSS_Generator: 4.90381383895874 LOSS_Discriminator: 0.10198291142781575\n",
            "ITERATION_NO.: 1036 LOSS_Generator: 3.7630691528320312 LOSS_Discriminator: 0.030050439139207203\n",
            "ITERATION_NO.: 1037 LOSS_Generator: 3.6453394889831543 LOSS_Discriminator: 0.05986905594666799\n",
            "ITERATION_NO.: 1038 LOSS_Generator: 4.600808143615723 LOSS_Discriminator: 0.1076217492421468\n",
            "ITERATION_NO.: 1039 LOSS_Generator: 5.322387218475342 LOSS_Discriminator: 0.01799885928630829\n",
            "ITERATION_NO.: 1040 LOSS_Generator: 5.467577934265137 LOSS_Discriminator: 0.08738673726717631\n",
            "ITERATION_NO.: 1041 LOSS_Generator: 6.261971950531006 LOSS_Discriminator: 0.011527096231778463\n",
            "ITERATION_NO.: 1042 LOSS_Generator: 6.294625282287598 LOSS_Discriminator: 0.08518439531326294\n",
            "ITERATION_NO.: 1043 LOSS_Generator: 5.874687194824219 LOSS_Discriminator: 0.08450165390968323\n",
            "ITERATION_NO.: 1044 LOSS_Generator: 5.476508140563965 LOSS_Discriminator: 0.08422144254048665\n",
            "ITERATION_NO.: 1045 LOSS_Generator: 4.525623798370361 LOSS_Discriminator: 0.10384615262349446\n",
            "ITERATION_NO.: 1046 LOSS_Generator: 3.377135753631592 LOSS_Discriminator: 0.1369843085606893\n",
            "ITERATION_NO.: 1047 LOSS_Generator: 4.157361030578613 LOSS_Discriminator: 0.09494503339131673\n",
            "ITERATION_NO.: 1048 LOSS_Generator: 4.473341464996338 LOSS_Discriminator: 0.04575282335281372\n",
            "ITERATION_NO.: 1049 LOSS_Generator: 5.709859848022461 LOSS_Discriminator: 0.033401452004909515\n",
            "ITERATION_NO.: 1050 LOSS_Generator: 5.832838535308838 LOSS_Discriminator: 0.07784442106882732\n",
            "ITERATION_NO.: 1051 LOSS_Generator: 6.14078426361084 LOSS_Discriminator: 0.10676053166389465\n",
            "ITERATION_NO.: 1052 LOSS_Generator: 5.560577392578125 LOSS_Discriminator: 0.16195942958196005\n",
            "ITERATION_NO.: 1053 LOSS_Generator: 4.414124965667725 LOSS_Discriminator: 0.011525133003791174\n",
            "ITERATION_NO.: 1054 LOSS_Generator: 3.7158923149108887 LOSS_Discriminator: 0.15558674931526184\n",
            "ITERATION_NO.: 1055 LOSS_Generator: 4.0734453201293945 LOSS_Discriminator: 0.11539414525032043\n",
            "ITERATION_NO.: 1056 LOSS_Generator: 5.536815643310547 LOSS_Discriminator: 0.07181058327356975\n",
            "ITERATION_NO.: 1057 LOSS_Generator: 5.715486526489258 LOSS_Discriminator: 0.07081103324890137\n",
            "ITERATION_NO.: 1058 LOSS_Generator: 6.149535179138184 LOSS_Discriminator: 0.03528653085231781\n",
            "ITERATION_NO.: 1059 LOSS_Generator: 6.767414093017578 LOSS_Discriminator: 0.006108885630965233\n",
            "ITERATION_NO.: 1060 LOSS_Generator: 6.929518699645996 LOSS_Discriminator: 0.0026158802211284637\n",
            "ITERATION_NO.: 1061 LOSS_Generator: 6.604139804840088 LOSS_Discriminator: 0.4299839735031128\n",
            "ITERATION_NO.: 1062 LOSS_Generator: 6.1171112060546875 LOSS_Discriminator: 0.003034895285964012\n",
            "ITERATION_NO.: 1063 LOSS_Generator: 5.513485431671143 LOSS_Discriminator: 0.07979829609394073\n",
            "ITERATION_NO.: 1064 LOSS_Generator: 3.9921927452087402 LOSS_Discriminator: 0.1726569135983785\n",
            "ITERATION_NO.: 1065 LOSS_Generator: 4.250300407409668 LOSS_Discriminator: 0.17059959967931113\n",
            "ITERATION_NO.: 1066 LOSS_Generator: 4.887619972229004 LOSS_Discriminator: 0.050676445166269936\n",
            "ITERATION_NO.: 1067 LOSS_Generator: 5.4307966232299805 LOSS_Discriminator: 0.14214580257733664\n",
            "ITERATION_NO.: 1068 LOSS_Generator: 5.307692527770996 LOSS_Discriminator: 0.09431174397468567\n",
            "ITERATION_NO.: 1069 LOSS_Generator: 5.195885181427002 LOSS_Discriminator: 0.05973906318346659\n",
            "ITERATION_NO.: 1070 LOSS_Generator: 4.841267108917236 LOSS_Discriminator: 0.0673215240240097\n",
            "ITERATION_NO.: 1071 LOSS_Generator: 4.9759521484375 LOSS_Discriminator: 0.023485901455084484\n",
            "ITERATION_NO.: 1072 LOSS_Generator: 5.137995719909668 LOSS_Discriminator: 0.09211931626001994\n",
            "ITERATION_NO.: 1073 LOSS_Generator: 4.55816650390625 LOSS_Discriminator: 0.0846852958202362\n",
            "ITERATION_NO.: 1074 LOSS_Generator: 4.66884183883667 LOSS_Discriminator: 0.04562212526798248\n",
            "ITERATION_NO.: 1075 LOSS_Generator: 4.652200698852539 LOSS_Discriminator: 0.08237385253111522\n",
            "ITERATION_NO.: 1076 LOSS_Generator: 5.163259506225586 LOSS_Discriminator: 0.05731503665447235\n",
            "ITERATION_NO.: 1077 LOSS_Generator: 6.199244499206543 LOSS_Discriminator: 0.18177088101704916\n",
            "ITERATION_NO.: 1078 LOSS_Generator: 6.347944259643555 LOSS_Discriminator: 0.007124550019701322\n",
            "ITERATION_NO.: 1079 LOSS_Generator: 5.895962715148926 LOSS_Discriminator: 0.06617788473765056\n",
            "ITERATION_NO.: 1080 LOSS_Generator: 5.9073052406311035 LOSS_Discriminator: 0.008258040994405746\n",
            "ITERATION_NO.: 1081 LOSS_Generator: 5.3423919677734375 LOSS_Discriminator: 0.08726113041241963\n",
            "ITERATION_NO.: 1082 LOSS_Generator: 5.178911209106445 LOSS_Discriminator: 0.18035211165746054\n",
            "ITERATION_NO.: 1083 LOSS_Generator: 4.6692304611206055 LOSS_Discriminator: 0.04181968172391256\n",
            "ITERATION_NO.: 1084 LOSS_Generator: 4.292535781860352 LOSS_Discriminator: 0.12378903230031331\n",
            "ITERATION_NO.: 1085 LOSS_Generator: 4.220350742340088 LOSS_Discriminator: 0.1436540683110555\n",
            "ITERATION_NO.: 1086 LOSS_Generator: 4.384953498840332 LOSS_Discriminator: 0.07852389911810558\n",
            "ITERATION_NO.: 1087 LOSS_Generator: 5.084803104400635 LOSS_Discriminator: 0.1672100822130839\n",
            "ITERATION_NO.: 1088 LOSS_Generator: 4.8843231201171875 LOSS_Discriminator: 0.12938855091730753\n",
            "ITERATION_NO.: 1089 LOSS_Generator: 5.164227485656738 LOSS_Discriminator: 0.024313437441984814\n",
            "ITERATION_NO.: 1090 LOSS_Generator: 4.806163787841797 LOSS_Discriminator: 0.027291042109330494\n",
            "ITERATION_NO.: 1091 LOSS_Generator: 4.680996894836426 LOSS_Discriminator: 0.02244878311951955\n",
            "ITERATION_NO.: 1092 LOSS_Generator: 5.353542327880859 LOSS_Discriminator: 0.0254556139310201\n",
            "ITERATION_NO.: 1093 LOSS_Generator: 5.789097785949707 LOSS_Discriminator: 0.057035466035207115\n",
            "ITERATION_NO.: 1094 LOSS_Generator: 5.047096252441406 LOSS_Discriminator: 0.2039672533671061\n",
            "ITERATION_NO.: 1095 LOSS_Generator: 4.372638702392578 LOSS_Discriminator: 0.07987355689207713\n",
            "ITERATION_NO.: 1096 LOSS_Generator: 4.971171855926514 LOSS_Discriminator: 0.06534567475318909\n",
            "ITERATION_NO.: 1097 LOSS_Generator: 5.313224792480469 LOSS_Discriminator: 0.09669286012649536\n",
            "ITERATION_NO.: 1098 LOSS_Generator: 6.427004814147949 LOSS_Discriminator: 0.013133189330498377\n",
            "ITERATION_NO.: 1099 LOSS_Generator: 6.96209192276001 LOSS_Discriminator: 0.02551328142484029\n",
            "ITERATION_NO.: 1100 LOSS_Generator: 6.550675392150879 LOSS_Discriminator: 0.19576863447825113\n",
            "ITERATION_NO.: 1101 LOSS_Generator: 6.347121238708496 LOSS_Discriminator: 0.06654787063598633\n",
            "ITERATION_NO.: 1102 LOSS_Generator: 5.50314998626709 LOSS_Discriminator: 0.1190739373366038\n",
            "ITERATION_NO.: 1103 LOSS_Generator: 4.869772911071777 LOSS_Discriminator: 0.01718576004107793\n",
            "ITERATION_NO.: 1104 LOSS_Generator: 4.64115047454834 LOSS_Discriminator: 0.06437045832475026\n",
            "ITERATION_NO.: 1105 LOSS_Generator: 4.012383937835693 LOSS_Discriminator: 0.14423254132270813\n",
            "ITERATION_NO.: 1106 LOSS_Generator: 4.845973014831543 LOSS_Discriminator: 0.1119212806224823\n",
            "ITERATION_NO.: 1107 LOSS_Generator: 5.4046735763549805 LOSS_Discriminator: 0.07883274058500926\n",
            "ITERATION_NO.: 1108 LOSS_Generator: 6.311978340148926 LOSS_Discriminator: 0.02001243457198143\n",
            "ITERATION_NO.: 1109 LOSS_Generator: 7.158655166625977 LOSS_Discriminator: 0.1128574808438619\n",
            "ITERATION_NO.: 1110 LOSS_Generator: 7.005643844604492 LOSS_Discriminator: 0.14226597547531128\n",
            "ITERATION_NO.: 1111 LOSS_Generator: 6.096686363220215 LOSS_Discriminator: 0.32886795202891034\n",
            "ITERATION_NO.: 1112 LOSS_Generator: 5.279223442077637 LOSS_Discriminator: 0.1360969841480255\n",
            "ITERATION_NO.: 1113 LOSS_Generator: 4.185329914093018 LOSS_Discriminator: 0.16260180870691934\n",
            "ITERATION_NO.: 1114 LOSS_Generator: 4.389196395874023 LOSS_Discriminator: 0.1715955932935079\n",
            "ITERATION_NO.: 1115 LOSS_Generator: 4.767463684082031 LOSS_Discriminator: 0.06262074907620747\n",
            "ITERATION_NO.: 1116 LOSS_Generator: 4.819477081298828 LOSS_Discriminator: 0.1490232745806376\n",
            "ITERATION_NO.: 1117 LOSS_Generator: 5.372845649719238 LOSS_Discriminator: 0.08490167061487834\n",
            "ITERATION_NO.: 1118 LOSS_Generator: 5.157018661499023 LOSS_Discriminator: 0.09162898858388265\n",
            "ITERATION_NO.: 1119 LOSS_Generator: 5.377729415893555 LOSS_Discriminator: 0.09163467089335124\n",
            "ITERATION_NO.: 1120 LOSS_Generator: 5.138314247131348 LOSS_Discriminator: 0.1956670880317688\n",
            "ITERATION_NO.: 1121 LOSS_Generator: 5.5161638259887695 LOSS_Discriminator: 0.04112586627403895\n",
            "ITERATION_NO.: 1122 LOSS_Generator: 3.9742908477783203 LOSS_Discriminator: 0.18887444337209067\n",
            "ITERATION_NO.: 1123 LOSS_Generator: 3.813093900680542 LOSS_Discriminator: 0.04568300644556681\n",
            "ITERATION_NO.: 1124 LOSS_Generator: 4.2139410972595215 LOSS_Discriminator: 0.08868512511253357\n",
            "ITERATION_NO.: 1125 LOSS_Generator: 4.635953903198242 LOSS_Discriminator: 0.15036718050638834\n",
            "ITERATION_NO.: 1126 LOSS_Generator: 4.592103481292725 LOSS_Discriminator: 0.028841284414132435\n",
            "ITERATION_NO.: 1127 LOSS_Generator: 4.424666404724121 LOSS_Discriminator: 0.026487909257411957\n",
            "ITERATION_NO.: 1128 LOSS_Generator: 5.36036491394043 LOSS_Discriminator: 0.015307960410912832\n",
            "ITERATION_NO.: 1129 LOSS_Generator: 5.735389232635498 LOSS_Discriminator: 0.05084444582462311\n",
            "ITERATION_NO.: 1130 LOSS_Generator: 5.71872091293335 LOSS_Discriminator: 0.1968715786933899\n",
            "ITERATION_NO.: 1131 LOSS_Generator: 4.776269912719727 LOSS_Discriminator: 0.05792886018753052\n",
            "ITERATION_NO.: 1132 LOSS_Generator: 4.823763847351074 LOSS_Discriminator: 0.03361830612023672\n",
            "ITERATION_NO.: 1133 LOSS_Generator: 4.4663848876953125 LOSS_Discriminator: 0.041199917594591774\n",
            "ITERATION_NO.: 1134 LOSS_Generator: 4.937455654144287 LOSS_Discriminator: 0.15800039966901144\n",
            "ITERATION_NO.: 1135 LOSS_Generator: 4.865859508514404 LOSS_Discriminator: 0.02007949724793434\n",
            "ITERATION_NO.: 1136 LOSS_Generator: 4.533282279968262 LOSS_Discriminator: 0.2269761562347412\n",
            "ITERATION_NO.: 1137 LOSS_Generator: 4.786910057067871 LOSS_Discriminator: 0.07079504430294037\n",
            "ITERATION_NO.: 1138 LOSS_Generator: 4.832902431488037 LOSS_Discriminator: 0.019812545428673427\n",
            "ITERATION_NO.: 1139 LOSS_Generator: 4.5096635818481445 LOSS_Discriminator: 0.21480369567871094\n",
            "ITERATION_NO.: 1140 LOSS_Generator: 4.300771236419678 LOSS_Discriminator: 0.15001444021860758\n",
            "ITERATION_NO.: 1141 LOSS_Generator: 3.6780734062194824 LOSS_Discriminator: 0.1983685294787089\n",
            "ITERATION_NO.: 1142 LOSS_Generator: 3.596364974975586 LOSS_Discriminator: 0.10838016867637634\n",
            "ITERATION_NO.: 1143 LOSS_Generator: 5.297558784484863 LOSS_Discriminator: 0.10408914089202881\n",
            "ITERATION_NO.: 1144 LOSS_Generator: 5.989027500152588 LOSS_Discriminator: 0.01855051393310229\n",
            "ITERATION_NO.: 1145 LOSS_Generator: 6.911456108093262 LOSS_Discriminator: 0.10620198647181193\n",
            "ITERATION_NO.: 1146 LOSS_Generator: 6.286496162414551 LOSS_Discriminator: 0.3516879876454671\n",
            "ITERATION_NO.: 1147 LOSS_Generator: 5.771149635314941 LOSS_Discriminator: 0.004114560162027677\n",
            "ITERATION_NO.: 1148 LOSS_Generator: 5.657748222351074 LOSS_Discriminator: 0.018688321113586426\n",
            "ITERATION_NO.: 1149 LOSS_Generator: 4.849501609802246 LOSS_Discriminator: 0.07293710112571716\n",
            "ITERATION_NO.: 1150 LOSS_Generator: 4.698450565338135 LOSS_Discriminator: 0.08187080919742584\n",
            "ITERATION_NO.: 1151 LOSS_Generator: 5.125364780426025 LOSS_Discriminator: 0.04324626922607422\n",
            "ITERATION_NO.: 1152 LOSS_Generator: 4.66096305847168 LOSS_Discriminator: 0.03521451850732168\n",
            "ITERATION_NO.: 1153 LOSS_Generator: 5.327099800109863 LOSS_Discriminator: 0.028006526331106823\n",
            "ITERATION_NO.: 1154 LOSS_Generator: 4.873387813568115 LOSS_Discriminator: 0.265502393245697\n",
            "ITERATION_NO.: 1155 LOSS_Generator: 3.888857841491699 LOSS_Discriminator: 0.15504849950472513\n",
            "ITERATION_NO.: 1156 LOSS_Generator: 3.8022334575653076 LOSS_Discriminator: 0.05886603891849518\n",
            "ITERATION_NO.: 1157 LOSS_Generator: 4.483401298522949 LOSS_Discriminator: 0.06351165970166524\n",
            "ITERATION_NO.: 1158 LOSS_Generator: 4.911870002746582 LOSS_Discriminator: 0.13916033506393433\n",
            "ITERATION_NO.: 1159 LOSS_Generator: 5.301419734954834 LOSS_Discriminator: 0.10510977109273274\n",
            "ITERATION_NO.: 1160 LOSS_Generator: 4.659039497375488 LOSS_Discriminator: 0.09662437438964844\n",
            "ITERATION_NO.: 1161 LOSS_Generator: 4.243504524230957 LOSS_Discriminator: 0.13308340311050415\n",
            "ITERATION_NO.: 1162 LOSS_Generator: 3.9739348888397217 LOSS_Discriminator: 0.06540082891782124\n",
            "ITERATION_NO.: 1163 LOSS_Generator: 4.2575788497924805 LOSS_Discriminator: 0.04069090882937113\n",
            "ITERATION_NO.: 1164 LOSS_Generator: 5.085624694824219 LOSS_Discriminator: 0.04484453797340393\n",
            "ITERATION_NO.: 1165 LOSS_Generator: 5.67598295211792 LOSS_Discriminator: 0.059413750966389976\n",
            "ITERATION_NO.: 1166 LOSS_Generator: 6.5254011154174805 LOSS_Discriminator: 0.03566500047842661\n",
            "ITERATION_NO.: 1167 LOSS_Generator: 6.680768013000488 LOSS_Discriminator: 0.021752330164114635\n",
            "ITERATION_NO.: 1168 LOSS_Generator: 6.475481033325195 LOSS_Discriminator: 0.004011635047694047\n",
            "ITERATION_NO.: 1169 LOSS_Generator: 7.062642574310303 LOSS_Discriminator: 0.025549034277598064\n",
            "ITERATION_NO.: 1170 LOSS_Generator: 6.800130844116211 LOSS_Discriminator: 0.023886457085609436\n",
            "ITERATION_NO.: 1171 LOSS_Generator: 6.570436477661133 LOSS_Discriminator: 0.09050055344899495\n",
            "ITERATION_NO.: 1172 LOSS_Generator: 6.403252601623535 LOSS_Discriminator: 0.0036497494826714196\n",
            "ITERATION_NO.: 1173 LOSS_Generator: 5.970598220825195 LOSS_Discriminator: 0.012788087129592896\n",
            "ITERATION_NO.: 1174 LOSS_Generator: 5.472574234008789 LOSS_Discriminator: 0.0623817245165507\n",
            "ITERATION_NO.: 1175 LOSS_Generator: 5.26359748840332 LOSS_Discriminator: 0.05868150293827057\n",
            "ITERATION_NO.: 1176 LOSS_Generator: 4.578342914581299 LOSS_Discriminator: 0.076922208070755\n",
            "ITERATION_NO.: 1177 LOSS_Generator: 4.320699691772461 LOSS_Discriminator: 0.17661571502685547\n",
            "ITERATION_NO.: 1178 LOSS_Generator: 4.277234077453613 LOSS_Discriminator: 0.10823256770769756\n",
            "ITERATION_NO.: 1179 LOSS_Generator: 4.050018787384033 LOSS_Discriminator: 0.2790100574493408\n",
            "ITERATION_NO.: 1180 LOSS_Generator: 4.391364097595215 LOSS_Discriminator: 0.046047747135162354\n",
            "ITERATION_NO.: 1181 LOSS_Generator: 4.730226516723633 LOSS_Discriminator: 0.026737314959367115\n",
            "ITERATION_NO.: 1182 LOSS_Generator: 4.886627674102783 LOSS_Discriminator: 0.37002233664194745\n",
            "ITERATION_NO.: 1183 LOSS_Generator: 4.416687965393066 LOSS_Discriminator: 0.0653044084707896\n",
            "ITERATION_NO.: 1184 LOSS_Generator: 4.124393463134766 LOSS_Discriminator: 0.08770450949668884\n",
            "ITERATION_NO.: 1185 LOSS_Generator: 5.276579856872559 LOSS_Discriminator: 0.06366328398386638\n",
            "ITERATION_NO.: 1186 LOSS_Generator: 6.017759323120117 LOSS_Discriminator: 0.08689624071121216\n",
            "ITERATION_NO.: 1187 LOSS_Generator: 6.294645309448242 LOSS_Discriminator: 0.05164817969004313\n",
            "ITERATION_NO.: 1188 LOSS_Generator: 5.744257926940918 LOSS_Discriminator: 0.17336148023605347\n",
            "ITERATION_NO.: 1189 LOSS_Generator: 6.079977989196777 LOSS_Discriminator: 0.01401404912273089\n",
            "ITERATION_NO.: 1190 LOSS_Generator: 5.336702823638916 LOSS_Discriminator: 0.022829343875249226\n",
            "ITERATION_NO.: 1191 LOSS_Generator: 5.248348712921143 LOSS_Discriminator: 0.013959214091300964\n",
            "ITERATION_NO.: 1192 LOSS_Generator: 5.5854949951171875 LOSS_Discriminator: 0.017269600182771683\n",
            "ITERATION_NO.: 1193 LOSS_Generator: 5.123766899108887 LOSS_Discriminator: 0.18414302666982016\n",
            "ITERATION_NO.: 1194 LOSS_Generator: 4.237505912780762 LOSS_Discriminator: 0.2947072585423787\n",
            "ITERATION_NO.: 1195 LOSS_Generator: 3.5828771591186523 LOSS_Discriminator: 0.08664991458257039\n",
            "ITERATION_NO.: 1196 LOSS_Generator: 3.681502103805542 LOSS_Discriminator: 0.06465466817220052\n",
            "ITERATION_NO.: 1197 LOSS_Generator: 4.749659538269043 LOSS_Discriminator: 0.17160813013712564\n",
            "ITERATION_NO.: 1198 LOSS_Generator: 5.392040252685547 LOSS_Discriminator: 0.026888189216454823\n",
            "ITERATION_NO.: 1199 LOSS_Generator: 5.867103576660156 LOSS_Discriminator: 0.23067772388458252\n",
            "ITERATION_NO.: 1200 LOSS_Generator: 4.949535846710205 LOSS_Discriminator: 0.1189190944035848\n",
            "ITERATION_NO.: 1201 LOSS_Generator: 4.035902500152588 LOSS_Discriminator: 0.13390424847602844\n",
            "ITERATION_NO.: 1202 LOSS_Generator: 3.472398281097412 LOSS_Discriminator: 0.1481658716996511\n",
            "ITERATION_NO.: 1203 LOSS_Generator: 4.488980770111084 LOSS_Discriminator: 0.09236015876134236\n",
            "ITERATION_NO.: 1204 LOSS_Generator: 6.082653999328613 LOSS_Discriminator: 0.03268041213353475\n",
            "ITERATION_NO.: 1205 LOSS_Generator: 6.446782112121582 LOSS_Discriminator: 0.09575652082761128\n",
            "ITERATION_NO.: 1206 LOSS_Generator: 6.598121643066406 LOSS_Discriminator: 0.004365145228803158\n",
            "ITERATION_NO.: 1207 LOSS_Generator: 6.537194728851318 LOSS_Discriminator: 0.08700993657112122\n",
            "ITERATION_NO.: 1208 LOSS_Generator: 6.845072269439697 LOSS_Discriminator: 0.07630361119906108\n",
            "ITERATION_NO.: 1209 LOSS_Generator: 5.733427047729492 LOSS_Discriminator: 0.21578800678253174\n",
            "ITERATION_NO.: 1210 LOSS_Generator: 4.610345363616943 LOSS_Discriminator: 0.2516602675120036\n",
            "ITERATION_NO.: 1211 LOSS_Generator: 3.875741481781006 LOSS_Discriminator: 0.11013282338778178\n",
            "ITERATION_NO.: 1212 LOSS_Generator: 4.462532997131348 LOSS_Discriminator: 0.1812239090601603\n",
            "ITERATION_NO.: 1213 LOSS_Generator: 4.901395320892334 LOSS_Discriminator: 0.07995931804180145\n",
            "ITERATION_NO.: 1214 LOSS_Generator: 5.60891056060791 LOSS_Discriminator: 0.1309997042020162\n",
            "ITERATION_NO.: 1215 LOSS_Generator: 5.7367424964904785 LOSS_Discriminator: 0.3030533989270528\n",
            "ITERATION_NO.: 1216 LOSS_Generator: 5.627960205078125 LOSS_Discriminator: 0.07692236204942067\n",
            "ITERATION_NO.: 1217 LOSS_Generator: 5.024280548095703 LOSS_Discriminator: 0.03126456836859385\n",
            "ITERATION_NO.: 1218 LOSS_Generator: 5.07765007019043 LOSS_Discriminator: 0.06023505826791128\n",
            "ITERATION_NO.: 1219 LOSS_Generator: 4.8373026847839355 LOSS_Discriminator: 0.0442311962445577\n",
            "ITERATION_NO.: 1220 LOSS_Generator: 4.798187255859375 LOSS_Discriminator: 0.049833844105402626\n",
            "ITERATION_NO.: 1221 LOSS_Generator: 4.966142654418945 LOSS_Discriminator: 0.10198551416397095\n",
            "ITERATION_NO.: 1222 LOSS_Generator: 5.129483222961426 LOSS_Discriminator: 0.07621859510739644\n",
            "ITERATION_NO.: 1223 LOSS_Generator: 5.173646926879883 LOSS_Discriminator: 0.05752528210481008\n",
            "ITERATION_NO.: 1224 LOSS_Generator: 5.510099411010742 LOSS_Discriminator: 0.04647026459376017\n",
            "ITERATION_NO.: 1225 LOSS_Generator: 4.964580535888672 LOSS_Discriminator: 0.0803879698117574\n",
            "ITERATION_NO.: 1226 LOSS_Generator: 4.610525608062744 LOSS_Discriminator: 0.13997963070869446\n",
            "ITERATION_NO.: 1227 LOSS_Generator: 4.868803024291992 LOSS_Discriminator: 0.06514147917429607\n",
            "ITERATION_NO.: 1228 LOSS_Generator: 5.260682106018066 LOSS_Discriminator: 0.04655983050664266\n",
            "ITERATION_NO.: 1229 LOSS_Generator: 5.203329086303711 LOSS_Discriminator: 0.07329097886880238\n",
            "ITERATION_NO.: 1230 LOSS_Generator: 5.744564533233643 LOSS_Discriminator: 0.054350703954696655\n",
            "ITERATION_NO.: 1231 LOSS_Generator: 5.228395462036133 LOSS_Discriminator: 0.0775631566842397\n",
            "ITERATION_NO.: 1232 LOSS_Generator: 4.825514793395996 LOSS_Discriminator: 0.02681621660788854\n",
            "ITERATION_NO.: 1233 LOSS_Generator: 5.037855625152588 LOSS_Discriminator: 0.02735660473505656\n",
            "ITERATION_NO.: 1234 LOSS_Generator: 4.921177387237549 LOSS_Discriminator: 0.041179085771242775\n",
            "ITERATION_NO.: 1235 LOSS_Generator: 4.843742370605469 LOSS_Discriminator: 0.15936234593391418\n",
            "ITERATION_NO.: 1236 LOSS_Generator: 4.785036087036133 LOSS_Discriminator: 0.09455060958862305\n",
            "ITERATION_NO.: 1237 LOSS_Generator: 4.77491569519043 LOSS_Discriminator: 0.04629392425219218\n",
            "ITERATION_NO.: 1238 LOSS_Generator: 4.494712829589844 LOSS_Discriminator: 0.04128255695104599\n",
            "ITERATION_NO.: 1239 LOSS_Generator: 4.790430068969727 LOSS_Discriminator: 0.25329991181691486\n",
            "ITERATION_NO.: 1240 LOSS_Generator: 4.549751281738281 LOSS_Discriminator: 0.11047487457593282\n",
            "ITERATION_NO.: 1241 LOSS_Generator: 4.792874336242676 LOSS_Discriminator: 0.2330646514892578\n",
            "ITERATION_NO.: 1242 LOSS_Generator: 4.367822647094727 LOSS_Discriminator: 0.11376883586247762\n",
            "ITERATION_NO.: 1243 LOSS_Generator: 4.245356559753418 LOSS_Discriminator: 0.21436134974161783\n",
            "ITERATION_NO.: 1244 LOSS_Generator: 4.3041672706604 LOSS_Discriminator: 0.08381040891011556\n",
            "ITERATION_NO.: 1245 LOSS_Generator: 4.380682468414307 LOSS_Discriminator: 0.04476441442966461\n",
            "ITERATION_NO.: 1246 LOSS_Generator: 5.4266204833984375 LOSS_Discriminator: 0.014900889247655869\n",
            "ITERATION_NO.: 1247 LOSS_Generator: 5.54585075378418 LOSS_Discriminator: 0.1321665644645691\n",
            "ITERATION_NO.: 1248 LOSS_Generator: 4.929245471954346 LOSS_Discriminator: 0.22124900420506796\n",
            "ITERATION_NO.: 1249 LOSS_Generator: 3.970219612121582 LOSS_Discriminator: 0.2779168089230855\n",
            "ITERATION_NO.: 1250 LOSS_Generator: 4.008445739746094 LOSS_Discriminator: 0.2130057414372762\n",
            "ITERATION_NO.: 1251 LOSS_Generator: 3.9054338932037354 LOSS_Discriminator: 0.1430038114388784\n",
            "ITERATION_NO.: 1252 LOSS_Generator: 5.731045246124268 LOSS_Discriminator: 0.019244571526845295\n",
            "ITERATION_NO.: 1253 LOSS_Generator: 5.879189491271973 LOSS_Discriminator: 0.1160224974155426\n",
            "ITERATION_NO.: 1254 LOSS_Generator: 5.969136714935303 LOSS_Discriminator: 0.07104140520095825\n",
            "ITERATION_NO.: 1255 LOSS_Generator: 5.975613117218018 LOSS_Discriminator: 0.10992323358853658\n",
            "ITERATION_NO.: 1256 LOSS_Generator: 5.486598014831543 LOSS_Discriminator: 0.039794752995173134\n",
            "ITERATION_NO.: 1257 LOSS_Generator: 5.212621688842773 LOSS_Discriminator: 0.017750494182109833\n",
            "ITERATION_NO.: 1258 LOSS_Generator: 4.933294296264648 LOSS_Discriminator: 0.056946213046709694\n",
            "ITERATION_NO.: 1259 LOSS_Generator: 5.009302139282227 LOSS_Discriminator: 0.058507040143013\n",
            "ITERATION_NO.: 1260 LOSS_Generator: 5.328794002532959 LOSS_Discriminator: 0.019796257217725117\n",
            "ITERATION_NO.: 1261 LOSS_Generator: 5.235937118530273 LOSS_Discriminator: 0.10098627209663391\n",
            "ITERATION_NO.: 1262 LOSS_Generator: 5.072206020355225 LOSS_Discriminator: 0.09524892767270406\n",
            "ITERATION_NO.: 1263 LOSS_Generator: 4.359645843505859 LOSS_Discriminator: 0.08614758650461833\n",
            "ITERATION_NO.: 1264 LOSS_Generator: 4.776645660400391 LOSS_Discriminator: 0.040679206450780235\n",
            "ITERATION_NO.: 1265 LOSS_Generator: 4.77679967880249 LOSS_Discriminator: 0.03764526297648748\n",
            "ITERATION_NO.: 1266 LOSS_Generator: 4.8578996658325195 LOSS_Discriminator: 0.08481725056966145\n",
            "ITERATION_NO.: 1267 LOSS_Generator: 4.533727645874023 LOSS_Discriminator: 0.016460494448741276\n",
            "ITERATION_NO.: 1268 LOSS_Generator: 4.823712348937988 LOSS_Discriminator: 0.025788351893424988\n",
            "ITERATION_NO.: 1269 LOSS_Generator: 5.0579657554626465 LOSS_Discriminator: 0.05868953466415405\n",
            "ITERATION_NO.: 1270 LOSS_Generator: 4.719973564147949 LOSS_Discriminator: 0.3065315882364909\n",
            "ITERATION_NO.: 1271 LOSS_Generator: 3.995234727859497 LOSS_Discriminator: 0.16534517208735147\n",
            "ITERATION_NO.: 1272 LOSS_Generator: 3.7986316680908203 LOSS_Discriminator: 0.1788731813430786\n",
            "ITERATION_NO.: 1273 LOSS_Generator: 3.2058205604553223 LOSS_Discriminator: 0.17919949690500894\n",
            "ITERATION_NO.: 1274 LOSS_Generator: 4.124222755432129 LOSS_Discriminator: 0.12495070695877075\n",
            "ITERATION_NO.: 1275 LOSS_Generator: 5.12479305267334 LOSS_Discriminator: 0.05048775672912598\n",
            "ITERATION_NO.: 1276 LOSS_Generator: 5.8068437576293945 LOSS_Discriminator: 0.3635869820912679\n",
            "ITERATION_NO.: 1277 LOSS_Generator: 5.413144588470459 LOSS_Discriminator: 0.46415483951568604\n",
            "ITERATION_NO.: 1278 LOSS_Generator: 3.9529836177825928 LOSS_Discriminator: 0.07290727893511455\n",
            "ITERATION_NO.: 1279 LOSS_Generator: 3.8271520137786865 LOSS_Discriminator: 0.07941348354021709\n",
            "ITERATION_NO.: 1280 LOSS_Generator: 4.04032564163208 LOSS_Discriminator: 0.20077002048492432\n",
            "ITERATION_NO.: 1281 LOSS_Generator: 4.418900966644287 LOSS_Discriminator: 0.1508237620194753\n",
            "ITERATION_NO.: 1282 LOSS_Generator: 4.760718822479248 LOSS_Discriminator: 0.26712791124979657\n",
            "ITERATION_NO.: 1283 LOSS_Generator: 3.968587636947632 LOSS_Discriminator: 0.16348019242286682\n",
            "ITERATION_NO.: 1284 LOSS_Generator: 4.572116851806641 LOSS_Discriminator: 0.14046162366867065\n",
            "ITERATION_NO.: 1285 LOSS_Generator: 3.946228504180908 LOSS_Discriminator: 0.10113195578257243\n",
            "ITERATION_NO.: 1286 LOSS_Generator: 4.037322044372559 LOSS_Discriminator: 0.11864868799845378\n",
            "ITERATION_NO.: 1287 LOSS_Generator: 4.79673957824707 LOSS_Discriminator: 0.08567591508229573\n",
            "ITERATION_NO.: 1288 LOSS_Generator: 4.82539176940918 LOSS_Discriminator: 0.1189972460269928\n",
            "ITERATION_NO.: 1289 LOSS_Generator: 5.160006523132324 LOSS_Discriminator: 0.027067703505357105\n",
            "ITERATION_NO.: 1290 LOSS_Generator: 5.977055549621582 LOSS_Discriminator: 0.013494521379470825\n",
            "ITERATION_NO.: 1291 LOSS_Generator: 5.7089433670043945 LOSS_Discriminator: 0.19264141718546549\n",
            "ITERATION_NO.: 1292 LOSS_Generator: 6.044580936431885 LOSS_Discriminator: 0.00822059934337934\n",
            "ITERATION_NO.: 1293 LOSS_Generator: 5.99177885055542 LOSS_Discriminator: 0.17269708712895712\n",
            "ITERATION_NO.: 1294 LOSS_Generator: 5.962429523468018 LOSS_Discriminator: 0.08598752816518147\n",
            "ITERATION_NO.: 1295 LOSS_Generator: 5.276808738708496 LOSS_Discriminator: 0.01004138340552648\n",
            "ITERATION_NO.: 1296 LOSS_Generator: 5.244624137878418 LOSS_Discriminator: 0.11545444528261821\n",
            "ITERATION_NO.: 1297 LOSS_Generator: 5.004655361175537 LOSS_Discriminator: 0.07126936316490173\n",
            "ITERATION_NO.: 1298 LOSS_Generator: 5.142992973327637 LOSS_Discriminator: 0.11965230107307434\n",
            "ITERATION_NO.: 1299 LOSS_Generator: 4.651697158813477 LOSS_Discriminator: 0.020874902606010437\n",
            "ITERATION_NO.: 1300 LOSS_Generator: 5.034363269805908 LOSS_Discriminator: 0.03981993099053701\n",
            "ITERATION_NO.: 1301 LOSS_Generator: 4.73845100402832 LOSS_Discriminator: 0.15568300088246664\n",
            "ITERATION_NO.: 1302 LOSS_Generator: 4.742163181304932 LOSS_Discriminator: 0.12012998263041179\n",
            "ITERATION_NO.: 1303 LOSS_Generator: 4.408384799957275 LOSS_Discriminator: 0.23219913244247437\n",
            "ITERATION_NO.: 1304 LOSS_Generator: 4.703624725341797 LOSS_Discriminator: 0.15655644734700522\n",
            "ITERATION_NO.: 1305 LOSS_Generator: 4.7794718742370605 LOSS_Discriminator: 0.03748282790184021\n",
            "ITERATION_NO.: 1306 LOSS_Generator: 4.884621620178223 LOSS_Discriminator: 0.019994025429089863\n",
            "ITERATION_NO.: 1307 LOSS_Generator: 5.2448272705078125 LOSS_Discriminator: 0.05882323284943899\n",
            "ITERATION_NO.: 1308 LOSS_Generator: 5.064085483551025 LOSS_Discriminator: 0.04526670773824056\n",
            "ITERATION_NO.: 1309 LOSS_Generator: 4.731339454650879 LOSS_Discriminator: 0.23703457911809286\n",
            "ITERATION_NO.: 1310 LOSS_Generator: 4.4412407875061035 LOSS_Discriminator: 0.1343118151028951\n",
            "ITERATION_NO.: 1311 LOSS_Generator: 3.7568857669830322 LOSS_Discriminator: 0.12460087736447652\n",
            "ITERATION_NO.: 1312 LOSS_Generator: 3.5189156532287598 LOSS_Discriminator: 0.27152403195699054\n",
            "ITERATION_NO.: 1313 LOSS_Generator: 4.934660911560059 LOSS_Discriminator: 0.05835044880708059\n",
            "ITERATION_NO.: 1314 LOSS_Generator: 5.641297340393066 LOSS_Discriminator: 0.07213619351387024\n",
            "ITERATION_NO.: 1315 LOSS_Generator: 6.388382434844971 LOSS_Discriminator: 0.10531734426816304\n",
            "ITERATION_NO.: 1316 LOSS_Generator: 6.128149032592773 LOSS_Discriminator: 0.1304709017276764\n",
            "ITERATION_NO.: 1317 LOSS_Generator: 5.812699317932129 LOSS_Discriminator: 0.23952247699101767\n",
            "ITERATION_NO.: 1318 LOSS_Generator: 5.0108184814453125 LOSS_Discriminator: 0.05335398515065511\n",
            "ITERATION_NO.: 1319 LOSS_Generator: 4.348553657531738 LOSS_Discriminator: 0.01868191361427307\n",
            "ITERATION_NO.: 1320 LOSS_Generator: 4.006808757781982 LOSS_Discriminator: 0.03140482554833094\n",
            "ITERATION_NO.: 1321 LOSS_Generator: 4.214928150177002 LOSS_Discriminator: 0.041258424520492554\n",
            "ITERATION_NO.: 1322 LOSS_Generator: 4.358943939208984 LOSS_Discriminator: 0.06053958833217621\n",
            "ITERATION_NO.: 1323 LOSS_Generator: 5.132569313049316 LOSS_Discriminator: 0.03330594797929128\n",
            "ITERATION_NO.: 1324 LOSS_Generator: 5.464364051818848 LOSS_Discriminator: 0.0781380186478297\n",
            "ITERATION_NO.: 1325 LOSS_Generator: 5.623453140258789 LOSS_Discriminator: 0.049833099047342934\n",
            "ITERATION_NO.: 1326 LOSS_Generator: 6.021847724914551 LOSS_Discriminator: 0.07262970010439555\n",
            "ITERATION_NO.: 1327 LOSS_Generator: 5.822229385375977 LOSS_Discriminator: 0.043927917877833046\n",
            "ITERATION_NO.: 1328 LOSS_Generator: 5.650339126586914 LOSS_Discriminator: 0.25827765464782715\n",
            "ITERATION_NO.: 1329 LOSS_Generator: 5.3212995529174805 LOSS_Discriminator: 0.018001238505045574\n",
            "ITERATION_NO.: 1330 LOSS_Generator: 5.030486583709717 LOSS_Discriminator: 0.1090494692325592\n",
            "ITERATION_NO.: 1331 LOSS_Generator: 4.739149570465088 LOSS_Discriminator: 0.07303280631701152\n",
            "ITERATION_NO.: 1332 LOSS_Generator: 4.1247968673706055 LOSS_Discriminator: 0.05148745079835256\n",
            "ITERATION_NO.: 1333 LOSS_Generator: 4.064242362976074 LOSS_Discriminator: 0.24559923013051352\n",
            "ITERATION_NO.: 1334 LOSS_Generator: 3.72872257232666 LOSS_Discriminator: 0.12448217471440633\n",
            "ITERATION_NO.: 1335 LOSS_Generator: 4.711055755615234 LOSS_Discriminator: 0.08957610527674358\n",
            "ITERATION_NO.: 1336 LOSS_Generator: 5.148128509521484 LOSS_Discriminator: 0.1392042636871338\n",
            "ITERATION_NO.: 1337 LOSS_Generator: 5.586794853210449 LOSS_Discriminator: 0.025933509071667988\n",
            "ITERATION_NO.: 1338 LOSS_Generator: 5.256786823272705 LOSS_Discriminator: 0.020838876565297444\n",
            "ITERATION_NO.: 1339 LOSS_Generator: 5.389030933380127 LOSS_Discriminator: 0.10201658805211385\n",
            "ITERATION_NO.: 1340 LOSS_Generator: 4.889114856719971 LOSS_Discriminator: 0.07566686471303304\n",
            "ITERATION_NO.: 1341 LOSS_Generator: 4.709615707397461 LOSS_Discriminator: 0.057688762744267784\n",
            "ITERATION_NO.: 1342 LOSS_Generator: 5.113332271575928 LOSS_Discriminator: 0.09062401453653972\n",
            "ITERATION_NO.: 1343 LOSS_Generator: 4.337826728820801 LOSS_Discriminator: 0.04634935657183329\n",
            "ITERATION_NO.: 1344 LOSS_Generator: 4.148892879486084 LOSS_Discriminator: 0.19476046164830527\n",
            "ITERATION_NO.: 1345 LOSS_Generator: 4.539323806762695 LOSS_Discriminator: 0.036512404680252075\n",
            "ITERATION_NO.: 1346 LOSS_Generator: 4.838220596313477 LOSS_Discriminator: 0.07492077847321828\n",
            "ITERATION_NO.: 1347 LOSS_Generator: 4.69864559173584 LOSS_Discriminator: 0.12069082260131836\n",
            "ITERATION_NO.: 1348 LOSS_Generator: 4.000034332275391 LOSS_Discriminator: 0.16715981562932333\n",
            "ITERATION_NO.: 1349 LOSS_Generator: 3.981170892715454 LOSS_Discriminator: 0.05823866526285807\n",
            "ITERATION_NO.: 1350 LOSS_Generator: 5.148159027099609 LOSS_Discriminator: 0.072477325797081\n",
            "ITERATION_NO.: 1351 LOSS_Generator: 5.988791465759277 LOSS_Discriminator: 0.07218785583972931\n",
            "ITERATION_NO.: 1352 LOSS_Generator: 6.10067081451416 LOSS_Discriminator: 0.11597779393196106\n",
            "ITERATION_NO.: 1353 LOSS_Generator: 6.095146179199219 LOSS_Discriminator: 0.13994010289510092\n",
            "ITERATION_NO.: 1354 LOSS_Generator: 5.6729888916015625 LOSS_Discriminator: 0.11415220300356548\n",
            "ITERATION_NO.: 1355 LOSS_Generator: 5.162628173828125 LOSS_Discriminator: 0.07602985699971516\n",
            "ITERATION_NO.: 1356 LOSS_Generator: 4.899721145629883 LOSS_Discriminator: 0.133771280447642\n",
            "ITERATION_NO.: 1357 LOSS_Generator: 4.665148735046387 LOSS_Discriminator: 0.08038267989953359\n",
            "ITERATION_NO.: 1358 LOSS_Generator: 3.7685747146606445 LOSS_Discriminator: 0.11484573284784953\n",
            "ITERATION_NO.: 1359 LOSS_Generator: 3.7573139667510986 LOSS_Discriminator: 0.09497206409772237\n",
            "ITERATION_NO.: 1360 LOSS_Generator: 4.646147727966309 LOSS_Discriminator: 0.10141436258951823\n",
            "ITERATION_NO.: 1361 LOSS_Generator: 5.016295433044434 LOSS_Discriminator: 0.027536516388257343\n",
            "ITERATION_NO.: 1362 LOSS_Generator: 5.321502685546875 LOSS_Discriminator: 0.10426714022954305\n",
            "ITERATION_NO.: 1363 LOSS_Generator: 4.94204044342041 LOSS_Discriminator: 0.3026961882909139\n",
            "ITERATION_NO.: 1364 LOSS_Generator: 4.453944206237793 LOSS_Discriminator: 0.0787210464477539\n",
            "ITERATION_NO.: 1365 LOSS_Generator: 3.9933829307556152 LOSS_Discriminator: 0.05176714062690735\n",
            "ITERATION_NO.: 1366 LOSS_Generator: 4.073650360107422 LOSS_Discriminator: 0.0631095419327418\n",
            "ITERATION_NO.: 1367 LOSS_Generator: 4.362100601196289 LOSS_Discriminator: 0.06683589518070221\n",
            "ITERATION_NO.: 1368 LOSS_Generator: 5.0824127197265625 LOSS_Discriminator: 0.02635858456293742\n",
            "ITERATION_NO.: 1369 LOSS_Generator: 5.597922325134277 LOSS_Discriminator: 0.01903539150953293\n",
            "ITERATION_NO.: 1370 LOSS_Generator: 5.520145416259766 LOSS_Discriminator: 0.09193900227546692\n",
            "ITERATION_NO.: 1371 LOSS_Generator: 6.080421447753906 LOSS_Discriminator: 0.015615060925483704\n",
            "ITERATION_NO.: 1372 LOSS_Generator: 5.424315452575684 LOSS_Discriminator: 0.08514981468518575\n",
            "ITERATION_NO.: 1373 LOSS_Generator: 5.490080833435059 LOSS_Discriminator: 0.12357600529988606\n",
            "ITERATION_NO.: 1374 LOSS_Generator: 4.878252983093262 LOSS_Discriminator: 0.10270616412162781\n",
            "ITERATION_NO.: 1375 LOSS_Generator: 4.41788387298584 LOSS_Discriminator: 0.040196272234121956\n",
            "ITERATION_NO.: 1376 LOSS_Generator: 3.6137447357177734 LOSS_Discriminator: 0.2312692403793335\n",
            "ITERATION_NO.: 1377 LOSS_Generator: 3.822768449783325 LOSS_Discriminator: 0.15363492568333945\n",
            "ITERATION_NO.: 1378 LOSS_Generator: 4.883781909942627 LOSS_Discriminator: 0.10753226280212402\n",
            "ITERATION_NO.: 1379 LOSS_Generator: 5.813897132873535 LOSS_Discriminator: 0.020533787707487743\n",
            "ITERATION_NO.: 1380 LOSS_Generator: 6.316444396972656 LOSS_Discriminator: 0.039961708088715873\n",
            "ITERATION_NO.: 1381 LOSS_Generator: 7.16069221496582 LOSS_Discriminator: 0.0058702776829401655\n",
            "ITERATION_NO.: 1382 LOSS_Generator: 6.8270463943481445 LOSS_Discriminator: 0.31363944212595624\n",
            "ITERATION_NO.: 1383 LOSS_Generator: 6.611048698425293 LOSS_Discriminator: 0.06684868037700653\n",
            "ITERATION_NO.: 1384 LOSS_Generator: 5.838572978973389 LOSS_Discriminator: 0.037334710359573364\n",
            "ITERATION_NO.: 1385 LOSS_Generator: 5.261196613311768 LOSS_Discriminator: 0.03442404419183731\n",
            "ITERATION_NO.: 1386 LOSS_Generator: 5.002592086791992 LOSS_Discriminator: 0.11496172348658244\n",
            "ITERATION_NO.: 1387 LOSS_Generator: 3.8661913871765137 LOSS_Discriminator: 0.10726553201675415\n",
            "ITERATION_NO.: 1388 LOSS_Generator: 4.0360822677612305 LOSS_Discriminator: 0.09622853994369507\n",
            "ITERATION_NO.: 1389 LOSS_Generator: 4.403340816497803 LOSS_Discriminator: 0.11546099185943604\n",
            "ITERATION_NO.: 1390 LOSS_Generator: 5.415316581726074 LOSS_Discriminator: 0.0413247545560201\n",
            "ITERATION_NO.: 1391 LOSS_Generator: 5.7696332931518555 LOSS_Discriminator: 0.045297076304753624\n",
            "ITERATION_NO.: 1392 LOSS_Generator: 5.60285758972168 LOSS_Discriminator: 0.17819837729136148\n",
            "ITERATION_NO.: 1393 LOSS_Generator: 5.427285671234131 LOSS_Discriminator: 0.09007731080055237\n",
            "ITERATION_NO.: 1394 LOSS_Generator: 4.928334712982178 LOSS_Discriminator: 0.028405462702115376\n",
            "ITERATION_NO.: 1395 LOSS_Generator: 4.875375747680664 LOSS_Discriminator: 0.06652013957500458\n",
            "ITERATION_NO.: 1396 LOSS_Generator: 4.378292083740234 LOSS_Discriminator: 0.11886116862297058\n",
            "ITERATION_NO.: 1397 LOSS_Generator: 4.644554615020752 LOSS_Discriminator: 0.07312069833278656\n",
            "ITERATION_NO.: 1398 LOSS_Generator: 4.490530967712402 LOSS_Discriminator: 0.05259298284848531\n",
            "ITERATION_NO.: 1399 LOSS_Generator: 4.509230613708496 LOSS_Discriminator: 0.17122300465901694\n",
            "ITERATION_NO.: 1400 LOSS_Generator: 4.78773832321167 LOSS_Discriminator: 0.12349515159924825\n",
            "ITERATION_NO.: 1401 LOSS_Generator: 4.287256240844727 LOSS_Discriminator: 0.09591833750406902\n",
            "ITERATION_NO.: 1402 LOSS_Generator: 4.417930603027344 LOSS_Discriminator: 0.09632674853006999\n",
            "ITERATION_NO.: 1403 LOSS_Generator: 4.022268295288086 LOSS_Discriminator: 0.04103071987628937\n",
            "ITERATION_NO.: 1404 LOSS_Generator: 3.4439070224761963 LOSS_Discriminator: 0.23472732305526733\n",
            "ITERATION_NO.: 1405 LOSS_Generator: 3.6060054302215576 LOSS_Discriminator: 0.12474408745765686\n",
            "ITERATION_NO.: 1406 LOSS_Generator: 4.223479270935059 LOSS_Discriminator: 0.09759563207626343\n",
            "ITERATION_NO.: 1407 LOSS_Generator: 5.069967269897461 LOSS_Discriminator: 0.08986210823059082\n",
            "ITERATION_NO.: 1408 LOSS_Generator: 4.65842342376709 LOSS_Discriminator: 0.23287526766459146\n",
            "ITERATION_NO.: 1409 LOSS_Generator: 4.624138832092285 LOSS_Discriminator: 0.05659988522529602\n",
            "ITERATION_NO.: 1410 LOSS_Generator: 4.409533977508545 LOSS_Discriminator: 0.12226625283559163\n",
            "ITERATION_NO.: 1411 LOSS_Generator: 4.597721099853516 LOSS_Discriminator: 0.11155766248703003\n",
            "ITERATION_NO.: 1412 LOSS_Generator: 5.113018989562988 LOSS_Discriminator: 0.20718506971995035\n",
            "ITERATION_NO.: 1413 LOSS_Generator: 5.21870231628418 LOSS_Discriminator: 0.04989716410636902\n",
            "ITERATION_NO.: 1414 LOSS_Generator: 5.271064758300781 LOSS_Discriminator: 0.08995211124420166\n",
            "ITERATION_NO.: 1415 LOSS_Generator: 4.762828350067139 LOSS_Discriminator: 0.15228508909543356\n",
            "ITERATION_NO.: 1416 LOSS_Generator: 4.63158655166626 LOSS_Discriminator: 0.04054907957712809\n",
            "ITERATION_NO.: 1417 LOSS_Generator: 4.625982284545898 LOSS_Discriminator: 0.029113357265790302\n",
            "ITERATION_NO.: 1418 LOSS_Generator: 4.552085876464844 LOSS_Discriminator: 0.09522481759389241\n",
            "ITERATION_NO.: 1419 LOSS_Generator: 4.975685119628906 LOSS_Discriminator: 0.036408464113871254\n",
            "ITERATION_NO.: 1420 LOSS_Generator: 4.483817100524902 LOSS_Discriminator: 0.1252173682053884\n",
            "ITERATION_NO.: 1421 LOSS_Generator: 4.699748516082764 LOSS_Discriminator: 0.024192775289217632\n",
            "ITERATION_NO.: 1422 LOSS_Generator: 5.416853427886963 LOSS_Discriminator: 0.02596668154001236\n",
            "ITERATION_NO.: 1423 LOSS_Generator: 5.125772953033447 LOSS_Discriminator: 0.16757245858510336\n",
            "ITERATION_NO.: 1424 LOSS_Generator: 5.374387741088867 LOSS_Discriminator: 0.025277671714623768\n",
            "ITERATION_NO.: 1425 LOSS_Generator: 5.020640850067139 LOSS_Discriminator: 0.040808344880739846\n",
            "ITERATION_NO.: 1426 LOSS_Generator: 5.0818986892700195 LOSS_Discriminator: 0.20301326115926108\n",
            "ITERATION_NO.: 1427 LOSS_Generator: 5.29673957824707 LOSS_Discriminator: 0.1405146320660909\n",
            "ITERATION_NO.: 1428 LOSS_Generator: 5.047589302062988 LOSS_Discriminator: 0.07102277874946594\n",
            "ITERATION_NO.: 1429 LOSS_Generator: 4.515610694885254 LOSS_Discriminator: 0.0626092404127121\n",
            "ITERATION_NO.: 1430 LOSS_Generator: 4.539058685302734 LOSS_Discriminator: 0.11031333605448405\n",
            "ITERATION_NO.: 1431 LOSS_Generator: 5.373128890991211 LOSS_Discriminator: 0.04688547054926554\n",
            "ITERATION_NO.: 1432 LOSS_Generator: 5.250256538391113 LOSS_Discriminator: 0.023640563090642292\n",
            "ITERATION_NO.: 1433 LOSS_Generator: 4.85065221786499 LOSS_Discriminator: 0.11948433518409729\n",
            "ITERATION_NO.: 1434 LOSS_Generator: 4.882180213928223 LOSS_Discriminator: 0.06679229935010274\n",
            "ITERATION_NO.: 1435 LOSS_Generator: 4.159786701202393 LOSS_Discriminator: 0.1683314045270284\n",
            "ITERATION_NO.: 1436 LOSS_Generator: 3.934147357940674 LOSS_Discriminator: 0.11794525384902954\n",
            "ITERATION_NO.: 1437 LOSS_Generator: 3.4497010707855225 LOSS_Discriminator: 0.2030694286028544\n",
            "ITERATION_NO.: 1438 LOSS_Generator: 4.212586402893066 LOSS_Discriminator: 0.07288403312365214\n",
            "ITERATION_NO.: 1439 LOSS_Generator: 4.900598049163818 LOSS_Discriminator: 0.03299792359272639\n",
            "ITERATION_NO.: 1440 LOSS_Generator: 5.274123668670654 LOSS_Discriminator: 0.15774977207183838\n",
            "ITERATION_NO.: 1441 LOSS_Generator: 5.371363639831543 LOSS_Discriminator: 0.010105195765693983\n",
            "ITERATION_NO.: 1442 LOSS_Generator: 5.756277084350586 LOSS_Discriminator: 0.07174659272034963\n",
            "ITERATION_NO.: 1443 LOSS_Generator: 5.698965549468994 LOSS_Discriminator: 0.07404798765977223\n",
            "ITERATION_NO.: 1444 LOSS_Generator: 5.196822166442871 LOSS_Discriminator: 0.1741372545560201\n",
            "ITERATION_NO.: 1445 LOSS_Generator: 3.9368720054626465 LOSS_Discriminator: 0.20948628584543863\n",
            "ITERATION_NO.: 1446 LOSS_Generator: 3.8704922199249268 LOSS_Discriminator: 0.1295386552810669\n",
            "ITERATION_NO.: 1447 LOSS_Generator: 3.5344066619873047 LOSS_Discriminator: 0.19154977798461914\n",
            "ITERATION_NO.: 1448 LOSS_Generator: 4.33905029296875 LOSS_Discriminator: 0.13864363233248392\n",
            "ITERATION_NO.: 1449 LOSS_Generator: 4.838592529296875 LOSS_Discriminator: 0.08148010571797688\n",
            "ITERATION_NO.: 1450 LOSS_Generator: 5.025356292724609 LOSS_Discriminator: 0.15043401718139648\n",
            "ITERATION_NO.: 1451 LOSS_Generator: 4.561368465423584 LOSS_Discriminator: 0.10064033667246501\n",
            "ITERATION_NO.: 1452 LOSS_Generator: 4.832023620605469 LOSS_Discriminator: 0.04930723706881205\n",
            "ITERATION_NO.: 1453 LOSS_Generator: 4.678977012634277 LOSS_Discriminator: 0.056843921542167664\n",
            "ITERATION_NO.: 1454 LOSS_Generator: 4.7408246994018555 LOSS_Discriminator: 0.2843019366264343\n",
            "ITERATION_NO.: 1455 LOSS_Generator: 4.1824235916137695 LOSS_Discriminator: 0.09429299831390381\n",
            "ITERATION_NO.: 1456 LOSS_Generator: 3.812321662902832 LOSS_Discriminator: 0.10596370697021484\n",
            "ITERATION_NO.: 1457 LOSS_Generator: 4.384056568145752 LOSS_Discriminator: 0.0966613491376241\n",
            "ITERATION_NO.: 1458 LOSS_Generator: 5.116398811340332 LOSS_Discriminator: 0.040511821707089744\n",
            "ITERATION_NO.: 1459 LOSS_Generator: 5.88512659072876 LOSS_Discriminator: 0.07601323227087657\n",
            "ITERATION_NO.: 1460 LOSS_Generator: 5.371799945831299 LOSS_Discriminator: 0.11480559905370076\n",
            "ITERATION_NO.: 1461 LOSS_Generator: 5.5204691886901855 LOSS_Discriminator: 0.005614102507630984\n",
            "ITERATION_NO.: 1462 LOSS_Generator: 5.625507354736328 LOSS_Discriminator: 0.06999725600083669\n",
            "ITERATION_NO.: 1463 LOSS_Generator: 5.110949993133545 LOSS_Discriminator: 0.0811553696791331\n",
            "ITERATION_NO.: 1464 LOSS_Generator: 4.221987724304199 LOSS_Discriminator: 0.142248272895813\n",
            "ITERATION_NO.: 1465 LOSS_Generator: 4.2451171875 LOSS_Discriminator: 0.027158523599306744\n",
            "ITERATION_NO.: 1466 LOSS_Generator: 4.346855163574219 LOSS_Discriminator: 0.04108326385418574\n",
            "ITERATION_NO.: 1467 LOSS_Generator: 5.168406009674072 LOSS_Discriminator: 0.026873899002869923\n",
            "ITERATION_NO.: 1468 LOSS_Generator: 5.466662406921387 LOSS_Discriminator: 0.05222458144028982\n",
            "ITERATION_NO.: 1469 LOSS_Generator: 5.0838751792907715 LOSS_Discriminator: 0.34316933155059814\n",
            "ITERATION_NO.: 1470 LOSS_Generator: 4.744553565979004 LOSS_Discriminator: 0.03174223750829697\n",
            "ITERATION_NO.: 1471 LOSS_Generator: 4.743809700012207 LOSS_Discriminator: 0.10280425349871318\n",
            "ITERATION_NO.: 1472 LOSS_Generator: 4.819019317626953 LOSS_Discriminator: 0.03571022550264994\n",
            "ITERATION_NO.: 1473 LOSS_Generator: 5.138535499572754 LOSS_Discriminator: 0.1351373791694641\n",
            "ITERATION_NO.: 1474 LOSS_Generator: 5.063018798828125 LOSS_Discriminator: 0.16119499007860819\n",
            "ITERATION_NO.: 1475 LOSS_Generator: 3.7322921752929688 LOSS_Discriminator: 0.2214850385983785\n",
            "ITERATION_NO.: 1476 LOSS_Generator: 3.3094635009765625 LOSS_Discriminator: 0.0935799777507782\n",
            "ITERATION_NO.: 1477 LOSS_Generator: 4.032004356384277 LOSS_Discriminator: 0.14697138468424478\n",
            "ITERATION_NO.: 1478 LOSS_Generator: 5.11773681640625 LOSS_Discriminator: 0.06868656476338704\n",
            "ITERATION_NO.: 1479 LOSS_Generator: 5.654935359954834 LOSS_Discriminator: 0.021866485476493835\n",
            "ITERATION_NO.: 1480 LOSS_Generator: 6.4740376472473145 LOSS_Discriminator: 0.01748691250880559\n",
            "ITERATION_NO.: 1481 LOSS_Generator: 6.246198654174805 LOSS_Discriminator: 0.1787953575452169\n",
            "ITERATION_NO.: 1482 LOSS_Generator: 5.672857284545898 LOSS_Discriminator: 0.10435984532038371\n",
            "ITERATION_NO.: 1483 LOSS_Generator: 4.601226329803467 LOSS_Discriminator: 0.15460661053657532\n",
            "ITERATION_NO.: 1484 LOSS_Generator: 3.450263261795044 LOSS_Discriminator: 0.06900455554326375\n",
            "ITERATION_NO.: 1485 LOSS_Generator: 3.8237452507019043 LOSS_Discriminator: 0.1918138066927592\n",
            "ITERATION_NO.: 1486 LOSS_Generator: 3.9923973083496094 LOSS_Discriminator: 0.06046496828397115\n",
            "ITERATION_NO.: 1487 LOSS_Generator: 5.593850135803223 LOSS_Discriminator: 0.030077427625656128\n",
            "ITERATION_NO.: 1488 LOSS_Generator: 6.2442121505737305 LOSS_Discriminator: 0.014381202558676401\n",
            "ITERATION_NO.: 1489 LOSS_Generator: 7.325448989868164 LOSS_Discriminator: 0.04375789066155752\n",
            "ITERATION_NO.: 1490 LOSS_Generator: 6.664997577667236 LOSS_Discriminator: 0.2855140765508016\n",
            "ITERATION_NO.: 1491 LOSS_Generator: 5.979161739349365 LOSS_Discriminator: 0.04078885167837143\n",
            "ITERATION_NO.: 1492 LOSS_Generator: 5.405367851257324 LOSS_Discriminator: 0.01239371175567309\n",
            "ITERATION_NO.: 1493 LOSS_Generator: 4.753867149353027 LOSS_Discriminator: 0.09236447016398112\n",
            "ITERATION_NO.: 1494 LOSS_Generator: 4.360325336456299 LOSS_Discriminator: 0.13633817434310913\n",
            "ITERATION_NO.: 1495 LOSS_Generator: 3.869966506958008 LOSS_Discriminator: 0.18895059823989868\n",
            "ITERATION_NO.: 1496 LOSS_Generator: 3.8515524864196777 LOSS_Discriminator: 0.06052579482396444\n",
            "ITERATION_NO.: 1497 LOSS_Generator: 4.500608444213867 LOSS_Discriminator: 0.13601704438527426\n",
            "ITERATION_NO.: 1498 LOSS_Generator: 4.955578327178955 LOSS_Discriminator: 0.04118633766969045\n",
            "ITERATION_NO.: 1499 LOSS_Generator: 6.058263301849365 LOSS_Discriminator: 0.21741004784901938\n",
            "ITERATION_NO.: 1500 LOSS_Generator: 5.844860076904297 LOSS_Discriminator: 0.09366169571876526\n",
            "ITERATION_NO.: 1501 LOSS_Generator: 5.549008369445801 LOSS_Discriminator: 0.10797557234764099\n",
            "ITERATION_NO.: 1502 LOSS_Generator: 5.904906272888184 LOSS_Discriminator: 0.08882550398508708\n",
            "ITERATION_NO.: 1503 LOSS_Generator: 4.852653503417969 LOSS_Discriminator: 0.12359873453776042\n",
            "ITERATION_NO.: 1504 LOSS_Generator: 4.588615417480469 LOSS_Discriminator: 0.10302112499872844\n",
            "ITERATION_NO.: 1505 LOSS_Generator: 3.669865608215332 LOSS_Discriminator: 0.1418675184249878\n",
            "ITERATION_NO.: 1506 LOSS_Generator: 3.940455198287964 LOSS_Discriminator: 0.1376283566157023\n",
            "ITERATION_NO.: 1507 LOSS_Generator: 5.118365287780762 LOSS_Discriminator: 0.0681300163269043\n",
            "ITERATION_NO.: 1508 LOSS_Generator: 6.110010147094727 LOSS_Discriminator: 0.01080154130856196\n",
            "ITERATION_NO.: 1509 LOSS_Generator: 6.4058942794799805 LOSS_Discriminator: 0.013371852536996206\n",
            "ITERATION_NO.: 1510 LOSS_Generator: 6.58034610748291 LOSS_Discriminator: 0.07470165689786275\n",
            "ITERATION_NO.: 1511 LOSS_Generator: 6.943053245544434 LOSS_Discriminator: 0.18126789728800455\n",
            "ITERATION_NO.: 1512 LOSS_Generator: 6.2810869216918945 LOSS_Discriminator: 0.1250138779481252\n",
            "ITERATION_NO.: 1513 LOSS_Generator: 5.978336811065674 LOSS_Discriminator: 0.1205855409304301\n",
            "ITERATION_NO.: 1514 LOSS_Generator: 5.291811943054199 LOSS_Discriminator: 0.228250781695048\n",
            "ITERATION_NO.: 1515 LOSS_Generator: 3.95955228805542 LOSS_Discriminator: 0.06617873907089233\n",
            "ITERATION_NO.: 1516 LOSS_Generator: 4.274604797363281 LOSS_Discriminator: 0.14591601490974426\n",
            "ITERATION_NO.: 1517 LOSS_Generator: 4.715761184692383 LOSS_Discriminator: 0.1637822985649109\n",
            "ITERATION_NO.: 1518 LOSS_Generator: 5.318506240844727 LOSS_Discriminator: 0.023835251728693645\n",
            "ITERATION_NO.: 1519 LOSS_Generator: 5.999787330627441 LOSS_Discriminator: 0.1796069343884786\n",
            "ITERATION_NO.: 1520 LOSS_Generator: 5.500741004943848 LOSS_Discriminator: 0.008184930930534998\n",
            "ITERATION_NO.: 1521 LOSS_Generator: 5.9905500411987305 LOSS_Discriminator: 0.015237698952356974\n",
            "ITERATION_NO.: 1522 LOSS_Generator: 5.94049072265625 LOSS_Discriminator: 0.04925873875617981\n",
            "ITERATION_NO.: 1523 LOSS_Generator: 6.077117919921875 LOSS_Discriminator: 0.1458312769730886\n",
            "ITERATION_NO.: 1524 LOSS_Generator: 6.159862518310547 LOSS_Discriminator: 0.02962799370288849\n",
            "ITERATION_NO.: 1525 LOSS_Generator: 4.787486553192139 LOSS_Discriminator: 0.34415388107299805\n",
            "ITERATION_NO.: 1526 LOSS_Generator: 4.556918621063232 LOSS_Discriminator: 0.048796420296033226\n",
            "ITERATION_NO.: 1527 LOSS_Generator: 4.948419094085693 LOSS_Discriminator: 0.08114925026893616\n",
            "ITERATION_NO.: 1528 LOSS_Generator: 4.960906028747559 LOSS_Discriminator: 0.21112521489461264\n",
            "ITERATION_NO.: 1529 LOSS_Generator: 4.625940322875977 LOSS_Discriminator: 0.09066482384999593\n",
            "ITERATION_NO.: 1530 LOSS_Generator: 4.693963527679443 LOSS_Discriminator: 0.13175400098164877\n",
            "ITERATION_NO.: 1531 LOSS_Generator: 4.422210693359375 LOSS_Discriminator: 0.19812866051991782\n",
            "ITERATION_NO.: 1532 LOSS_Generator: 4.4366607666015625 LOSS_Discriminator: 0.10845961173375447\n",
            "ITERATION_NO.: 1533 LOSS_Generator: 4.2868876457214355 LOSS_Discriminator: 0.10997389753659566\n",
            "ITERATION_NO.: 1534 LOSS_Generator: 4.691376209259033 LOSS_Discriminator: 0.0729529857635498\n",
            "ITERATION_NO.: 1535 LOSS_Generator: 5.0680437088012695 LOSS_Discriminator: 0.03475406269232432\n",
            "ITERATION_NO.: 1536 LOSS_Generator: 6.187963008880615 LOSS_Discriminator: 0.09062323967615764\n",
            "ITERATION_NO.: 1537 LOSS_Generator: 6.297401428222656 LOSS_Discriminator: 0.005684000129501025\n",
            "ITERATION_NO.: 1538 LOSS_Generator: 6.13482666015625 LOSS_Discriminator: 0.23123065630594888\n",
            "ITERATION_NO.: 1539 LOSS_Generator: 5.125282287597656 LOSS_Discriminator: 0.09050314625104268\n",
            "ITERATION_NO.: 1540 LOSS_Generator: 4.637768745422363 LOSS_Discriminator: 0.011154960840940475\n",
            "ITERATION_NO.: 1541 LOSS_Generator: 4.288074493408203 LOSS_Discriminator: 0.019310325384140015\n",
            "ITERATION_NO.: 1542 LOSS_Generator: 4.436713218688965 LOSS_Discriminator: 0.06507125993569692\n",
            "ITERATION_NO.: 1543 LOSS_Generator: 4.8321733474731445 LOSS_Discriminator: 0.03910283247629801\n",
            "ITERATION_NO.: 1544 LOSS_Generator: 4.686643600463867 LOSS_Discriminator: 0.051408360401789345\n",
            "ITERATION_NO.: 1545 LOSS_Generator: 4.497900009155273 LOSS_Discriminator: 0.18915084997812906\n",
            "ITERATION_NO.: 1546 LOSS_Generator: 4.548870086669922 LOSS_Discriminator: 0.1254178285598755\n",
            "ITERATION_NO.: 1547 LOSS_Generator: 4.457074165344238 LOSS_Discriminator: 0.022735513746738434\n",
            "ITERATION_NO.: 1548 LOSS_Generator: 4.3496928215026855 LOSS_Discriminator: 0.17410707473754883\n",
            "ITERATION_NO.: 1549 LOSS_Generator: 3.235417366027832 LOSS_Discriminator: 0.3104715943336487\n",
            "ITERATION_NO.: 1550 LOSS_Generator: 3.3596608638763428 LOSS_Discriminator: 0.09223944942156474\n",
            "ITERATION_NO.: 1551 LOSS_Generator: 4.857455730438232 LOSS_Discriminator: 0.20191613833109537\n",
            "ITERATION_NO.: 1552 LOSS_Generator: 5.634464740753174 LOSS_Discriminator: 0.12771078944206238\n",
            "ITERATION_NO.: 1553 LOSS_Generator: 6.1000518798828125 LOSS_Discriminator: 0.12359492977460225\n",
            "ITERATION_NO.: 1554 LOSS_Generator: 5.775539875030518 LOSS_Discriminator: 0.004111456063886483\n",
            "ITERATION_NO.: 1555 LOSS_Generator: 6.2597761154174805 LOSS_Discriminator: 0.11512569586435954\n",
            "ITERATION_NO.: 1556 LOSS_Generator: 5.532600402832031 LOSS_Discriminator: 0.2768528461456299\n",
            "ITERATION_NO.: 1557 LOSS_Generator: 5.111903190612793 LOSS_Discriminator: 0.00811749572555224\n",
            "ITERATION_NO.: 1558 LOSS_Generator: 4.460643291473389 LOSS_Discriminator: 0.13282271226247153\n",
            "ITERATION_NO.: 1559 LOSS_Generator: 4.0720624923706055 LOSS_Discriminator: 0.05317425727844238\n",
            "ITERATION_NO.: 1560 LOSS_Generator: 4.2265520095825195 LOSS_Discriminator: 0.07705740133921306\n",
            "ITERATION_NO.: 1561 LOSS_Generator: 4.791057586669922 LOSS_Discriminator: 0.13422833879788718\n",
            "ITERATION_NO.: 1562 LOSS_Generator: 4.651701927185059 LOSS_Discriminator: 0.3044522205988566\n",
            "ITERATION_NO.: 1563 LOSS_Generator: 4.649233818054199 LOSS_Discriminator: 0.08612629771232605\n",
            "ITERATION_NO.: 1564 LOSS_Generator: 4.818826198577881 LOSS_Discriminator: 0.028063689668973286\n",
            "ITERATION_NO.: 1565 LOSS_Generator: 4.695666790008545 LOSS_Discriminator: 0.018947763989369076\n",
            "ITERATION_NO.: 1566 LOSS_Generator: 5.276876449584961 LOSS_Discriminator: 0.019262269139289856\n",
            "ITERATION_NO.: 1567 LOSS_Generator: 5.523634910583496 LOSS_Discriminator: 0.008730145171284676\n",
            "ITERATION_NO.: 1568 LOSS_Generator: 6.066834449768066 LOSS_Discriminator: 0.0418081929286321\n",
            "ITERATION_NO.: 1569 LOSS_Generator: 5.999987602233887 LOSS_Discriminator: 0.005883026868104935\n",
            "ITERATION_NO.: 1570 LOSS_Generator: 5.773216247558594 LOSS_Discriminator: 0.12503714362780252\n",
            "ITERATION_NO.: 1571 LOSS_Generator: 5.121542453765869 LOSS_Discriminator: 0.01607299968600273\n",
            "ITERATION_NO.: 1572 LOSS_Generator: 4.438634872436523 LOSS_Discriminator: 0.17222936948140463\n",
            "ITERATION_NO.: 1573 LOSS_Generator: 3.6770620346069336 LOSS_Discriminator: 0.052958438793818154\n",
            "ITERATION_NO.: 1574 LOSS_Generator: 3.987607955932617 LOSS_Discriminator: 0.10516531268755595\n",
            "ITERATION_NO.: 1575 LOSS_Generator: 4.762748718261719 LOSS_Discriminator: 0.071713720758756\n",
            "ITERATION_NO.: 1576 LOSS_Generator: 5.943787574768066 LOSS_Discriminator: 0.05749174455801646\n",
            "ITERATION_NO.: 1577 LOSS_Generator: 6.839016914367676 LOSS_Discriminator: 0.0070945508778095245\n",
            "ITERATION_NO.: 1578 LOSS_Generator: 6.609335422515869 LOSS_Discriminator: 0.37612322966257733\n",
            "ITERATION_NO.: 1579 LOSS_Generator: 5.7501678466796875 LOSS_Discriminator: 0.12425500154495239\n",
            "ITERATION_NO.: 1580 LOSS_Generator: 5.330187797546387 LOSS_Discriminator: 0.22069595257441202\n",
            "ITERATION_NO.: 1581 LOSS_Generator: 3.9756624698638916 LOSS_Discriminator: 0.1590744952360789\n",
            "ITERATION_NO.: 1582 LOSS_Generator: 3.2368364334106445 LOSS_Discriminator: 0.07354843616485596\n",
            "ITERATION_NO.: 1583 LOSS_Generator: 4.458674430847168 LOSS_Discriminator: 0.14160233736038208\n",
            "ITERATION_NO.: 1584 LOSS_Generator: 4.468138694763184 LOSS_Discriminator: 0.0791072944800059\n",
            "ITERATION_NO.: 1585 LOSS_Generator: 4.912247180938721 LOSS_Discriminator: 0.07715017596880595\n",
            "ITERATION_NO.: 1586 LOSS_Generator: 5.113043308258057 LOSS_Discriminator: 0.022776752710342407\n",
            "ITERATION_NO.: 1587 LOSS_Generator: 5.6312174797058105 LOSS_Discriminator: 0.13832126061121622\n",
            "ITERATION_NO.: 1588 LOSS_Generator: 6.007184028625488 LOSS_Discriminator: 0.017001489798227947\n",
            "ITERATION_NO.: 1589 LOSS_Generator: 5.782384872436523 LOSS_Discriminator: 0.012213360518217087\n",
            "ITERATION_NO.: 1590 LOSS_Generator: 5.808350563049316 LOSS_Discriminator: 0.20226991176605225\n",
            "ITERATION_NO.: 1591 LOSS_Generator: 4.78475284576416 LOSS_Discriminator: 0.011033564805984497\n",
            "ITERATION_NO.: 1592 LOSS_Generator: 4.9977192878723145 LOSS_Discriminator: 0.04852926234404246\n",
            "ITERATION_NO.: 1593 LOSS_Generator: 5.159966945648193 LOSS_Discriminator: 0.11833878358205159\n",
            "ITERATION_NO.: 1594 LOSS_Generator: 4.623517990112305 LOSS_Discriminator: 0.03608699142932892\n",
            "ITERATION_NO.: 1595 LOSS_Generator: 4.828760147094727 LOSS_Discriminator: 0.0662601391474406\n",
            "ITERATION_NO.: 1596 LOSS_Generator: 4.716055393218994 LOSS_Discriminator: 0.07801546653111775\n",
            "ITERATION_NO.: 1597 LOSS_Generator: 4.682907581329346 LOSS_Discriminator: 0.05824525157610575\n",
            "ITERATION_NO.: 1598 LOSS_Generator: 4.692803859710693 LOSS_Discriminator: 0.03899263838926951\n",
            "ITERATION_NO.: 1599 LOSS_Generator: 4.937145233154297 LOSS_Discriminator: 0.15579708417256674\n",
            "ITERATION_NO.: 1600 LOSS_Generator: 5.073700904846191 LOSS_Discriminator: 0.10164849956830342\n",
            "ITERATION_NO.: 1601 LOSS_Generator: 5.330666542053223 LOSS_Discriminator: 0.10835826396942139\n",
            "ITERATION_NO.: 1602 LOSS_Generator: 4.785090446472168 LOSS_Discriminator: 0.03127549837032954\n",
            "ITERATION_NO.: 1603 LOSS_Generator: 4.381689548492432 LOSS_Discriminator: 0.19097298383712769\n",
            "ITERATION_NO.: 1604 LOSS_Generator: 4.344980716705322 LOSS_Discriminator: 0.20494500796000162\n",
            "ITERATION_NO.: 1605 LOSS_Generator: 4.237027645111084 LOSS_Discriminator: 0.03956703841686249\n",
            "ITERATION_NO.: 1606 LOSS_Generator: 4.610461711883545 LOSS_Discriminator: 0.1136612594127655\n",
            "ITERATION_NO.: 1607 LOSS_Generator: 5.182747840881348 LOSS_Discriminator: 0.01860858251651128\n",
            "ITERATION_NO.: 1608 LOSS_Generator: 5.209424018859863 LOSS_Discriminator: 0.20266429583231607\n",
            "ITERATION_NO.: 1609 LOSS_Generator: 4.93265438079834 LOSS_Discriminator: 0.10230581959088643\n",
            "ITERATION_NO.: 1610 LOSS_Generator: 4.59202241897583 LOSS_Discriminator: 0.03640177597602209\n",
            "ITERATION_NO.: 1611 LOSS_Generator: 4.617434501647949 LOSS_Discriminator: 0.17275812228520712\n",
            "ITERATION_NO.: 1612 LOSS_Generator: 4.420917510986328 LOSS_Discriminator: 0.05971001088619232\n",
            "ITERATION_NO.: 1613 LOSS_Generator: 4.404283046722412 LOSS_Discriminator: 0.22562833627065024\n",
            "ITERATION_NO.: 1614 LOSS_Generator: 3.965287685394287 LOSS_Discriminator: 0.16198028127352396\n",
            "ITERATION_NO.: 1615 LOSS_Generator: 4.012486457824707 LOSS_Discriminator: 0.20358421405156454\n",
            "ITERATION_NO.: 1616 LOSS_Generator: 4.568295478820801 LOSS_Discriminator: 0.08254903554916382\n",
            "ITERATION_NO.: 1617 LOSS_Generator: 5.4126996994018555 LOSS_Discriminator: 0.09108645717302959\n",
            "ITERATION_NO.: 1618 LOSS_Generator: 5.928678512573242 LOSS_Discriminator: 0.04812923073768616\n",
            "ITERATION_NO.: 1619 LOSS_Generator: 5.817150592803955 LOSS_Discriminator: 0.13286556800206503\n",
            "ITERATION_NO.: 1620 LOSS_Generator: 5.5758137702941895 LOSS_Discriminator: 0.13940860827763876\n",
            "ITERATION_NO.: 1621 LOSS_Generator: 5.778401851654053 LOSS_Discriminator: 0.008481708665688833\n",
            "ITERATION_NO.: 1622 LOSS_Generator: 5.72133731842041 LOSS_Discriminator: 0.006887936343749364\n",
            "ITERATION_NO.: 1623 LOSS_Generator: 5.529917240142822 LOSS_Discriminator: 0.008722266182303429\n",
            "ITERATION_NO.: 1624 LOSS_Generator: 5.859161376953125 LOSS_Discriminator: 0.007908744116624197\n",
            "ITERATION_NO.: 1625 LOSS_Generator: 6.094444274902344 LOSS_Discriminator: 0.007511908188462257\n",
            "ITERATION_NO.: 1626 LOSS_Generator: 5.767056465148926 LOSS_Discriminator: 0.11043703556060791\n",
            "ITERATION_NO.: 1627 LOSS_Generator: 5.638354301452637 LOSS_Discriminator: 0.06626672049363454\n",
            "ITERATION_NO.: 1628 LOSS_Generator: 5.787919998168945 LOSS_Discriminator: 0.012917943298816681\n",
            "ITERATION_NO.: 1629 LOSS_Generator: 5.301125526428223 LOSS_Discriminator: 0.08409147461255391\n",
            "ITERATION_NO.: 1630 LOSS_Generator: 4.936697483062744 LOSS_Discriminator: 0.023354622224966686\n",
            "ITERATION_NO.: 1631 LOSS_Generator: 4.508667945861816 LOSS_Discriminator: 0.11908976236979167\n",
            "ITERATION_NO.: 1632 LOSS_Generator: 4.70426082611084 LOSS_Discriminator: 0.024444520473480225\n",
            "ITERATION_NO.: 1633 LOSS_Generator: 4.29002046585083 LOSS_Discriminator: 0.14803746342658997\n",
            "ITERATION_NO.: 1634 LOSS_Generator: 4.435403823852539 LOSS_Discriminator: 0.07053913672765096\n",
            "ITERATION_NO.: 1635 LOSS_Generator: 4.6366376876831055 LOSS_Discriminator: 0.3055438796679179\n",
            "ITERATION_NO.: 1636 LOSS_Generator: 3.8469250202178955 LOSS_Discriminator: 0.12060761451721191\n",
            "ITERATION_NO.: 1637 LOSS_Generator: 4.710657119750977 LOSS_Discriminator: 0.09963602821032207\n",
            "ITERATION_NO.: 1638 LOSS_Generator: 4.439648151397705 LOSS_Discriminator: 0.08769420782725017\n",
            "ITERATION_NO.: 1639 LOSS_Generator: 5.237983703613281 LOSS_Discriminator: 0.17618536949157715\n",
            "ITERATION_NO.: 1640 LOSS_Generator: 5.630621910095215 LOSS_Discriminator: 0.06720104316870372\n",
            "ITERATION_NO.: 1641 LOSS_Generator: 5.929359436035156 LOSS_Discriminator: 0.07310254871845245\n",
            "ITERATION_NO.: 1642 LOSS_Generator: 6.256563186645508 LOSS_Discriminator: 0.11066651344299316\n",
            "ITERATION_NO.: 1643 LOSS_Generator: 5.836663246154785 LOSS_Discriminator: 0.005813744540015857\n",
            "ITERATION_NO.: 1644 LOSS_Generator: 5.680490493774414 LOSS_Discriminator: 0.09763364990552266\n",
            "ITERATION_NO.: 1645 LOSS_Generator: 5.326089859008789 LOSS_Discriminator: 0.1318611999352773\n",
            "ITERATION_NO.: 1646 LOSS_Generator: 5.193610668182373 LOSS_Discriminator: 0.05820915102958679\n",
            "ITERATION_NO.: 1647 LOSS_Generator: 4.673525810241699 LOSS_Discriminator: 0.21101228396097818\n",
            "ITERATION_NO.: 1648 LOSS_Generator: 4.5920610427856445 LOSS_Discriminator: 0.07897935310999553\n",
            "ITERATION_NO.: 1649 LOSS_Generator: 4.355690956115723 LOSS_Discriminator: 0.06733629107475281\n",
            "ITERATION_NO.: 1650 LOSS_Generator: 5.31380033493042 LOSS_Discriminator: 0.030692266921202343\n",
            "ITERATION_NO.: 1651 LOSS_Generator: 5.553987979888916 LOSS_Discriminator: 0.20461181799570718\n",
            "ITERATION_NO.: 1652 LOSS_Generator: 6.08435583114624 LOSS_Discriminator: 0.009039442365368208\n",
            "ITERATION_NO.: 1653 LOSS_Generator: 6.1874895095825195 LOSS_Discriminator: 0.005854743843277295\n",
            "ITERATION_NO.: 1654 LOSS_Generator: 6.389042854309082 LOSS_Discriminator: 0.005401559794942538\n",
            "ITERATION_NO.: 1655 LOSS_Generator: 6.252472877502441 LOSS_Discriminator: 0.19327290852864584\n",
            "ITERATION_NO.: 1656 LOSS_Generator: 5.622969627380371 LOSS_Discriminator: 0.1025896966457367\n",
            "ITERATION_NO.: 1657 LOSS_Generator: 4.8539886474609375 LOSS_Discriminator: 0.19671674569447836\n",
            "ITERATION_NO.: 1658 LOSS_Generator: 4.15715217590332 LOSS_Discriminator: 0.040209474662939705\n",
            "ITERATION_NO.: 1659 LOSS_Generator: 4.490253448486328 LOSS_Discriminator: 0.06620331108570099\n",
            "ITERATION_NO.: 1660 LOSS_Generator: 4.527987003326416 LOSS_Discriminator: 0.04744217793146769\n",
            "ITERATION_NO.: 1661 LOSS_Generator: 5.785310745239258 LOSS_Discriminator: 0.011373539765675863\n",
            "ITERATION_NO.: 1662 LOSS_Generator: 6.079023361206055 LOSS_Discriminator: 0.06707065304120381\n",
            "ITERATION_NO.: 1663 LOSS_Generator: 6.555606842041016 LOSS_Discriminator: 0.004740763145188491\n",
            "ITERATION_NO.: 1664 LOSS_Generator: 6.645566940307617 LOSS_Discriminator: 0.002717382895449797\n",
            "ITERATION_NO.: 1665 LOSS_Generator: 6.519543647766113 LOSS_Discriminator: 0.16710748275121054\n",
            "ITERATION_NO.: 1666 LOSS_Generator: 6.509531497955322 LOSS_Discriminator: 0.0029847165569663048\n",
            "ITERATION_NO.: 1667 LOSS_Generator: 6.56345796585083 LOSS_Discriminator: 0.0033718720078468323\n",
            "ITERATION_NO.: 1668 LOSS_Generator: 5.894709587097168 LOSS_Discriminator: 0.009027772893508276\n",
            "ITERATION_NO.: 1669 LOSS_Generator: 5.954848289489746 LOSS_Discriminator: 0.01833520084619522\n",
            "ITERATION_NO.: 1670 LOSS_Generator: 6.089132308959961 LOSS_Discriminator: 0.19070265690485635\n",
            "ITERATION_NO.: 1671 LOSS_Generator: 5.830587387084961 LOSS_Discriminator: 0.013466487328211466\n",
            "ITERATION_NO.: 1672 LOSS_Generator: 5.735452175140381 LOSS_Discriminator: 0.1956749161084493\n",
            "ITERATION_NO.: 1673 LOSS_Generator: 5.373874187469482 LOSS_Discriminator: 0.05520269274711609\n",
            "ITERATION_NO.: 1674 LOSS_Generator: 5.640104293823242 LOSS_Discriminator: 0.18513411283493042\n",
            "ITERATION_NO.: 1675 LOSS_Generator: 4.646104335784912 LOSS_Discriminator: 0.13179210821787515\n",
            "ITERATION_NO.: 1676 LOSS_Generator: 4.216766357421875 LOSS_Discriminator: 0.04360508918762207\n",
            "ITERATION_NO.: 1677 LOSS_Generator: 4.538829803466797 LOSS_Discriminator: 0.1256230572859446\n",
            "ITERATION_NO.: 1678 LOSS_Generator: 4.621813774108887 LOSS_Discriminator: 0.1793843905131022\n",
            "ITERATION_NO.: 1679 LOSS_Generator: 4.293658256530762 LOSS_Discriminator: 0.22935465971628824\n",
            "ITERATION_NO.: 1680 LOSS_Generator: 4.491290092468262 LOSS_Discriminator: 0.08411492904027303\n",
            "ITERATION_NO.: 1681 LOSS_Generator: 4.923173427581787 LOSS_Discriminator: 0.08991527557373047\n",
            "ITERATION_NO.: 1682 LOSS_Generator: 5.156764030456543 LOSS_Discriminator: 0.05955644448598226\n",
            "ITERATION_NO.: 1683 LOSS_Generator: 4.923178672790527 LOSS_Discriminator: 0.10496972004572551\n",
            "ITERATION_NO.: 1684 LOSS_Generator: 4.4897565841674805 LOSS_Discriminator: 0.09690920511881511\n",
            "ITERATION_NO.: 1685 LOSS_Generator: 4.28016471862793 LOSS_Discriminator: 0.1580631136894226\n",
            "ITERATION_NO.: 1686 LOSS_Generator: 3.7571358680725098 LOSS_Discriminator: 0.156643807888031\n",
            "ITERATION_NO.: 1687 LOSS_Generator: 3.7727432250976562 LOSS_Discriminator: 0.11835276087125142\n",
            "ITERATION_NO.: 1688 LOSS_Generator: 5.460823059082031 LOSS_Discriminator: 0.030627412100632984\n",
            "ITERATION_NO.: 1689 LOSS_Generator: 5.717620372772217 LOSS_Discriminator: 0.11326442162195842\n",
            "ITERATION_NO.: 1690 LOSS_Generator: 6.071690082550049 LOSS_Discriminator: 0.03944587210814158\n",
            "ITERATION_NO.: 1691 LOSS_Generator: 5.790292739868164 LOSS_Discriminator: 0.1726935108502706\n",
            "ITERATION_NO.: 1692 LOSS_Generator: 4.537917137145996 LOSS_Discriminator: 0.14757094780604044\n",
            "ITERATION_NO.: 1693 LOSS_Generator: 3.9020626544952393 LOSS_Discriminator: 0.05844339728355408\n",
            "ITERATION_NO.: 1694 LOSS_Generator: 4.7684431076049805 LOSS_Discriminator: 0.06028072535991669\n",
            "ITERATION_NO.: 1695 LOSS_Generator: 5.115625381469727 LOSS_Discriminator: 0.02228415012359619\n",
            "ITERATION_NO.: 1696 LOSS_Generator: 5.279590606689453 LOSS_Discriminator: 0.011641270170609156\n",
            "ITERATION_NO.: 1697 LOSS_Generator: 5.5070295333862305 LOSS_Discriminator: 0.09807472427686055\n",
            "ITERATION_NO.: 1698 LOSS_Generator: 5.260277271270752 LOSS_Discriminator: 0.14160077770551047\n",
            "ITERATION_NO.: 1699 LOSS_Generator: 3.9610509872436523 LOSS_Discriminator: 0.14271093408266702\n",
            "ITERATION_NO.: 1700 LOSS_Generator: 3.2818922996520996 LOSS_Discriminator: 0.1425111492474874\n",
            "ITERATION_NO.: 1701 LOSS_Generator: 4.655242919921875 LOSS_Discriminator: 0.10527716080347697\n",
            "ITERATION_NO.: 1702 LOSS_Generator: 5.380143165588379 LOSS_Discriminator: 0.025065089265505474\n",
            "ITERATION_NO.: 1703 LOSS_Generator: 6.064087867736816 LOSS_Discriminator: 0.10306631525357564\n",
            "ITERATION_NO.: 1704 LOSS_Generator: 6.407783508300781 LOSS_Discriminator: 0.06292272607485454\n",
            "ITERATION_NO.: 1705 LOSS_Generator: 6.276252269744873 LOSS_Discriminator: 0.0037829910094539323\n",
            "ITERATION_NO.: 1706 LOSS_Generator: 5.839526176452637 LOSS_Discriminator: 0.08911801377932231\n",
            "ITERATION_NO.: 1707 LOSS_Generator: 4.9114484786987305 LOSS_Discriminator: 0.13444762428601584\n",
            "ITERATION_NO.: 1708 LOSS_Generator: 3.8258094787597656 LOSS_Discriminator: 0.0926793913046519\n",
            "ITERATION_NO.: 1709 LOSS_Generator: 4.118930816650391 LOSS_Discriminator: 0.21145657698313394\n",
            "ITERATION_NO.: 1710 LOSS_Generator: 5.399343490600586 LOSS_Discriminator: 0.06131286919116974\n",
            "ITERATION_NO.: 1711 LOSS_Generator: 6.025473594665527 LOSS_Discriminator: 0.0604667067527771\n",
            "ITERATION_NO.: 1712 LOSS_Generator: 6.1291584968566895 LOSS_Discriminator: 0.15197322765986124\n",
            "ITERATION_NO.: 1713 LOSS_Generator: 5.652000904083252 LOSS_Discriminator: 0.02229805787404378\n",
            "ITERATION_NO.: 1714 LOSS_Generator: 4.856504917144775 LOSS_Discriminator: 0.0065282223125298815\n",
            "ITERATION_NO.: 1715 LOSS_Generator: 5.30086612701416 LOSS_Discriminator: 0.046091859539349876\n",
            "ITERATION_NO.: 1716 LOSS_Generator: 4.977410316467285 LOSS_Discriminator: 0.13002137343088785\n",
            "ITERATION_NO.: 1717 LOSS_Generator: 4.271237373352051 LOSS_Discriminator: 0.028185332814852398\n",
            "ITERATION_NO.: 1718 LOSS_Generator: 4.242337703704834 LOSS_Discriminator: 0.06885069608688354\n",
            "ITERATION_NO.: 1719 LOSS_Generator: 4.900942802429199 LOSS_Discriminator: 0.05219625929991404\n",
            "ITERATION_NO.: 1720 LOSS_Generator: 5.588221549987793 LOSS_Discriminator: 0.025363877415657043\n",
            "ITERATION_NO.: 1721 LOSS_Generator: 5.818553924560547 LOSS_Discriminator: 0.06328240533669789\n",
            "ITERATION_NO.: 1722 LOSS_Generator: 5.765057563781738 LOSS_Discriminator: 0.015203028917312622\n",
            "ITERATION_NO.: 1723 LOSS_Generator: 5.668423652648926 LOSS_Discriminator: 0.14827088514963785\n",
            "ITERATION_NO.: 1724 LOSS_Generator: 5.22643518447876 LOSS_Discriminator: 0.032596901059150696\n",
            "ITERATION_NO.: 1725 LOSS_Generator: 4.6674275398254395 LOSS_Discriminator: 0.06193337341149648\n",
            "ITERATION_NO.: 1726 LOSS_Generator: 4.362622261047363 LOSS_Discriminator: 0.04059274991353353\n",
            "ITERATION_NO.: 1727 LOSS_Generator: 5.1691694259643555 LOSS_Discriminator: 0.049954116344451904\n",
            "ITERATION_NO.: 1728 LOSS_Generator: 5.981675624847412 LOSS_Discriminator: 0.011297351370255152\n",
            "ITERATION_NO.: 1729 LOSS_Generator: 6.296673774719238 LOSS_Discriminator: 0.009919586901863417\n",
            "ITERATION_NO.: 1730 LOSS_Generator: 6.279665946960449 LOSS_Discriminator: 0.21430555979410806\n",
            "ITERATION_NO.: 1731 LOSS_Generator: 5.943272590637207 LOSS_Discriminator: 0.20458942651748657\n",
            "ITERATION_NO.: 1732 LOSS_Generator: 4.724984169006348 LOSS_Discriminator: 0.03319872170686722\n",
            "ITERATION_NO.: 1733 LOSS_Generator: 3.911618709564209 LOSS_Discriminator: 0.05196211735407511\n",
            "ITERATION_NO.: 1734 LOSS_Generator: 4.099370002746582 LOSS_Discriminator: 0.05649039645989736\n",
            "ITERATION_NO.: 1735 LOSS_Generator: 4.585600852966309 LOSS_Discriminator: 0.07978584865729015\n",
            "ITERATION_NO.: 1736 LOSS_Generator: 5.344722747802734 LOSS_Discriminator: 0.061807528138160706\n",
            "ITERATION_NO.: 1737 LOSS_Generator: 6.057914733886719 LOSS_Discriminator: 0.19090783596038818\n",
            "ITERATION_NO.: 1738 LOSS_Generator: 5.796630859375 LOSS_Discriminator: 0.20786714553833008\n",
            "ITERATION_NO.: 1739 LOSS_Generator: 5.336195468902588 LOSS_Discriminator: 0.015666291117668152\n",
            "ITERATION_NO.: 1740 LOSS_Generator: 5.256253242492676 LOSS_Discriminator: 0.04331221183141073\n",
            "ITERATION_NO.: 1741 LOSS_Generator: 4.899105548858643 LOSS_Discriminator: 0.01390286535024643\n",
            "ITERATION_NO.: 1742 LOSS_Generator: 5.242415428161621 LOSS_Discriminator: 0.022678265968958538\n",
            "ITERATION_NO.: 1743 LOSS_Generator: 5.374687194824219 LOSS_Discriminator: 0.029994065562884014\n",
            "ITERATION_NO.: 1744 LOSS_Generator: 5.328004837036133 LOSS_Discriminator: 0.10031022628148396\n",
            "ITERATION_NO.: 1745 LOSS_Generator: 5.195165157318115 LOSS_Discriminator: 0.012264817953109741\n",
            "ITERATION_NO.: 1746 LOSS_Generator: 5.177872657775879 LOSS_Discriminator: 0.07051439086596172\n",
            "ITERATION_NO.: 1747 LOSS_Generator: 4.7208051681518555 LOSS_Discriminator: 0.01908169314265251\n",
            "ITERATION_NO.: 1748 LOSS_Generator: 5.250130653381348 LOSS_Discriminator: 0.03927760571241379\n",
            "ITERATION_NO.: 1749 LOSS_Generator: 5.735063552856445 LOSS_Discriminator: 0.012114992986122767\n",
            "ITERATION_NO.: 1750 LOSS_Generator: 5.868391990661621 LOSS_Discriminator: 0.1839643120765686\n",
            "ITERATION_NO.: 1751 LOSS_Generator: 4.825991630554199 LOSS_Discriminator: 0.09920452038447063\n",
            "ITERATION_NO.: 1752 LOSS_Generator: 4.090909957885742 LOSS_Discriminator: 0.022399793068567913\n",
            "ITERATION_NO.: 1753 LOSS_Generator: 3.9319512844085693 LOSS_Discriminator: 0.19054585695266724\n",
            "ITERATION_NO.: 1754 LOSS_Generator: 3.9892101287841797 LOSS_Discriminator: 0.09774130582809448\n",
            "ITERATION_NO.: 1755 LOSS_Generator: 4.6947736740112305 LOSS_Discriminator: 0.07563265164693196\n",
            "ITERATION_NO.: 1756 LOSS_Generator: 5.188988208770752 LOSS_Discriminator: 0.13719322284062704\n",
            "ITERATION_NO.: 1757 LOSS_Generator: 5.813643455505371 LOSS_Discriminator: 0.01871318245927493\n",
            "ITERATION_NO.: 1758 LOSS_Generator: 5.78407096862793 LOSS_Discriminator: 0.04251198470592499\n",
            "ITERATION_NO.: 1759 LOSS_Generator: 5.996532440185547 LOSS_Discriminator: 0.025450209776560467\n",
            "ITERATION_NO.: 1760 LOSS_Generator: 5.836435794830322 LOSS_Discriminator: 0.006489725783467293\n",
            "ITERATION_NO.: 1761 LOSS_Generator: 6.077996730804443 LOSS_Discriminator: 0.016379306713740032\n",
            "ITERATION_NO.: 1762 LOSS_Generator: 6.294473648071289 LOSS_Discriminator: 0.014324950675169626\n",
            "ITERATION_NO.: 1763 LOSS_Generator: 6.595590591430664 LOSS_Discriminator: 0.08458791176478068\n",
            "ITERATION_NO.: 1764 LOSS_Generator: 5.46512508392334 LOSS_Discriminator: 0.34496498107910156\n",
            "ITERATION_NO.: 1765 LOSS_Generator: 4.0149126052856445 LOSS_Discriminator: 0.06696431835492452\n",
            "ITERATION_NO.: 1766 LOSS_Generator: 4.2731499671936035 LOSS_Discriminator: 0.08052546779314677\n",
            "ITERATION_NO.: 1767 LOSS_Generator: 5.230224609375 LOSS_Discriminator: 0.08968156576156616\n",
            "ITERATION_NO.: 1768 LOSS_Generator: 6.231529712677002 LOSS_Discriminator: 0.13672267397244772\n",
            "ITERATION_NO.: 1769 LOSS_Generator: 6.3044891357421875 LOSS_Discriminator: 0.013900933166344961\n",
            "ITERATION_NO.: 1770 LOSS_Generator: 6.68834114074707 LOSS_Discriminator: 0.21385546525319418\n",
            "ITERATION_NO.: 1771 LOSS_Generator: 6.310937881469727 LOSS_Discriminator: 0.34107168515523273\n",
            "ITERATION_NO.: 1772 LOSS_Generator: 5.109445571899414 LOSS_Discriminator: 0.3298737406730652\n",
            "ITERATION_NO.: 1773 LOSS_Generator: 3.546722412109375 LOSS_Discriminator: 0.13817537824312845\n",
            "ITERATION_NO.: 1774 LOSS_Generator: 2.679651975631714 LOSS_Discriminator: 0.1517914136250814\n",
            "ITERATION_NO.: 1775 LOSS_Generator: 3.3096277713775635 LOSS_Discriminator: 0.24343200524648032\n",
            "ITERATION_NO.: 1776 LOSS_Generator: 4.484159469604492 LOSS_Discriminator: 0.07644231120745341\n",
            "ITERATION_NO.: 1777 LOSS_Generator: 6.271496772766113 LOSS_Discriminator: 0.01759853959083557\n",
            "ITERATION_NO.: 1778 LOSS_Generator: 7.146400451660156 LOSS_Discriminator: 0.015283926079670588\n",
            "ITERATION_NO.: 1779 LOSS_Generator: 6.96854305267334 LOSS_Discriminator: 0.26456864674886066\n",
            "ITERATION_NO.: 1780 LOSS_Generator: 7.407651901245117 LOSS_Discriminator: 0.0017465854374070962\n",
            "ITERATION_NO.: 1781 LOSS_Generator: 7.295772552490234 LOSS_Discriminator: 0.2298884391784668\n",
            "ITERATION_NO.: 1782 LOSS_Generator: 6.1423163414001465 LOSS_Discriminator: 0.1429527203241984\n",
            "ITERATION_NO.: 1783 LOSS_Generator: 5.61080265045166 LOSS_Discriminator: 0.010116379087169966\n",
            "ITERATION_NO.: 1784 LOSS_Generator: 5.580842971801758 LOSS_Discriminator: 0.06041371822357178\n",
            "ITERATION_NO.: 1785 LOSS_Generator: 4.877197265625 LOSS_Discriminator: 0.10580289363861084\n",
            "ITERATION_NO.: 1786 LOSS_Generator: 5.068254470825195 LOSS_Discriminator: 0.046913474798202515\n",
            "ITERATION_NO.: 1787 LOSS_Generator: 4.367033958435059 LOSS_Discriminator: 0.2639177441596985\n",
            "ITERATION_NO.: 1788 LOSS_Generator: 3.5071628093719482 LOSS_Discriminator: 0.11403729518254598\n",
            "ITERATION_NO.: 1789 LOSS_Generator: 3.763495922088623 LOSS_Discriminator: 0.13921295603116354\n",
            "ITERATION_NO.: 1790 LOSS_Generator: 3.9068775177001953 LOSS_Discriminator: 0.13255786895751953\n",
            "ITERATION_NO.: 1791 LOSS_Generator: 4.271440029144287 LOSS_Discriminator: 0.13707280158996582\n",
            "ITERATION_NO.: 1792 LOSS_Generator: 5.158384323120117 LOSS_Discriminator: 0.016396782050530117\n",
            "ITERATION_NO.: 1793 LOSS_Generator: 5.017548561096191 LOSS_Discriminator: 0.1823220451672872\n",
            "ITERATION_NO.: 1794 LOSS_Generator: 5.277022838592529 LOSS_Discriminator: 0.031864906350771584\n",
            "ITERATION_NO.: 1795 LOSS_Generator: 4.645624160766602 LOSS_Discriminator: 0.11474037170410156\n",
            "ITERATION_NO.: 1796 LOSS_Generator: 4.728872299194336 LOSS_Discriminator: 0.06544121603171031\n",
            "ITERATION_NO.: 1797 LOSS_Generator: 3.907466173171997 LOSS_Discriminator: 0.12816983461380005\n",
            "ITERATION_NO.: 1798 LOSS_Generator: 3.292243480682373 LOSS_Discriminator: 0.14059887329737344\n",
            "ITERATION_NO.: 1799 LOSS_Generator: 3.7883548736572266 LOSS_Discriminator: 0.040239075819651283\n",
            "ITERATION_NO.: 1800 LOSS_Generator: 4.36829948425293 LOSS_Discriminator: 0.07401793201764424\n",
            "ITERATION_NO.: 1801 LOSS_Generator: 5.319445610046387 LOSS_Discriminator: 0.09174086650212605\n",
            "ITERATION_NO.: 1802 LOSS_Generator: 5.388851165771484 LOSS_Discriminator: 0.2956196467081706\n",
            "ITERATION_NO.: 1803 LOSS_Generator: 5.430054664611816 LOSS_Discriminator: 0.016568568845589954\n",
            "ITERATION_NO.: 1804 LOSS_Generator: 4.8242974281311035 LOSS_Discriminator: 0.055604745944341026\n",
            "ITERATION_NO.: 1805 LOSS_Generator: 5.264004707336426 LOSS_Discriminator: 0.04242044190565745\n",
            "ITERATION_NO.: 1806 LOSS_Generator: 5.140578269958496 LOSS_Discriminator: 0.12334895133972168\n",
            "ITERATION_NO.: 1807 LOSS_Generator: 4.990911483764648 LOSS_Discriminator: 0.08375760912895203\n",
            "ITERATION_NO.: 1808 LOSS_Generator: 4.565317153930664 LOSS_Discriminator: 0.10425268610318501\n",
            "ITERATION_NO.: 1809 LOSS_Generator: 3.8562088012695312 LOSS_Discriminator: 0.1451626420021057\n",
            "ITERATION_NO.: 1810 LOSS_Generator: 3.260927677154541 LOSS_Discriminator: 0.12371512254079182\n",
            "ITERATION_NO.: 1811 LOSS_Generator: 3.7997074127197266 LOSS_Discriminator: 0.15667750438054404\n",
            "ITERATION_NO.: 1812 LOSS_Generator: 4.990723133087158 LOSS_Discriminator: 0.05963014562924703\n",
            "ITERATION_NO.: 1813 LOSS_Generator: 5.856966972351074 LOSS_Discriminator: 0.016612668832143147\n",
            "ITERATION_NO.: 1814 LOSS_Generator: 6.10513973236084 LOSS_Discriminator: 0.31375060478846234\n",
            "ITERATION_NO.: 1815 LOSS_Generator: 6.274506092071533 LOSS_Discriminator: 0.15454309185345969\n",
            "ITERATION_NO.: 1816 LOSS_Generator: 5.894714832305908 LOSS_Discriminator: 0.12497401237487793\n",
            "ITERATION_NO.: 1817 LOSS_Generator: 5.115863800048828 LOSS_Discriminator: 0.01906747743487358\n",
            "ITERATION_NO.: 1818 LOSS_Generator: 4.734939098358154 LOSS_Discriminator: 0.013224529723326365\n",
            "ITERATION_NO.: 1819 LOSS_Generator: 4.614306449890137 LOSS_Discriminator: 0.05704200267791748\n",
            "ITERATION_NO.: 1820 LOSS_Generator: 4.116390705108643 LOSS_Discriminator: 0.041244350373744965\n",
            "ITERATION_NO.: 1821 LOSS_Generator: 5.051128387451172 LOSS_Discriminator: 0.040903148551781975\n",
            "ITERATION_NO.: 1822 LOSS_Generator: 5.1379075050354 LOSS_Discriminator: 0.14548742771148682\n",
            "ITERATION_NO.: 1823 LOSS_Generator: 4.980539798736572 LOSS_Discriminator: 0.0998294750849406\n",
            "ITERATION_NO.: 1824 LOSS_Generator: 4.656039237976074 LOSS_Discriminator: 0.08584307630856831\n",
            "ITERATION_NO.: 1825 LOSS_Generator: 4.9523725509643555 LOSS_Discriminator: 0.07253958284854889\n",
            "ITERATION_NO.: 1826 LOSS_Generator: 4.99522066116333 LOSS_Discriminator: 0.03771571318308512\n",
            "ITERATION_NO.: 1827 LOSS_Generator: 5.444189071655273 LOSS_Discriminator: 0.03334525724252065\n",
            "ITERATION_NO.: 1828 LOSS_Generator: 5.824211120605469 LOSS_Discriminator: 0.010384277130166689\n",
            "ITERATION_NO.: 1829 LOSS_Generator: 6.110136985778809 LOSS_Discriminator: 0.00531068816781044\n",
            "ITERATION_NO.: 1830 LOSS_Generator: 5.735116958618164 LOSS_Discriminator: 0.18233120441436768\n",
            "ITERATION_NO.: 1831 LOSS_Generator: 5.318154335021973 LOSS_Discriminator: 0.029444565375645954\n",
            "ITERATION_NO.: 1832 LOSS_Generator: 5.0289130210876465 LOSS_Discriminator: 0.13989985982577005\n",
            "ITERATION_NO.: 1833 LOSS_Generator: 4.099537372589111 LOSS_Discriminator: 0.1235041618347168\n",
            "ITERATION_NO.: 1834 LOSS_Generator: 4.787458896636963 LOSS_Discriminator: 0.07995445032914479\n",
            "ITERATION_NO.: 1835 LOSS_Generator: 5.630173206329346 LOSS_Discriminator: 0.019225527842839558\n",
            "ITERATION_NO.: 1836 LOSS_Generator: 6.358621597290039 LOSS_Discriminator: 0.12831361095110574\n",
            "ITERATION_NO.: 1837 LOSS_Generator: 6.0346832275390625 LOSS_Discriminator: 0.006583049272497495\n",
            "ITERATION_NO.: 1838 LOSS_Generator: 6.017343997955322 LOSS_Discriminator: 0.10711399714152019\n",
            "ITERATION_NO.: 1839 LOSS_Generator: 5.4035186767578125 LOSS_Discriminator: 0.03184129546085993\n",
            "ITERATION_NO.: 1840 LOSS_Generator: 5.126858711242676 LOSS_Discriminator: 0.134896586338679\n",
            "ITERATION_NO.: 1841 LOSS_Generator: 4.786931037902832 LOSS_Discriminator: 0.1325114667415619\n",
            "ITERATION_NO.: 1842 LOSS_Generator: 4.091251373291016 LOSS_Discriminator: 0.049622525771458946\n",
            "ITERATION_NO.: 1843 LOSS_Generator: 3.9661502838134766 LOSS_Discriminator: 0.07173352440198262\n",
            "ITERATION_NO.: 1844 LOSS_Generator: 5.445774555206299 LOSS_Discriminator: 0.05653585493564606\n",
            "ITERATION_NO.: 1845 LOSS_Generator: 5.494267463684082 LOSS_Discriminator: 0.1663779616355896\n",
            "ITERATION_NO.: 1846 LOSS_Generator: 5.294253826141357 LOSS_Discriminator: 0.017001360654830933\n",
            "ITERATION_NO.: 1847 LOSS_Generator: 5.4111738204956055 LOSS_Discriminator: 0.04033118486404419\n",
            "ITERATION_NO.: 1848 LOSS_Generator: 4.937290191650391 LOSS_Discriminator: 0.1127110222975413\n",
            "ITERATION_NO.: 1849 LOSS_Generator: 4.989881992340088 LOSS_Discriminator: 0.03790597120920817\n",
            "ITERATION_NO.: 1850 LOSS_Generator: 4.682612419128418 LOSS_Discriminator: 0.042409429947535195\n",
            "ITERATION_NO.: 1851 LOSS_Generator: 4.368629455566406 LOSS_Discriminator: 0.08947329719861348\n",
            "ITERATION_NO.: 1852 LOSS_Generator: 4.958245754241943 LOSS_Discriminator: 0.07328918079535167\n",
            "ITERATION_NO.: 1853 LOSS_Generator: 4.871119499206543 LOSS_Discriminator: 0.06146345535914103\n",
            "ITERATION_NO.: 1854 LOSS_Generator: 4.857622146606445 LOSS_Discriminator: 0.1849849820137024\n",
            "ITERATION_NO.: 1855 LOSS_Generator: 4.630160331726074 LOSS_Discriminator: 0.13295269012451172\n",
            "ITERATION_NO.: 1856 LOSS_Generator: 4.606535911560059 LOSS_Discriminator: 0.08175464967886607\n",
            "ITERATION_NO.: 1857 LOSS_Generator: 4.981339931488037 LOSS_Discriminator: 0.02552785227696101\n",
            "ITERATION_NO.: 1858 LOSS_Generator: 5.275221824645996 LOSS_Discriminator: 0.20558043320973715\n",
            "ITERATION_NO.: 1859 LOSS_Generator: 5.21804141998291 LOSS_Discriminator: 0.01947375883658727\n",
            "ITERATION_NO.: 1860 LOSS_Generator: 4.912555694580078 LOSS_Discriminator: 0.03137780477603277\n",
            "ITERATION_NO.: 1861 LOSS_Generator: 5.2251787185668945 LOSS_Discriminator: 0.037506399055322014\n",
            "ITERATION_NO.: 1862 LOSS_Generator: 5.168408393859863 LOSS_Discriminator: 0.12932255864143372\n",
            "ITERATION_NO.: 1863 LOSS_Generator: 4.81443452835083 LOSS_Discriminator: 0.012114310016234716\n",
            "ITERATION_NO.: 1864 LOSS_Generator: 4.575568199157715 LOSS_Discriminator: 0.1451748013496399\n",
            "ITERATION_NO.: 1865 LOSS_Generator: 4.266993999481201 LOSS_Discriminator: 0.06753179430961609\n",
            "ITERATION_NO.: 1866 LOSS_Generator: 4.213024139404297 LOSS_Discriminator: 0.07168220480283101\n",
            "ITERATION_NO.: 1867 LOSS_Generator: 4.718164443969727 LOSS_Discriminator: 0.027069754898548126\n",
            "ITERATION_NO.: 1868 LOSS_Generator: 5.319896221160889 LOSS_Discriminator: 0.09350154797236125\n",
            "ITERATION_NO.: 1869 LOSS_Generator: 5.314655303955078 LOSS_Discriminator: 0.13912885387738547\n",
            "ITERATION_NO.: 1870 LOSS_Generator: 5.101597785949707 LOSS_Discriminator: 0.14226268728574118\n",
            "ITERATION_NO.: 1871 LOSS_Generator: 4.06303071975708 LOSS_Discriminator: 0.11924836039543152\n",
            "ITERATION_NO.: 1872 LOSS_Generator: 4.087626934051514 LOSS_Discriminator: 0.13719048102696738\n",
            "ITERATION_NO.: 1873 LOSS_Generator: 3.911917209625244 LOSS_Discriminator: 0.10223587354024251\n",
            "ITERATION_NO.: 1874 LOSS_Generator: 4.167635917663574 LOSS_Discriminator: 0.10212345918019612\n",
            "ITERATION_NO.: 1875 LOSS_Generator: 5.268474578857422 LOSS_Discriminator: 0.12273932496706645\n",
            "EPOCH OVER: 29\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Image_no 1\n",
            "Image_no 2\n",
            "Image_no 3\n",
            "Image_no 4\n",
            "Image_no 5\n",
            "Image_no 6\n",
            "Image_no 7\n",
            "Image_no 8\n",
            "Image_no 9\n",
            "Image_no 10\n",
            "Image_no 11\n",
            "Image_no 12\n",
            "Image_no 13\n",
            "Image_no 14\n",
            "Image_no 15\n",
            "Image_no 16\n",
            "Image_no 17\n",
            "Image_no 18\n",
            "Image_no 19\n",
            "Image_no 20\n",
            "Image_no 21\n",
            "Image_no 22\n",
            "Image_no 23\n",
            "Image_no 24\n",
            "Image_no 25\n",
            "Image_no 26\n",
            "Image_no 27\n",
            "Image_no 28\n",
            "Image_no 29\n",
            "Image_no 30\n",
            "Image_no 31\n",
            "Image_no 32\n",
            "Image_no 33\n",
            "Image_no 34\n",
            "Image_no 35\n",
            "Image_no 36\n",
            "Image_no 37\n",
            "Image_no 38\n",
            "Image_no 39\n",
            "Image_no 40\n",
            "Image_no 41\n",
            "Image_no 42\n",
            "Image_no 43\n",
            "Image_no 44\n",
            "Image_no 45\n",
            "Image_no 46\n",
            "Image_no 47\n",
            "Image_no 48\n",
            "Image_no 49\n",
            "Image_no 50\n",
            "Image_no 51\n",
            "Image_no 52\n",
            "Image_no 53\n",
            "Image_no 54\n",
            "Image_no 55\n",
            "Image_no 56\n",
            "Image_no 57\n",
            "Image_no 58\n",
            "Image_no 59\n",
            "Image_no 60\n",
            "Image_no 61\n",
            "Image_no 62\n",
            "Image_no 63\n",
            "Image_no 64\n",
            "Image_no 65\n",
            "Image_no 66\n",
            "Image_no 67\n",
            "Image_no 68\n",
            "Image_no 69\n",
            "Image_no 70\n",
            "Image_no 71\n",
            "Image_no 72\n",
            "Image_no 73\n",
            "Image_no 74\n",
            "Image_no 75\n",
            "Image_no 76\n",
            "Image_no 77\n",
            "Image_no 78\n",
            "Image_no 79\n",
            "Image_no 80\n",
            "Image_no 81\n",
            "Image_no 82\n",
            "Image_no 83\n",
            "Image_no 84\n",
            "Image_no 85\n",
            "Image_no 86\n",
            "Image_no 87\n",
            "Image_no 88\n",
            "Image_no 89\n",
            "Image_no 90\n",
            "Image_no 91\n",
            "Image_no 92\n",
            "Image_no 93\n",
            "Image_no 94\n",
            "Image_no 95\n",
            "Image_no 96\n",
            "Image_no 97\n",
            "Image_no 98\n",
            "Image_no 99\n",
            "Image_no 100\n",
            "Image_no 101\n",
            "Image_no 102\n",
            "Image_no 103\n",
            "Image_no 104\n",
            "Image_no 105\n",
            "Image_no 106\n",
            "Image_no 107\n",
            "Image_no 108\n",
            "Image_no 109\n",
            "Image_no 110\n",
            "Image_no 111\n",
            "Image_no 112\n",
            "Image_no 113\n",
            "Image_no 114\n",
            "Image_no 115\n",
            "Image_no 116\n",
            "Image_no 117\n",
            "Image_no 118\n",
            "Image_no 119\n",
            "Image_no 120\n",
            "Image_no 121\n",
            "Image_no 122\n",
            "Image_no 123\n",
            "Image_no 124\n",
            "Image_no 125\n",
            "Image_no 126\n",
            "Image_no 127\n",
            "Image_no 128\n",
            "Image_no 129\n",
            "Image_no 130\n",
            "Image_no 131\n",
            "Image_no 132\n",
            "Image_no 133\n",
            "Image_no 134\n",
            "Image_no 135\n",
            "Image_no 136\n",
            "Image_no 137\n",
            "Image_no 138\n",
            "Image_no 139\n",
            "Image_no 140\n",
            "Image_no 141\n",
            "Image_no 142\n",
            "Image_no 143\n",
            "Image_no 144\n",
            "Image_no 145\n",
            "Image_no 146\n",
            "Image_no 147\n",
            "Image_no 148\n",
            "Image_no 149\n",
            "Image_no 150\n",
            "Image_no 151\n",
            "Image_no 152\n",
            "Image_no 153\n",
            "Image_no 154\n",
            "Image_no 155\n",
            "Image_no 156\n",
            "Image_no 157\n",
            "Image_no 158\n",
            "Image_no 159\n",
            "Image_no 160\n",
            "Image_no 161\n",
            "Image_no 162\n",
            "Image_no 163\n",
            "Image_no 164\n",
            "Image_no 165\n",
            "Image_no 166\n",
            "Image_no 167\n",
            "Image_no 168\n",
            "Image_no 169\n",
            "Image_no 170\n",
            "Image_no 171\n",
            "Image_no 172\n",
            "Image_no 173\n",
            "Image_no 174\n",
            "Image_no 175\n",
            "Image_no 176\n",
            "Image_no 177\n",
            "Image_no 178\n",
            "Image_no 179\n",
            "Image_no 180\n",
            "Image_no 181\n",
            "Image_no 182\n",
            "Image_no 183\n",
            "Image_no 184\n",
            "Image_no 185\n",
            "Image_no 186\n",
            "Image_no 187\n",
            "Image_no 188\n",
            "Image_no 189\n",
            "Image_no 190\n",
            "Image_no 191\n",
            "Image_no 192\n",
            "Image_no 193\n",
            "Image_no 194\n",
            "Image_no 195\n",
            "Image_no 196\n",
            "Image_no 197\n",
            "Image_no 198\n",
            "Image_no 199\n",
            "Image_no 200\n",
            "Image_no 201\n",
            "Image_no 202\n",
            "Image_no 203\n",
            "Image_no 204\n",
            "Image_no 205\n",
            "Image_no 206\n",
            "Image_no 207\n",
            "Image_no 208\n",
            "Image_no 209\n",
            "Image_no 210\n",
            "Image_no 211\n",
            "Image_no 212\n",
            "Image_no 213\n",
            "Image_no 214\n",
            "Image_no 215\n",
            "Image_no 216\n",
            "Image_no 217\n",
            "Image_no 218\n",
            "Image_no 219\n",
            "Image_no 220\n",
            "Image_no 221\n",
            "Image_no 222\n",
            "Image_no 223\n",
            "Image_no 224\n",
            "Image_no 225\n",
            "Image_no 226\n",
            "Image_no 227\n",
            "Image_no 228\n",
            "Image_no 229\n",
            "Image_no 230\n",
            "Image_no 231\n",
            "Image_no 232\n",
            "Image_no 233\n",
            "Image_no 234\n",
            "Image_no 235\n",
            "Image_no 236\n",
            "Image_no 237\n",
            "Image_no 238\n",
            "Image_no 239\n",
            "Image_no 240\n",
            "Image_no 241\n",
            "Image_no 242\n",
            "Image_no 243\n",
            "Image_no 244\n",
            "Image_no 245\n",
            "Image_no 246\n",
            "Image_no 247\n",
            "Image_no 248\n",
            "Image_no 249\n",
            "Image_no 250\n",
            "Image_no 251\n",
            "Image_no 252\n",
            "Image_no 253\n",
            "Image_no 254\n",
            "Image_no 255\n",
            "Image_no 256\n",
            "Image_no 257\n",
            "Image_no 258\n",
            "Image_no 259\n",
            "Image_no 260\n",
            "Image_no 261\n",
            "Image_no 262\n",
            "Image_no 263\n",
            "Image_no 264\n",
            "Image_no 265\n",
            "Image_no 266\n",
            "Image_no 267\n",
            "Image_no 268\n",
            "Image_no 269\n",
            "Image_no 270\n",
            "Image_no 271\n",
            "Image_no 272\n",
            "Image_no 273\n",
            "Image_no 274\n",
            "Image_no 275\n",
            "Image_no 276\n",
            "Image_no 277\n",
            "Image_no 278\n",
            "Image_no 279\n",
            "Image_no 280\n",
            "Image_no 281\n",
            "Image_no 282\n",
            "Image_no 283\n",
            "Image_no 284\n",
            "Image_no 285\n",
            "Image_no 286\n",
            "Image_no 287\n",
            "Image_no 288\n",
            "Image_no 289\n",
            "Image_no 290\n",
            "Image_no 291\n",
            "Image_no 292\n",
            "Image_no 293\n",
            "Image_no 294\n",
            "Image_no 295\n",
            "Image_no 296\n",
            "Image_no 297\n",
            "Image_no 298\n",
            "Image_no 299\n",
            "Image_no 300\n",
            "Image_no 301\n",
            "Image_no 302\n",
            "Image_no 303\n",
            "Image_no 304\n",
            "Image_no 305\n",
            "Image_no 306\n",
            "Image_no 307\n",
            "Image_no 308\n",
            "Image_no 309\n",
            "Image_no 310\n",
            "Image_no 311\n",
            "Image_no 312\n",
            "Image_no 313\n",
            "Image_no 314\n",
            "Image_no 315\n",
            "Image_no 316\n",
            "Image_no 317\n",
            "Image_no 318\n",
            "Image_no 319\n",
            "Image_no 320\n",
            "Image_no 321\n",
            "Image_no 322\n",
            "Image_no 323\n",
            "Image_no 324\n",
            "Image_no 325\n",
            "Image_no 326\n",
            "Image_no 327\n",
            "Image_no 328\n",
            "Image_no 329\n",
            "Image_no 330\n",
            "Image_no 331\n",
            "Image_no 332\n",
            "Image_no 333\n",
            "Image_no 334\n",
            "Image_no 335\n",
            "Image_no 336\n",
            "Image_no 337\n",
            "Image_no 338\n",
            "Image_no 339\n",
            "Image_no 340\n",
            "Image_no 341\n",
            "Image_no 342\n",
            "Image_no 343\n",
            "Image_no 344\n",
            "Image_no 345\n",
            "Image_no 346\n",
            "Image_no 347\n",
            "Image_no 348\n",
            "Image_no 349\n",
            "Image_no 350\n",
            "Image_no 351\n",
            "Image_no 352\n",
            "Image_no 353\n",
            "Image_no 354\n",
            "Image_no 355\n",
            "Image_no 356\n",
            "Image_no 357\n",
            "Image_no 358\n",
            "Image_no 359\n",
            "Image_no 360\n",
            "Image_no 361\n",
            "Image_no 362\n",
            "Image_no 363\n",
            "Image_no 364\n",
            "Image_no 365\n",
            "Image_no 366\n",
            "Image_no 367\n",
            "Image_no 368\n",
            "Image_no 369\n",
            "Image_no 370\n",
            "Image_no 371\n",
            "Image_no 372\n",
            "Image_no 373\n",
            "Image_no 374\n",
            "Image_no 375\n",
            "Image_no 376\n",
            "Image_no 377\n",
            "Image_no 378\n",
            "Image_no 379\n",
            "Image_no 380\n",
            "Image_no 381\n",
            "Image_no 382\n",
            "Image_no 383\n",
            "Image_no 384\n",
            "Image_no 385\n",
            "Image_no 386\n",
            "Image_no 387\n",
            "Image_no 388\n",
            "Image_no 389\n",
            "Image_no 390\n",
            "Image_no 391\n",
            "Image_no 392\n",
            "Image_no 393\n",
            "Image_no 394\n",
            "Image_no 395\n",
            "Image_no 396\n",
            "Image_no 397\n",
            "Image_no 398\n",
            "Image_no 399\n",
            "Image_no 400\n",
            "Image_no 401\n",
            "Image_no 402\n",
            "Image_no 403\n",
            "Image_no 404\n",
            "Image_no 405\n",
            "Image_no 406\n",
            "Image_no 407\n",
            "Image_no 408\n",
            "Image_no 409\n",
            "Image_no 410\n",
            "Image_no 411\n",
            "Image_no 412\n",
            "Image_no 413\n",
            "Image_no 414\n",
            "Image_no 415\n",
            "Image_no 416\n",
            "Image_no 417\n",
            "Image_no 418\n",
            "Image_no 419\n",
            "Image_no 420\n",
            "Image_no 421\n",
            "Image_no 422\n",
            "Image_no 423\n",
            "Image_no 424\n",
            "Image_no 425\n",
            "Image_no 426\n",
            "Image_no 427\n",
            "Image_no 428\n",
            "Image_no 429\n",
            "Image_no 430\n",
            "Image_no 431\n",
            "Image_no 432\n",
            "Image_no 433\n",
            "Image_no 434\n",
            "Image_no 435\n",
            "Image_no 436\n",
            "Image_no 437\n",
            "Image_no 438\n",
            "Image_no 439\n",
            "Image_no 440\n",
            "Image_no 441\n",
            "Image_no 442\n",
            "Image_no 443\n",
            "Image_no 444\n",
            "Image_no 445\n",
            "Image_no 446\n",
            "Image_no 447\n",
            "Image_no 448\n",
            "Image_no 449\n",
            "Image_no 450\n",
            "Image_no 451\n",
            "Image_no 452\n",
            "Image_no 453\n",
            "Image_no 454\n",
            "Image_no 455\n",
            "Image_no 456\n",
            "Image_no 457\n",
            "Image_no 458\n",
            "Image_no 459\n",
            "Image_no 460\n",
            "Image_no 461\n",
            "Image_no 462\n",
            "Image_no 463\n",
            "Image_no 464\n",
            "Image_no 465\n",
            "Image_no 466\n",
            "Image_no 467\n",
            "Image_no 468\n",
            "Image_no 469\n",
            "Image_no 470\n",
            "Image_no 471\n",
            "Image_no 472\n",
            "Image_no 473\n",
            "Image_no 474\n",
            "Image_no 475\n",
            "Image_no 476\n",
            "Image_no 477\n",
            "Image_no 478\n",
            "Image_no 479\n",
            "Image_no 480\n",
            "Image_no 481\n",
            "Image_no 482\n",
            "Image_no 483\n",
            "Image_no 484\n",
            "Image_no 485\n",
            "Image_no 486\n",
            "Image_no 487\n",
            "Image_no 488\n",
            "Image_no 489\n",
            "Image_no 490\n",
            "Image_no 491\n",
            "Image_no 492\n",
            "Image_no 493\n",
            "Image_no 494\n",
            "Image_no 495\n",
            "Image_no 496\n",
            "Image_no 497\n",
            "Image_no 498\n",
            "Image_no 499\n",
            "Image_no 500\n",
            "Image_no 501\n",
            "Image_no 502\n",
            "Image_no 503\n",
            "Image_no 504\n",
            "Image_no 505\n",
            "Image_no 506\n",
            "Image_no 507\n",
            "Image_no 508\n",
            "Image_no 509\n",
            "Image_no 510\n",
            "Image_no 511\n",
            "Image_no 512\n",
            "Image_no 513\n",
            "Image_no 514\n",
            "Image_no 515\n",
            "Image_no 516\n",
            "Image_no 517\n",
            "Image_no 518\n",
            "Image_no 519\n",
            "Image_no 520\n",
            "Image_no 521\n",
            "Image_no 522\n",
            "Image_no 523\n",
            "Image_no 524\n",
            "Image_no 525\n",
            "Image_no 526\n",
            "Image_no 527\n",
            "Image_no 528\n",
            "Image_no 529\n",
            "Image_no 530\n",
            "Image_no 531\n",
            "Image_no 532\n",
            "Image_no 533\n",
            "Image_no 534\n",
            "Image_no 535\n",
            "Image_no 536\n",
            "Image_no 537\n",
            "Image_no 538\n",
            "Image_no 539\n",
            "Image_no 540\n",
            "Image_no 541\n",
            "Image_no 542\n",
            "Image_no 543\n",
            "Image_no 544\n",
            "Image_no 545\n",
            "Image_no 546\n",
            "Image_no 547\n",
            "Image_no 548\n",
            "Image_no 549\n",
            "Image_no 550\n",
            "Image_no 551\n",
            "Image_no 552\n",
            "Image_no 553\n",
            "Image_no 554\n",
            "Image_no 555\n",
            "Image_no 556\n",
            "Image_no 557\n",
            "Image_no 558\n",
            "Image_no 559\n",
            "Image_no 560\n",
            "Image_no 561\n",
            "Image_no 562\n",
            "Image_no 563\n",
            "Image_no 564\n",
            "Image_no 565\n",
            "Image_no 566\n",
            "Image_no 567\n",
            "Image_no 568\n",
            "Image_no 569\n",
            "Image_no 570\n",
            "Image_no 571\n",
            "Image_no 572\n",
            "Image_no 573\n",
            "Image_no 574\n",
            "Image_no 575\n",
            "Image_no 576\n",
            "Image_no 577\n",
            "Image_no 578\n",
            "Image_no 579\n",
            "Image_no 580\n",
            "Image_no 581\n",
            "Image_no 582\n",
            "Image_no 583\n",
            "Image_no 584\n",
            "Image_no 585\n",
            "Image_no 586\n",
            "Image_no 587\n",
            "Image_no 588\n",
            "Image_no 589\n",
            "Image_no 590\n",
            "Image_no 591\n",
            "Image_no 592\n",
            "Image_no 593\n",
            "Image_no 594\n",
            "Image_no 595\n",
            "Image_no 596\n",
            "Image_no 597\n",
            "Image_no 598\n",
            "Image_no 599\n",
            "Image_no 600\n",
            "Image_no 601\n",
            "Image_no 602\n",
            "Image_no 603\n",
            "Image_no 604\n",
            "Image_no 605\n",
            "Image_no 606\n",
            "Image_no 607\n",
            "Image_no 608\n",
            "Image_no 609\n",
            "Image_no 610\n",
            "Image_no 611\n",
            "Image_no 612\n",
            "Image_no 613\n",
            "Image_no 614\n",
            "Image_no 615\n",
            "Image_no 616\n",
            "Image_no 617\n",
            "Image_no 618\n",
            "Image_no 619\n",
            "Image_no 620\n",
            "Image_no 621\n",
            "Image_no 622\n",
            "Image_no 623\n",
            "Image_no 624\n",
            "Image_no 625\n",
            "Image_no 626\n",
            "Image_no 627\n",
            "Image_no 628\n",
            "Image_no 629\n",
            "Image_no 630\n",
            "Image_no 631\n",
            "Image_no 632\n",
            "Image_no 633\n",
            "Image_no 634\n",
            "Image_no 635\n",
            "Image_no 636\n",
            "Image_no 637\n",
            "Image_no 638\n",
            "Image_no 639\n",
            "Image_no 640\n",
            "Image_no 641\n",
            "Image_no 642\n",
            "Image_no 643\n",
            "Image_no 644\n",
            "Image_no 645\n",
            "Image_no 646\n",
            "Image_no 647\n",
            "Image_no 648\n",
            "Image_no 649\n",
            "Image_no 650\n",
            "Image_no 651\n",
            "Image_no 652\n",
            "Image_no 653\n",
            "Image_no 654\n",
            "Image_no 655\n",
            "Image_no 656\n",
            "Image_no 657\n",
            "Image_no 658\n",
            "Image_no 659\n",
            "Image_no 660\n",
            "Image_no 661\n",
            "Image_no 662\n",
            "Image_no 663\n",
            "Image_no 664\n",
            "Image_no 665\n",
            "Image_no 666\n",
            "Image_no 667\n",
            "Image_no 668\n",
            "Image_no 669\n",
            "Image_no 670\n",
            "Image_no 671\n",
            "Image_no 672\n",
            "Image_no 673\n",
            "Image_no 674\n",
            "Image_no 675\n",
            "Image_no 676\n",
            "Image_no 677\n",
            "Image_no 678\n",
            "Image_no 679\n",
            "Image_no 680\n",
            "Image_no 681\n",
            "Image_no 682\n",
            "Image_no 683\n",
            "Image_no 684\n",
            "Image_no 685\n",
            "Image_no 686\n",
            "Image_no 687\n",
            "Image_no 688\n",
            "Image_no 689\n",
            "Image_no 690\n",
            "Image_no 691\n",
            "Image_no 692\n",
            "Image_no 693\n",
            "Image_no 694\n",
            "Image_no 695\n",
            "Image_no 696\n",
            "Image_no 697\n",
            "Image_no 698\n",
            "Image_no 699\n",
            "Image_no 700\n",
            "Image_no 701\n",
            "Image_no 702\n",
            "Image_no 703\n",
            "Image_no 704\n",
            "Image_no 705\n",
            "Image_no 706\n",
            "Image_no 707\n",
            "Image_no 708\n",
            "Image_no 709\n",
            "Image_no 710\n",
            "Image_no 711\n",
            "Image_no 712\n",
            "Image_no 713\n",
            "Image_no 714\n",
            "Image_no 715\n",
            "Image_no 716\n",
            "Image_no 717\n",
            "Image_no 718\n",
            "Image_no 719\n",
            "Image_no 720\n",
            "Image_no 721\n",
            "Image_no 722\n",
            "Image_no 723\n",
            "Image_no 724\n",
            "Image_no 725\n",
            "Image_no 726\n",
            "Image_no 727\n",
            "Image_no 728\n",
            "Image_no 729\n",
            "Image_no 730\n",
            "Image_no 731\n",
            "Image_no 732\n",
            "Image_no 733\n",
            "Image_no 734\n",
            "Image_no 735\n",
            "Image_no 736\n",
            "Image_no 737\n",
            "Image_no 738\n",
            "Image_no 739\n",
            "Image_no 740\n",
            "Image_no 741\n",
            "Image_no 742\n",
            "Image_no 743\n",
            "Image_no 744\n",
            "Image_no 745\n",
            "Image_no 746\n",
            "Image_no 747\n",
            "Image_no 748\n",
            "Image_no 749\n",
            "Image_no 750\n",
            "Image_no 751\n",
            "Image_no 752\n",
            "Image_no 753\n",
            "Image_no 754\n",
            "Image_no 755\n",
            "Image_no 756\n",
            "Image_no 757\n",
            "Image_no 758\n",
            "Image_no 759\n",
            "Image_no 760\n",
            "Image_no 761\n",
            "Image_no 762\n",
            "Image_no 763\n",
            "Image_no 764\n",
            "Image_no 765\n",
            "Image_no 766\n",
            "Image_no 767\n",
            "Image_no 768\n",
            "Image_no 769\n",
            "Image_no 770\n",
            "Image_no 771\n",
            "Image_no 772\n",
            "Image_no 773\n",
            "Image_no 774\n",
            "Image_no 775\n",
            "Image_no 776\n",
            "Image_no 777\n",
            "Image_no 778\n",
            "Image_no 779\n",
            "Image_no 780\n",
            "Image_no 781\n",
            "Image_no 782\n",
            "Image_no 783\n",
            "Image_no 784\n",
            "Image_no 785\n",
            "Image_no 786\n",
            "Image_no 787\n",
            "Image_no 788\n",
            "Image_no 789\n",
            "Image_no 790\n",
            "Image_no 791\n",
            "Image_no 792\n",
            "Image_no 793\n",
            "Image_no 794\n",
            "Image_no 795\n",
            "Image_no 796\n",
            "Image_no 797\n",
            "Image_no 798\n",
            "Image_no 799\n",
            "Image_no 800\n",
            "Image_no 801\n",
            "Image_no 802\n",
            "Image_no 803\n",
            "Image_no 804\n",
            "Image_no 805\n",
            "Image_no 806\n",
            "Image_no 807\n",
            "Image_no 808\n",
            "Image_no 809\n",
            "Image_no 810\n",
            "Image_no 811\n",
            "Image_no 812\n",
            "Image_no 813\n",
            "Image_no 814\n",
            "Image_no 815\n",
            "Image_no 816\n",
            "Image_no 817\n",
            "Image_no 818\n",
            "Image_no 819\n",
            "Image_no 820\n",
            "Image_no 821\n",
            "Image_no 822\n",
            "Image_no 823\n",
            "Image_no 824\n",
            "Image_no 825\n",
            "Image_no 826\n",
            "Image_no 827\n",
            "Image_no 828\n",
            "Image_no 829\n",
            "Image_no 830\n",
            "Image_no 831\n",
            "Image_no 832\n",
            "Image_no 833\n",
            "Image_no 834\n",
            "Image_no 835\n",
            "Image_no 836\n",
            "Image_no 837\n",
            "Image_no 838\n",
            "Image_no 839\n",
            "Image_no 840\n",
            "Image_no 841\n",
            "Image_no 842\n",
            "Image_no 843\n",
            "Image_no 844\n",
            "Image_no 845\n",
            "Image_no 846\n",
            "Image_no 847\n",
            "Image_no 848\n",
            "Image_no 849\n",
            "Image_no 850\n",
            "Image_no 851\n",
            "Image_no 852\n",
            "Image_no 853\n",
            "Image_no 854\n",
            "Image_no 855\n",
            "Image_no 856\n",
            "Image_no 857\n",
            "Image_no 858\n",
            "Image_no 859\n",
            "Image_no 860\n",
            "Image_no 861\n",
            "Image_no 862\n",
            "Image_no 863\n",
            "Image_no 864\n",
            "Image_no 865\n",
            "Image_no 866\n",
            "Image_no 867\n",
            "Image_no 868\n",
            "Image_no 869\n",
            "Image_no 870\n",
            "Image_no 871\n",
            "Image_no 872\n",
            "Image_no 873\n",
            "Image_no 874\n",
            "Image_no 875\n",
            "Image_no 876\n",
            "Image_no 877\n",
            "Image_no 878\n",
            "Image_no 879\n",
            "Image_no 880\n",
            "Image_no 881\n",
            "Image_no 882\n",
            "Image_no 883\n",
            "Image_no 884\n",
            "Image_no 885\n",
            "Image_no 886\n",
            "Image_no 887\n",
            "Image_no 888\n",
            "Image_no 889\n",
            "Image_no 890\n",
            "Image_no 891\n",
            "Image_no 892\n",
            "Image_no 893\n",
            "Image_no 894\n",
            "Image_no 895\n",
            "Image_no 896\n",
            "Image_no 897\n",
            "Image_no 898\n",
            "Image_no 899\n",
            "Image_no 900\n",
            "Image_no 901\n",
            "Image_no 902\n",
            "Image_no 903\n",
            "Image_no 904\n",
            "Image_no 905\n",
            "Image_no 906\n",
            "Image_no 907\n",
            "Image_no 908\n",
            "Image_no 909\n",
            "Image_no 910\n",
            "Image_no 911\n",
            "Image_no 912\n",
            "Image_no 913\n",
            "Image_no 914\n",
            "Image_no 915\n",
            "Image_no 916\n",
            "Image_no 917\n",
            "Image_no 918\n",
            "Image_no 919\n",
            "Image_no 920\n",
            "Image_no 921\n",
            "Image_no 922\n",
            "Image_no 923\n",
            "Image_no 924\n",
            "Image_no 925\n",
            "Image_no 926\n",
            "Image_no 927\n",
            "Image_no 928\n",
            "Image_no 929\n",
            "Image_no 930\n",
            "Image_no 931\n",
            "Image_no 932\n",
            "Image_no 933\n",
            "Image_no 934\n",
            "Image_no 935\n",
            "Image_no 936\n",
            "Image_no 937\n",
            "Image_no 938\n",
            "Image_no 939\n",
            "Image_no 940\n",
            "Image_no 941\n",
            "Image_no 942\n",
            "Image_no 943\n",
            "Image_no 944\n",
            "Image_no 945\n",
            "Image_no 946\n",
            "Image_no 947\n",
            "Image_no 948\n",
            "Image_no 949\n",
            "Image_no 950\n",
            "Image_no 951\n",
            "Image_no 952\n",
            "Image_no 953\n",
            "Image_no 954\n",
            "Image_no 955\n",
            "Image_no 956\n",
            "Image_no 957\n",
            "Image_no 958\n",
            "Image_no 959\n",
            "Image_no 960\n",
            "Image_no 961\n",
            "Image_no 962\n",
            "Image_no 963\n",
            "Image_no 964\n",
            "Image_no 965\n",
            "Image_no 966\n",
            "Image_no 967\n",
            "Image_no 968\n",
            "Image_no 969\n",
            "Image_no 970\n",
            "Image_no 971\n",
            "Image_no 972\n",
            "Image_no 973\n",
            "Image_no 974\n",
            "Image_no 975\n",
            "Image_no 976\n",
            "Image_no 977\n",
            "Image_no 978\n",
            "Image_no 979\n",
            "Image_no 980\n",
            "Image_no 981\n",
            "Image_no 982\n",
            "Image_no 983\n",
            "Image_no 984\n",
            "Image_no 985\n",
            "Image_no 986\n",
            "Image_no 987\n",
            "Image_no 988\n",
            "Image_no 989\n",
            "Image_no 990\n",
            "Image_no 991\n",
            "Image_no 992\n",
            "Image_no 993\n",
            "Image_no 994\n",
            "Image_no 995\n",
            "Image_no 996\n",
            "Image_no 997\n",
            "Image_no 998\n",
            "Image_no 999\n",
            "Image_no 1000\n",
            "Image_no 1001\n",
            "Image_no 1002\n",
            "Image_no 1003\n",
            "Image_no 1004\n",
            "Image_no 1005\n",
            "Image_no 1006\n",
            "Image_no 1007\n",
            "Image_no 1008\n",
            "Image_no 1009\n",
            "Image_no 1010\n",
            "Image_no 1011\n",
            "Image_no 1012\n",
            "Image_no 1013\n",
            "Image_no 1014\n",
            "Image_no 1015\n",
            "Image_no 1016\n",
            "Image_no 1017\n",
            "Image_no 1018\n",
            "Image_no 1019\n",
            "Image_no 1020\n",
            "Image_no 1021\n",
            "Image_no 1022\n",
            "Image_no 1023\n",
            "Image_no 1024\n",
            "Image_no 1025\n",
            "Image_no 1026\n",
            "Image_no 1027\n",
            "Image_no 1028\n",
            "Image_no 1029\n",
            "Image_no 1030\n",
            "Image_no 1031\n",
            "Image_no 1032\n",
            "Image_no 1033\n",
            "Image_no 1034\n",
            "Image_no 1035\n",
            "Image_no 1036\n",
            "Image_no 1037\n",
            "Image_no 1038\n",
            "Image_no 1039\n",
            "Image_no 1040\n",
            "Image_no 1041\n",
            "Image_no 1042\n",
            "Image_no 1043\n",
            "Image_no 1044\n",
            "Image_no 1045\n",
            "Image_no 1046\n",
            "Image_no 1047\n",
            "Image_no 1048\n",
            "Image_no 1049\n",
            "Image_no 1050\n",
            "Image_no 1051\n",
            "Image_no 1052\n",
            "Image_no 1053\n",
            "Image_no 1054\n",
            "Image_no 1055\n",
            "Image_no 1056\n",
            "Image_no 1057\n",
            "Image_no 1058\n",
            "Image_no 1059\n",
            "Image_no 1060\n",
            "Image_no 1061\n",
            "Image_no 1062\n",
            "Image_no 1063\n",
            "Image_no 1064\n",
            "Image_no 1065\n",
            "Image_no 1066\n",
            "Image_no 1067\n",
            "Image_no 1068\n",
            "Image_no 1069\n",
            "Image_no 1070\n",
            "Image_no 1071\n",
            "Image_no 1072\n",
            "Image_no 1073\n",
            "Image_no 1074\n",
            "Image_no 1075\n",
            "Image_no 1076\n",
            "Image_no 1077\n",
            "Image_no 1078\n",
            "Image_no 1079\n",
            "Image_no 1080\n",
            "Image_no 1081\n",
            "Image_no 1082\n",
            "Image_no 1083\n",
            "Image_no 1084\n",
            "Image_no 1085\n",
            "Image_no 1086\n",
            "Image_no 1087\n",
            "Image_no 1088\n",
            "Image_no 1089\n",
            "Image_no 1090\n",
            "Image_no 1091\n",
            "Image_no 1092\n",
            "Image_no 1093\n",
            "Image_no 1094\n",
            "Image_no 1095\n",
            "Image_no 1096\n",
            "Image_no 1097\n",
            "Image_no 1098\n",
            "Image_no 1099\n",
            "Image_no 1100\n",
            "Image_no 1101\n",
            "Image_no 1102\n",
            "Image_no 1103\n",
            "Image_no 1104\n",
            "Image_no 1105\n",
            "Image_no 1106\n",
            "Image_no 1107\n",
            "Image_no 1108\n",
            "Image_no 1109\n",
            "Image_no 1110\n",
            "Image_no 1111\n",
            "Image_no 1112\n",
            "Image_no 1113\n",
            "Image_no 1114\n",
            "Image_no 1115\n",
            "Image_no 1116\n",
            "Image_no 1117\n",
            "Image_no 1118\n",
            "Image_no 1119\n",
            "Image_no 1120\n",
            "Image_no 1121\n",
            "Image_no 1122\n",
            "Image_no 1123\n",
            "Image_no 1124\n",
            "Image_no 1125\n",
            "Image_no 1126\n",
            "Image_no 1127\n",
            "Image_no 1128\n",
            "Image_no 1129\n",
            "Image_no 1130\n",
            "Image_no 1131\n",
            "Image_no 1132\n",
            "Image_no 1133\n",
            "Image_no 1134\n",
            "Image_no 1135\n",
            "Image_no 1136\n",
            "Image_no 1137\n",
            "Image_no 1138\n",
            "Image_no 1139\n",
            "Image_no 1140\n",
            "Image_no 1141\n",
            "Image_no 1142\n",
            "Image_no 1143\n",
            "Image_no 1144\n",
            "Image_no 1145\n",
            "Image_no 1146\n",
            "Image_no 1147\n",
            "Image_no 1148\n",
            "Image_no 1149\n",
            "Image_no 1150\n",
            "Image_no 1151\n",
            "Image_no 1152\n",
            "Image_no 1153\n",
            "Image_no 1154\n",
            "Image_no 1155\n",
            "Image_no 1156\n",
            "Image_no 1157\n",
            "Image_no 1158\n",
            "Image_no 1159\n",
            "Image_no 1160\n",
            "Image_no 1161\n",
            "Image_no 1162\n",
            "Image_no 1163\n",
            "Image_no 1164\n",
            "Image_no 1165\n",
            "Image_no 1166\n",
            "Image_no 1167\n",
            "Image_no 1168\n",
            "Image_no 1169\n",
            "Image_no 1170\n",
            "Image_no 1171\n",
            "Image_no 1172\n",
            "Image_no 1173\n",
            "Image_no 1174\n",
            "Image_no 1175\n",
            "Image_no 1176\n",
            "Image_no 1177\n",
            "Image_no 1178\n",
            "Image_no 1179\n",
            "Image_no 1180\n",
            "Image_no 1181\n",
            "Image_no 1182\n",
            "Image_no 1183\n",
            "Image_no 1184\n",
            "Image_no 1185\n",
            "Image_no 1186\n",
            "Image_no 1187\n",
            "Image_no 1188\n",
            "Image_no 1189\n",
            "Image_no 1190\n",
            "Image_no 1191\n",
            "Image_no 1192\n",
            "Image_no 1193\n",
            "Image_no 1194\n",
            "Image_no 1195\n",
            "Image_no 1196\n",
            "Image_no 1197\n",
            "Image_no 1198\n",
            "Image_no 1199\n",
            "Image_no 1200\n",
            "Image_no 1201\n",
            "Image_no 1202\n",
            "Image_no 1203\n",
            "Image_no 1204\n",
            "Image_no 1205\n",
            "Image_no 1206\n",
            "Image_no 1207\n",
            "Image_no 1208\n",
            "Image_no 1209\n",
            "Image_no 1210\n",
            "Image_no 1211\n",
            "Image_no 1212\n",
            "Image_no 1213\n",
            "Image_no 1214\n",
            "Image_no 1215\n",
            "Image_no 1216\n",
            "Image_no 1217\n",
            "Image_no 1218\n",
            "Image_no 1219\n",
            "Image_no 1220\n",
            "Image_no 1221\n",
            "Image_no 1222\n",
            "Image_no 1223\n",
            "Image_no 1224\n",
            "Image_no 1225\n",
            "Image_no 1226\n",
            "Image_no 1227\n",
            "Image_no 1228\n",
            "Image_no 1229\n",
            "Image_no 1230\n",
            "Image_no 1231\n",
            "Image_no 1232\n",
            "Image_no 1233\n",
            "Image_no 1234\n",
            "Image_no 1235\n",
            "Image_no 1236\n",
            "Image_no 1237\n",
            "Image_no 1238\n",
            "Image_no 1239\n",
            "Image_no 1240\n",
            "Image_no 1241\n",
            "Image_no 1242\n",
            "Image_no 1243\n",
            "Image_no 1244\n",
            "Image_no 1245\n",
            "Image_no 1246\n",
            "Image_no 1247\n",
            "Image_no 1248\n",
            "Image_no 1249\n",
            "Image_no 1250\n",
            "Image_no 1251\n",
            "Image_no 1252\n",
            "Image_no 1253\n",
            "Image_no 1254\n",
            "Image_no 1255\n",
            "Image_no 1256\n",
            "Image_no 1257\n",
            "Image_no 1258\n",
            "Image_no 1259\n",
            "Image_no 1260\n",
            "Image_no 1261\n",
            "Image_no 1262\n",
            "Image_no 1263\n",
            "Image_no 1264\n",
            "Image_no 1265\n",
            "Image_no 1266\n",
            "Image_no 1267\n",
            "Image_no 1268\n",
            "Image_no 1269\n",
            "Image_no 1270\n",
            "Image_no 1271\n",
            "Image_no 1272\n",
            "Image_no 1273\n",
            "Image_no 1274\n",
            "Image_no 1275\n",
            "Image_no 1276\n",
            "Image_no 1277\n",
            "Image_no 1278\n",
            "Image_no 1279\n",
            "Image_no 1280\n",
            "Image_no 1281\n",
            "Image_no 1282\n",
            "Image_no 1283\n",
            "Image_no 1284\n",
            "Image_no 1285\n",
            "Image_no 1286\n",
            "Image_no 1287\n",
            "Image_no 1288\n",
            "Image_no 1289\n",
            "Image_no 1290\n",
            "Image_no 1291\n",
            "Image_no 1292\n",
            "Image_no 1293\n",
            "Image_no 1294\n",
            "Image_no 1295\n",
            "Image_no 1296\n",
            "Image_no 1297\n",
            "Image_no 1298\n",
            "Image_no 1299\n",
            "Image_no 1300\n",
            "Image_no 1301\n",
            "Image_no 1302\n",
            "Image_no 1303\n",
            "Image_no 1304\n",
            "Image_no 1305\n",
            "Image_no 1306\n",
            "Image_no 1307\n",
            "Image_no 1308\n",
            "Image_no 1309\n",
            "Image_no 1310\n",
            "Image_no 1311\n",
            "Image_no 1312\n",
            "Image_no 1313\n",
            "Image_no 1314\n",
            "Image_no 1315\n",
            "Image_no 1316\n",
            "Image_no 1317\n",
            "Image_no 1318\n",
            "Image_no 1319\n",
            "Image_no 1320\n",
            "Image_no 1321\n",
            "Image_no 1322\n",
            "Image_no 1323\n",
            "Image_no 1324\n",
            "Image_no 1325\n",
            "Image_no 1326\n",
            "Image_no 1327\n",
            "Image_no 1328\n",
            "Image_no 1329\n",
            "Image_no 1330\n",
            "Image_no 1331\n",
            "Image_no 1332\n",
            "Image_no 1333\n",
            "Image_no 1334\n",
            "Image_no 1335\n",
            "Image_no 1336\n",
            "Image_no 1337\n",
            "Image_no 1338\n",
            "Image_no 1339\n",
            "Image_no 1340\n",
            "Image_no 1341\n",
            "Image_no 1342\n",
            "Image_no 1343\n",
            "Image_no 1344\n",
            "Image_no 1345\n",
            "Image_no 1346\n",
            "Image_no 1347\n",
            "Image_no 1348\n",
            "Image_no 1349\n",
            "Image_no 1350\n",
            "Image_no 1351\n",
            "Image_no 1352\n",
            "Image_no 1353\n",
            "Image_no 1354\n",
            "Image_no 1355\n",
            "Image_no 1356\n",
            "Image_no 1357\n",
            "Image_no 1358\n",
            "Image_no 1359\n",
            "Image_no 1360\n",
            "Image_no 1361\n",
            "Image_no 1362\n",
            "Image_no 1363\n",
            "Image_no 1364\n",
            "Image_no 1365\n",
            "Image_no 1366\n",
            "Image_no 1367\n",
            "Image_no 1368\n",
            "Image_no 1369\n",
            "Image_no 1370\n",
            "Image_no 1371\n",
            "Image_no 1372\n",
            "Image_no 1373\n",
            "Image_no 1374\n",
            "Image_no 1375\n",
            "Image_no 1376\n",
            "Image_no 1377\n",
            "Image_no 1378\n",
            "Image_no 1379\n",
            "Image_no 1380\n",
            "Image_no 1381\n",
            "Image_no 1382\n",
            "Image_no 1383\n",
            "Image_no 1384\n",
            "Image_no 1385\n",
            "Image_no 1386\n",
            "Image_no 1387\n",
            "Image_no 1388\n",
            "Image_no 1389\n",
            "Image_no 1390\n",
            "Image_no 1391\n",
            "Image_no 1392\n",
            "Image_no 1393\n",
            "Image_no 1394\n",
            "Image_no 1395\n",
            "Image_no 1396\n",
            "Image_no 1397\n",
            "Image_no 1398\n",
            "Image_no 1399\n",
            "Image_no 1400\n",
            "Image_no 1401\n",
            "Image_no 1402\n",
            "Image_no 1403\n",
            "Image_no 1404\n",
            "Image_no 1405\n",
            "Image_no 1406\n",
            "Image_no 1407\n",
            "Image_no 1408\n",
            "Image_no 1409\n",
            "Image_no 1410\n",
            "Image_no 1411\n",
            "Image_no 1412\n",
            "Image_no 1413\n",
            "Image_no 1414\n",
            "Image_no 1415\n",
            "Image_no 1416\n",
            "Image_no 1417\n",
            "Image_no 1418\n",
            "Image_no 1419\n",
            "Image_no 1420\n",
            "Image_no 1421\n",
            "Image_no 1422\n",
            "Image_no 1423\n",
            "Image_no 1424\n",
            "Image_no 1425\n",
            "Image_no 1426\n",
            "Image_no 1427\n",
            "Image_no 1428\n",
            "Image_no 1429\n",
            "Image_no 1430\n",
            "Image_no 1431\n",
            "Image_no 1432\n",
            "Image_no 1433\n",
            "Image_no 1434\n",
            "Image_no 1435\n",
            "Image_no 1436\n",
            "Image_no 1437\n",
            "Image_no 1438\n",
            "Image_no 1439\n",
            "Image_no 1440\n",
            "Image_no 1441\n",
            "Image_no 1442\n",
            "Image_no 1443\n",
            "Image_no 1444\n",
            "Image_no 1445\n",
            "Image_no 1446\n",
            "Image_no 1447\n",
            "Image_no 1448\n",
            "Image_no 1449\n",
            "Image_no 1450\n",
            "Image_no 1451\n",
            "Image_no 1452\n",
            "Image_no 1453\n",
            "Image_no 1454\n",
            "Image_no 1455\n",
            "Image_no 1456\n",
            "Image_no 1457\n",
            "Image_no 1458\n",
            "Image_no 1459\n",
            "Image_no 1460\n",
            "Image_no 1461\n",
            "Image_no 1462\n",
            "Image_no 1463\n",
            "Image_no 1464\n",
            "Image_no 1465\n",
            "Image_no 1466\n",
            "Image_no 1467\n",
            "Image_no 1468\n",
            "Image_no 1469\n",
            "Image_no 1470\n",
            "Image_no 1471\n",
            "Image_no 1472\n",
            "Image_no 1473\n",
            "Image_no 1474\n",
            "Image_no 1475\n",
            "Image_no 1476\n",
            "Image_no 1477\n",
            "Image_no 1478\n",
            "Image_no 1479\n",
            "Image_no 1480\n",
            "Image_no 1481\n",
            "Image_no 1482\n",
            "Image_no 1483\n",
            "Image_no 1484\n",
            "Image_no 1485\n",
            "Image_no 1486\n",
            "Image_no 1487\n",
            "Image_no 1488\n",
            "Image_no 1489\n",
            "Image_no 1490\n",
            "Image_no 1491\n",
            "Image_no 1492\n",
            "Image_no 1493\n",
            "Image_no 1494\n",
            "Image_no 1495\n",
            "Image_no 1496\n",
            "Image_no 1497\n",
            "Image_no 1498\n",
            "Image_no 1499\n",
            "Image_no 1500\n",
            "Image_no 1501\n",
            "Image_no 1502\n",
            "Image_no 1503\n",
            "Image_no 1504\n",
            "Image_no 1505\n",
            "Image_no 1506\n",
            "Image_no 1507\n",
            "Image_no 1508\n",
            "Image_no 1509\n",
            "Image_no 1510\n",
            "Image_no 1511\n",
            "Image_no 1512\n",
            "Image_no 1513\n",
            "Image_no 1514\n",
            "Image_no 1515\n",
            "Image_no 1516\n",
            "Image_no 1517\n",
            "Image_no 1518\n",
            "Image_no 1519\n",
            "Image_no 1520\n",
            "Image_no 1521\n",
            "Image_no 1522\n",
            "Image_no 1523\n",
            "Image_no 1524\n",
            "Image_no 1525\n",
            "Image_no 1526\n",
            "Image_no 1527\n",
            "Image_no 1528\n",
            "Image_no 1529\n",
            "Image_no 1530\n",
            "Image_no 1531\n",
            "Image_no 1532\n",
            "Image_no 1533\n",
            "Image_no 1534\n",
            "Image_no 1535\n",
            "Image_no 1536\n",
            "Image_no 1537\n",
            "Image_no 1538\n",
            "Image_no 1539\n",
            "Image_no 1540\n",
            "Image_no 1541\n",
            "Image_no 1542\n",
            "Image_no 1543\n",
            "Image_no 1544\n",
            "Image_no 1545\n",
            "Image_no 1546\n",
            "Image_no 1547\n",
            "Image_no 1548\n",
            "Image_no 1549\n",
            "Image_no 1550\n",
            "Image_no 1551\n",
            "Image_no 1552\n",
            "Image_no 1553\n",
            "Image_no 1554\n",
            "Image_no 1555\n",
            "Image_no 1556\n",
            "Image_no 1557\n",
            "Image_no 1558\n",
            "Image_no 1559\n",
            "Image_no 1560\n",
            "Image_no 1561\n",
            "Image_no 1562\n",
            "Image_no 1563\n",
            "Image_no 1564\n",
            "Image_no 1565\n",
            "Image_no 1566\n",
            "Image_no 1567\n",
            "Image_no 1568\n",
            "Image_no 1569\n",
            "Image_no 1570\n",
            "Image_no 1571\n",
            "Image_no 1572\n",
            "Image_no 1573\n",
            "Image_no 1574\n",
            "Image_no 1575\n",
            "Image_no 1576\n",
            "Image_no 1577\n",
            "Image_no 1578\n",
            "Image_no 1579\n",
            "Image_no 1580\n",
            "Image_no 1581\n",
            "Image_no 1582\n",
            "Image_no 1583\n",
            "Image_no 1584\n",
            "Image_no 1585\n",
            "Image_no 1586\n",
            "Image_no 1587\n",
            "Image_no 1588\n",
            "Image_no 1589\n",
            "Image_no 1590\n",
            "Image_no 1591\n",
            "Image_no 1592\n",
            "Image_no 1593\n",
            "Image_no 1594\n",
            "Image_no 1595\n",
            "Image_no 1596\n",
            "Image_no 1597\n",
            "Image_no 1598\n",
            "Image_no 1599\n",
            "Image_no 1600\n",
            "Image_no 1601\n",
            "Image_no 1602\n",
            "Image_no 1603\n",
            "Image_no 1604\n",
            "Image_no 1605\n",
            "Image_no 1606\n",
            "Image_no 1607\n",
            "Image_no 1608\n",
            "Image_no 1609\n",
            "Image_no 1610\n",
            "Image_no 1611\n",
            "Image_no 1612\n",
            "Image_no 1613\n",
            "Image_no 1614\n",
            "Image_no 1615\n",
            "Image_no 1616\n",
            "Image_no 1617\n",
            "Image_no 1618\n",
            "Image_no 1619\n",
            "Image_no 1620\n",
            "Image_no 1621\n",
            "Image_no 1622\n",
            "Image_no 1623\n",
            "Image_no 1624\n",
            "Image_no 1625\n",
            "Image_no 1626\n",
            "Image_no 1627\n",
            "Image_no 1628\n",
            "Image_no 1629\n",
            "Image_no 1630\n",
            "Image_no 1631\n",
            "Image_no 1632\n",
            "Image_no 1633\n",
            "Image_no 1634\n",
            "Image_no 1635\n",
            "Image_no 1636\n",
            "Image_no 1637\n",
            "Image_no 1638\n",
            "Image_no 1639\n",
            "Image_no 1640\n",
            "Image_no 1641\n",
            "Image_no 1642\n",
            "Image_no 1643\n",
            "Image_no 1644\n",
            "Image_no 1645\n",
            "Image_no 1646\n",
            "Image_no 1647\n",
            "Image_no 1648\n",
            "Image_no 1649\n",
            "Image_no 1650\n",
            "Image_no 1651\n",
            "Image_no 1652\n",
            "Image_no 1653\n",
            "Image_no 1654\n",
            "Image_no 1655\n",
            "Image_no 1656\n",
            "Image_no 1657\n",
            "Image_no 1658\n",
            "Image_no 1659\n",
            "Image_no 1660\n",
            "Image_no 1661\n",
            "Image_no 1662\n",
            "Image_no 1663\n",
            "Image_no 1664\n",
            "Image_no 1665\n",
            "Image_no 1666\n",
            "Image_no 1667\n",
            "Image_no 1668\n",
            "Image_no 1669\n",
            "Image_no 1670\n",
            "Image_no 1671\n",
            "Image_no 1672\n",
            "Image_no 1673\n",
            "Image_no 1674\n",
            "Image_no 1675\n",
            "Image_no 1676\n",
            "Image_no 1677\n",
            "Image_no 1678\n",
            "Image_no 1679\n",
            "Image_no 1680\n",
            "Image_no 1681\n",
            "Image_no 1682\n",
            "Image_no 1683\n",
            "Image_no 1684\n",
            "Image_no 1685\n",
            "Image_no 1686\n",
            "Image_no 1687\n",
            "Image_no 1688\n",
            "Image_no 1689\n",
            "Image_no 1690\n",
            "Image_no 1691\n",
            "Image_no 1692\n",
            "Image_no 1693\n",
            "Image_no 1694\n",
            "Image_no 1695\n",
            "Image_no 1696\n",
            "Image_no 1697\n",
            "Image_no 1698\n",
            "Image_no 1699\n",
            "Image_no 1700\n",
            "Image_no 1701\n",
            "Image_no 1702\n",
            "Image_no 1703\n",
            "Image_no 1704\n",
            "Image_no 1705\n",
            "Image_no 1706\n",
            "Image_no 1707\n",
            "Image_no 1708\n",
            "Image_no 1709\n",
            "Image_no 1710\n",
            "Image_no 1711\n",
            "Image_no 1712\n",
            "Image_no 1713\n",
            "Image_no 1714\n",
            "Image_no 1715\n",
            "Image_no 1716\n",
            "Image_no 1717\n",
            "Image_no 1718\n",
            "Image_no 1719\n",
            "Image_no 1720\n",
            "Image_no 1721\n",
            "Image_no 1722\n",
            "Image_no 1723\n",
            "Image_no 1724\n",
            "Image_no 1725\n",
            "Image_no 1726\n",
            "Image_no 1727\n",
            "Image_no 1728\n",
            "Image_no 1729\n",
            "Image_no 1730\n",
            "Image_no 1731\n",
            "Image_no 1732\n",
            "Image_no 1733\n",
            "Image_no 1734\n",
            "Image_no 1735\n",
            "Image_no 1736\n",
            "Image_no 1737\n",
            "Image_no 1738\n",
            "Image_no 1739\n",
            "Image_no 1740\n",
            "Image_no 1741\n",
            "Image_no 1742\n",
            "Image_no 1743\n",
            "Image_no 1744\n",
            "Image_no 1745\n",
            "Image_no 1746\n",
            "Image_no 1747\n",
            "Image_no 1748\n",
            "Image_no 1749\n",
            "Image_no 1750\n",
            "Image_no 1751\n",
            "Image_no 1752\n",
            "Image_no 1753\n",
            "Image_no 1754\n",
            "Image_no 1755\n",
            "Image_no 1756\n",
            "Image_no 1757\n",
            "Image_no 1758\n",
            "Image_no 1759\n",
            "Image_no 1760\n",
            "Image_no 1761\n",
            "Image_no 1762\n",
            "Image_no 1763\n",
            "Image_no 1764\n",
            "Image_no 1765\n",
            "Image_no 1766\n",
            "Image_no 1767\n",
            "Image_no 1768\n",
            "Image_no 1769\n",
            "Image_no 1770\n",
            "Image_no 1771\n",
            "Image_no 1772\n",
            "Image_no 1773\n",
            "Image_no 1774\n",
            "Image_no 1775\n",
            "Image_no 1776\n",
            "Image_no 1777\n",
            "Image_no 1778\n",
            "Image_no 1779\n",
            "Image_no 1780\n",
            "Image_no 1781\n",
            "Image_no 1782\n",
            "Image_no 1783\n",
            "Image_no 1784\n",
            "Image_no 1785\n",
            "Image_no 1786\n",
            "Image_no 1787\n",
            "Image_no 1788\n",
            "Image_no 1789\n",
            "Image_no 1790\n",
            "Image_no 1791\n",
            "Image_no 1792\n",
            "Image_no 1793\n",
            "Image_no 1794\n",
            "Image_no 1795\n",
            "Image_no 1796\n",
            "Image_no 1797\n",
            "Image_no 1798\n",
            "Image_no 1799\n",
            "Image_no 1800\n",
            "Image_no 1801\n",
            "Image_no 1802\n",
            "Image_no 1803\n",
            "Image_no 1804\n",
            "Image_no 1805\n",
            "Image_no 1806\n",
            "Image_no 1807\n",
            "Image_no 1808\n",
            "Image_no 1809\n",
            "Image_no 1810\n",
            "Image_no 1811\n",
            "Image_no 1812\n",
            "Image_no 1813\n",
            "Image_no 1814\n",
            "Image_no 1815\n",
            "Image_no 1816\n",
            "Image_no 1817\n",
            "Image_no 1818\n",
            "Image_no 1819\n",
            "Image_no 1820\n",
            "Image_no 1821\n",
            "Image_no 1822\n",
            "Image_no 1823\n",
            "Image_no 1824\n",
            "Image_no 1825\n",
            "Image_no 1826\n",
            "Image_no 1827\n",
            "Image_no 1828\n",
            "Image_no 1829\n",
            "Image_no 1830\n",
            "Image_no 1831\n",
            "Image_no 1832\n",
            "Image_no 1833\n",
            "Image_no 1834\n",
            "Image_no 1835\n",
            "Image_no 1836\n",
            "Image_no 1837\n",
            "Image_no 1838\n",
            "Image_no 1839\n",
            "Image_no 1840\n",
            "Image_no 1841\n",
            "Image_no 1842\n",
            "Image_no 1843\n",
            "Image_no 1844\n",
            "Image_no 1845\n",
            "Image_no 1846\n",
            "Image_no 1847\n",
            "Image_no 1848\n",
            "Image_no 1849\n",
            "Image_no 1850\n",
            "Image_no 1851\n",
            "Image_no 1852\n",
            "Image_no 1853\n",
            "Image_no 1854\n",
            "Image_no 1855\n",
            "Image_no 1856\n",
            "Image_no 1857\n",
            "Image_no 1858\n",
            "Image_no 1859\n",
            "Image_no 1860\n",
            "Image_no 1861\n",
            "Image_no 1862\n",
            "Image_no 1863\n",
            "Image_no 1864\n",
            "Image_no 1865\n",
            "Image_no 1866\n",
            "Image_no 1867\n",
            "Image_no 1868\n",
            "Image_no 1869\n",
            "Image_no 1870\n",
            "Image_no 1871\n",
            "Image_no 1872\n",
            "Image_no 1873\n",
            "Image_no 1874\n",
            "Image_no 1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJa_mpGvAKH",
        "colab_type": "code",
        "outputId": "f76e38d0-82c3-4d90-f469-477edf47812d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "\n",
        "#test image visualization\n",
        "img=cv2.imread('testing1000.png')\n",
        "plt.imshow(img)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7c002891d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADLCAYAAACVv9NEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQh9debltAVr",
        "colab_type": "text"
      },
      "source": [
        "WE CAN SEE THE MIXING OCCURNG IN THE LATENT SPACE AND ESPECIALLY BECAUSE OF LOW DIMENSIONALITY 522 THOUGH THE POINTS IN LATENT SPACE ARE DISCRETE AND KIND OF ONE TO ONE MAPPING BUT THEY STILL SEEM TO BE QUITE CONTINUOUS. TAKING A HIGHER DIMENSIONAL DATASET MAY SHOW THE SPARSITY IN THE LATENT IN A BETTER WAY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkKRbjiJ7AOX",
        "colab_type": "code",
        "outputId": "c251fdbe-124e-4a00-f193-8b8b7f468478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "#layer3 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAHTCAYAAABiN8IeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emKAhl5VTgyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iq1TfmdTg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer2 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer2_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vZmGsyTg3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer1 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HJvYuG-CYE",
        "colab_type": "text"
      },
      "source": [
        "SO IT SEEMS THAT THE LATENT SPACE IS VERY CONTINUOUS BUT IN REALITY IT IS DISCRETE IN CASE OF A NORMAL AUTOENCODER TO VISUALIZE IT WE NEED A LARGER DIMENSION DATASET TO PROOVE IT WE NEED TO PLOT THE LATENT SPACE BY CONVERTING IT INTO TWO DIMENSIONS USING PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvIql91TgwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WZqSH9vTgtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axzG2TQeOhNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}