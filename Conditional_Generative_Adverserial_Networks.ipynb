{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional Generative Adverserial Networks",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7DpqASl3BHKYt+hxA0j0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5cdde0005f640198f07d33cc539b109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb3b84479c1941e3a89244199e9f56d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89b6aa9d150d4d2baa13f504007f30c7",
              "IPY_MODEL_e4373d3dec744d06bc0db15dc2851092"
            ]
          }
        },
        "fb3b84479c1941e3a89244199e9f56d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89b6aa9d150d4d2baa13f504007f30c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a2086037181473e8a0208c2f8b0983c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb7bd248dbe543debf32942c58589be3"
          }
        },
        "e4373d3dec744d06bc0db15dc2851092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e32446ef6e8247d68c034a159816fced",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:03&lt;00:00, 3082732.41it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7352d979e2e4df89e2029148ab72918"
          }
        },
        "3a2086037181473e8a0208c2f8b0983c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb7bd248dbe543debf32942c58589be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e32446ef6e8247d68c034a159816fced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7352d979e2e4df89e2029148ab72918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70347836dd974b6caea2dbfd2978397c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0cb9c0483e14cb0bf725733d42baf1d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95b90501ce8a474e99994a964be8743c",
              "IPY_MODEL_726b975c55664c508a2bc3104ee510d9"
            ]
          }
        },
        "f0cb9c0483e14cb0bf725733d42baf1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95b90501ce8a474e99994a964be8743c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_40407bf2a9be45fea79bed580acdd349",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b227fe520cdb4612a728ad8dbd626120"
          }
        },
        "726b975c55664c508a2bc3104ee510d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75d4b4b176be4af68bd30b35f859d33e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 99764.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07f78c68dc0242a8acfaeb413c28a860"
          }
        },
        "40407bf2a9be45fea79bed580acdd349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b227fe520cdb4612a728ad8dbd626120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75d4b4b176be4af68bd30b35f859d33e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07f78c68dc0242a8acfaeb413c28a860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c163f6e1aa14bfab9deff80b0c3bd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bb0e17c8daff4093a4ccf23d921d9134",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92dc6eb9054a4286b7b8012779902ea0",
              "IPY_MODEL_e36b1aa9e2004798b1b32082d2e26dd8"
            ]
          }
        },
        "bb0e17c8daff4093a4ccf23d921d9134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92dc6eb9054a4286b7b8012779902ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d85056e2b98c42879d4bfbdc21813a1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d69dcb0b75aa45ab8d749e7742bd2b2c"
          }
        },
        "e36b1aa9e2004798b1b32082d2e26dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cfe66093707047a4b94eb2d3b65cfffe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:01&lt;00:00, 1161221.85it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1eb26f035ee467ab7a266cd49407f08"
          }
        },
        "d85056e2b98c42879d4bfbdc21813a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d69dcb0b75aa45ab8d749e7742bd2b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfe66093707047a4b94eb2d3b65cfffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1eb26f035ee467ab7a266cd49407f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e5639f3f16e42eb9f2fe1ff5cfb4c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7526fbf010a4c0095bb86e13262d1b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b0b9ec6f8eba4abab4ae5fc802d946e3",
              "IPY_MODEL_e4188cd3cdd94b6fbb16b3aa39063a58"
            ]
          }
        },
        "a7526fbf010a4c0095bb86e13262d1b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0b9ec6f8eba4abab4ae5fc802d946e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_929a4b8dc5784540bd1848be498ef6b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96e7432cec534cba8c987aba6d54e244"
          }
        },
        "e4188cd3cdd94b6fbb16b3aa39063a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23484fda95774e61af48ab819d2ff175",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 19600.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b419d25d2a2460886c0ab4414a3b28d"
          }
        },
        "929a4b8dc5784540bd1848be498ef6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96e7432cec534cba8c987aba6d54e244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23484fda95774e61af48ab819d2ff175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b419d25d2a2460886c0ab4414a3b28d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyakjain0112/Generative-models-papers-and-code/blob/master/Conditional_Generative_Adverserial_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryF6KNvfIwsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all requirements\n",
        "import torch as torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu as Relu\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pi-2xlo2im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "f5cdde0005f640198f07d33cc539b109",
            "fb3b84479c1941e3a89244199e9f56d3",
            "89b6aa9d150d4d2baa13f504007f30c7",
            "e4373d3dec744d06bc0db15dc2851092",
            "3a2086037181473e8a0208c2f8b0983c",
            "eb7bd248dbe543debf32942c58589be3",
            "e32446ef6e8247d68c034a159816fced",
            "a7352d979e2e4df89e2029148ab72918",
            "70347836dd974b6caea2dbfd2978397c",
            "f0cb9c0483e14cb0bf725733d42baf1d",
            "95b90501ce8a474e99994a964be8743c",
            "726b975c55664c508a2bc3104ee510d9",
            "40407bf2a9be45fea79bed580acdd349",
            "b227fe520cdb4612a728ad8dbd626120",
            "75d4b4b176be4af68bd30b35f859d33e",
            "07f78c68dc0242a8acfaeb413c28a860",
            "5c163f6e1aa14bfab9deff80b0c3bd6d",
            "bb0e17c8daff4093a4ccf23d921d9134",
            "92dc6eb9054a4286b7b8012779902ea0",
            "e36b1aa9e2004798b1b32082d2e26dd8",
            "d85056e2b98c42879d4bfbdc21813a1c",
            "d69dcb0b75aa45ab8d749e7742bd2b2c",
            "cfe66093707047a4b94eb2d3b65cfffe",
            "d1eb26f035ee467ab7a266cd49407f08",
            "3e5639f3f16e42eb9f2fe1ff5cfb4c01",
            "a7526fbf010a4c0095bb86e13262d1b2",
            "b0b9ec6f8eba4abab4ae5fc802d946e3",
            "e4188cd3cdd94b6fbb16b3aa39063a58",
            "929a4b8dc5784540bd1848be498ef6b8",
            "96e7432cec534cba8c987aba6d54e244",
            "23484fda95774e61af48ab819d2ff175",
            "0b419d25d2a2460886c0ab4414a3b28d"
          ]
        },
        "outputId": "832b17e0-4da8-44d5-dc73-d54f46d27396"
      },
      "source": [
        "#using the inbuilt Dataset class where all data is loaded into the cpu memory and using getitem we can get the data by just passing the corresponding index4\n",
        "\n",
        "#VARIABLE=DATASET\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5cdde0005f640198f07d33cc539b109",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70347836dd974b6caea2dbfd2978397c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c163f6e1aa14bfab9deff80b0c3bd6d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e5639f3f16e42eb9f2fe1ff5cfb4c01",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuEHh_4vpfh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading all the data using Dataloaader inbuilt class it loads all the data again into the cpu memory without making a copy because it uses iter for it and even if suffle examples is true then also no cpy is \n",
        "#required it just reshuffles the order of the iteraror in the list. \n",
        "#NOTE: only when we will load the data for training in the training class then the data will be needed to be loaded in the gpu memory\n",
        "batch_size=100\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO6OQlJluQy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3285cb3f-2286-4281-b6fb-47bf2aceb972"
      },
      "source": [
        "\n",
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_generator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=100):\n",
        "    super(model_generator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.mean=100\n",
        "    self.class_size=10\n",
        "    self.std=100\n",
        "    self.g_input_without_label=100\n",
        "    self.g_input_dim=110\n",
        "    self.g_output_dim=self.filter_size*self.filter_size\n",
        "    self.batcher=1\n",
        "    self.labeler_batch_size=torch.from_numpy(np.zeros([self.batch_size,self.class_size],dtype='float32')).cuda()\n",
        "    self.labeler_batcher=torch.from_numpy(np.zeros([self.batcher,self.class_size],dtype='float32')).cuda()\n",
        "    self.fc1 = nn.Linear(self.g_input_dim, 128)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, self.g_output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,x_input,label,latent_space=False,latent=None,marker=False,plot=False,batcher=1):\n",
        "    #taking x_input as input for coding simplicity but not using it in any way\n",
        "    if latent_space==True or marker==True or plot==True:\n",
        "      self.x=(torch.randn(batcher,self.g_input_without_label)).cuda()\n",
        "      vec=self.labeler_batcher\n",
        "      vec[0][int(label)]=1\n",
        "      self.x=torch.cat((self.x,vec),1)\n",
        "\n",
        "    else:\n",
        "      self.x=(torch.randn(self.batch_size,self.g_input_without_label)).cuda()\n",
        "      vec=self.labeler_batch_size\n",
        "      for i in range(self.batch_size):\n",
        "        vec[i][int(label[i].item())]=1\n",
        "      self.x=torch.cat((self.x,vec),1)\n",
        "    \n",
        "    x = F.leaky_relu(self.fc1(self.x), 0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    return torch.tanh(self.fc4(x))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atPKIhw3CY_M",
        "colab_type": "code",
        "outputId": "5affa2d9-a3f7-4cac-dc6d-d1c91c7ed43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_discriminator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=100):\n",
        "    super(model_discriminator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "\n",
        "    self.class_size=10\n",
        "    self.d_input_dim=self.filter_size*self.filter_size+self.class_size\n",
        "\n",
        "    self.fc1 = nn.Linear(self.d_input_dim, 1024)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "\n",
        "  def forward(self,x,latent_space=False,latent=None,marker=False,plot=False):\n",
        "      x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "      if plot==True:\n",
        "        return x\n",
        "      if latent_space==True:\n",
        "        return x\n",
        "      if marker==True:\n",
        "        x=latent\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      return torch.sigmoid(self.fc4(x))\n",
        "\"\"\"\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x=self.layer3(x)\n",
        "    x=Relu(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.layer4(x)\n",
        "    x=Relu(x)\n",
        "    x=self.layer5(x)\n",
        "    x=torch.sigmoid(x)\n",
        "    #print(x.size())\n",
        "    #x=x.cpu().detach().numpy()\n",
        "    #x=np.array(np.amax(x,axis=1))\n",
        "    \n",
        "    \n",
        "    #print(x.size())\n",
        "\n",
        "    return x\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    if plot==True:\\n      return x\\n    if latent_space==True:\\n      return x\\n    if marker==True:\\n      x=latent\\n    x=self.layer3(x)\\n    x=Relu(x)\\n    x=torch.flatten(x,1)\\n    x=self.layer4(x)\\n    x=Relu(x)\\n    x=self.layer5(x)\\n    x=torch.sigmoid(x)\\n    #print(x.size())\\n    #x=x.cpu().detach().numpy()\\n    #x=np.array(np.amax(x,axis=1))\\n    \\n    \\n    #print(x.size())\\n\\n    return x\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zeJguY2gvG",
        "colab_type": "code",
        "outputId": "9ede507b-3aa8-4f9b-dc65-038fa5642418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "class model_generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(model_generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class model_discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(model_discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))\n",
        "        \n",
        "\"\"\"\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nclass model_generator(nn.Module):\\n    def __init__(self, g_input_dim, g_output_dim):\\n        super(model_generator, self).__init__()       \\n        self.fc1 = nn.Linear(g_input_dim, 256)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\\n\\n    \\n    # forward method\\n    def forward(self, x): \\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        return torch.tanh(self.fc4(x))\\n    \\nclass model_discriminator(nn.Module):\\n    def __init__(self, d_input_dim):\\n        super(model_discriminator, self).__init__()\\n        self.fc1 = nn.Linear(d_input_dim, 1024)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, 1)\\n    \\n    # forward method\\n    def forward(self, x):\\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        return torch.sigmoid(self.fc4(x))\\n        \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lMPEo6izHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#latent space visualization\n",
        "class modify_space(object):\n",
        "  def __init__(self,importer,test_gen,label=1,label2=2,num_latents=10):\n",
        "    self.batch_size=1\n",
        "    self.label=label\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.label2=label2\n",
        "    self.test_gen=test_gen\n",
        "    self.num_latents=num_latents\n",
        "    self.net_generator=importer.net_generator\n",
        "    self.net_discriminator=importer.net_discriminator\n",
        "    self.generate()\n",
        "    self.latent()\n",
        "\n",
        "\n",
        "  def generate(self):\n",
        "    w=0\n",
        "    e=0\n",
        "    print(1)\n",
        "    for (image,label) in self.test_gen:\n",
        "      if w==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label:\n",
        "          self.image=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "\n",
        "          self.label=label[i].cuda()\n",
        "          w=1\n",
        "          break\n",
        "    for (image,label) in self.test_gen:\n",
        "      if e==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label2:\n",
        "          self.image2=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "          self.label2=label[i].cuda()\n",
        "          e=1\n",
        "          break\n",
        "\n",
        "  def latent(self):\n",
        "    latent1=self.net_generator(self.image,latent_space=True,label=self.label)\n",
        "    latent2=self.net_generator(self.image2,latent_space=True,label=self.label2)\n",
        "    for count in range(self.num_latents+1):\n",
        "      input_latent=(latent1*(count/(self.num_latents))+latent2*(1-count/self.num_latents))\n",
        "      output=self.net_generator(self.image2,latent=input_latent,marker=True,label=self.label)\n",
        "      output=output.view(1, 1,self.filter_size ,self.filter_size ) \n",
        "      save_image(output,'latent_layer1_'+str(count)+'.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4mZAqyQgp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualizing the latent space points in two dimenions using principal component analysis\n",
        "a=0\n",
        "class plotter(object):\n",
        "  def __init__(self,render):\n",
        "    self.batch_size=1\n",
        "    self.test_gen=test_gen\n",
        "    self.net_generator=render.net_generator\n",
        "    self.net_discriminator=render.net_discriminator\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.latent=[]\n",
        "    self.latent_size=256\n",
        "    self.label_array=[]\n",
        "    self.visualize()\n",
        "   \n",
        "  def visualize(self):\n",
        "    for (image,label) in self.test_gen:\n",
        "      for i in range(len(label)):\n",
        "        latent=self.net_generator(np.reshape(image[i],[self.batch_size,self.num_layers*self.filter_size*self.filter_size]).cuda(),plot=True,label=label[i].item()).cpu().detach().numpy()\n",
        "        latent_x=np.reshape(latent,(self.latent_size))\n",
        "        self.latent.append(latent_x)\n",
        "        self.label_array.append(label[i].item())\n",
        "        #feat_cols = ['feature'+str(i) for i in range(latent_x.shape[1])]\n",
        "        #latent=pd.DataFrame(latent_x,columns=feat_cols)\n",
        "        #print(latent.shape)\n",
        "        #print(latent)\n",
        "\n",
        "    pca_latent = PCA(n_components=2)\n",
        "  \n",
        "    twodim_pca = pca_latent.fit_transform(np.array(self.latent))\n",
        "    colormap = np.array(['r', 'g', 'b','pink','orange','lightblue','black','brown','yellow','white'])\n",
        "    for i in range(10):\n",
        "      temp1=[]\n",
        "      temp2=[]\n",
        "      \n",
        "      for j in range(len(self.label_array)):\n",
        "\n",
        "        if self.label_array[j]==i:\n",
        "          temp1.append(twodim_pca[j][0])\n",
        "          temp2.append(twodim_pca[j][1]) \n",
        "      plt.scatter(temp1,temp2,c=colormap[i])\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_OekwHJ5W2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ea8cea3f-2a41-41b4-da3e-4641cf780e4d"
      },
      "source": [
        "#MAIN DO NOT TOUCH IT\n",
        "net_generator=model_generator().cuda()\n",
        "net_discriminator=model_discriminator().cuda()\n",
        "lr=0.00005\n",
        "opt_generator=torch.optim.Adam(net_generator.parameters(),lr)\n",
        "opt_discriminator=torch.optim.Adam(net_discriminator.parameters(),lr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8JeXOyqY-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If we are building a class without using any functionality of any other class then object can be written it means that the class will return an object type dataset\n",
        "#However if we write like data.Datasets of Sampler then it means that it will have functionality of the superclass data.Datasets or Sampler however ultimately these super classes will also return an object.\n",
        "#so here in a way we are returning an object of the super class via the sub class.\n",
        "#BUILDING A SIMPLE AUTOENCODER WHICH JUST AIMS AT RECONSTRUCTION WITHOUT ANY STOCHASTICITY\n",
        "\n",
        "\n",
        "class training(object):\n",
        "\n",
        "  def __init__(self,epochs=10,train_g=train_gen,test_g=test_gen,generate_latent=False,plot=False,opt_generator=opt_generator,opt_discriminator=opt_discriminator):\n",
        "    self.epochs=epochs\n",
        "    self.lr=lr\n",
        "    self.train_gen=train_g\n",
        "    self.batch_size=100\n",
        "    self.test_gen=test_g\n",
        "    self.device='cuda'\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.net_generator=net_generator\n",
        "    self.net_discriminator=net_discriminator\n",
        "    self.loss=torch.nn.BCELoss()\n",
        "    self.opt_discriminator=opt_discriminator\n",
        "    self.opt_generator=opt_generator\n",
        "    self.generation_iters=1\n",
        "    self.d_iters=1\n",
        "    self.class_size=10\n",
        "    self.labeler=torch.from_numpy(np.zeros([self.batch_size,self.class_size],dtype='float32')).cuda()\n",
        "    self.train()\n",
        "    \n",
        "    if generate_latent==True:\n",
        "      modify_space(self,test_gen)\n",
        "    if plot==True:\n",
        "      plotter(self)\n",
        "    self.test()\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    for epoch_no in range(self.epochs):\n",
        "      counter=0\n",
        "      for (image,label) in self.train_gen:\n",
        "        img=image.to(self.device)\n",
        "        lab_real=torch.from_numpy(np.ones([self.batch_size,1],np.float32)).to(self.device)\n",
        "        lab_fake=torch.from_numpy(np.zeros([self.batch_size,1],np.float32)).to(self.device)\n",
        "\n",
        "        #train_discriminator\n",
        "        #self.opt_generator.zero_grad()\n",
        "\n",
        "        self.opt_discriminator.zero_grad()\n",
        "        \"\"\"random_choice=np.random.choice([0,1])\n",
        "        if random_choice==0:\n",
        "          input_label=lab_fake\n",
        "          input_img=output_generator[:,:,:27,:27]\n",
        "          print(\"fake_image\",input_img.size())\n",
        "        else:\n",
        "          input_label=lab_real\n",
        "          input_img=img[:,:,:27,:27]\n",
        "          print(\"real_image\",input_img.size())\n",
        "          \"\"\"\n",
        "        loss_d=0\n",
        "        for i in range(self.d_iters):  \n",
        "          output_generator=self.net_generator(img,label).to(self.device)\n",
        "          vec=self.labeler\n",
        "          for i in range(self.batch_size):\n",
        "            vec[i][int(label[i].item())]=1\n",
        "\n",
        "          output_generator=torch.cat((output_generator,vec),1)\n",
        "\n",
        "          logits_fake = self.net_discriminator(output_generator).to(self.device)\n",
        "\n",
        "\n",
        "          logits_real = self.net_discriminator(torch.cat((img.view(self.batch_size,self.num_layers*self.filter_size*self.filter_size),vec),1)).to(self.device)\n",
        "\n",
        "          loss_discriminator_real = self.loss(logits_real,lab_real)\n",
        "          loss_discriminator_fake = self.loss(logits_fake,lab_fake)\n",
        "          loss_discriminator=loss_discriminator_real+loss_discriminator_fake\n",
        "          loss_discriminator.backward(retain_graph=True)\n",
        "          self.opt_discriminator.step()\n",
        "          loss_d+=loss_discriminator\n",
        "        #discriminator one iteration complete\n",
        "\n",
        "        #train_generator\n",
        "        tot_loss=0\n",
        "        for i in range(self.generation_iters):\n",
        "          img=0\n",
        "          self.opt_generator.zero_grad()\n",
        "          #self.opt_discriminator.zero_grad()\n",
        "          output_generator=self.net_generator(img,label).to(self.device)\n",
        "          output_generator=torch.cat((output_generator,vec),1)\n",
        "          #print(\"fake_image\",output_generator.size())\n",
        "          logit_disc = self.net_discriminator(output_generator).to(self.device)\n",
        "          loss_generator = self.loss(logit_disc,lab_real)\n",
        "          tot_loss+=loss_generator\n",
        "          loss_generator.backward()\n",
        "          self.opt_generator.step()\n",
        "        #One generator iteration completed\n",
        "        counter+=1\n",
        "        print(\"ITERATION_NO.:\",counter ,\"LOSS_Generator:\",tot_loss.item()/self.generation_iters ,\"LOSS_Discriminator:\",loss_d.item()/self.d_iters)\n",
        "      print(\"EPOCH OVER:\",epoch_no)\n",
        "   \n",
        "  def test(self):\n",
        "    count=0\n",
        "    for (image,label) in self.train_gen:\n",
        "      img=image[0].to(self.device)\n",
        "      lab=label.to(self.device)\n",
        "      outputs = self.net_generator(img,label).view(100,1,28,28)\n",
        "      count+=1\n",
        "      print(\"Image_no\",count)\n",
        "      if count%100==0:\n",
        "        save_image(outputs,'testing'+str(count)+'.png')\n",
        "       \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUJezNR8OFw",
        "colab_type": "code",
        "outputId": "20772a77-b661-42f8-bad2-63cf7cf8501c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True,generate_latent=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.887015342712402 LOSS_Discriminator: 0.13386225700378418\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.649950981140137 LOSS_Discriminator: 0.12466742098331451\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.28140115737915 LOSS_Discriminator: 0.1985200196504593\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.207391738891602 LOSS_Discriminator: 0.15910644829273224\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.7836456298828125 LOSS_Discriminator: 0.18276044726371765\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.36057186126709 LOSS_Discriminator: 0.16632086038589478\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.759139537811279 LOSS_Discriminator: 0.17054980993270874\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.501724720001221 LOSS_Discriminator: 0.09245511889457703\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.1782050132751465 LOSS_Discriminator: 0.22587543725967407\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.387893199920654 LOSS_Discriminator: 0.10334436595439911\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.5834479331970215 LOSS_Discriminator: 0.14592742919921875\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.289676666259766 LOSS_Discriminator: 0.22973106801509857\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.187404632568359 LOSS_Discriminator: 0.18470844626426697\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.714200496673584 LOSS_Discriminator: 0.1083485409617424\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.4150919914245605 LOSS_Discriminator: 0.07157470285892487\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.656025409698486 LOSS_Discriminator: 0.2105247676372528\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.343142032623291 LOSS_Discriminator: 0.14980581402778625\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.382339954376221 LOSS_Discriminator: 0.14283543825149536\n",
            "ITERATION_NO.: 430 LOSS_Generator: 4.846466541290283 LOSS_Discriminator: 0.07866056263446808\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.190721035003662 LOSS_Discriminator: 0.11093659698963165\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.219649791717529 LOSS_Discriminator: 0.20977741479873657\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.1880950927734375 LOSS_Discriminator: 0.13557130098342896\n",
            "ITERATION_NO.: 434 LOSS_Generator: 5.073035717010498 LOSS_Discriminator: 0.11085584759712219\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.310137748718262 LOSS_Discriminator: 0.1022975891828537\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.679354667663574 LOSS_Discriminator: 0.21258559823036194\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.9907426834106445 LOSS_Discriminator: 0.12593241035938263\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.071822643280029 LOSS_Discriminator: 0.16140280663967133\n",
            "ITERATION_NO.: 439 LOSS_Generator: 5.760551929473877 LOSS_Discriminator: 0.13040752708911896\n",
            "ITERATION_NO.: 440 LOSS_Generator: 5.970884323120117 LOSS_Discriminator: 0.16819752752780914\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.100429058074951 LOSS_Discriminator: 0.1772708147764206\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.513132095336914 LOSS_Discriminator: 0.16393953561782837\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.344836235046387 LOSS_Discriminator: 0.1011970192193985\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.364936351776123 LOSS_Discriminator: 0.14486193656921387\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.431891918182373 LOSS_Discriminator: 0.12735170125961304\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.076075077056885 LOSS_Discriminator: 0.12039843946695328\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.473275184631348 LOSS_Discriminator: 0.15732166171073914\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.084230422973633 LOSS_Discriminator: 0.2197485864162445\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.894423961639404 LOSS_Discriminator: 0.14028121531009674\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.491405010223389 LOSS_Discriminator: 0.05829176306724548\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.8669352531433105 LOSS_Discriminator: 0.18351255357265472\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.136693954467773 LOSS_Discriminator: 0.12862278521060944\n",
            "ITERATION_NO.: 453 LOSS_Generator: 6.0936150550842285 LOSS_Discriminator: 0.19859731197357178\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.781539440155029 LOSS_Discriminator: 0.0998881459236145\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.923635959625244 LOSS_Discriminator: 0.17996662855148315\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.990555286407471 LOSS_Discriminator: 0.10934368520975113\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.016653060913086 LOSS_Discriminator: 0.09866323322057724\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.159101963043213 LOSS_Discriminator: 0.14633747935295105\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.612699508666992 LOSS_Discriminator: 0.1769554615020752\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.4293293952941895 LOSS_Discriminator: 0.07424838095903397\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.671866416931152 LOSS_Discriminator: 0.23818102478981018\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.360029697418213 LOSS_Discriminator: 0.06526029855012894\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.410860538482666 LOSS_Discriminator: 0.18271905183792114\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.958304405212402 LOSS_Discriminator: 0.14795735478401184\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.668404579162598 LOSS_Discriminator: 0.19966742396354675\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.803394794464111 LOSS_Discriminator: 0.15534216165542603\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.477631092071533 LOSS_Discriminator: 0.12004096806049347\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.765458583831787 LOSS_Discriminator: 0.1908906102180481\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.344843864440918 LOSS_Discriminator: 0.12483508884906769\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.265628814697266 LOSS_Discriminator: 0.07264938950538635\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.660553455352783 LOSS_Discriminator: 0.12685930728912354\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.447699069976807 LOSS_Discriminator: 0.14772409200668335\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.933584213256836 LOSS_Discriminator: 0.18334029614925385\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.679560661315918 LOSS_Discriminator: 0.22355082631111145\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.51228141784668 LOSS_Discriminator: 0.10676507651805878\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.968329906463623 LOSS_Discriminator: 0.08749137818813324\n",
            "ITERATION_NO.: 477 LOSS_Generator: 6.132064342498779 LOSS_Discriminator: 0.23304934799671173\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.585495471954346 LOSS_Discriminator: 0.14046204090118408\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.864046573638916 LOSS_Discriminator: 0.0830436572432518\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.491377353668213 LOSS_Discriminator: 0.08372758328914642\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.051267147064209 LOSS_Discriminator: 0.3101051449775696\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.660135269165039 LOSS_Discriminator: 0.20584046840667725\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.29377555847168 LOSS_Discriminator: 0.12017375230789185\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.3059306144714355 LOSS_Discriminator: 0.10409190505743027\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.419232368469238 LOSS_Discriminator: 0.25708508491516113\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.596532821655273 LOSS_Discriminator: 0.17676392197608948\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.989395618438721 LOSS_Discriminator: 0.1463020145893097\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.591841220855713 LOSS_Discriminator: 0.14333808422088623\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.184231758117676 LOSS_Discriminator: 0.07951085269451141\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.292605400085449 LOSS_Discriminator: 0.09682734310626984\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.989428520202637 LOSS_Discriminator: 0.08644852042198181\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.672348499298096 LOSS_Discriminator: 0.1264738142490387\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.925947189331055 LOSS_Discriminator: 0.12109357118606567\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.684725761413574 LOSS_Discriminator: 0.09851612150669098\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.06387996673584 LOSS_Discriminator: 0.1592482626438141\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.982989311218262 LOSS_Discriminator: 0.18541137874126434\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.696685314178467 LOSS_Discriminator: 0.17635199427604675\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.564211845397949 LOSS_Discriminator: 0.2867588996887207\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.105557918548584 LOSS_Discriminator: 0.09631205350160599\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.516077041625977 LOSS_Discriminator: 0.12085723876953125\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.752639293670654 LOSS_Discriminator: 0.18172354996204376\n",
            "ITERATION_NO.: 502 LOSS_Generator: 6.124112606048584 LOSS_Discriminator: 0.2153671681880951\n",
            "ITERATION_NO.: 503 LOSS_Generator: 6.034792423248291 LOSS_Discriminator: 0.23046600818634033\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.967189788818359 LOSS_Discriminator: 0.19698694348335266\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.829473972320557 LOSS_Discriminator: 0.19035062193870544\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.132840156555176 LOSS_Discriminator: 0.21138323843479156\n",
            "ITERATION_NO.: 507 LOSS_Generator: 4.952083110809326 LOSS_Discriminator: 0.11461866647005081\n",
            "ITERATION_NO.: 508 LOSS_Generator: 4.6283464431762695 LOSS_Discriminator: 0.09726518392562866\n",
            "ITERATION_NO.: 509 LOSS_Generator: 4.579771041870117 LOSS_Discriminator: 0.18910834193229675\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.292629241943359 LOSS_Discriminator: 0.19920077919960022\n",
            "ITERATION_NO.: 511 LOSS_Generator: 4.743068695068359 LOSS_Discriminator: 0.16960471868515015\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.630172252655029 LOSS_Discriminator: 0.2480584681034088\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.453855514526367 LOSS_Discriminator: 0.13405156135559082\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.5775146484375 LOSS_Discriminator: 0.1680511236190796\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.908810615539551 LOSS_Discriminator: 0.15269945561885834\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.253108501434326 LOSS_Discriminator: 0.13489821553230286\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.739797592163086 LOSS_Discriminator: 0.15449988842010498\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.394474029541016 LOSS_Discriminator: 0.13222438097000122\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.024099349975586 LOSS_Discriminator: 0.060966018587350845\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.166059970855713 LOSS_Discriminator: 0.15404437482357025\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.171120643615723 LOSS_Discriminator: 0.23823362588882446\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.66800594329834 LOSS_Discriminator: 0.09458526223897934\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.539602279663086 LOSS_Discriminator: 0.22873950004577637\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.46890115737915 LOSS_Discriminator: 0.24038350582122803\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.704494476318359 LOSS_Discriminator: 0.2680652439594269\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.846944808959961 LOSS_Discriminator: 0.07041878998279572\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.574695587158203 LOSS_Discriminator: 0.17276883125305176\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.5425310134887695 LOSS_Discriminator: 0.03834662586450577\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.46052360534668 LOSS_Discriminator: 0.14166918396949768\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.607629299163818 LOSS_Discriminator: 0.1482248306274414\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.493960380554199 LOSS_Discriminator: 0.15873146057128906\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.639558792114258 LOSS_Discriminator: 0.0887976586818695\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.657099723815918 LOSS_Discriminator: 0.11221322417259216\n",
            "ITERATION_NO.: 534 LOSS_Generator: 6.2680463790893555 LOSS_Discriminator: 0.06177079305052757\n",
            "ITERATION_NO.: 535 LOSS_Generator: 6.064003944396973 LOSS_Discriminator: 0.10950350761413574\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.104071140289307 LOSS_Discriminator: 0.12129251658916473\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.560764789581299 LOSS_Discriminator: 0.14409077167510986\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.428564071655273 LOSS_Discriminator: 0.15810713171958923\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.459105968475342 LOSS_Discriminator: 0.15735478699207306\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.2028093338012695 LOSS_Discriminator: 0.08566505461931229\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.2138352394104 LOSS_Discriminator: 0.11376802623271942\n",
            "ITERATION_NO.: 542 LOSS_Generator: 4.861887454986572 LOSS_Discriminator: 0.08607468754053116\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.386372089385986 LOSS_Discriminator: 0.14015930891036987\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.708454608917236 LOSS_Discriminator: 0.11385144293308258\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.568988800048828 LOSS_Discriminator: 0.09724538773298264\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.504734516143799 LOSS_Discriminator: 0.18060413002967834\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.127920627593994 LOSS_Discriminator: 0.12069566547870636\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.2273993492126465 LOSS_Discriminator: 0.1688299924135208\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.625064849853516 LOSS_Discriminator: 0.13129399716854095\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.683739185333252 LOSS_Discriminator: 0.16271372139453888\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.7106547355651855 LOSS_Discriminator: 0.11186103522777557\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.395758628845215 LOSS_Discriminator: 0.1264001727104187\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.783804893493652 LOSS_Discriminator: 0.20927590131759644\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.120422840118408 LOSS_Discriminator: 0.13002419471740723\n",
            "ITERATION_NO.: 555 LOSS_Generator: 4.666907787322998 LOSS_Discriminator: 0.12320789694786072\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.194947719573975 LOSS_Discriminator: 0.15000325441360474\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.214817047119141 LOSS_Discriminator: 0.25775203108787537\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.467544078826904 LOSS_Discriminator: 0.20674437284469604\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.327878475189209 LOSS_Discriminator: 0.15673349797725677\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.797427177429199 LOSS_Discriminator: 0.2706695795059204\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.420687198638916 LOSS_Discriminator: 0.2058125138282776\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.719873428344727 LOSS_Discriminator: 0.13403654098510742\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.224443435668945 LOSS_Discriminator: 0.11417094618082047\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.50175666809082 LOSS_Discriminator: 0.06592340022325516\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.580362319946289 LOSS_Discriminator: 0.15342283248901367\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.7475762367248535 LOSS_Discriminator: 0.15578563511371613\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.8887038230896 LOSS_Discriminator: 0.17367148399353027\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.847427845001221 LOSS_Discriminator: 0.14416347444057465\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.328598499298096 LOSS_Discriminator: 0.14956030249595642\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.265149116516113 LOSS_Discriminator: 0.1710277497768402\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.5866193771362305 LOSS_Discriminator: 0.12608182430267334\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.392736911773682 LOSS_Discriminator: 0.2215452939271927\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.115182399749756 LOSS_Discriminator: 0.23353157937526703\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.2620954513549805 LOSS_Discriminator: 0.13084858655929565\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.378261566162109 LOSS_Discriminator: 0.1827356517314911\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.4192047119140625 LOSS_Discriminator: 0.18084987998008728\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.930529594421387 LOSS_Discriminator: 0.13731032609939575\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.795665264129639 LOSS_Discriminator: 0.20171818137168884\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.843896389007568 LOSS_Discriminator: 0.1465446949005127\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.672561168670654 LOSS_Discriminator: 0.10619813948869705\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.735612869262695 LOSS_Discriminator: 0.1318894922733307\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.726062774658203 LOSS_Discriminator: 0.11200793087482452\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.326602935791016 LOSS_Discriminator: 0.18570873141288757\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.165634632110596 LOSS_Discriminator: 0.11585596203804016\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.484200954437256 LOSS_Discriminator: 0.19354836642742157\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.752469062805176 LOSS_Discriminator: 0.2653977870941162\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.756452560424805 LOSS_Discriminator: 0.10901211202144623\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.867950916290283 LOSS_Discriminator: 0.10015195608139038\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.8383870124816895 LOSS_Discriminator: 0.1333032250404358\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.817132949829102 LOSS_Discriminator: 0.12038554251194\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.523215293884277 LOSS_Discriminator: 0.09117288887500763\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.582056045532227 LOSS_Discriminator: 0.1649644672870636\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.8352952003479 LOSS_Discriminator: 0.1519874483346939\n",
            "ITERATION_NO.: 594 LOSS_Generator: 5.866971492767334 LOSS_Discriminator: 0.14437691867351532\n",
            "ITERATION_NO.: 595 LOSS_Generator: 6.143831253051758 LOSS_Discriminator: 0.09189765155315399\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.244560718536377 LOSS_Discriminator: 0.14156755805015564\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.559730052947998 LOSS_Discriminator: 0.11623388528823853\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.933800220489502 LOSS_Discriminator: 0.15436813235282898\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.391337871551514 LOSS_Discriminator: 0.08603139221668243\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.479612827301025 LOSS_Discriminator: 0.2835952043533325\n",
            "EPOCH OVER: 1\n",
            "ITERATION_NO.: 1 LOSS_Generator: 4.847112655639648 LOSS_Discriminator: 0.13568638265132904\n",
            "ITERATION_NO.: 2 LOSS_Generator: 4.990050792694092 LOSS_Discriminator: 0.16221928596496582\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.169022083282471 LOSS_Discriminator: 0.21762984991073608\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.653803825378418 LOSS_Discriminator: 0.13196715712547302\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.89741325378418 LOSS_Discriminator: 0.15674620866775513\n",
            "ITERATION_NO.: 6 LOSS_Generator: 6.0940656661987305 LOSS_Discriminator: 0.158111572265625\n",
            "ITERATION_NO.: 7 LOSS_Generator: 6.280698299407959 LOSS_Discriminator: 0.0684385821223259\n",
            "ITERATION_NO.: 8 LOSS_Generator: 6.112585067749023 LOSS_Discriminator: 0.14435124397277832\n",
            "ITERATION_NO.: 9 LOSS_Generator: 6.013257026672363 LOSS_Discriminator: 0.19383849203586578\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.436801910400391 LOSS_Discriminator: 0.062107499688863754\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.005292892456055 LOSS_Discriminator: 0.14422890543937683\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.423669815063477 LOSS_Discriminator: 0.15294882655143738\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.263861179351807 LOSS_Discriminator: 0.15638698637485504\n",
            "ITERATION_NO.: 14 LOSS_Generator: 4.8377485275268555 LOSS_Discriminator: 0.19125603139400482\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.396384239196777 LOSS_Discriminator: 0.20235534012317657\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.368769645690918 LOSS_Discriminator: 0.1293056607246399\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.294514179229736 LOSS_Discriminator: 0.06416887044906616\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.60478401184082 LOSS_Discriminator: 0.2366047501564026\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.6806416511535645 LOSS_Discriminator: 0.14139500260353088\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.72081184387207 LOSS_Discriminator: 0.08780555427074432\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.3807148933410645 LOSS_Discriminator: 0.15915459394454956\n",
            "ITERATION_NO.: 22 LOSS_Generator: 6.157003402709961 LOSS_Discriminator: 0.10435548424720764\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.71450138092041 LOSS_Discriminator: 0.12033173441886902\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.435703754425049 LOSS_Discriminator: 0.08764687925577164\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.434931755065918 LOSS_Discriminator: 0.06270009279251099\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.1486334800720215 LOSS_Discriminator: 0.1677255779504776\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.031744480133057 LOSS_Discriminator: 0.1509755402803421\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.123028755187988 LOSS_Discriminator: 0.13071414828300476\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.303354263305664 LOSS_Discriminator: 0.1827516257762909\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.370915412902832 LOSS_Discriminator: 0.08372454345226288\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.1416168212890625 LOSS_Discriminator: 0.07843580842018127\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.318210601806641 LOSS_Discriminator: 0.046470120549201965\n",
            "ITERATION_NO.: 33 LOSS_Generator: 4.85865592956543 LOSS_Discriminator: 0.07935032993555069\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.31411600112915 LOSS_Discriminator: 0.1339806616306305\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.669073581695557 LOSS_Discriminator: 0.12711665034294128\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.604841232299805 LOSS_Discriminator: 0.2044510841369629\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.822460174560547 LOSS_Discriminator: 0.12416939437389374\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.7018866539001465 LOSS_Discriminator: 0.13705512881278992\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.172874927520752 LOSS_Discriminator: 0.08519783616065979\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.8175177574157715 LOSS_Discriminator: 0.0637894868850708\n",
            "ITERATION_NO.: 41 LOSS_Generator: 6.140176773071289 LOSS_Discriminator: 0.11621750891208649\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.394431114196777 LOSS_Discriminator: 0.18314391374588013\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.631715774536133 LOSS_Discriminator: 0.13372787833213806\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.022007942199707 LOSS_Discriminator: 0.1505998969078064\n",
            "ITERATION_NO.: 45 LOSS_Generator: 4.931337833404541 LOSS_Discriminator: 0.2068382203578949\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.19571590423584 LOSS_Discriminator: 0.1625974476337433\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.024364471435547 LOSS_Discriminator: 0.09182506799697876\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.1298828125 LOSS_Discriminator: 0.1457817405462265\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.104411602020264 LOSS_Discriminator: 0.14862340688705444\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.392165660858154 LOSS_Discriminator: 0.20634213089942932\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.550070762634277 LOSS_Discriminator: 0.12113937735557556\n",
            "ITERATION_NO.: 52 LOSS_Generator: 5.386984825134277 LOSS_Discriminator: 0.09279131889343262\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.062628269195557 LOSS_Discriminator: 0.1994715929031372\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.397232532501221 LOSS_Discriminator: 0.24967646598815918\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.05106782913208 LOSS_Discriminator: 0.14606723189353943\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.340598106384277 LOSS_Discriminator: 0.12200336903333664\n",
            "ITERATION_NO.: 57 LOSS_Generator: 4.957576751708984 LOSS_Discriminator: 0.1889418363571167\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.078840732574463 LOSS_Discriminator: 0.09511551260948181\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.065943717956543 LOSS_Discriminator: 0.12711092829704285\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.366866588592529 LOSS_Discriminator: 0.14679056406021118\n",
            "ITERATION_NO.: 61 LOSS_Generator: 5.5969343185424805 LOSS_Discriminator: 0.16473405063152313\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.003090858459473 LOSS_Discriminator: 0.10389666259288788\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.379299163818359 LOSS_Discriminator: 0.1113274097442627\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.1015472412109375 LOSS_Discriminator: 0.1441502571105957\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.113913536071777 LOSS_Discriminator: 0.08447294682264328\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.656388759613037 LOSS_Discriminator: 0.20486868917942047\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.1209187507629395 LOSS_Discriminator: 0.09110981225967407\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.045073986053467 LOSS_Discriminator: 0.12460575997829437\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.0315141677856445 LOSS_Discriminator: 0.09965991973876953\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.25809383392334 LOSS_Discriminator: 0.09318871051073074\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.258882522583008 LOSS_Discriminator: 0.12201464176177979\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.420541286468506 LOSS_Discriminator: 0.17516177892684937\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.986097812652588 LOSS_Discriminator: 0.15503573417663574\n",
            "ITERATION_NO.: 74 LOSS_Generator: 6.049939155578613 LOSS_Discriminator: 0.10591094195842743\n",
            "ITERATION_NO.: 75 LOSS_Generator: 6.032942295074463 LOSS_Discriminator: 0.1836795210838318\n",
            "ITERATION_NO.: 76 LOSS_Generator: 6.088243007659912 LOSS_Discriminator: 0.13742688298225403\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.176520824432373 LOSS_Discriminator: 0.21155212819576263\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.4833664894104 LOSS_Discriminator: 0.12935081124305725\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.173223972320557 LOSS_Discriminator: 0.12035103142261505\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.19091796875 LOSS_Discriminator: 0.10700692236423492\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.010676860809326 LOSS_Discriminator: 0.11148912459611893\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.209985256195068 LOSS_Discriminator: 0.07371419668197632\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.2624945640563965 LOSS_Discriminator: 0.19731321930885315\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.552292346954346 LOSS_Discriminator: 0.18342268466949463\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.3359785079956055 LOSS_Discriminator: 0.08464838564395905\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.444832801818848 LOSS_Discriminator: 0.12021701037883759\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.251528263092041 LOSS_Discriminator: 0.18425145745277405\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.222810745239258 LOSS_Discriminator: 0.09147109091281891\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.459383010864258 LOSS_Discriminator: 0.14430882036685944\n",
            "ITERATION_NO.: 90 LOSS_Generator: 4.999025344848633 LOSS_Discriminator: 0.11403775215148926\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.179569721221924 LOSS_Discriminator: 0.07685136795043945\n",
            "ITERATION_NO.: 92 LOSS_Generator: 4.950193405151367 LOSS_Discriminator: 0.12065331637859344\n",
            "ITERATION_NO.: 93 LOSS_Generator: 4.770279407501221 LOSS_Discriminator: 0.12254150211811066\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.054034233093262 LOSS_Discriminator: 0.13643993437290192\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.5971550941467285 LOSS_Discriminator: 0.2085885852575302\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.309299468994141 LOSS_Discriminator: 0.09228131175041199\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.6801910400390625 LOSS_Discriminator: 0.10953682661056519\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.966411590576172 LOSS_Discriminator: 0.061099596321582794\n",
            "ITERATION_NO.: 99 LOSS_Generator: 5.929776191711426 LOSS_Discriminator: 0.2146817445755005\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.014552116394043 LOSS_Discriminator: 0.1419980823993683\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.805419445037842 LOSS_Discriminator: 0.0956972986459732\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.960689067840576 LOSS_Discriminator: 0.14904984831809998\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.876748085021973 LOSS_Discriminator: 0.10744043439626694\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.055298328399658 LOSS_Discriminator: 0.14536668360233307\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.045899391174316 LOSS_Discriminator: 0.16862279176712036\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.183440685272217 LOSS_Discriminator: 0.18735407292842865\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.622925281524658 LOSS_Discriminator: 0.08675617724657059\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.964963912963867 LOSS_Discriminator: 0.07823298126459122\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.844815731048584 LOSS_Discriminator: 0.11817207932472229\n",
            "ITERATION_NO.: 110 LOSS_Generator: 6.101278781890869 LOSS_Discriminator: 0.06667985767126083\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.998235702514648 LOSS_Discriminator: 0.07620088011026382\n",
            "ITERATION_NO.: 112 LOSS_Generator: 6.152230739593506 LOSS_Discriminator: 0.22575969994068146\n",
            "ITERATION_NO.: 113 LOSS_Generator: 6.0989861488342285 LOSS_Discriminator: 0.11845337599515915\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.726240158081055 LOSS_Discriminator: 0.30115532875061035\n",
            "ITERATION_NO.: 115 LOSS_Generator: 6.461793899536133 LOSS_Discriminator: 0.0843077003955841\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.869212627410889 LOSS_Discriminator: 0.18070870637893677\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.717369556427002 LOSS_Discriminator: 0.07296714186668396\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.632729530334473 LOSS_Discriminator: 0.05700024217367172\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.0983805656433105 LOSS_Discriminator: 0.1562783420085907\n",
            "ITERATION_NO.: 120 LOSS_Generator: 4.969278812408447 LOSS_Discriminator: 0.07727278023958206\n",
            "ITERATION_NO.: 121 LOSS_Generator: 4.970389366149902 LOSS_Discriminator: 0.11934831738471985\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.349188804626465 LOSS_Discriminator: 0.19764947891235352\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.61356258392334 LOSS_Discriminator: 0.18144211173057556\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.410804271697998 LOSS_Discriminator: 0.13186082243919373\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.741954803466797 LOSS_Discriminator: 0.10151849687099457\n",
            "ITERATION_NO.: 126 LOSS_Generator: 6.142836093902588 LOSS_Discriminator: 0.24094730615615845\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.918633460998535 LOSS_Discriminator: 0.17761632800102234\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.864636421203613 LOSS_Discriminator: 0.1514054238796234\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.936328887939453 LOSS_Discriminator: 0.08034394681453705\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.724381923675537 LOSS_Discriminator: 0.1262567937374115\n",
            "ITERATION_NO.: 131 LOSS_Generator: 5.6742072105407715 LOSS_Discriminator: 0.12550139427185059\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.735380172729492 LOSS_Discriminator: 0.23671603202819824\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.940383434295654 LOSS_Discriminator: 0.11785661429166794\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.762263774871826 LOSS_Discriminator: 0.1242619976401329\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.578348159790039 LOSS_Discriminator: 0.1279180645942688\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.502507209777832 LOSS_Discriminator: 0.1358800232410431\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.994950771331787 LOSS_Discriminator: 0.18443384766578674\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.395390510559082 LOSS_Discriminator: 0.18015018105506897\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.081119537353516 LOSS_Discriminator: 0.17891553044319153\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.884312152862549 LOSS_Discriminator: 0.13981014490127563\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.536492347717285 LOSS_Discriminator: 0.13541164994239807\n",
            "ITERATION_NO.: 142 LOSS_Generator: 6.058731555938721 LOSS_Discriminator: 0.12010936439037323\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.626811981201172 LOSS_Discriminator: 0.06817743182182312\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.174429416656494 LOSS_Discriminator: 0.15776701271533966\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.085385799407959 LOSS_Discriminator: 0.12444238364696503\n",
            "ITERATION_NO.: 146 LOSS_Generator: 4.752542495727539 LOSS_Discriminator: 0.13139161467552185\n",
            "ITERATION_NO.: 147 LOSS_Generator: 4.621240615844727 LOSS_Discriminator: 0.21448975801467896\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.257050037384033 LOSS_Discriminator: 0.22037267684936523\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.352680206298828 LOSS_Discriminator: 0.14995484054088593\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.4468770027160645 LOSS_Discriminator: 0.19968152046203613\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.379075527191162 LOSS_Discriminator: 0.1590747833251953\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.76715087890625 LOSS_Discriminator: 0.08304180204868317\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.520244121551514 LOSS_Discriminator: 0.21001800894737244\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.635781288146973 LOSS_Discriminator: 0.15913420915603638\n",
            "ITERATION_NO.: 155 LOSS_Generator: 6.0462846755981445 LOSS_Discriminator: 0.12073290348052979\n",
            "ITERATION_NO.: 156 LOSS_Generator: 5.737545013427734 LOSS_Discriminator: 0.09123498201370239\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.382781982421875 LOSS_Discriminator: 0.19693969190120697\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.135409355163574 LOSS_Discriminator: 0.12483540922403336\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.375438213348389 LOSS_Discriminator: 0.10198007524013519\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.208256721496582 LOSS_Discriminator: 0.0822112113237381\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.338339805603027 LOSS_Discriminator: 0.10685975849628448\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.0135955810546875 LOSS_Discriminator: 0.12200307101011276\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.37650203704834 LOSS_Discriminator: 0.09691409766674042\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.4041972160339355 LOSS_Discriminator: 0.25943875312805176\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.234580039978027 LOSS_Discriminator: 0.155584454536438\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.3987250328063965 LOSS_Discriminator: 0.13958728313446045\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.433914661407471 LOSS_Discriminator: 0.18740016222000122\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.466865062713623 LOSS_Discriminator: 0.1466819941997528\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.422428607940674 LOSS_Discriminator: 0.11795228719711304\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.309223651885986 LOSS_Discriminator: 0.1316121369600296\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.198119640350342 LOSS_Discriminator: 0.20227277278900146\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.294000625610352 LOSS_Discriminator: 0.14547264575958252\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.731678009033203 LOSS_Discriminator: 0.07452677190303802\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.571008682250977 LOSS_Discriminator: 0.14811085164546967\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.208023548126221 LOSS_Discriminator: 0.16924278438091278\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.58380126953125 LOSS_Discriminator: 0.1258663833141327\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.497608661651611 LOSS_Discriminator: 0.18500064313411713\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.262645721435547 LOSS_Discriminator: 0.23497451841831207\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.643629550933838 LOSS_Discriminator: 0.12696439027786255\n",
            "ITERATION_NO.: 180 LOSS_Generator: 4.8293867111206055 LOSS_Discriminator: 0.23441296815872192\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.001387119293213 LOSS_Discriminator: 0.17980694770812988\n",
            "ITERATION_NO.: 182 LOSS_Generator: 4.9074883460998535 LOSS_Discriminator: 0.12695413827896118\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.042980670928955 LOSS_Discriminator: 0.14618685841560364\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.2301812171936035 LOSS_Discriminator: 0.24796411395072937\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.401482105255127 LOSS_Discriminator: 0.27598631381988525\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.729150295257568 LOSS_Discriminator: 0.1328006386756897\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.720484733581543 LOSS_Discriminator: 0.14190927147865295\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.625680923461914 LOSS_Discriminator: 0.18365070223808289\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.689466953277588 LOSS_Discriminator: 0.1073032096028328\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.381112575531006 LOSS_Discriminator: 0.2351493537425995\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.727582931518555 LOSS_Discriminator: 0.13573896884918213\n",
            "ITERATION_NO.: 192 LOSS_Generator: 4.9604291915893555 LOSS_Discriminator: 0.1416243612766266\n",
            "ITERATION_NO.: 193 LOSS_Generator: 4.8613080978393555 LOSS_Discriminator: 0.12984319031238556\n",
            "ITERATION_NO.: 194 LOSS_Generator: 4.899913787841797 LOSS_Discriminator: 0.17464017868041992\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.2694878578186035 LOSS_Discriminator: 0.15245170891284943\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.279998302459717 LOSS_Discriminator: 0.19853147864341736\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.103453159332275 LOSS_Discriminator: 0.13483475148677826\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.408109188079834 LOSS_Discriminator: 0.1424543410539627\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.5251145362854 LOSS_Discriminator: 0.26896417140960693\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.800350189208984 LOSS_Discriminator: 0.09572070837020874\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.583260536193848 LOSS_Discriminator: 0.10501392185688019\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.460936069488525 LOSS_Discriminator: 0.07185089588165283\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.83653450012207 LOSS_Discriminator: 0.11309580504894257\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.7494425773620605 LOSS_Discriminator: 0.09946512430906296\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.907540321350098 LOSS_Discriminator: 0.1125202625989914\n",
            "ITERATION_NO.: 206 LOSS_Generator: 6.067033290863037 LOSS_Discriminator: 0.21177223324775696\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.875143527984619 LOSS_Discriminator: 0.12302593886852264\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.432642936706543 LOSS_Discriminator: 0.20287635922431946\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.505960464477539 LOSS_Discriminator: 0.22860464453697205\n",
            "ITERATION_NO.: 210 LOSS_Generator: 4.98068904876709 LOSS_Discriminator: 0.11503664404153824\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.1858625411987305 LOSS_Discriminator: 0.17630106210708618\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.6432108879089355 LOSS_Discriminator: 0.2118808776140213\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.923242092132568 LOSS_Discriminator: 0.17046460509300232\n",
            "ITERATION_NO.: 214 LOSS_Generator: 6.601314067840576 LOSS_Discriminator: 0.21115148067474365\n",
            "ITERATION_NO.: 215 LOSS_Generator: 6.606085777282715 LOSS_Discriminator: 0.12396654486656189\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.394774913787842 LOSS_Discriminator: 0.1447126716375351\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.960752487182617 LOSS_Discriminator: 0.30729976296424866\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.453853607177734 LOSS_Discriminator: 0.2226284146308899\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.1527299880981445 LOSS_Discriminator: 0.14715370535850525\n",
            "ITERATION_NO.: 220 LOSS_Generator: 4.767031192779541 LOSS_Discriminator: 0.15754292905330658\n",
            "ITERATION_NO.: 221 LOSS_Generator: 4.996212959289551 LOSS_Discriminator: 0.16676543653011322\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.179808139801025 LOSS_Discriminator: 0.12970726191997528\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.8204474449157715 LOSS_Discriminator: 0.12007379531860352\n",
            "ITERATION_NO.: 224 LOSS_Generator: 5.918336391448975 LOSS_Discriminator: 0.17377500236034393\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.7127580642700195 LOSS_Discriminator: 0.13029541075229645\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.178636074066162 LOSS_Discriminator: 0.06271466612815857\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.401398181915283 LOSS_Discriminator: 0.1008102297782898\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.904127597808838 LOSS_Discriminator: 0.11986732482910156\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.951387405395508 LOSS_Discriminator: 0.20992180705070496\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.297552108764648 LOSS_Discriminator: 0.14833641052246094\n",
            "ITERATION_NO.: 231 LOSS_Generator: 4.759593963623047 LOSS_Discriminator: 0.20710402727127075\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.110628128051758 LOSS_Discriminator: 0.24501311779022217\n",
            "ITERATION_NO.: 233 LOSS_Generator: 4.867290019989014 LOSS_Discriminator: 0.19093826413154602\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.021062850952148 LOSS_Discriminator: 0.1705811619758606\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.942176342010498 LOSS_Discriminator: 0.10443126410245895\n",
            "ITERATION_NO.: 236 LOSS_Generator: 6.5179667472839355 LOSS_Discriminator: 0.12194930762052536\n",
            "ITERATION_NO.: 237 LOSS_Generator: 6.952228546142578 LOSS_Discriminator: 0.1173294186592102\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.849214553833008 LOSS_Discriminator: 0.25466009974479675\n",
            "ITERATION_NO.: 239 LOSS_Generator: 6.8862762451171875 LOSS_Discriminator: 0.06844046711921692\n",
            "ITERATION_NO.: 240 LOSS_Generator: 6.481616973876953 LOSS_Discriminator: 0.22009113430976868\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.975991725921631 LOSS_Discriminator: 0.11343106627464294\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.231106758117676 LOSS_Discriminator: 0.11476732790470123\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.454340934753418 LOSS_Discriminator: 0.0688576027750969\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.117730617523193 LOSS_Discriminator: 0.20553728938102722\n",
            "ITERATION_NO.: 245 LOSS_Generator: 5.286306381225586 LOSS_Discriminator: 0.15948954224586487\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.384702205657959 LOSS_Discriminator: 0.15501390397548676\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.202514171600342 LOSS_Discriminator: 0.18267454206943512\n",
            "ITERATION_NO.: 248 LOSS_Generator: 6.098613739013672 LOSS_Discriminator: 0.1254962980747223\n",
            "ITERATION_NO.: 249 LOSS_Generator: 6.198507785797119 LOSS_Discriminator: 0.1354457437992096\n",
            "ITERATION_NO.: 250 LOSS_Generator: 6.374207973480225 LOSS_Discriminator: 0.2582431435585022\n",
            "ITERATION_NO.: 251 LOSS_Generator: 6.334430694580078 LOSS_Discriminator: 0.22886881232261658\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.562769889831543 LOSS_Discriminator: 0.2017383575439453\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.259521961212158 LOSS_Discriminator: 0.18818606436252594\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.822939395904541 LOSS_Discriminator: 0.0736590027809143\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.754826068878174 LOSS_Discriminator: 0.14699167013168335\n",
            "ITERATION_NO.: 256 LOSS_Generator: 5.622464656829834 LOSS_Discriminator: 0.19535312056541443\n",
            "ITERATION_NO.: 257 LOSS_Generator: 4.791663646697998 LOSS_Discriminator: 0.14804363250732422\n",
            "ITERATION_NO.: 258 LOSS_Generator: 5.722938060760498 LOSS_Discriminator: 0.09839105606079102\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.282928943634033 LOSS_Discriminator: 0.15926697850227356\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.460573196411133 LOSS_Discriminator: 0.19285652041435242\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.920778274536133 LOSS_Discriminator: 0.21801137924194336\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.211397647857666 LOSS_Discriminator: 0.21337786316871643\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.3744964599609375 LOSS_Discriminator: 0.10824872553348541\n",
            "ITERATION_NO.: 264 LOSS_Generator: 6.158626556396484 LOSS_Discriminator: 0.12735986709594727\n",
            "ITERATION_NO.: 265 LOSS_Generator: 6.223405838012695 LOSS_Discriminator: 0.11065298318862915\n",
            "ITERATION_NO.: 266 LOSS_Generator: 6.091537952423096 LOSS_Discriminator: 0.0882028341293335\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.704583168029785 LOSS_Discriminator: 0.14341604709625244\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.947830677032471 LOSS_Discriminator: 0.21119095385074615\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.796598434448242 LOSS_Discriminator: 0.17074623703956604\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.226114273071289 LOSS_Discriminator: 0.0431080088019371\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.347194194793701 LOSS_Discriminator: 0.1599242091178894\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.472836971282959 LOSS_Discriminator: 0.23611792922019958\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.141952991485596 LOSS_Discriminator: 0.1487623155117035\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.2568230628967285 LOSS_Discriminator: 0.15971384942531586\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.013533592224121 LOSS_Discriminator: 0.23556533455848694\n",
            "ITERATION_NO.: 276 LOSS_Generator: 4.884951591491699 LOSS_Discriminator: 0.1393926441669464\n",
            "ITERATION_NO.: 277 LOSS_Generator: 4.978667736053467 LOSS_Discriminator: 0.08942032605409622\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.41935920715332 LOSS_Discriminator: 0.2323942482471466\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.322622776031494 LOSS_Discriminator: 0.11805926263332367\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.330097198486328 LOSS_Discriminator: 0.1373022198677063\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.476374626159668 LOSS_Discriminator: 0.1040211096405983\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.241456508636475 LOSS_Discriminator: 0.11523209512233734\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.5549092292785645 LOSS_Discriminator: 0.17092478275299072\n",
            "ITERATION_NO.: 284 LOSS_Generator: 6.093980312347412 LOSS_Discriminator: 0.14457044005393982\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.297598361968994 LOSS_Discriminator: 0.10438399016857147\n",
            "ITERATION_NO.: 286 LOSS_Generator: 6.113531589508057 LOSS_Discriminator: 0.1684473156929016\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.841805458068848 LOSS_Discriminator: 0.11165768653154373\n",
            "ITERATION_NO.: 288 LOSS_Generator: 6.02290153503418 LOSS_Discriminator: 0.14149267971515656\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.741714954376221 LOSS_Discriminator: 0.21610555052757263\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.5372314453125 LOSS_Discriminator: 0.16040438413619995\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.4043498039245605 LOSS_Discriminator: 0.08635762333869934\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.208320140838623 LOSS_Discriminator: 0.18749970197677612\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.502914905548096 LOSS_Discriminator: 0.03893313556909561\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.23027229309082 LOSS_Discriminator: 0.11905819177627563\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.5687127113342285 LOSS_Discriminator: 0.12438549101352692\n",
            "ITERATION_NO.: 296 LOSS_Generator: 4.957350254058838 LOSS_Discriminator: 0.12554395198822021\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.8847527503967285 LOSS_Discriminator: 0.09175248444080353\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.359090805053711 LOSS_Discriminator: 0.13330641388893127\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.845076084136963 LOSS_Discriminator: 0.20606543123722076\n",
            "ITERATION_NO.: 300 LOSS_Generator: 6.133354663848877 LOSS_Discriminator: 0.22620037198066711\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.655766010284424 LOSS_Discriminator: 0.20703889429569244\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.251922130584717 LOSS_Discriminator: 0.14839276671409607\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.185219764709473 LOSS_Discriminator: 0.16174760460853577\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.5240983963012695 LOSS_Discriminator: 0.20501232147216797\n",
            "ITERATION_NO.: 305 LOSS_Generator: 5.624223232269287 LOSS_Discriminator: 0.12603212893009186\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.991486072540283 LOSS_Discriminator: 0.10419553518295288\n",
            "ITERATION_NO.: 307 LOSS_Generator: 6.394876003265381 LOSS_Discriminator: 0.2666483521461487\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.15174674987793 LOSS_Discriminator: 0.17913216352462769\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.788756847381592 LOSS_Discriminator: 0.12969854474067688\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.6683173179626465 LOSS_Discriminator: 0.06375204771757126\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.978738784790039 LOSS_Discriminator: 0.11094178259372711\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.799290657043457 LOSS_Discriminator: 0.11636236310005188\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.160698413848877 LOSS_Discriminator: 0.12512947618961334\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.673013687133789 LOSS_Discriminator: 0.1325397938489914\n",
            "ITERATION_NO.: 315 LOSS_Generator: 5.152996063232422 LOSS_Discriminator: 0.10231560468673706\n",
            "ITERATION_NO.: 316 LOSS_Generator: 5.438378810882568 LOSS_Discriminator: 0.20286032557487488\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.845970630645752 LOSS_Discriminator: 0.10189758986234665\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.703343391418457 LOSS_Discriminator: 0.16047364473342896\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.679861068725586 LOSS_Discriminator: 0.06747420877218246\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.753526210784912 LOSS_Discriminator: 0.14407873153686523\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.442843437194824 LOSS_Discriminator: 0.08456988632678986\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.863211631774902 LOSS_Discriminator: 0.11309237778186798\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.601346015930176 LOSS_Discriminator: 0.21794354915618896\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.893069267272949 LOSS_Discriminator: 0.13061155378818512\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.515130996704102 LOSS_Discriminator: 0.14835649728775024\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.858729839324951 LOSS_Discriminator: 0.07424597442150116\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.4907426834106445 LOSS_Discriminator: 0.13778141140937805\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.883028507232666 LOSS_Discriminator: 0.17692795395851135\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.746011734008789 LOSS_Discriminator: 0.11692897230386734\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.609373092651367 LOSS_Discriminator: 0.08682063221931458\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.631463050842285 LOSS_Discriminator: 0.18493478000164032\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.75030517578125 LOSS_Discriminator: 0.07059281319379807\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.271819591522217 LOSS_Discriminator: 0.18519270420074463\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.490561485290527 LOSS_Discriminator: 0.14865577220916748\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.421105861663818 LOSS_Discriminator: 0.24287977814674377\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.879706382751465 LOSS_Discriminator: 0.17533943057060242\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.2884979248046875 LOSS_Discriminator: 0.21073144674301147\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.484415054321289 LOSS_Discriminator: 0.2347918450832367\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.758239269256592 LOSS_Discriminator: 0.18359017372131348\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.4543962478637695 LOSS_Discriminator: 0.18694338202476501\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.939774990081787 LOSS_Discriminator: 0.05785934999585152\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.981171131134033 LOSS_Discriminator: 0.10626190900802612\n",
            "ITERATION_NO.: 343 LOSS_Generator: 6.142025947570801 LOSS_Discriminator: 0.10649299621582031\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.7317376136779785 LOSS_Discriminator: 0.1790795922279358\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.829391956329346 LOSS_Discriminator: 0.19128413498401642\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.744094371795654 LOSS_Discriminator: 0.10791440308094025\n",
            "ITERATION_NO.: 347 LOSS_Generator: 6.000048637390137 LOSS_Discriminator: 0.22721341252326965\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.929165840148926 LOSS_Discriminator: 0.24093018472194672\n",
            "ITERATION_NO.: 349 LOSS_Generator: 4.9800286293029785 LOSS_Discriminator: 0.09980389475822449\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.222215175628662 LOSS_Discriminator: 0.22831383347511292\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.304670333862305 LOSS_Discriminator: 0.2158699929714203\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.2275285720825195 LOSS_Discriminator: 0.20686882734298706\n",
            "ITERATION_NO.: 353 LOSS_Generator: 4.749894618988037 LOSS_Discriminator: 0.1650993973016739\n",
            "ITERATION_NO.: 354 LOSS_Generator: 4.9263739585876465 LOSS_Discriminator: 0.11120389401912689\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.155186176300049 LOSS_Discriminator: 0.1979498416185379\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.830644130706787 LOSS_Discriminator: 0.18029676377773285\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.362203598022461 LOSS_Discriminator: 0.150665283203125\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.61184024810791 LOSS_Discriminator: 0.24174657464027405\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.61638069152832 LOSS_Discriminator: 0.09460790455341339\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.908745288848877 LOSS_Discriminator: 0.12279047071933746\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.5189409255981445 LOSS_Discriminator: 0.12770944833755493\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.921832084655762 LOSS_Discriminator: 0.14813929796218872\n",
            "ITERATION_NO.: 363 LOSS_Generator: 5.9211201667785645 LOSS_Discriminator: 0.16971445083618164\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.6764235496521 LOSS_Discriminator: 0.17223073542118073\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.53176212310791 LOSS_Discriminator: 0.12413211166858673\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.834446430206299 LOSS_Discriminator: 0.13273215293884277\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.45993709564209 LOSS_Discriminator: 0.13787919282913208\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.810505390167236 LOSS_Discriminator: 0.13291344046592712\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.674660682678223 LOSS_Discriminator: 0.18439942598342896\n",
            "ITERATION_NO.: 370 LOSS_Generator: 6.201210975646973 LOSS_Discriminator: 0.09371379762887955\n",
            "ITERATION_NO.: 371 LOSS_Generator: 6.005859851837158 LOSS_Discriminator: 0.19823375344276428\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.510196685791016 LOSS_Discriminator: 0.18798531591892242\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.309504985809326 LOSS_Discriminator: 0.14304761588573456\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.384380340576172 LOSS_Discriminator: 0.1799604743719101\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.14517879486084 LOSS_Discriminator: 0.13925552368164062\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.131509304046631 LOSS_Discriminator: 0.20063239336013794\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.5137038230896 LOSS_Discriminator: 0.19271016120910645\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.106398105621338 LOSS_Discriminator: 0.19200880825519562\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.039806365966797 LOSS_Discriminator: 0.26579803228378296\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.542343616485596 LOSS_Discriminator: 0.08266915380954742\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.514042854309082 LOSS_Discriminator: 0.10858792066574097\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.770580291748047 LOSS_Discriminator: 0.13109064102172852\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.818601131439209 LOSS_Discriminator: 0.1319483369588852\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.6919732093811035 LOSS_Discriminator: 0.10906896740198135\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.561314105987549 LOSS_Discriminator: 0.15292593836784363\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.997888088226318 LOSS_Discriminator: 0.1438084989786148\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.288408279418945 LOSS_Discriminator: 0.0658428817987442\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.335540771484375 LOSS_Discriminator: 0.09699634462594986\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.167771816253662 LOSS_Discriminator: 0.12136361002922058\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.2763991355896 LOSS_Discriminator: 0.11232961714267731\n",
            "ITERATION_NO.: 391 LOSS_Generator: 5.485128879547119 LOSS_Discriminator: 0.15811866521835327\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.029937744140625 LOSS_Discriminator: 0.1618439108133316\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.332310199737549 LOSS_Discriminator: 0.14441703259944916\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.846699237823486 LOSS_Discriminator: 0.1331031620502472\n",
            "ITERATION_NO.: 395 LOSS_Generator: 6.0133185386657715 LOSS_Discriminator: 0.14491873979568481\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.221463680267334 LOSS_Discriminator: 0.22401496767997742\n",
            "ITERATION_NO.: 397 LOSS_Generator: 6.036544322967529 LOSS_Discriminator: 0.18462972342967987\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.670730113983154 LOSS_Discriminator: 0.18316254019737244\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.590146541595459 LOSS_Discriminator: 0.23898755013942719\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.4467997550964355 LOSS_Discriminator: 0.1722956895828247\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.036846160888672 LOSS_Discriminator: 0.14102144539356232\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.2005534172058105 LOSS_Discriminator: 0.1540912538766861\n",
            "ITERATION_NO.: 403 LOSS_Generator: 4.838775157928467 LOSS_Discriminator: 0.11391045898199081\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.393965244293213 LOSS_Discriminator: 0.12082222104072571\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.04401159286499 LOSS_Discriminator: 0.08795973658561707\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.5875773429870605 LOSS_Discriminator: 0.10868894308805466\n",
            "ITERATION_NO.: 407 LOSS_Generator: 6.056003570556641 LOSS_Discriminator: 0.13754720985889435\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.944477558135986 LOSS_Discriminator: 0.18123170733451843\n",
            "ITERATION_NO.: 409 LOSS_Generator: 6.264535427093506 LOSS_Discriminator: 0.09482228010892868\n",
            "ITERATION_NO.: 410 LOSS_Generator: 6.477663040161133 LOSS_Discriminator: 0.10433875024318695\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.950897216796875 LOSS_Discriminator: 0.27127882838249207\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.88145637512207 LOSS_Discriminator: 0.14871107041835785\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.681944370269775 LOSS_Discriminator: 0.16496309638023376\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.3297600746154785 LOSS_Discriminator: 0.1045663058757782\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.511240005493164 LOSS_Discriminator: 0.12195934355258942\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.210654258728027 LOSS_Discriminator: 0.13169151544570923\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.790679454803467 LOSS_Discriminator: 0.04700823873281479\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.771572113037109 LOSS_Discriminator: 0.11450816690921783\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.734348297119141 LOSS_Discriminator: 0.05282709747552872\n",
            "ITERATION_NO.: 420 LOSS_Generator: 6.098133087158203 LOSS_Discriminator: 0.17508584260940552\n",
            "ITERATION_NO.: 421 LOSS_Generator: 6.795627593994141 LOSS_Discriminator: 0.19429895281791687\n",
            "ITERATION_NO.: 422 LOSS_Generator: 6.583260536193848 LOSS_Discriminator: 0.125231072306633\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.342716217041016 LOSS_Discriminator: 0.2935296595096588\n",
            "ITERATION_NO.: 424 LOSS_Generator: 6.236624717712402 LOSS_Discriminator: 0.1326596587896347\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.478175640106201 LOSS_Discriminator: 0.1522122323513031\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.1170973777771 LOSS_Discriminator: 0.11342232674360275\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.34682559967041 LOSS_Discriminator: 0.11179331690073013\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.734523296356201 LOSS_Discriminator: 0.11282046884298325\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.391928672790527 LOSS_Discriminator: 0.09376917779445648\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.478870391845703 LOSS_Discriminator: 0.24249579012393951\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.8793487548828125 LOSS_Discriminator: 0.15816527605056763\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.696752548217773 LOSS_Discriminator: 0.08662646263837814\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.517998695373535 LOSS_Discriminator: 0.2013898640871048\n",
            "ITERATION_NO.: 434 LOSS_Generator: 6.001194000244141 LOSS_Discriminator: 0.15223440527915955\n",
            "ITERATION_NO.: 435 LOSS_Generator: 6.180233955383301 LOSS_Discriminator: 0.06393206119537354\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.07204532623291 LOSS_Discriminator: 0.12229099869728088\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.8498125076293945 LOSS_Discriminator: 0.10055214166641235\n",
            "ITERATION_NO.: 438 LOSS_Generator: 5.613022327423096 LOSS_Discriminator: 0.09583596140146255\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.1641082763671875 LOSS_Discriminator: 0.12015262991189957\n",
            "ITERATION_NO.: 440 LOSS_Generator: 5.566586971282959 LOSS_Discriminator: 0.2226334512233734\n",
            "ITERATION_NO.: 441 LOSS_Generator: 5.425034046173096 LOSS_Discriminator: 0.19034938514232635\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.0965576171875 LOSS_Discriminator: 0.1882651001214981\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.446371078491211 LOSS_Discriminator: 0.23534056544303894\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.598425388336182 LOSS_Discriminator: 0.13327756524085999\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.552913188934326 LOSS_Discriminator: 0.20919159054756165\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.862904071807861 LOSS_Discriminator: 0.13814035058021545\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.613110542297363 LOSS_Discriminator: 0.13899651169776917\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.704921245574951 LOSS_Discriminator: 0.08559583127498627\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.8038105964660645 LOSS_Discriminator: 0.10850609838962555\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.785256385803223 LOSS_Discriminator: 0.17963477969169617\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.30295467376709 LOSS_Discriminator: 0.1689618080854416\n",
            "ITERATION_NO.: 452 LOSS_Generator: 5.5318193435668945 LOSS_Discriminator: 0.1434536576271057\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.1524457931518555 LOSS_Discriminator: 0.10657825320959091\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.334435939788818 LOSS_Discriminator: 0.07015367597341537\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.555823802947998 LOSS_Discriminator: 0.2013920545578003\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.668026924133301 LOSS_Discriminator: 0.19613370299339294\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.040552139282227 LOSS_Discriminator: 0.24454361200332642\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.819750785827637 LOSS_Discriminator: 0.06597873568534851\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.943040370941162 LOSS_Discriminator: 0.12843522429466248\n",
            "ITERATION_NO.: 460 LOSS_Generator: 6.112790584564209 LOSS_Discriminator: 0.21790435910224915\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.7179765701293945 LOSS_Discriminator: 0.18612709641456604\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.2841033935546875 LOSS_Discriminator: 0.09392698854207993\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.46042537689209 LOSS_Discriminator: 0.08224892616271973\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.672103404998779 LOSS_Discriminator: 0.06187098100781441\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.105983734130859 LOSS_Discriminator: 0.19710665941238403\n",
            "ITERATION_NO.: 466 LOSS_Generator: 4.984731674194336 LOSS_Discriminator: 0.1501193642616272\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.372152805328369 LOSS_Discriminator: 0.07702158391475677\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.298748970031738 LOSS_Discriminator: 0.14367720484733582\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.229747772216797 LOSS_Discriminator: 0.17839226126670837\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.826550483703613 LOSS_Discriminator: 0.07754591107368469\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.441866397857666 LOSS_Discriminator: 0.25997114181518555\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.894679546356201 LOSS_Discriminator: 0.1936585009098053\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.662425994873047 LOSS_Discriminator: 0.06192290782928467\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.685076713562012 LOSS_Discriminator: 0.1726248413324356\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.419497489929199 LOSS_Discriminator: 0.08773227035999298\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.3473992347717285 LOSS_Discriminator: 0.12864118814468384\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.035276889801025 LOSS_Discriminator: 0.12082995474338531\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.093979358673096 LOSS_Discriminator: 0.09777702391147614\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.846451282501221 LOSS_Discriminator: 0.14768266677856445\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.094892978668213 LOSS_Discriminator: 0.1890476644039154\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.490146636962891 LOSS_Discriminator: 0.19174984097480774\n",
            "ITERATION_NO.: 482 LOSS_Generator: 6.02076530456543 LOSS_Discriminator: 0.18581081926822662\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.5450615882873535 LOSS_Discriminator: 0.22123479843139648\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.699114799499512 LOSS_Discriminator: 0.16775888204574585\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.698559761047363 LOSS_Discriminator: 0.09981057047843933\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.472558498382568 LOSS_Discriminator: 0.13581787049770355\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.007676601409912 LOSS_Discriminator: 0.1558908373117447\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.9322333335876465 LOSS_Discriminator: 0.17786940932273865\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.633074760437012 LOSS_Discriminator: 0.13771675527095795\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.260246753692627 LOSS_Discriminator: 0.1648157238960266\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.618082046508789 LOSS_Discriminator: 0.07408739626407623\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.562415599822998 LOSS_Discriminator: 0.1656034141778946\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.492316246032715 LOSS_Discriminator: 0.1366284191608429\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.519742488861084 LOSS_Discriminator: 0.17981231212615967\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.698379039764404 LOSS_Discriminator: 0.12253955751657486\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.4944167137146 LOSS_Discriminator: 0.1724885106086731\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.666784763336182 LOSS_Discriminator: 0.13585969805717468\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.2436981201171875 LOSS_Discriminator: 0.21381106972694397\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.868912220001221 LOSS_Discriminator: 0.1747053563594818\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.569767951965332 LOSS_Discriminator: 0.24760153889656067\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.701842784881592 LOSS_Discriminator: 0.07735975086688995\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.457947254180908 LOSS_Discriminator: 0.1374199092388153\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.943567276000977 LOSS_Discriminator: 0.11706618964672089\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.252247333526611 LOSS_Discriminator: 0.23474711179733276\n",
            "ITERATION_NO.: 505 LOSS_Generator: 4.9585161209106445 LOSS_Discriminator: 0.11883815377950668\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.13370418548584 LOSS_Discriminator: 0.13239046931266785\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.416271209716797 LOSS_Discriminator: 0.1423615664243698\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.187715530395508 LOSS_Discriminator: 0.12597212195396423\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.461760997772217 LOSS_Discriminator: 0.12832915782928467\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.591463565826416 LOSS_Discriminator: 0.18032607436180115\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.432275295257568 LOSS_Discriminator: 0.11626642942428589\n",
            "ITERATION_NO.: 512 LOSS_Generator: 6.0925703048706055 LOSS_Discriminator: 0.18725693225860596\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.891510009765625 LOSS_Discriminator: 0.2548510432243347\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.183111190795898 LOSS_Discriminator: 0.14704027771949768\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.782761096954346 LOSS_Discriminator: 0.13803020119667053\n",
            "ITERATION_NO.: 516 LOSS_Generator: 4.875143527984619 LOSS_Discriminator: 0.06667564809322357\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.014536380767822 LOSS_Discriminator: 0.049800850450992584\n",
            "ITERATION_NO.: 518 LOSS_Generator: 4.944929122924805 LOSS_Discriminator: 0.2222672402858734\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.341508865356445 LOSS_Discriminator: 0.15376150608062744\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.441375255584717 LOSS_Discriminator: 0.2161111980676651\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.462335586547852 LOSS_Discriminator: 0.10029272735118866\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.435592174530029 LOSS_Discriminator: 0.10178208351135254\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.3563055992126465 LOSS_Discriminator: 0.13351742923259735\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.655029773712158 LOSS_Discriminator: 0.1876058429479599\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.455654144287109 LOSS_Discriminator: 0.16346919536590576\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.903263092041016 LOSS_Discriminator: 0.14505460858345032\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.752209663391113 LOSS_Discriminator: 0.10222207754850388\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.445999622344971 LOSS_Discriminator: 0.14828619360923767\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.897968292236328 LOSS_Discriminator: 0.10530377179384232\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.495805263519287 LOSS_Discriminator: 0.13293743133544922\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.739299297332764 LOSS_Discriminator: 0.17480720579624176\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.899812698364258 LOSS_Discriminator: 0.16438792645931244\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.668712139129639 LOSS_Discriminator: 0.17560064792633057\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.889657020568848 LOSS_Discriminator: 0.1696721315383911\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.091991424560547 LOSS_Discriminator: 0.11881668865680695\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.127445697784424 LOSS_Discriminator: 0.0865270122885704\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.189952373504639 LOSS_Discriminator: 0.16009297966957092\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.629627227783203 LOSS_Discriminator: 0.19573897123336792\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.58489990234375 LOSS_Discriminator: 0.11192752420902252\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.780348300933838 LOSS_Discriminator: 0.09926263988018036\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.86937141418457 LOSS_Discriminator: 0.10573004186153412\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.347158908843994 LOSS_Discriminator: 0.14059381186962128\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.745584487915039 LOSS_Discriminator: 0.16573545336723328\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.808205604553223 LOSS_Discriminator: 0.10340206325054169\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.586815357208252 LOSS_Discriminator: 0.17894132435321808\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.869239330291748 LOSS_Discriminator: 0.10495611280202866\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.244630336761475 LOSS_Discriminator: 0.08648762851953506\n",
            "ITERATION_NO.: 548 LOSS_Generator: 4.697448253631592 LOSS_Discriminator: 0.18563874065876007\n",
            "ITERATION_NO.: 549 LOSS_Generator: 4.899871349334717 LOSS_Discriminator: 0.13229995965957642\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.490402698516846 LOSS_Discriminator: 0.149465411901474\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.931757926940918 LOSS_Discriminator: 0.1123829334974289\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.72895622253418 LOSS_Discriminator: 0.1327100694179535\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.857152938842773 LOSS_Discriminator: 0.16343365609645844\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.134115219116211 LOSS_Discriminator: 0.11244803667068481\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.816120624542236 LOSS_Discriminator: 0.16314437985420227\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.837775707244873 LOSS_Discriminator: 0.19662445783615112\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.339484214782715 LOSS_Discriminator: 0.14866456389427185\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.629591464996338 LOSS_Discriminator: 0.09299037605524063\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.097139358520508 LOSS_Discriminator: 0.04974288120865822\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.880746364593506 LOSS_Discriminator: 0.16794085502624512\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.374690055847168 LOSS_Discriminator: 0.14970970153808594\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.639842987060547 LOSS_Discriminator: 0.1383862942457199\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.142863750457764 LOSS_Discriminator: 0.18507224321365356\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.228969573974609 LOSS_Discriminator: 0.1999817192554474\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.807864189147949 LOSS_Discriminator: 0.2598487436771393\n",
            "ITERATION_NO.: 566 LOSS_Generator: 6.080986976623535 LOSS_Discriminator: 0.15953662991523743\n",
            "ITERATION_NO.: 567 LOSS_Generator: 6.0873589515686035 LOSS_Discriminator: 0.1498349905014038\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.842235565185547 LOSS_Discriminator: 0.18343470990657806\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.28862190246582 LOSS_Discriminator: 0.10979916900396347\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.6403350830078125 LOSS_Discriminator: 0.15864399075508118\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.598061561584473 LOSS_Discriminator: 0.08834979683160782\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.315191745758057 LOSS_Discriminator: 0.1481863558292389\n",
            "ITERATION_NO.: 573 LOSS_Generator: 4.955112934112549 LOSS_Discriminator: 0.10714520514011383\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.555395603179932 LOSS_Discriminator: 0.14114515483379364\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.875673770904541 LOSS_Discriminator: 0.272216796875\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.870588779449463 LOSS_Discriminator: 0.11822694540023804\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.338984489440918 LOSS_Discriminator: 0.09481930732727051\n",
            "ITERATION_NO.: 578 LOSS_Generator: 6.246034145355225 LOSS_Discriminator: 0.09844328463077545\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.979186534881592 LOSS_Discriminator: 0.12392333149909973\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.497706890106201 LOSS_Discriminator: 0.2832827866077423\n",
            "ITERATION_NO.: 581 LOSS_Generator: 6.283847808837891 LOSS_Discriminator: 0.1449608951807022\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.702563285827637 LOSS_Discriminator: 0.08333151042461395\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.573575973510742 LOSS_Discriminator: 0.18253310024738312\n",
            "ITERATION_NO.: 584 LOSS_Generator: 5.4373273849487305 LOSS_Discriminator: 0.1832503080368042\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.322004318237305 LOSS_Discriminator: 0.19261951744556427\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.554556846618652 LOSS_Discriminator: 0.1445116102695465\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.362411975860596 LOSS_Discriminator: 0.18564683198928833\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.791309356689453 LOSS_Discriminator: 0.1625206470489502\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.26528787612915 LOSS_Discriminator: 0.08423930406570435\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.429197788238525 LOSS_Discriminator: 0.15537552535533905\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.746577739715576 LOSS_Discriminator: 0.1338680535554886\n",
            "ITERATION_NO.: 592 LOSS_Generator: 6.410837173461914 LOSS_Discriminator: 0.17575597763061523\n",
            "ITERATION_NO.: 593 LOSS_Generator: 6.493984222412109 LOSS_Discriminator: 0.09180000424385071\n",
            "ITERATION_NO.: 594 LOSS_Generator: 6.200579643249512 LOSS_Discriminator: 0.10457966476678848\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.785221576690674 LOSS_Discriminator: 0.11437775939702988\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.75119686126709 LOSS_Discriminator: 0.16277852654457092\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.0742292404174805 LOSS_Discriminator: 0.08195607364177704\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.079177379608154 LOSS_Discriminator: 0.1152721717953682\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.1912078857421875 LOSS_Discriminator: 0.10976706445217133\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.5464324951171875 LOSS_Discriminator: 0.1188913881778717\n",
            "EPOCH OVER: 2\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.224207878112793 LOSS_Discriminator: 0.1312161535024643\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.203670501708984 LOSS_Discriminator: 0.11147668957710266\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.7049455642700195 LOSS_Discriminator: 0.20737597346305847\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.282186985015869 LOSS_Discriminator: 0.09494137018918991\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.329095363616943 LOSS_Discriminator: 0.10112500190734863\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.446409702301025 LOSS_Discriminator: 0.2323610782623291\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.596173286437988 LOSS_Discriminator: 0.10629911720752716\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.456842422485352 LOSS_Discriminator: 0.18575820326805115\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.724761486053467 LOSS_Discriminator: 0.11786790937185287\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.855576992034912 LOSS_Discriminator: 0.12828722596168518\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.615508556365967 LOSS_Discriminator: 0.12796053290367126\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.540414333343506 LOSS_Discriminator: 0.13803978264331818\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.133089542388916 LOSS_Discriminator: 0.15145519375801086\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.012803554534912 LOSS_Discriminator: 0.10921673476696014\n",
            "ITERATION_NO.: 15 LOSS_Generator: 4.688477039337158 LOSS_Discriminator: 0.13915473222732544\n",
            "ITERATION_NO.: 16 LOSS_Generator: 4.756999969482422 LOSS_Discriminator: 0.12286581099033356\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.1869354248046875 LOSS_Discriminator: 0.13683651387691498\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.072302341461182 LOSS_Discriminator: 0.17333140969276428\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.341324329376221 LOSS_Discriminator: 0.10867076367139816\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.50212287902832 LOSS_Discriminator: 0.07858945429325104\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.384613037109375 LOSS_Discriminator: 0.1166321188211441\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.459173202514648 LOSS_Discriminator: 0.1757960468530655\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.915622711181641 LOSS_Discriminator: 0.12715739011764526\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.765701770782471 LOSS_Discriminator: 0.14250700175762177\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.736920356750488 LOSS_Discriminator: 0.20027752220630646\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.442519664764404 LOSS_Discriminator: 0.21650272607803345\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.792296886444092 LOSS_Discriminator: 0.06697118282318115\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.404697418212891 LOSS_Discriminator: 0.15293613076210022\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.5492730140686035 LOSS_Discriminator: 0.075593002140522\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.770883560180664 LOSS_Discriminator: 0.23168784379959106\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.8153395652771 LOSS_Discriminator: 0.16756197810173035\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.750507831573486 LOSS_Discriminator: 0.2915540933609009\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.932374954223633 LOSS_Discriminator: 0.09195482730865479\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.6681036949157715 LOSS_Discriminator: 0.1445855051279068\n",
            "ITERATION_NO.: 35 LOSS_Generator: 6.034573554992676 LOSS_Discriminator: 0.1254730075597763\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.820253849029541 LOSS_Discriminator: 0.1600400060415268\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.506886005401611 LOSS_Discriminator: 0.08266167342662811\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.58769416809082 LOSS_Discriminator: 0.08714531362056732\n",
            "ITERATION_NO.: 39 LOSS_Generator: 5.295094013214111 LOSS_Discriminator: 0.17213888466358185\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.073586940765381 LOSS_Discriminator: 0.1496673822402954\n",
            "ITERATION_NO.: 41 LOSS_Generator: 4.5857038497924805 LOSS_Discriminator: 0.16618789732456207\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.287482738494873 LOSS_Discriminator: 0.27271729707717896\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.590785980224609 LOSS_Discriminator: 0.21787017583847046\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.824533462524414 LOSS_Discriminator: 0.21736498177051544\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.11439323425293 LOSS_Discriminator: 0.17846305668354034\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.9940643310546875 LOSS_Discriminator: 0.2140699326992035\n",
            "ITERATION_NO.: 47 LOSS_Generator: 6.2098283767700195 LOSS_Discriminator: 0.12226216495037079\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.847951889038086 LOSS_Discriminator: 0.11266137659549713\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.828166961669922 LOSS_Discriminator: 0.14726896584033966\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.2315568923950195 LOSS_Discriminator: 0.1854683756828308\n",
            "ITERATION_NO.: 51 LOSS_Generator: 4.858984470367432 LOSS_Discriminator: 0.08258390426635742\n",
            "ITERATION_NO.: 52 LOSS_Generator: 4.327352523803711 LOSS_Discriminator: 0.2000577598810196\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.031248569488525 LOSS_Discriminator: 0.23375210165977478\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.084162712097168 LOSS_Discriminator: 0.24789568781852722\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.826891899108887 LOSS_Discriminator: 0.09091193974018097\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.530828952789307 LOSS_Discriminator: 0.1398114114999771\n",
            "ITERATION_NO.: 57 LOSS_Generator: 6.082802772521973 LOSS_Discriminator: 0.07763884961605072\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.98923397064209 LOSS_Discriminator: 0.18166810274124146\n",
            "ITERATION_NO.: 59 LOSS_Generator: 6.402511119842529 LOSS_Discriminator: 0.16863176226615906\n",
            "ITERATION_NO.: 60 LOSS_Generator: 6.1881513595581055 LOSS_Discriminator: 0.2620388865470886\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.197248935699463 LOSS_Discriminator: 0.09049615263938904\n",
            "ITERATION_NO.: 62 LOSS_Generator: 5.792496204376221 LOSS_Discriminator: 0.18202494084835052\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.610563278198242 LOSS_Discriminator: 0.16870686411857605\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.106308937072754 LOSS_Discriminator: 0.12420671433210373\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.259317398071289 LOSS_Discriminator: 0.10116316378116608\n",
            "ITERATION_NO.: 66 LOSS_Generator: 4.9163408279418945 LOSS_Discriminator: 0.16491559147834778\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.4904680252075195 LOSS_Discriminator: 0.11657360196113586\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.84271240234375 LOSS_Discriminator: 0.10983239114284515\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.7750115394592285 LOSS_Discriminator: 0.08869966864585876\n",
            "ITERATION_NO.: 70 LOSS_Generator: 6.277060508728027 LOSS_Discriminator: 0.15278679132461548\n",
            "ITERATION_NO.: 71 LOSS_Generator: 6.23806619644165 LOSS_Discriminator: 0.19665786623954773\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.795835018157959 LOSS_Discriminator: 0.12018747627735138\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.815308094024658 LOSS_Discriminator: 0.20439863204956055\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.263772487640381 LOSS_Discriminator: 0.12786166369915009\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.357922554016113 LOSS_Discriminator: 0.05846722424030304\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.123425483703613 LOSS_Discriminator: 0.15616661310195923\n",
            "ITERATION_NO.: 77 LOSS_Generator: 4.868243217468262 LOSS_Discriminator: 0.1394919753074646\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.240764141082764 LOSS_Discriminator: 0.16786833107471466\n",
            "ITERATION_NO.: 79 LOSS_Generator: 4.8299641609191895 LOSS_Discriminator: 0.30469265580177307\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.403817176818848 LOSS_Discriminator: 0.10641433298587799\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.709873676300049 LOSS_Discriminator: 0.13872501254081726\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.7100749015808105 LOSS_Discriminator: 0.07445217669010162\n",
            "ITERATION_NO.: 83 LOSS_Generator: 6.2717108726501465 LOSS_Discriminator: 0.1265096515417099\n",
            "ITERATION_NO.: 84 LOSS_Generator: 6.2037763595581055 LOSS_Discriminator: 0.20360597968101501\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.602654457092285 LOSS_Discriminator: 0.20319059491157532\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.730352401733398 LOSS_Discriminator: 0.10006918758153915\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.900056838989258 LOSS_Discriminator: 0.23585695028305054\n",
            "ITERATION_NO.: 88 LOSS_Generator: 4.974740028381348 LOSS_Discriminator: 0.14369645714759827\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.25810432434082 LOSS_Discriminator: 0.1624215841293335\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.409883499145508 LOSS_Discriminator: 0.2364373505115509\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.481358528137207 LOSS_Discriminator: 0.1065911129117012\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.199960231781006 LOSS_Discriminator: 0.1411522477865219\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.464194297790527 LOSS_Discriminator: 0.1596144735813141\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.345310211181641 LOSS_Discriminator: 0.18763913214206696\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.345618724822998 LOSS_Discriminator: 0.0762067586183548\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.6068596839904785 LOSS_Discriminator: 0.11611540615558624\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.461492538452148 LOSS_Discriminator: 0.19040150940418243\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.577767372131348 LOSS_Discriminator: 0.21425151824951172\n",
            "ITERATION_NO.: 99 LOSS_Generator: 5.4313812255859375 LOSS_Discriminator: 0.13351956009864807\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.686532497406006 LOSS_Discriminator: 0.1350645124912262\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.693655490875244 LOSS_Discriminator: 0.09960786253213882\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.829754829406738 LOSS_Discriminator: 0.13308864831924438\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.84918212890625 LOSS_Discriminator: 0.09011543542146683\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.516735076904297 LOSS_Discriminator: 0.12606410682201385\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.627399921417236 LOSS_Discriminator: 0.12328878045082092\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.504100322723389 LOSS_Discriminator: 0.14229030907154083\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.521600723266602 LOSS_Discriminator: 0.13205204904079437\n",
            "ITERATION_NO.: 108 LOSS_Generator: 4.986120700836182 LOSS_Discriminator: 0.1485823094844818\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.877025604248047 LOSS_Discriminator: 0.14705541729927063\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.324192047119141 LOSS_Discriminator: 0.11407411098480225\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.298313140869141 LOSS_Discriminator: 0.07454650849103928\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.4458537101745605 LOSS_Discriminator: 0.08811633288860321\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.791874408721924 LOSS_Discriminator: 0.18008136749267578\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.475121974945068 LOSS_Discriminator: 0.2241562157869339\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.956312656402588 LOSS_Discriminator: 0.07593484967947006\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.3001604080200195 LOSS_Discriminator: 0.1289834976196289\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.417054653167725 LOSS_Discriminator: 0.20602771639823914\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.094743251800537 LOSS_Discriminator: 0.1144481748342514\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.0581278800964355 LOSS_Discriminator: 0.07549490034580231\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.343927383422852 LOSS_Discriminator: 0.1912788450717926\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.155351638793945 LOSS_Discriminator: 0.08150309324264526\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.443553924560547 LOSS_Discriminator: 0.1847667396068573\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.526522636413574 LOSS_Discriminator: 0.09032565355300903\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.571725368499756 LOSS_Discriminator: 0.17092406749725342\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.432011127471924 LOSS_Discriminator: 0.19477006793022156\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.731564521789551 LOSS_Discriminator: 0.18188130855560303\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.598395824432373 LOSS_Discriminator: 0.13576292991638184\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.350401401519775 LOSS_Discriminator: 0.238019198179245\n",
            "ITERATION_NO.: 129 LOSS_Generator: 6.262148857116699 LOSS_Discriminator: 0.18604691326618195\n",
            "ITERATION_NO.: 130 LOSS_Generator: 6.041901111602783 LOSS_Discriminator: 0.22148892283439636\n",
            "ITERATION_NO.: 131 LOSS_Generator: 5.950451850891113 LOSS_Discriminator: 0.13308116793632507\n",
            "ITERATION_NO.: 132 LOSS_Generator: 6.10401725769043 LOSS_Discriminator: 0.11875450611114502\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.680777072906494 LOSS_Discriminator: 0.1451175957918167\n",
            "ITERATION_NO.: 134 LOSS_Generator: 6.288459300994873 LOSS_Discriminator: 0.22147130966186523\n",
            "ITERATION_NO.: 135 LOSS_Generator: 6.116999626159668 LOSS_Discriminator: 0.14365831017494202\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.598546981811523 LOSS_Discriminator: 0.187807098031044\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.9465765953063965 LOSS_Discriminator: 0.07523779571056366\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.613616943359375 LOSS_Discriminator: 0.22514408826828003\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.710792064666748 LOSS_Discriminator: 0.23999756574630737\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.650163650512695 LOSS_Discriminator: 0.14158639311790466\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.329875946044922 LOSS_Discriminator: 0.22701627016067505\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.763813018798828 LOSS_Discriminator: 0.0944070816040039\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.827641010284424 LOSS_Discriminator: 0.12243308126926422\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.338019371032715 LOSS_Discriminator: 0.11926469951868057\n",
            "ITERATION_NO.: 145 LOSS_Generator: 6.614607334136963 LOSS_Discriminator: 0.19392764568328857\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.3241658210754395 LOSS_Discriminator: 0.14007443189620972\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.992233753204346 LOSS_Discriminator: 0.05882076919078827\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.56173038482666 LOSS_Discriminator: 0.20387960970401764\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.836404323577881 LOSS_Discriminator: 0.1284877359867096\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.9322896003723145 LOSS_Discriminator: 0.08787582814693451\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.084161758422852 LOSS_Discriminator: 0.10032166540622711\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.330288887023926 LOSS_Discriminator: 0.11559869349002838\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.352680683135986 LOSS_Discriminator: 0.30457818508148193\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.524719715118408 LOSS_Discriminator: 0.094693124294281\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.852667808532715 LOSS_Discriminator: 0.11486031115055084\n",
            "ITERATION_NO.: 156 LOSS_Generator: 6.204675674438477 LOSS_Discriminator: 0.20016321539878845\n",
            "ITERATION_NO.: 157 LOSS_Generator: 6.257833957672119 LOSS_Discriminator: 0.10348593443632126\n",
            "ITERATION_NO.: 158 LOSS_Generator: 6.0041913986206055 LOSS_Discriminator: 0.13166972994804382\n",
            "ITERATION_NO.: 159 LOSS_Generator: 6.267336368560791 LOSS_Discriminator: 0.15424251556396484\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.761298179626465 LOSS_Discriminator: 0.28118032217025757\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.884387969970703 LOSS_Discriminator: 0.11910036206245422\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.687091827392578 LOSS_Discriminator: 0.1290939450263977\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.705526351928711 LOSS_Discriminator: 0.2196805477142334\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.358087062835693 LOSS_Discriminator: 0.20986008644104004\n",
            "ITERATION_NO.: 165 LOSS_Generator: 6.013506889343262 LOSS_Discriminator: 0.10551796108484268\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.96700382232666 LOSS_Discriminator: 0.16905619204044342\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.461568832397461 LOSS_Discriminator: 0.13895650207996368\n",
            "ITERATION_NO.: 168 LOSS_Generator: 6.411235332489014 LOSS_Discriminator: 0.19687865674495697\n",
            "ITERATION_NO.: 169 LOSS_Generator: 6.314161777496338 LOSS_Discriminator: 0.21299414336681366\n",
            "ITERATION_NO.: 170 LOSS_Generator: 6.412665843963623 LOSS_Discriminator: 0.20598770678043365\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.911798000335693 LOSS_Discriminator: 0.14817297458648682\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.385809898376465 LOSS_Discriminator: 0.079997718334198\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.048408508300781 LOSS_Discriminator: 0.21610689163208008\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.343429088592529 LOSS_Discriminator: 0.16427847743034363\n",
            "ITERATION_NO.: 175 LOSS_Generator: 4.803738594055176 LOSS_Discriminator: 0.22990423440933228\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.476529598236084 LOSS_Discriminator: 0.3659786880016327\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.417280197143555 LOSS_Discriminator: 0.12319325655698776\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.833990097045898 LOSS_Discriminator: 0.12345528602600098\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.638780117034912 LOSS_Discriminator: 0.10776916146278381\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.818365097045898 LOSS_Discriminator: 0.12421157956123352\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.943326473236084 LOSS_Discriminator: 0.1814008355140686\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.651151657104492 LOSS_Discriminator: 0.05827498435974121\n",
            "ITERATION_NO.: 183 LOSS_Generator: 4.701028823852539 LOSS_Discriminator: 0.11809836328029633\n",
            "ITERATION_NO.: 184 LOSS_Generator: 4.65821647644043 LOSS_Discriminator: 0.04619947820901871\n",
            "ITERATION_NO.: 185 LOSS_Generator: 4.85737419128418 LOSS_Discriminator: 0.18750523030757904\n",
            "ITERATION_NO.: 186 LOSS_Generator: 4.751896858215332 LOSS_Discriminator: 0.09021929651498795\n",
            "ITERATION_NO.: 187 LOSS_Generator: 4.9963274002075195 LOSS_Discriminator: 0.13607291877269745\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.533648490905762 LOSS_Discriminator: 0.19407393038272858\n",
            "ITERATION_NO.: 189 LOSS_Generator: 6.0598039627075195 LOSS_Discriminator: 0.09620267152786255\n",
            "ITERATION_NO.: 190 LOSS_Generator: 6.321882247924805 LOSS_Discriminator: 0.2563975155353546\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.963229179382324 LOSS_Discriminator: 0.18508900701999664\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.108642101287842 LOSS_Discriminator: 0.22155830264091492\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.738748073577881 LOSS_Discriminator: 0.17496538162231445\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.651413440704346 LOSS_Discriminator: 0.21864113211631775\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.271811485290527 LOSS_Discriminator: 0.17609000205993652\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.302537441253662 LOSS_Discriminator: 0.10495908558368683\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.098675727844238 LOSS_Discriminator: 0.10780608654022217\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.3826799392700195 LOSS_Discriminator: 0.13698852062225342\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.393127918243408 LOSS_Discriminator: 0.18025045096874237\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.535479545593262 LOSS_Discriminator: 0.25408869981765747\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.207364559173584 LOSS_Discriminator: 0.11384440213441849\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.256894111633301 LOSS_Discriminator: 0.2044663280248642\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.526741981506348 LOSS_Discriminator: 0.15383771061897278\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.716325759887695 LOSS_Discriminator: 0.16537389159202576\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.450040340423584 LOSS_Discriminator: 0.1327773779630661\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.7592058181762695 LOSS_Discriminator: 0.18136560916900635\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.646894454956055 LOSS_Discriminator: 0.1020694151520729\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.394112586975098 LOSS_Discriminator: 0.21908560395240784\n",
            "ITERATION_NO.: 209 LOSS_Generator: 4.9235711097717285 LOSS_Discriminator: 0.1193990558385849\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.304977893829346 LOSS_Discriminator: 0.11048127710819244\n",
            "ITERATION_NO.: 211 LOSS_Generator: 4.799323558807373 LOSS_Discriminator: 0.0811479389667511\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.664356708526611 LOSS_Discriminator: 0.17789730429649353\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.381923198699951 LOSS_Discriminator: 0.1876770257949829\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.501514434814453 LOSS_Discriminator: 0.18840980529785156\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.419766902923584 LOSS_Discriminator: 0.26433032751083374\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.957342147827148 LOSS_Discriminator: 0.10378935933113098\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.835789680480957 LOSS_Discriminator: 0.10661347955465317\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.848568916320801 LOSS_Discriminator: 0.1468605250120163\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.840707302093506 LOSS_Discriminator: 0.18709036707878113\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.7887163162231445 LOSS_Discriminator: 0.1488276869058609\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.283300399780273 LOSS_Discriminator: 0.11323294043540955\n",
            "ITERATION_NO.: 222 LOSS_Generator: 4.984870910644531 LOSS_Discriminator: 0.20675119757652283\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.120703220367432 LOSS_Discriminator: 0.18642789125442505\n",
            "ITERATION_NO.: 224 LOSS_Generator: 4.944822311401367 LOSS_Discriminator: 0.09092134237289429\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.306567192077637 LOSS_Discriminator: 0.14429877698421478\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.640314102172852 LOSS_Discriminator: 0.13850560784339905\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.084173202514648 LOSS_Discriminator: 0.09560712426900864\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.766737461090088 LOSS_Discriminator: 0.20083120465278625\n",
            "ITERATION_NO.: 229 LOSS_Generator: 6.387566089630127 LOSS_Discriminator: 0.09209506213665009\n",
            "ITERATION_NO.: 230 LOSS_Generator: 6.138929843902588 LOSS_Discriminator: 0.13912877440452576\n",
            "ITERATION_NO.: 231 LOSS_Generator: 6.09945821762085 LOSS_Discriminator: 0.11328060179948807\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.79581356048584 LOSS_Discriminator: 0.1774449348449707\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.438671112060547 LOSS_Discriminator: 0.22813262045383453\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.336358070373535 LOSS_Discriminator: 0.12284095585346222\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.140711784362793 LOSS_Discriminator: 0.10978738218545914\n",
            "ITERATION_NO.: 236 LOSS_Generator: 4.7678632736206055 LOSS_Discriminator: 0.10113532841205597\n",
            "ITERATION_NO.: 237 LOSS_Generator: 5.147193431854248 LOSS_Discriminator: 0.11576348543167114\n",
            "ITERATION_NO.: 238 LOSS_Generator: 4.976215362548828 LOSS_Discriminator: 0.16669133305549622\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.537480354309082 LOSS_Discriminator: 0.19374774396419525\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.801532745361328 LOSS_Discriminator: 0.157279372215271\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.131402969360352 LOSS_Discriminator: 0.15241160988807678\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.688938617706299 LOSS_Discriminator: 0.04202853515744209\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.685574531555176 LOSS_Discriminator: 0.11783063411712646\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.349635124206543 LOSS_Discriminator: 0.17450764775276184\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.268252372741699 LOSS_Discriminator: 0.2038688361644745\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.353265285491943 LOSS_Discriminator: 0.1453995555639267\n",
            "ITERATION_NO.: 247 LOSS_Generator: 6.13865852355957 LOSS_Discriminator: 0.11824662983417511\n",
            "ITERATION_NO.: 248 LOSS_Generator: 6.146684646606445 LOSS_Discriminator: 0.27799373865127563\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.587121963500977 LOSS_Discriminator: 0.19770848751068115\n",
            "ITERATION_NO.: 250 LOSS_Generator: 4.705184459686279 LOSS_Discriminator: 0.15373259782791138\n",
            "ITERATION_NO.: 251 LOSS_Generator: 4.980364799499512 LOSS_Discriminator: 0.06896565854549408\n",
            "ITERATION_NO.: 252 LOSS_Generator: 4.397627353668213 LOSS_Discriminator: 0.14422951638698578\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.106696605682373 LOSS_Discriminator: 0.2978387176990509\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.092772006988525 LOSS_Discriminator: 0.19554133713245392\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.859154224395752 LOSS_Discriminator: 0.12844063341617584\n",
            "ITERATION_NO.: 256 LOSS_Generator: 6.109163761138916 LOSS_Discriminator: 0.1399930864572525\n",
            "ITERATION_NO.: 257 LOSS_Generator: 6.291012763977051 LOSS_Discriminator: 0.1486775428056717\n",
            "ITERATION_NO.: 258 LOSS_Generator: 6.882903575897217 LOSS_Discriminator: 0.11485286802053452\n",
            "ITERATION_NO.: 259 LOSS_Generator: 6.743138313293457 LOSS_Discriminator: 0.23170679807662964\n",
            "ITERATION_NO.: 260 LOSS_Generator: 6.693084239959717 LOSS_Discriminator: 0.09690043330192566\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.152164936065674 LOSS_Discriminator: 0.20757603645324707\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.499603748321533 LOSS_Discriminator: 0.11500184237957001\n",
            "ITERATION_NO.: 263 LOSS_Generator: 4.759354114532471 LOSS_Discriminator: 0.2045416533946991\n",
            "ITERATION_NO.: 264 LOSS_Generator: 4.428083419799805 LOSS_Discriminator: 0.17231789231300354\n",
            "ITERATION_NO.: 265 LOSS_Generator: 4.4073591232299805 LOSS_Discriminator: 0.2200663685798645\n",
            "ITERATION_NO.: 266 LOSS_Generator: 4.740754127502441 LOSS_Discriminator: 0.1713590919971466\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.240828037261963 LOSS_Discriminator: 0.1971529871225357\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.771706581115723 LOSS_Discriminator: 0.15865221619606018\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.7187724113464355 LOSS_Discriminator: 0.06374464929103851\n",
            "ITERATION_NO.: 270 LOSS_Generator: 6.896360397338867 LOSS_Discriminator: 0.12165579199790955\n",
            "ITERATION_NO.: 271 LOSS_Generator: 6.705729961395264 LOSS_Discriminator: 0.1687997281551361\n",
            "ITERATION_NO.: 272 LOSS_Generator: 6.206884384155273 LOSS_Discriminator: 0.09766741096973419\n",
            "ITERATION_NO.: 273 LOSS_Generator: 6.073665618896484 LOSS_Discriminator: 0.2871364653110504\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.513766288757324 LOSS_Discriminator: 0.16907864809036255\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.318840503692627 LOSS_Discriminator: 0.1691211462020874\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.192011833190918 LOSS_Discriminator: 0.1973567008972168\n",
            "ITERATION_NO.: 277 LOSS_Generator: 4.81549072265625 LOSS_Discriminator: 0.12925589084625244\n",
            "ITERATION_NO.: 278 LOSS_Generator: 4.94788122177124 LOSS_Discriminator: 0.1089833676815033\n",
            "ITERATION_NO.: 279 LOSS_Generator: 4.661170959472656 LOSS_Discriminator: 0.14972704648971558\n",
            "ITERATION_NO.: 280 LOSS_Generator: 4.8151092529296875 LOSS_Discriminator: 0.13780850172042847\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.3937458992004395 LOSS_Discriminator: 0.1373804658651352\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.895583629608154 LOSS_Discriminator: 0.19232970476150513\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.9427995681762695 LOSS_Discriminator: 0.1208503246307373\n",
            "ITERATION_NO.: 284 LOSS_Generator: 6.2644805908203125 LOSS_Discriminator: 0.10148860514163971\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.734714984893799 LOSS_Discriminator: 0.21015921235084534\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.069673538208008 LOSS_Discriminator: 0.11120394617319107\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.305179595947266 LOSS_Discriminator: 0.2088850736618042\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.352978706359863 LOSS_Discriminator: 0.18489539623260498\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.435791015625 LOSS_Discriminator: 0.20205171406269073\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.273709297180176 LOSS_Discriminator: 0.10573416948318481\n",
            "ITERATION_NO.: 291 LOSS_Generator: 4.934351921081543 LOSS_Discriminator: 0.06897023320198059\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.618480205535889 LOSS_Discriminator: 0.11376409232616425\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.726706504821777 LOSS_Discriminator: 0.060037050396203995\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.8257951736450195 LOSS_Discriminator: 0.11293283849954605\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.5320820808410645 LOSS_Discriminator: 0.10080040246248245\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.335381031036377 LOSS_Discriminator: 0.15453368425369263\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.4875006675720215 LOSS_Discriminator: 0.1542501151561737\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.852585315704346 LOSS_Discriminator: 0.19334626197814941\n",
            "ITERATION_NO.: 299 LOSS_Generator: 6.0591559410095215 LOSS_Discriminator: 0.08257487416267395\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.8671674728393555 LOSS_Discriminator: 0.1233089417219162\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.648910999298096 LOSS_Discriminator: 0.18160319328308105\n",
            "ITERATION_NO.: 302 LOSS_Generator: 4.781368732452393 LOSS_Discriminator: 0.0814291313290596\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.1492156982421875 LOSS_Discriminator: 0.1327521800994873\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.111976146697998 LOSS_Discriminator: 0.1576448231935501\n",
            "ITERATION_NO.: 305 LOSS_Generator: 4.811829090118408 LOSS_Discriminator: 0.05648566409945488\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.164528846740723 LOSS_Discriminator: 0.16699214279651642\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.042186737060547 LOSS_Discriminator: 0.18790003657341003\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.45168924331665 LOSS_Discriminator: 0.14348208904266357\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.418662071228027 LOSS_Discriminator: 0.13024593889713287\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.572815418243408 LOSS_Discriminator: 0.186386838555336\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.929986953735352 LOSS_Discriminator: 0.18591910600662231\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.192831039428711 LOSS_Discriminator: 0.17157398164272308\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.639459609985352 LOSS_Discriminator: 0.10738767683506012\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.328742027282715 LOSS_Discriminator: 0.17235563695430756\n",
            "ITERATION_NO.: 315 LOSS_Generator: 5.432331562042236 LOSS_Discriminator: 0.12426599115133286\n",
            "ITERATION_NO.: 316 LOSS_Generator: 5.520789623260498 LOSS_Discriminator: 0.11476727575063705\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.7281494140625 LOSS_Discriminator: 0.05626398324966431\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.559728622436523 LOSS_Discriminator: 0.16209328174591064\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.852097034454346 LOSS_Discriminator: 0.11952481418848038\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.747148513793945 LOSS_Discriminator: 0.14316526055335999\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.1624979972839355 LOSS_Discriminator: 0.16246512532234192\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.674116134643555 LOSS_Discriminator: 0.1873367726802826\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.554765224456787 LOSS_Discriminator: 0.17144256830215454\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.535716533660889 LOSS_Discriminator: 0.1111333817243576\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.398488998413086 LOSS_Discriminator: 0.09601110219955444\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.275052547454834 LOSS_Discriminator: 0.12408272922039032\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.225709915161133 LOSS_Discriminator: 0.17734487354755402\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.178351879119873 LOSS_Discriminator: 0.10865572839975357\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.519512176513672 LOSS_Discriminator: 0.14463086426258087\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.708676815032959 LOSS_Discriminator: 0.1521739512681961\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.929160118103027 LOSS_Discriminator: 0.12455621361732483\n",
            "ITERATION_NO.: 332 LOSS_Generator: 6.184983730316162 LOSS_Discriminator: 0.1104913130402565\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.695061683654785 LOSS_Discriminator: 0.14202874898910522\n",
            "ITERATION_NO.: 334 LOSS_Generator: 6.378924369812012 LOSS_Discriminator: 0.1215970516204834\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.850780010223389 LOSS_Discriminator: 0.25988852977752686\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.373693466186523 LOSS_Discriminator: 0.13816693425178528\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.43955135345459 LOSS_Discriminator: 0.25043314695358276\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.043330192565918 LOSS_Discriminator: 0.08417540788650513\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.339763641357422 LOSS_Discriminator: 0.1013679951429367\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.152730941772461 LOSS_Discriminator: 0.11345858871936798\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.332775115966797 LOSS_Discriminator: 0.167332261800766\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.996010780334473 LOSS_Discriminator: 0.14134886860847473\n",
            "ITERATION_NO.: 343 LOSS_Generator: 6.277649402618408 LOSS_Discriminator: 0.14964549243450165\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.94688606262207 LOSS_Discriminator: 0.12232151627540588\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.881254196166992 LOSS_Discriminator: 0.1743147224187851\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.705854892730713 LOSS_Discriminator: 0.1355714499950409\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.518702507019043 LOSS_Discriminator: 0.12762346863746643\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.075351715087891 LOSS_Discriminator: 0.08658936619758606\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.211684703826904 LOSS_Discriminator: 0.19787228107452393\n",
            "ITERATION_NO.: 350 LOSS_Generator: 4.9946608543396 LOSS_Discriminator: 0.12408885359764099\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.3214545249938965 LOSS_Discriminator: 0.0860615149140358\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.339849948883057 LOSS_Discriminator: 0.10337629169225693\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.773614883422852 LOSS_Discriminator: 0.16722705960273743\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.614329814910889 LOSS_Discriminator: 0.12318640947341919\n",
            "ITERATION_NO.: 355 LOSS_Generator: 6.484896659851074 LOSS_Discriminator: 0.16700375080108643\n",
            "ITERATION_NO.: 356 LOSS_Generator: 6.362870693206787 LOSS_Discriminator: 0.06802039593458176\n",
            "ITERATION_NO.: 357 LOSS_Generator: 6.061558246612549 LOSS_Discriminator: 0.07098967581987381\n",
            "ITERATION_NO.: 358 LOSS_Generator: 6.145969867706299 LOSS_Discriminator: 0.15553531050682068\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.69069766998291 LOSS_Discriminator: 0.1047820970416069\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.995345592498779 LOSS_Discriminator: 0.2095443606376648\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.348630905151367 LOSS_Discriminator: 0.1231711357831955\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.079647064208984 LOSS_Discriminator: 0.2279607504606247\n",
            "ITERATION_NO.: 363 LOSS_Generator: 4.884204387664795 LOSS_Discriminator: 0.1282031536102295\n",
            "ITERATION_NO.: 364 LOSS_Generator: 4.977936744689941 LOSS_Discriminator: 0.15580123662948608\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.4479899406433105 LOSS_Discriminator: 0.14095118641853333\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.38832950592041 LOSS_Discriminator: 0.1403551995754242\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.789297103881836 LOSS_Discriminator: 0.1683504581451416\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.888440132141113 LOSS_Discriminator: 0.13267764449119568\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.7319560050964355 LOSS_Discriminator: 0.22175481915473938\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.661285877227783 LOSS_Discriminator: 0.09137386083602905\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.522704601287842 LOSS_Discriminator: 0.19246432185173035\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.341182708740234 LOSS_Discriminator: 0.10543737560510635\n",
            "ITERATION_NO.: 373 LOSS_Generator: 4.927041530609131 LOSS_Discriminator: 0.18480050563812256\n",
            "ITERATION_NO.: 374 LOSS_Generator: 4.9650797843933105 LOSS_Discriminator: 0.08711740374565125\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.566462993621826 LOSS_Discriminator: 0.1257043331861496\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.598301887512207 LOSS_Discriminator: 0.10596049576997757\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.700118541717529 LOSS_Discriminator: 0.2213161587715149\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.60667610168457 LOSS_Discriminator: 0.14143052697181702\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.935237407684326 LOSS_Discriminator: 0.1125725656747818\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.801191806793213 LOSS_Discriminator: 0.28435173630714417\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.140476703643799 LOSS_Discriminator: 0.22012245655059814\n",
            "ITERATION_NO.: 382 LOSS_Generator: 4.610580921173096 LOSS_Discriminator: 0.06695627421140671\n",
            "ITERATION_NO.: 383 LOSS_Generator: 4.660977840423584 LOSS_Discriminator: 0.1997717320919037\n",
            "ITERATION_NO.: 384 LOSS_Generator: 4.850011825561523 LOSS_Discriminator: 0.13620500266551971\n",
            "ITERATION_NO.: 385 LOSS_Generator: 4.787558078765869 LOSS_Discriminator: 0.18850703537464142\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.204596996307373 LOSS_Discriminator: 0.1628914475440979\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.934661388397217 LOSS_Discriminator: 0.1916375607252121\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.9793243408203125 LOSS_Discriminator: 0.21121540665626526\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.307583808898926 LOSS_Discriminator: 0.2538047432899475\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.678747653961182 LOSS_Discriminator: 0.24134650826454163\n",
            "ITERATION_NO.: 391 LOSS_Generator: 5.674443244934082 LOSS_Discriminator: 0.3081538677215576\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.02191162109375 LOSS_Discriminator: 0.15060517191886902\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.152038097381592 LOSS_Discriminator: 0.19738870859146118\n",
            "ITERATION_NO.: 394 LOSS_Generator: 4.401016712188721 LOSS_Discriminator: 0.19209864735603333\n",
            "ITERATION_NO.: 395 LOSS_Generator: 4.710777282714844 LOSS_Discriminator: 0.21434654295444489\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.033489227294922 LOSS_Discriminator: 0.14952798187732697\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.3331499099731445 LOSS_Discriminator: 0.20606441795825958\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.573808670043945 LOSS_Discriminator: 0.18963953852653503\n",
            "ITERATION_NO.: 399 LOSS_Generator: 6.032200336456299 LOSS_Discriminator: 0.27339333295822144\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.42753791809082 LOSS_Discriminator: 0.2780333161354065\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.79168701171875 LOSS_Discriminator: 0.1772516965866089\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.711268901824951 LOSS_Discriminator: 0.13810816407203674\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.473987579345703 LOSS_Discriminator: 0.08693263679742813\n",
            "ITERATION_NO.: 404 LOSS_Generator: 4.846383094787598 LOSS_Discriminator: 0.15955767035484314\n",
            "ITERATION_NO.: 405 LOSS_Generator: 4.825732231140137 LOSS_Discriminator: 0.20967459678649902\n",
            "ITERATION_NO.: 406 LOSS_Generator: 4.709499835968018 LOSS_Discriminator: 0.24407470226287842\n",
            "ITERATION_NO.: 407 LOSS_Generator: 4.500990390777588 LOSS_Discriminator: 0.11356645077466965\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.064098358154297 LOSS_Discriminator: 0.125273197889328\n",
            "ITERATION_NO.: 409 LOSS_Generator: 4.639580249786377 LOSS_Discriminator: 0.34743136167526245\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.1847052574157715 LOSS_Discriminator: 0.12238624691963196\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.461763381958008 LOSS_Discriminator: 0.13335862755775452\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.662722587585449 LOSS_Discriminator: 0.12101082503795624\n",
            "ITERATION_NO.: 413 LOSS_Generator: 6.009072303771973 LOSS_Discriminator: 0.22382326424121857\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.096601486206055 LOSS_Discriminator: 0.16082237660884857\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.186798095703125 LOSS_Discriminator: 0.19149532914161682\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.331310272216797 LOSS_Discriminator: 0.16997219622135162\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.425197601318359 LOSS_Discriminator: 0.31023842096328735\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.296365261077881 LOSS_Discriminator: 0.12834838032722473\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.202348232269287 LOSS_Discriminator: 0.1853564828634262\n",
            "ITERATION_NO.: 420 LOSS_Generator: 4.903329849243164 LOSS_Discriminator: 0.09045334160327911\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.240684986114502 LOSS_Discriminator: 0.21136894822120667\n",
            "ITERATION_NO.: 422 LOSS_Generator: 4.5983195304870605 LOSS_Discriminator: 0.21727418899536133\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.347369194030762 LOSS_Discriminator: 0.16038568317890167\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.896707057952881 LOSS_Discriminator: 0.18752510845661163\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.045342922210693 LOSS_Discriminator: 0.1638784557580948\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.956280708312988 LOSS_Discriminator: 0.17455503344535828\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.574511528015137 LOSS_Discriminator: 0.18546415865421295\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.1438446044921875 LOSS_Discriminator: 0.205863356590271\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.851537704467773 LOSS_Discriminator: 0.2561558187007904\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.6809821128845215 LOSS_Discriminator: 0.09753692150115967\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.0761332511901855 LOSS_Discriminator: 0.10127389430999756\n",
            "ITERATION_NO.: 432 LOSS_Generator: 4.9125261306762695 LOSS_Discriminator: 0.13993653655052185\n",
            "ITERATION_NO.: 433 LOSS_Generator: 4.566130638122559 LOSS_Discriminator: 0.17413702607154846\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.888583183288574 LOSS_Discriminator: 0.14009180665016174\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.076600074768066 LOSS_Discriminator: 0.20098812878131866\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.116837978363037 LOSS_Discriminator: 0.20325618982315063\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.858712196350098 LOSS_Discriminator: 0.11473838984966278\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.057816028594971 LOSS_Discriminator: 0.1870780736207962\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.247806549072266 LOSS_Discriminator: 0.18169277906417847\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.27302360534668 LOSS_Discriminator: 0.221175879240036\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.251852989196777 LOSS_Discriminator: 0.17951112985610962\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.685770034790039 LOSS_Discriminator: 0.1771167665719986\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.7619524002075195 LOSS_Discriminator: 0.09919625520706177\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.4699177742004395 LOSS_Discriminator: 0.19107790291309357\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.236368179321289 LOSS_Discriminator: 0.13551566004753113\n",
            "ITERATION_NO.: 446 LOSS_Generator: 4.538419723510742 LOSS_Discriminator: 0.1354045569896698\n",
            "ITERATION_NO.: 447 LOSS_Generator: 4.952368259429932 LOSS_Discriminator: 0.148918017745018\n",
            "ITERATION_NO.: 448 LOSS_Generator: 4.989480972290039 LOSS_Discriminator: 0.14629767835140228\n",
            "ITERATION_NO.: 449 LOSS_Generator: 4.807561874389648 LOSS_Discriminator: 0.19956818222999573\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.506853103637695 LOSS_Discriminator: 0.07669961452484131\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.78361701965332 LOSS_Discriminator: 0.1968248188495636\n",
            "ITERATION_NO.: 452 LOSS_Generator: 5.585434436798096 LOSS_Discriminator: 0.1363813877105713\n",
            "ITERATION_NO.: 453 LOSS_Generator: 6.014507293701172 LOSS_Discriminator: 0.14728951454162598\n",
            "ITERATION_NO.: 454 LOSS_Generator: 6.143069267272949 LOSS_Discriminator: 0.1330503523349762\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.878455638885498 LOSS_Discriminator: 0.17979781329631805\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.542301177978516 LOSS_Discriminator: 0.11037570238113403\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.578812122344971 LOSS_Discriminator: 0.19826972484588623\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.214015483856201 LOSS_Discriminator: 0.07772268354892731\n",
            "ITERATION_NO.: 459 LOSS_Generator: 4.945611953735352 LOSS_Discriminator: 0.06248798966407776\n",
            "ITERATION_NO.: 460 LOSS_Generator: 4.8704447746276855 LOSS_Discriminator: 0.19819889962673187\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.133390426635742 LOSS_Discriminator: 0.0792253315448761\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.086927890777588 LOSS_Discriminator: 0.12619489431381226\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.315894603729248 LOSS_Discriminator: 0.049939192831516266\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.278923988342285 LOSS_Discriminator: 0.11473588645458221\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.611584663391113 LOSS_Discriminator: 0.11843756586313248\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.0271148681640625 LOSS_Discriminator: 0.17255540192127228\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.832766056060791 LOSS_Discriminator: 0.10773082077503204\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.969109058380127 LOSS_Discriminator: 0.13341200351715088\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.51213264465332 LOSS_Discriminator: 0.16031983494758606\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.813961029052734 LOSS_Discriminator: 0.09685512632131577\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.801130294799805 LOSS_Discriminator: 0.16768640279769897\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.445987701416016 LOSS_Discriminator: 0.12645719945430756\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.2931623458862305 LOSS_Discriminator: 0.1719849705696106\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.850763320922852 LOSS_Discriminator: 0.1194860190153122\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.361934661865234 LOSS_Discriminator: 0.1899605393409729\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.638424873352051 LOSS_Discriminator: 0.09969941526651382\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.509420871734619 LOSS_Discriminator: 0.182221919298172\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.485257148742676 LOSS_Discriminator: 0.12062940001487732\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.57661247253418 LOSS_Discriminator: 0.18502312898635864\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.849600791931152 LOSS_Discriminator: 0.1230020523071289\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.620940685272217 LOSS_Discriminator: 0.15604497492313385\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.358002185821533 LOSS_Discriminator: 0.10914649069309235\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.589445114135742 LOSS_Discriminator: 0.12239953875541687\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.223707675933838 LOSS_Discriminator: 0.24303370714187622\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.626920223236084 LOSS_Discriminator: 0.0768364742398262\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.488783359527588 LOSS_Discriminator: 0.06571666151285172\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.501545429229736 LOSS_Discriminator: 0.09872958809137344\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.526254177093506 LOSS_Discriminator: 0.21973776817321777\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.562953948974609 LOSS_Discriminator: 0.18442733585834503\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.4152960777282715 LOSS_Discriminator: 0.12559424340724945\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.713939189910889 LOSS_Discriminator: 0.09981858730316162\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.708230018615723 LOSS_Discriminator: 0.19500145316123962\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.576327323913574 LOSS_Discriminator: 0.09359240531921387\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.190019130706787 LOSS_Discriminator: 0.1522512435913086\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.133087158203125 LOSS_Discriminator: 0.13423755764961243\n",
            "ITERATION_NO.: 496 LOSS_Generator: 4.9348530769348145 LOSS_Discriminator: 0.22786706686019897\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.370813369750977 LOSS_Discriminator: 0.17794732749462128\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.212829113006592 LOSS_Discriminator: 0.11561152338981628\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.458832263946533 LOSS_Discriminator: 0.119338259100914\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.793577194213867 LOSS_Discriminator: 0.0708007887005806\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.4773101806640625 LOSS_Discriminator: 0.14102640748023987\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.5087103843688965 LOSS_Discriminator: 0.1267063319683075\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.163877487182617 LOSS_Discriminator: 0.11509408801794052\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.571793079376221 LOSS_Discriminator: 0.21081329882144928\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.58842658996582 LOSS_Discriminator: 0.11918529868125916\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.328863143920898 LOSS_Discriminator: 0.1348543018102646\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.552992343902588 LOSS_Discriminator: 0.0820927694439888\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.040474891662598 LOSS_Discriminator: 0.1864849328994751\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.7983784675598145 LOSS_Discriminator: 0.22228752076625824\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.8594536781311035 LOSS_Discriminator: 0.09337471425533295\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.924154281616211 LOSS_Discriminator: 0.09132005274295807\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.302536487579346 LOSS_Discriminator: 0.12540899217128754\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.245317459106445 LOSS_Discriminator: 0.09317845106124878\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.524998188018799 LOSS_Discriminator: 0.17188948392868042\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.113381385803223 LOSS_Discriminator: 0.11670747399330139\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.152696132659912 LOSS_Discriminator: 0.14305873215198517\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.190248966217041 LOSS_Discriminator: 0.1361595243215561\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.2978620529174805 LOSS_Discriminator: 0.19333696365356445\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.037009239196777 LOSS_Discriminator: 0.10771891474723816\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.713220596313477 LOSS_Discriminator: 0.18842194974422455\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.550018310546875 LOSS_Discriminator: 0.185111865401268\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.900379657745361 LOSS_Discriminator: 0.12481458485126495\n",
            "ITERATION_NO.: 523 LOSS_Generator: 6.152801990509033 LOSS_Discriminator: 0.12374305725097656\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.818448543548584 LOSS_Discriminator: 0.28733497858047485\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.944615364074707 LOSS_Discriminator: 0.114532969892025\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.566809177398682 LOSS_Discriminator: 0.10793799161911011\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.910450458526611 LOSS_Discriminator: 0.16334116458892822\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.54665470123291 LOSS_Discriminator: 0.10015562176704407\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.760400772094727 LOSS_Discriminator: 0.15891097486019135\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.762412071228027 LOSS_Discriminator: 0.07746604084968567\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.808202743530273 LOSS_Discriminator: 0.12301613390445709\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.960413932800293 LOSS_Discriminator: 0.12339207530021667\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.774148464202881 LOSS_Discriminator: 0.14197850227355957\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.8678812980651855 LOSS_Discriminator: 0.224191814661026\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.78237771987915 LOSS_Discriminator: 0.05663001537322998\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.396945953369141 LOSS_Discriminator: 0.12806454300880432\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.625631809234619 LOSS_Discriminator: 0.09959950298070908\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.852436542510986 LOSS_Discriminator: 0.12930913269519806\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.471179008483887 LOSS_Discriminator: 0.1585298478603363\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.249790191650391 LOSS_Discriminator: 0.13912218809127808\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.099920749664307 LOSS_Discriminator: 0.15435323119163513\n",
            "ITERATION_NO.: 542 LOSS_Generator: 4.95706844329834 LOSS_Discriminator: 0.15837495028972626\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.265324592590332 LOSS_Discriminator: 0.20092596113681793\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.0346221923828125 LOSS_Discriminator: 0.11924721300601959\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.046419143676758 LOSS_Discriminator: 0.17812201380729675\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.353919506072998 LOSS_Discriminator: 0.08310441672801971\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.381195068359375 LOSS_Discriminator: 0.15313482284545898\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.411032199859619 LOSS_Discriminator: 0.11404028534889221\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.364810943603516 LOSS_Discriminator: 0.34267657995224\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.068695545196533 LOSS_Discriminator: 0.14159178733825684\n",
            "ITERATION_NO.: 551 LOSS_Generator: 6.161070346832275 LOSS_Discriminator: 0.15323352813720703\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.1040120124816895 LOSS_Discriminator: 0.11918724328279495\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.218488693237305 LOSS_Discriminator: 0.2549249529838562\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.869720458984375 LOSS_Discriminator: 0.08207095414400101\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.706441879272461 LOSS_Discriminator: 0.0733303502202034\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.98930549621582 LOSS_Discriminator: 0.20241963863372803\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.665449619293213 LOSS_Discriminator: 0.17531153559684753\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.344141006469727 LOSS_Discriminator: 0.13446877896785736\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.581713676452637 LOSS_Discriminator: 0.19455575942993164\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.748046398162842 LOSS_Discriminator: 0.1437048316001892\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.122684955596924 LOSS_Discriminator: 0.0935884565114975\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.834560394287109 LOSS_Discriminator: 0.10760155320167542\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.467602729797363 LOSS_Discriminator: 0.1028437614440918\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.798924446105957 LOSS_Discriminator: 0.1980920284986496\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.875564098358154 LOSS_Discriminator: 0.2056945264339447\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.400355815887451 LOSS_Discriminator: 0.15157774090766907\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.345605850219727 LOSS_Discriminator: 0.13311389088630676\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.6815056800842285 LOSS_Discriminator: 0.17012658715248108\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.065495491027832 LOSS_Discriminator: 0.09426166117191315\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.705466270446777 LOSS_Discriminator: 0.08512970805168152\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.819863319396973 LOSS_Discriminator: 0.1351088583469391\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.51779842376709 LOSS_Discriminator: 0.2234247922897339\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.3682708740234375 LOSS_Discriminator: 0.1565505862236023\n",
            "ITERATION_NO.: 574 LOSS_Generator: 4.9334716796875 LOSS_Discriminator: 0.2050146609544754\n",
            "ITERATION_NO.: 575 LOSS_Generator: 4.946272373199463 LOSS_Discriminator: 0.18395014107227325\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.1564621925354 LOSS_Discriminator: 0.17894572019577026\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.416441440582275 LOSS_Discriminator: 0.22596639394760132\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.423340320587158 LOSS_Discriminator: 0.11013555526733398\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.948619365692139 LOSS_Discriminator: 0.12357502430677414\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.140145301818848 LOSS_Discriminator: 0.11579637229442596\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.8208208084106445 LOSS_Discriminator: 0.09199173748493195\n",
            "ITERATION_NO.: 582 LOSS_Generator: 6.3947224617004395 LOSS_Discriminator: 0.139377623796463\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.092258930206299 LOSS_Discriminator: 0.12628653645515442\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.05410623550415 LOSS_Discriminator: 0.1524968296289444\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.598250865936279 LOSS_Discriminator: 0.08714259415864944\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.531638145446777 LOSS_Discriminator: 0.148967906832695\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.377219200134277 LOSS_Discriminator: 0.11176933348178864\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.345552921295166 LOSS_Discriminator: 0.09495650976896286\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.023609638214111 LOSS_Discriminator: 0.09629040956497192\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.044541835784912 LOSS_Discriminator: 0.11078576743602753\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.343553066253662 LOSS_Discriminator: 0.10969453305006027\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.292888641357422 LOSS_Discriminator: 0.12081315368413925\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.748027324676514 LOSS_Discriminator: 0.07882107049226761\n",
            "ITERATION_NO.: 594 LOSS_Generator: 5.887065410614014 LOSS_Discriminator: 0.09596693515777588\n",
            "ITERATION_NO.: 595 LOSS_Generator: 6.543107986450195 LOSS_Discriminator: 0.22370323538780212\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.965579032897949 LOSS_Discriminator: 0.08899472653865814\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.307703971862793 LOSS_Discriminator: 0.1442556381225586\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.338164806365967 LOSS_Discriminator: 0.23033028841018677\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.926678657531738 LOSS_Discriminator: 0.12819424271583557\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.54278564453125 LOSS_Discriminator: 0.04160325229167938\n",
            "EPOCH OVER: 3\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.467031478881836 LOSS_Discriminator: 0.16649684309959412\n",
            "ITERATION_NO.: 2 LOSS_Generator: 4.859781265258789 LOSS_Discriminator: 0.10679756850004196\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.258218765258789 LOSS_Discriminator: 0.0901818498969078\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.494517803192139 LOSS_Discriminator: 0.13964049518108368\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.550290584564209 LOSS_Discriminator: 0.16281719505786896\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.252946853637695 LOSS_Discriminator: 0.16625738143920898\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.910952568054199 LOSS_Discriminator: 0.19739338755607605\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.892880916595459 LOSS_Discriminator: 0.1366885006427765\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.786970615386963 LOSS_Discriminator: 0.18338653445243835\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.1513895988464355 LOSS_Discriminator: 0.1421286016702652\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.516567230224609 LOSS_Discriminator: 0.13931134343147278\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.206182479858398 LOSS_Discriminator: 0.1721486896276474\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.291211128234863 LOSS_Discriminator: 0.13781338930130005\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.160544395446777 LOSS_Discriminator: 0.19025620818138123\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.021520137786865 LOSS_Discriminator: 0.2907685935497284\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.318062782287598 LOSS_Discriminator: 0.18998385965824127\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.091926574707031 LOSS_Discriminator: 0.14167065918445587\n",
            "ITERATION_NO.: 18 LOSS_Generator: 4.9422831535339355 LOSS_Discriminator: 0.07159751653671265\n",
            "ITERATION_NO.: 19 LOSS_Generator: 4.913403034210205 LOSS_Discriminator: 0.0995398461818695\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.170117378234863 LOSS_Discriminator: 0.16727954149246216\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.221113681793213 LOSS_Discriminator: 0.07579554617404938\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.566710948944092 LOSS_Discriminator: 0.1687469333410263\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.697568893432617 LOSS_Discriminator: 0.11353442817926407\n",
            "ITERATION_NO.: 24 LOSS_Generator: 6.002283096313477 LOSS_Discriminator: 0.15198804438114166\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.751355171203613 LOSS_Discriminator: 0.1531234085559845\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.933289051055908 LOSS_Discriminator: 0.1641276478767395\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.837624549865723 LOSS_Discriminator: 0.20027285814285278\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.8923821449279785 LOSS_Discriminator: 0.18373653292655945\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.440567493438721 LOSS_Discriminator: 0.17589572072029114\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.034753799438477 LOSS_Discriminator: 0.1717585176229477\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.701509952545166 LOSS_Discriminator: 0.13550564646720886\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.648642063140869 LOSS_Discriminator: 0.0704549252986908\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.726337432861328 LOSS_Discriminator: 0.1403428614139557\n",
            "ITERATION_NO.: 34 LOSS_Generator: 4.941755294799805 LOSS_Discriminator: 0.23593181371688843\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.629824638366699 LOSS_Discriminator: 0.13989055156707764\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.369900703430176 LOSS_Discriminator: 0.13942091166973114\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.482487678527832 LOSS_Discriminator: 0.17269295454025269\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.825849533081055 LOSS_Discriminator: 0.18335837125778198\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.023001194000244 LOSS_Discriminator: 0.20405840873718262\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.8746113777160645 LOSS_Discriminator: 0.1332291066646576\n",
            "ITERATION_NO.: 41 LOSS_Generator: 6.446437358856201 LOSS_Discriminator: 0.1206618994474411\n",
            "ITERATION_NO.: 42 LOSS_Generator: 6.27810525894165 LOSS_Discriminator: 0.15297754108905792\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.013071060180664 LOSS_Discriminator: 0.08483302593231201\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.2057318687438965 LOSS_Discriminator: 0.17121602594852448\n",
            "ITERATION_NO.: 45 LOSS_Generator: 5.697632312774658 LOSS_Discriminator: 0.07769990712404251\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.710839748382568 LOSS_Discriminator: 0.13900193572044373\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.5225510597229 LOSS_Discriminator: 0.20164622366428375\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.409826278686523 LOSS_Discriminator: 0.14156700670719147\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.9576416015625 LOSS_Discriminator: 0.21154145896434784\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.564813613891602 LOSS_Discriminator: 0.04981674998998642\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.979762077331543 LOSS_Discriminator: 0.13157184422016144\n",
            "ITERATION_NO.: 52 LOSS_Generator: 6.019333362579346 LOSS_Discriminator: 0.13590091466903687\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.864832878112793 LOSS_Discriminator: 0.13552820682525635\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.619917392730713 LOSS_Discriminator: 0.27755361795425415\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.7929534912109375 LOSS_Discriminator: 0.21053101122379303\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.042416095733643 LOSS_Discriminator: 0.173635333776474\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.377896308898926 LOSS_Discriminator: 0.1076161116361618\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.605050563812256 LOSS_Discriminator: 0.10058131814002991\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.670799732208252 LOSS_Discriminator: 0.12138771265745163\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.8812689781188965 LOSS_Discriminator: 0.16112661361694336\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.353140830993652 LOSS_Discriminator: 0.16804635524749756\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.213759899139404 LOSS_Discriminator: 0.17656663060188293\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.688457489013672 LOSS_Discriminator: 0.0829395055770874\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.575245380401611 LOSS_Discriminator: 0.2109682410955429\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.67268180847168 LOSS_Discriminator: 0.15642070770263672\n",
            "ITERATION_NO.: 66 LOSS_Generator: 4.9935784339904785 LOSS_Discriminator: 0.0697460025548935\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.0412678718566895 LOSS_Discriminator: 0.11591523885726929\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.223110198974609 LOSS_Discriminator: 0.14692778885364532\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.171860218048096 LOSS_Discriminator: 0.22520287334918976\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.674396991729736 LOSS_Discriminator: 0.17314913868904114\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.345561981201172 LOSS_Discriminator: 0.13394734263420105\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.797744274139404 LOSS_Discriminator: 0.08800050616264343\n",
            "ITERATION_NO.: 73 LOSS_Generator: 6.008755683898926 LOSS_Discriminator: 0.1702573448419571\n",
            "ITERATION_NO.: 74 LOSS_Generator: 6.038498401641846 LOSS_Discriminator: 0.10030961036682129\n",
            "ITERATION_NO.: 75 LOSS_Generator: 6.317811489105225 LOSS_Discriminator: 0.26996976137161255\n",
            "ITERATION_NO.: 76 LOSS_Generator: 6.105682373046875 LOSS_Discriminator: 0.17303137481212616\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.324962615966797 LOSS_Discriminator: 0.26821815967559814\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.353149890899658 LOSS_Discriminator: 0.10820327699184418\n",
            "ITERATION_NO.: 79 LOSS_Generator: 4.813928127288818 LOSS_Discriminator: 0.22535842657089233\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.621226787567139 LOSS_Discriminator: 0.09776797890663147\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.515982151031494 LOSS_Discriminator: 0.1539633721113205\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.122970104217529 LOSS_Discriminator: 0.15928126871585846\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.906127452850342 LOSS_Discriminator: 0.09596964716911316\n",
            "ITERATION_NO.: 84 LOSS_Generator: 6.0767364501953125 LOSS_Discriminator: 0.25032076239585876\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.6040239334106445 LOSS_Discriminator: 0.21663090586662292\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.89875602722168 LOSS_Discriminator: 0.09967382252216339\n",
            "ITERATION_NO.: 87 LOSS_Generator: 6.212213039398193 LOSS_Discriminator: 0.11788755655288696\n",
            "ITERATION_NO.: 88 LOSS_Generator: 6.170543670654297 LOSS_Discriminator: 0.14383631944656372\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.4474053382873535 LOSS_Discriminator: 0.21388667821884155\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.408315658569336 LOSS_Discriminator: 0.2155945599079132\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.6681694984436035 LOSS_Discriminator: 0.14305219054222107\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.328636646270752 LOSS_Discriminator: 0.09151328355073929\n",
            "ITERATION_NO.: 93 LOSS_Generator: 4.991466045379639 LOSS_Discriminator: 0.14085449278354645\n",
            "ITERATION_NO.: 94 LOSS_Generator: 4.9615325927734375 LOSS_Discriminator: 0.10969459265470505\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.101108551025391 LOSS_Discriminator: 0.16835448145866394\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.579227924346924 LOSS_Discriminator: 0.10599081963300705\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.827298641204834 LOSS_Discriminator: 0.12188307195901871\n",
            "ITERATION_NO.: 98 LOSS_Generator: 6.433917045593262 LOSS_Discriminator: 0.12942881882190704\n",
            "ITERATION_NO.: 99 LOSS_Generator: 6.099661827087402 LOSS_Discriminator: 0.18676042556762695\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.3239850997924805 LOSS_Discriminator: 0.07795748859643936\n",
            "ITERATION_NO.: 101 LOSS_Generator: 6.642541408538818 LOSS_Discriminator: 0.09036639332771301\n",
            "ITERATION_NO.: 102 LOSS_Generator: 6.506608963012695 LOSS_Discriminator: 0.10811498016119003\n",
            "ITERATION_NO.: 103 LOSS_Generator: 6.5165791511535645 LOSS_Discriminator: 0.14272594451904297\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.930994033813477 LOSS_Discriminator: 0.27562105655670166\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.61541748046875 LOSS_Discriminator: 0.20667243003845215\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.611778736114502 LOSS_Discriminator: 0.08559297025203705\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.257415771484375 LOSS_Discriminator: 0.05354481190443039\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.329739570617676 LOSS_Discriminator: 0.20103859901428223\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.2061872482299805 LOSS_Discriminator: 0.13010916113853455\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.138430118560791 LOSS_Discriminator: 0.2668529450893402\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.564803600311279 LOSS_Discriminator: 0.10077838599681854\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.632693290710449 LOSS_Discriminator: 0.09726820141077042\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.429802417755127 LOSS_Discriminator: 0.15036113560199738\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.755319118499756 LOSS_Discriminator: 0.2006533145904541\n",
            "ITERATION_NO.: 115 LOSS_Generator: 6.020496845245361 LOSS_Discriminator: 0.1563192754983902\n",
            "ITERATION_NO.: 116 LOSS_Generator: 6.472973823547363 LOSS_Discriminator: 0.1486542969942093\n",
            "ITERATION_NO.: 117 LOSS_Generator: 6.037327766418457 LOSS_Discriminator: 0.20516127347946167\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.894804000854492 LOSS_Discriminator: 0.23621860146522522\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.885711193084717 LOSS_Discriminator: 0.14978010952472687\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.5007758140563965 LOSS_Discriminator: 0.21138715744018555\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.52003288269043 LOSS_Discriminator: 0.13412873446941376\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.6042938232421875 LOSS_Discriminator: 0.19763043522834778\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.383852958679199 LOSS_Discriminator: 0.1632116436958313\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.644548416137695 LOSS_Discriminator: 0.18042141199111938\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.656607151031494 LOSS_Discriminator: 0.12739548087120056\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.616815567016602 LOSS_Discriminator: 0.10905595123767853\n",
            "ITERATION_NO.: 127 LOSS_Generator: 6.1935319900512695 LOSS_Discriminator: 0.15278008580207825\n",
            "ITERATION_NO.: 128 LOSS_Generator: 6.163182258605957 LOSS_Discriminator: 0.12242023646831512\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.756548881530762 LOSS_Discriminator: 0.2385750263929367\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.872220039367676 LOSS_Discriminator: 0.19705337285995483\n",
            "ITERATION_NO.: 131 LOSS_Generator: 5.3157806396484375 LOSS_Discriminator: 0.1107873022556305\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.281169891357422 LOSS_Discriminator: 0.07182471454143524\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.529289722442627 LOSS_Discriminator: 0.1611352562904358\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.058315277099609 LOSS_Discriminator: 0.12545596063137054\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.253786563873291 LOSS_Discriminator: 0.1737748086452484\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.641523838043213 LOSS_Discriminator: 0.2182740867137909\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.677117347717285 LOSS_Discriminator: 0.12320742011070251\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.807882308959961 LOSS_Discriminator: 0.11484114080667496\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.0735273361206055 LOSS_Discriminator: 0.1554541289806366\n",
            "ITERATION_NO.: 140 LOSS_Generator: 6.305455207824707 LOSS_Discriminator: 0.15172915160655975\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.538334369659424 LOSS_Discriminator: 0.1385488212108612\n",
            "ITERATION_NO.: 142 LOSS_Generator: 6.227033615112305 LOSS_Discriminator: 0.18883606791496277\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.982296943664551 LOSS_Discriminator: 0.08581306040287018\n",
            "ITERATION_NO.: 144 LOSS_Generator: 6.222198486328125 LOSS_Discriminator: 0.12608219683170319\n",
            "ITERATION_NO.: 145 LOSS_Generator: 6.287182807922363 LOSS_Discriminator: 0.1392163634300232\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.150885105133057 LOSS_Discriminator: 0.12427645921707153\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.819161891937256 LOSS_Discriminator: 0.08608497679233551\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.373500823974609 LOSS_Discriminator: 0.09663381427526474\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.409444808959961 LOSS_Discriminator: 0.11600358784198761\n",
            "ITERATION_NO.: 150 LOSS_Generator: 4.714472770690918 LOSS_Discriminator: 0.19183985888957977\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.212216377258301 LOSS_Discriminator: 0.22948068380355835\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.069322109222412 LOSS_Discriminator: 0.12377174198627472\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.398967266082764 LOSS_Discriminator: 0.10449118167161942\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.590388774871826 LOSS_Discriminator: 0.21365642547607422\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.335559844970703 LOSS_Discriminator: 0.09139259159564972\n",
            "ITERATION_NO.: 156 LOSS_Generator: 5.722168445587158 LOSS_Discriminator: 0.11202099174261093\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.468281269073486 LOSS_Discriminator: 0.06971889734268188\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.680088520050049 LOSS_Discriminator: 0.148783877491951\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.9613823890686035 LOSS_Discriminator: 0.0799364224076271\n",
            "ITERATION_NO.: 160 LOSS_Generator: 6.126779556274414 LOSS_Discriminator: 0.13545434176921844\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.756206035614014 LOSS_Discriminator: 0.10214586555957794\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.958748817443848 LOSS_Discriminator: 0.13626277446746826\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.704919338226318 LOSS_Discriminator: 0.10657934844493866\n",
            "ITERATION_NO.: 164 LOSS_Generator: 6.154048442840576 LOSS_Discriminator: 0.11517807841300964\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.677460193634033 LOSS_Discriminator: 0.11582870781421661\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.771214008331299 LOSS_Discriminator: 0.14140678942203522\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.404038906097412 LOSS_Discriminator: 0.15263336896896362\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.3885345458984375 LOSS_Discriminator: 0.1380005180835724\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.575896739959717 LOSS_Discriminator: 0.10807105153799057\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.106633186340332 LOSS_Discriminator: 0.11859472095966339\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.378354549407959 LOSS_Discriminator: 0.11107926070690155\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.580698013305664 LOSS_Discriminator: 0.2014189064502716\n",
            "ITERATION_NO.: 173 LOSS_Generator: 6.471452713012695 LOSS_Discriminator: 0.14191535115242004\n",
            "ITERATION_NO.: 174 LOSS_Generator: 6.338705062866211 LOSS_Discriminator: 0.11874173581600189\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.513465404510498 LOSS_Discriminator: 0.1554897129535675\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.401849269866943 LOSS_Discriminator: 0.12252533435821533\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.6190972328186035 LOSS_Discriminator: 0.14320753514766693\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.397512435913086 LOSS_Discriminator: 0.14098407328128815\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.292118549346924 LOSS_Discriminator: 0.07368271052837372\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.490477085113525 LOSS_Discriminator: 0.17039361596107483\n",
            "ITERATION_NO.: 181 LOSS_Generator: 4.502289295196533 LOSS_Discriminator: 0.10937446355819702\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.029797554016113 LOSS_Discriminator: 0.16209927201271057\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.153291702270508 LOSS_Discriminator: 0.1168699860572815\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.4357686042785645 LOSS_Discriminator: 0.17565381526947021\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.974033355712891 LOSS_Discriminator: 0.09411027282476425\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.505227565765381 LOSS_Discriminator: 0.11614379286766052\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.003962516784668 LOSS_Discriminator: 0.07330481708049774\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.700112342834473 LOSS_Discriminator: 0.15342959761619568\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.119156360626221 LOSS_Discriminator: 0.19797402620315552\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.416850566864014 LOSS_Discriminator: 0.07804321497678757\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.4000244140625 LOSS_Discriminator: 0.19638529419898987\n",
            "ITERATION_NO.: 192 LOSS_Generator: 5.762945652008057 LOSS_Discriminator: 0.16078068315982819\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.160286903381348 LOSS_Discriminator: 0.23300565779209137\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.079131603240967 LOSS_Discriminator: 0.2031705528497696\n",
            "ITERATION_NO.: 195 LOSS_Generator: 6.015181064605713 LOSS_Discriminator: 0.1487427055835724\n",
            "ITERATION_NO.: 196 LOSS_Generator: 6.27800178527832 LOSS_Discriminator: 0.25563013553619385\n",
            "ITERATION_NO.: 197 LOSS_Generator: 6.267245292663574 LOSS_Discriminator: 0.09985555708408356\n",
            "ITERATION_NO.: 198 LOSS_Generator: 6.201469898223877 LOSS_Discriminator: 0.21166744828224182\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.99824333190918 LOSS_Discriminator: 0.24137873947620392\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.383756160736084 LOSS_Discriminator: 0.22978802025318146\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.431299209594727 LOSS_Discriminator: 0.10591500997543335\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.244404315948486 LOSS_Discriminator: 0.172329843044281\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.191310405731201 LOSS_Discriminator: 0.18875932693481445\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.723556041717529 LOSS_Discriminator: 0.19595834612846375\n",
            "ITERATION_NO.: 205 LOSS_Generator: 6.311039447784424 LOSS_Discriminator: 0.15032339096069336\n",
            "ITERATION_NO.: 206 LOSS_Generator: 6.187382221221924 LOSS_Discriminator: 0.09055575728416443\n",
            "ITERATION_NO.: 207 LOSS_Generator: 6.608281135559082 LOSS_Discriminator: 0.13432465493679047\n",
            "ITERATION_NO.: 208 LOSS_Generator: 6.483684062957764 LOSS_Discriminator: 0.1609853208065033\n",
            "ITERATION_NO.: 209 LOSS_Generator: 6.58663272857666 LOSS_Discriminator: 0.31406232714653015\n",
            "ITERATION_NO.: 210 LOSS_Generator: 6.225853443145752 LOSS_Discriminator: 0.12089990079402924\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.690962314605713 LOSS_Discriminator: 0.14381912350654602\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.521164417266846 LOSS_Discriminator: 0.16926345229148865\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.031508922576904 LOSS_Discriminator: 0.09817777574062347\n",
            "ITERATION_NO.: 214 LOSS_Generator: 4.66463041305542 LOSS_Discriminator: 0.23751556873321533\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.358455657958984 LOSS_Discriminator: 0.10122110694646835\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.225520133972168 LOSS_Discriminator: 0.10739979892969131\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.489848613739014 LOSS_Discriminator: 0.12081732600927353\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.120731353759766 LOSS_Discriminator: 0.09179693460464478\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.054117202758789 LOSS_Discriminator: 0.2082763910293579\n",
            "ITERATION_NO.: 220 LOSS_Generator: 6.170129776000977 LOSS_Discriminator: 0.10072890669107437\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.85483455657959 LOSS_Discriminator: 0.12600888311862946\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.16807746887207 LOSS_Discriminator: 0.1881733536720276\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.816286087036133 LOSS_Discriminator: 0.09663936495780945\n",
            "ITERATION_NO.: 224 LOSS_Generator: 5.821542263031006 LOSS_Discriminator: 0.19271084666252136\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.328486442565918 LOSS_Discriminator: 0.07870838046073914\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.270496845245361 LOSS_Discriminator: 0.18888860940933228\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.292135715484619 LOSS_Discriminator: 0.10946031659841537\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.180026054382324 LOSS_Discriminator: 0.2358548641204834\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.590836048126221 LOSS_Discriminator: 0.16266784071922302\n",
            "ITERATION_NO.: 230 LOSS_Generator: 6.015402317047119 LOSS_Discriminator: 0.11043502390384674\n",
            "ITERATION_NO.: 231 LOSS_Generator: 6.1822357177734375 LOSS_Discriminator: 0.144967719912529\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.9134697914123535 LOSS_Discriminator: 0.10419443249702454\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.824803352355957 LOSS_Discriminator: 0.09353111684322357\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.4180145263671875 LOSS_Discriminator: 0.09405320882797241\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.6490797996521 LOSS_Discriminator: 0.15242889523506165\n",
            "ITERATION_NO.: 236 LOSS_Generator: 5.217097282409668 LOSS_Discriminator: 0.06383061408996582\n",
            "ITERATION_NO.: 237 LOSS_Generator: 5.422083854675293 LOSS_Discriminator: 0.22444646060466766\n",
            "ITERATION_NO.: 238 LOSS_Generator: 5.61411190032959 LOSS_Discriminator: 0.10482662916183472\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.462713718414307 LOSS_Discriminator: 0.1159168928861618\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.7191972732543945 LOSS_Discriminator: 0.10867809504270554\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.865768909454346 LOSS_Discriminator: 0.12224148213863373\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.5047783851623535 LOSS_Discriminator: 0.07636323571205139\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.488814830780029 LOSS_Discriminator: 0.0842042863368988\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.624287128448486 LOSS_Discriminator: 0.2441381961107254\n",
            "ITERATION_NO.: 245 LOSS_Generator: 5.577638626098633 LOSS_Discriminator: 0.10542885959148407\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.3218793869018555 LOSS_Discriminator: 0.14894311130046844\n",
            "ITERATION_NO.: 247 LOSS_Generator: 4.770843505859375 LOSS_Discriminator: 0.16322895884513855\n",
            "ITERATION_NO.: 248 LOSS_Generator: 4.530285358428955 LOSS_Discriminator: 0.12524646520614624\n",
            "ITERATION_NO.: 249 LOSS_Generator: 4.820719242095947 LOSS_Discriminator: 0.12470455467700958\n",
            "ITERATION_NO.: 250 LOSS_Generator: 4.664510726928711 LOSS_Discriminator: 0.07663868367671967\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.327567100524902 LOSS_Discriminator: 0.1706177145242691\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.9421000480651855 LOSS_Discriminator: 0.14400210976600647\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.090214729309082 LOSS_Discriminator: 0.11598086357116699\n",
            "ITERATION_NO.: 254 LOSS_Generator: 6.2969441413879395 LOSS_Discriminator: 0.218292698264122\n",
            "ITERATION_NO.: 255 LOSS_Generator: 6.356197357177734 LOSS_Discriminator: 0.12073764204978943\n",
            "ITERATION_NO.: 256 LOSS_Generator: 6.314011096954346 LOSS_Discriminator: 0.1755419373512268\n",
            "ITERATION_NO.: 257 LOSS_Generator: 5.892699718475342 LOSS_Discriminator: 0.12630492448806763\n",
            "ITERATION_NO.: 258 LOSS_Generator: 5.565878391265869 LOSS_Discriminator: 0.18363142013549805\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.695866584777832 LOSS_Discriminator: 0.14329683780670166\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.712636947631836 LOSS_Discriminator: 0.12810641527175903\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.7764201164245605 LOSS_Discriminator: 0.1184418797492981\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.382731914520264 LOSS_Discriminator: 0.11514081805944443\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.834682464599609 LOSS_Discriminator: 0.15639865398406982\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.849020004272461 LOSS_Discriminator: 0.24161282181739807\n",
            "ITERATION_NO.: 265 LOSS_Generator: 5.380103588104248 LOSS_Discriminator: 0.16198357939720154\n",
            "ITERATION_NO.: 266 LOSS_Generator: 5.312572479248047 LOSS_Discriminator: 0.08606313914060593\n",
            "ITERATION_NO.: 267 LOSS_Generator: 4.75592041015625 LOSS_Discriminator: 0.1763879954814911\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.3135271072387695 LOSS_Discriminator: 0.1572457253932953\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.1564812660217285 LOSS_Discriminator: 0.10821907222270966\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.173403263092041 LOSS_Discriminator: 0.1003522276878357\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.200503349304199 LOSS_Discriminator: 0.18812434375286102\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.477331638336182 LOSS_Discriminator: 0.18883708119392395\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.752460956573486 LOSS_Discriminator: 0.11365889757871628\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.775722503662109 LOSS_Discriminator: 0.12808355689048767\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.523406982421875 LOSS_Discriminator: 0.1720297932624817\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.845909595489502 LOSS_Discriminator: 0.18369397521018982\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.332922458648682 LOSS_Discriminator: 0.11330461502075195\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.002803802490234 LOSS_Discriminator: 0.20883716642856598\n",
            "ITERATION_NO.: 279 LOSS_Generator: 4.5073442459106445 LOSS_Discriminator: 0.25325292348861694\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.2770843505859375 LOSS_Discriminator: 0.2420305609703064\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.226185321807861 LOSS_Discriminator: 0.15705366432666779\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.861499786376953 LOSS_Discriminator: 0.0746128261089325\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.507251739501953 LOSS_Discriminator: 0.17072242498397827\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.791944980621338 LOSS_Discriminator: 0.16012749075889587\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.891692638397217 LOSS_Discriminator: 0.14302128553390503\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.615733623504639 LOSS_Discriminator: 0.1145818680524826\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.409714221954346 LOSS_Discriminator: 0.21007610857486725\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.510927200317383 LOSS_Discriminator: 0.10806538164615631\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.409409999847412 LOSS_Discriminator: 0.09210406988859177\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.954371452331543 LOSS_Discriminator: 0.08879615366458893\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.728395462036133 LOSS_Discriminator: 0.20123109221458435\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.565560340881348 LOSS_Discriminator: 0.22082245349884033\n",
            "ITERATION_NO.: 293 LOSS_Generator: 6.033214569091797 LOSS_Discriminator: 0.17223528027534485\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.0697712898254395 LOSS_Discriminator: 0.16946354508399963\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.5652241706848145 LOSS_Discriminator: 0.15162238478660583\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.4613189697265625 LOSS_Discriminator: 0.20874285697937012\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.8902764320373535 LOSS_Discriminator: 0.12113771587610245\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.450002670288086 LOSS_Discriminator: 0.2327297031879425\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.836449146270752 LOSS_Discriminator: 0.24908220767974854\n",
            "ITERATION_NO.: 300 LOSS_Generator: 6.2695393562316895 LOSS_Discriminator: 0.13549205660820007\n",
            "ITERATION_NO.: 301 LOSS_Generator: 6.079500198364258 LOSS_Discriminator: 0.10425133258104324\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.1073174476623535 LOSS_Discriminator: 0.25564512610435486\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.646681308746338 LOSS_Discriminator: 0.21607495844364166\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.929567337036133 LOSS_Discriminator: 0.15631432831287384\n",
            "ITERATION_NO.: 305 LOSS_Generator: 4.570099830627441 LOSS_Discriminator: 0.1549554318189621\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.25502872467041 LOSS_Discriminator: 0.1923837959766388\n",
            "ITERATION_NO.: 307 LOSS_Generator: 4.77589225769043 LOSS_Discriminator: 0.19905933737754822\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.065158843994141 LOSS_Discriminator: 0.16307523846626282\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.2541632652282715 LOSS_Discriminator: 0.18483227491378784\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.432106971740723 LOSS_Discriminator: 0.23095539212226868\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.709041118621826 LOSS_Discriminator: 0.19683879613876343\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.76216983795166 LOSS_Discriminator: 0.18649205565452576\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.88778829574585 LOSS_Discriminator: 0.12494786083698273\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.534096717834473 LOSS_Discriminator: 0.13476911187171936\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.389688014984131 LOSS_Discriminator: 0.10333466529846191\n",
            "ITERATION_NO.: 316 LOSS_Generator: 5.7184858322143555 LOSS_Discriminator: 0.17632301151752472\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.686392784118652 LOSS_Discriminator: 0.17452767491340637\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.194040298461914 LOSS_Discriminator: 0.17538666725158691\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.214993953704834 LOSS_Discriminator: 0.22116324305534363\n",
            "ITERATION_NO.: 320 LOSS_Generator: 4.906004905700684 LOSS_Discriminator: 0.17520493268966675\n",
            "ITERATION_NO.: 321 LOSS_Generator: 4.829240798950195 LOSS_Discriminator: 0.09434082359075546\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.03474235534668 LOSS_Discriminator: 0.22553586959838867\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.201363563537598 LOSS_Discriminator: 0.15288914740085602\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.5022172927856445 LOSS_Discriminator: 0.1951616406440735\n",
            "ITERATION_NO.: 325 LOSS_Generator: 6.305800914764404 LOSS_Discriminator: 0.06046873331069946\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.989087104797363 LOSS_Discriminator: 0.11581628024578094\n",
            "ITERATION_NO.: 327 LOSS_Generator: 6.1949615478515625 LOSS_Discriminator: 0.12315617501735687\n",
            "ITERATION_NO.: 328 LOSS_Generator: 6.151699066162109 LOSS_Discriminator: 0.18068081140518188\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.646676540374756 LOSS_Discriminator: 0.17671726644039154\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.171797275543213 LOSS_Discriminator: 0.18542379140853882\n",
            "ITERATION_NO.: 331 LOSS_Generator: 4.909590244293213 LOSS_Discriminator: 0.20572718977928162\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.037558078765869 LOSS_Discriminator: 0.19380435347557068\n",
            "ITERATION_NO.: 333 LOSS_Generator: 4.8754377365112305 LOSS_Discriminator: 0.23679742217063904\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.505459785461426 LOSS_Discriminator: 0.13004738092422485\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.344792366027832 LOSS_Discriminator: 0.10593150556087494\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.292541980743408 LOSS_Discriminator: 0.1938323676586151\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.9365153312683105 LOSS_Discriminator: 0.10349707305431366\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.801916599273682 LOSS_Discriminator: 0.11170001327991486\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.349865913391113 LOSS_Discriminator: 0.12897828221321106\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.65554141998291 LOSS_Discriminator: 0.10779204964637756\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.543243885040283 LOSS_Discriminator: 0.1348506510257721\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.0554518699646 LOSS_Discriminator: 0.1623760163784027\n",
            "ITERATION_NO.: 343 LOSS_Generator: 4.909104824066162 LOSS_Discriminator: 0.1417902708053589\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.1818976402282715 LOSS_Discriminator: 0.19018098711967468\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.013429164886475 LOSS_Discriminator: 0.2736506164073944\n",
            "ITERATION_NO.: 346 LOSS_Generator: 4.720273494720459 LOSS_Discriminator: 0.20515769720077515\n",
            "ITERATION_NO.: 347 LOSS_Generator: 4.939798355102539 LOSS_Discriminator: 0.16689711809158325\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.789313793182373 LOSS_Discriminator: 0.07596202194690704\n",
            "ITERATION_NO.: 349 LOSS_Generator: 6.031599044799805 LOSS_Discriminator: 0.11369239538908005\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.9409003257751465 LOSS_Discriminator: 0.1146359071135521\n",
            "ITERATION_NO.: 351 LOSS_Generator: 6.135585784912109 LOSS_Discriminator: 0.1371079534292221\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.668298244476318 LOSS_Discriminator: 0.17620359361171722\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.810093879699707 LOSS_Discriminator: 0.16216206550598145\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.18044900894165 LOSS_Discriminator: 0.21088755130767822\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.085150241851807 LOSS_Discriminator: 0.1618853658437729\n",
            "ITERATION_NO.: 356 LOSS_Generator: 4.461159706115723 LOSS_Discriminator: 0.2672611176967621\n",
            "ITERATION_NO.: 357 LOSS_Generator: 4.9108052253723145 LOSS_Discriminator: 0.26625680923461914\n",
            "ITERATION_NO.: 358 LOSS_Generator: 4.776545524597168 LOSS_Discriminator: 0.18668808043003082\n",
            "ITERATION_NO.: 359 LOSS_Generator: 4.617374897003174 LOSS_Discriminator: 0.174966961145401\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.372941493988037 LOSS_Discriminator: 0.14686399698257446\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.2534708976745605 LOSS_Discriminator: 0.220417782664299\n",
            "ITERATION_NO.: 362 LOSS_Generator: 6.146700382232666 LOSS_Discriminator: 0.11975939571857452\n",
            "ITERATION_NO.: 363 LOSS_Generator: 6.227980136871338 LOSS_Discriminator: 0.1679401397705078\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.863811016082764 LOSS_Discriminator: 0.18019422888755798\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.999090194702148 LOSS_Discriminator: 0.16614021360874176\n",
            "ITERATION_NO.: 366 LOSS_Generator: 4.901640892028809 LOSS_Discriminator: 0.3043290376663208\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.387773036956787 LOSS_Discriminator: 0.06033111736178398\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.47055196762085 LOSS_Discriminator: 0.19122618436813354\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.164731025695801 LOSS_Discriminator: 0.19221524894237518\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.477653980255127 LOSS_Discriminator: 0.149128720164299\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.295600414276123 LOSS_Discriminator: 0.14405080676078796\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.204890727996826 LOSS_Discriminator: 0.12973502278327942\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.661975860595703 LOSS_Discriminator: 0.20824295282363892\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.389567852020264 LOSS_Discriminator: 0.14777180552482605\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.446389198303223 LOSS_Discriminator: 0.14389759302139282\n",
            "ITERATION_NO.: 376 LOSS_Generator: 6.1760759353637695 LOSS_Discriminator: 0.06611467897891998\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.837247848510742 LOSS_Discriminator: 0.14003042876720428\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.950862407684326 LOSS_Discriminator: 0.2019352912902832\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.8301310539245605 LOSS_Discriminator: 0.14071685075759888\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.770232677459717 LOSS_Discriminator: 0.14659743010997772\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.626806735992432 LOSS_Discriminator: 0.27490681409835815\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.070205211639404 LOSS_Discriminator: 0.11811459064483643\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.065029144287109 LOSS_Discriminator: 0.1546853482723236\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.401381015777588 LOSS_Discriminator: 0.15206784009933472\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.093853950500488 LOSS_Discriminator: 0.15409332513809204\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.182651996612549 LOSS_Discriminator: 0.11014057695865631\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.20771598815918 LOSS_Discriminator: 0.17565150558948517\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.4199323654174805 LOSS_Discriminator: 0.09956178814172745\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.317111968994141 LOSS_Discriminator: 0.21794112026691437\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.587953090667725 LOSS_Discriminator: 0.09726984053850174\n",
            "ITERATION_NO.: 391 LOSS_Generator: 5.794478893280029 LOSS_Discriminator: 0.13767680525779724\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.227138519287109 LOSS_Discriminator: 0.17941272258758545\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.248530387878418 LOSS_Discriminator: 0.1577797830104828\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.159143447875977 LOSS_Discriminator: 0.1285848766565323\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.196989059448242 LOSS_Discriminator: 0.13173523545265198\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.330139636993408 LOSS_Discriminator: 0.29055988788604736\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.729337692260742 LOSS_Discriminator: 0.15882325172424316\n",
            "ITERATION_NO.: 398 LOSS_Generator: 6.053831577301025 LOSS_Discriminator: 0.17476531863212585\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.3667168617248535 LOSS_Discriminator: 0.09248276799917221\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.743925094604492 LOSS_Discriminator: 0.25676092505455017\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.8178486824035645 LOSS_Discriminator: 0.1372063308954239\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.5992631912231445 LOSS_Discriminator: 0.09672249853610992\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.151074409484863 LOSS_Discriminator: 0.08712074160575867\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.326897621154785 LOSS_Discriminator: 0.1499265730381012\n",
            "ITERATION_NO.: 405 LOSS_Generator: 4.889077186584473 LOSS_Discriminator: 0.07349730283021927\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.1861701011657715 LOSS_Discriminator: 0.17177903652191162\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.068518161773682 LOSS_Discriminator: 0.08451753854751587\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.423980712890625 LOSS_Discriminator: 0.1020718663930893\n",
            "ITERATION_NO.: 409 LOSS_Generator: 5.337692737579346 LOSS_Discriminator: 0.12166435271501541\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.401719570159912 LOSS_Discriminator: 0.14346227049827576\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.377831935882568 LOSS_Discriminator: 0.15021100640296936\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.194613456726074 LOSS_Discriminator: 0.21578392386436462\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.190464496612549 LOSS_Discriminator: 0.14788109064102173\n",
            "ITERATION_NO.: 414 LOSS_Generator: 4.96835470199585 LOSS_Discriminator: 0.13415241241455078\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.076528549194336 LOSS_Discriminator: 0.12473544478416443\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.38442325592041 LOSS_Discriminator: 0.2007756233215332\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.597210884094238 LOSS_Discriminator: 0.24911242723464966\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.475649833679199 LOSS_Discriminator: 0.16055208444595337\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.6221442222595215 LOSS_Discriminator: 0.13513973355293274\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.5480732917785645 LOSS_Discriminator: 0.14087368547916412\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.5592041015625 LOSS_Discriminator: 0.08923327177762985\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.9060797691345215 LOSS_Discriminator: 0.1659543365240097\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.7586822509765625 LOSS_Discriminator: 0.19224253296852112\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.64047908782959 LOSS_Discriminator: 0.16588906943798065\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.543545722961426 LOSS_Discriminator: 0.05987422913312912\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.356260776519775 LOSS_Discriminator: 0.2382204532623291\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.9799089431762695 LOSS_Discriminator: 0.18494229018688202\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.657495498657227 LOSS_Discriminator: 0.17200952768325806\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.740602016448975 LOSS_Discriminator: 0.13357290625572205\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.554905891418457 LOSS_Discriminator: 0.19105364382266998\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.489786148071289 LOSS_Discriminator: 0.13629403710365295\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.058586120605469 LOSS_Discriminator: 0.14621490240097046\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.351567268371582 LOSS_Discriminator: 0.10268581658601761\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.902545928955078 LOSS_Discriminator: 0.11893387883901596\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.567162036895752 LOSS_Discriminator: 0.13282985985279083\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.514467239379883 LOSS_Discriminator: 0.14988306164741516\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.036921977996826 LOSS_Discriminator: 0.1861124336719513\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.072464466094971 LOSS_Discriminator: 0.027343399822711945\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.848196029663086 LOSS_Discriminator: 0.08012401312589645\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.237448215484619 LOSS_Discriminator: 0.18795324862003326\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.451608657836914 LOSS_Discriminator: 0.1028420478105545\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.0754570960998535 LOSS_Discriminator: 0.10112869739532471\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.939403533935547 LOSS_Discriminator: 0.15945063531398773\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.582347393035889 LOSS_Discriminator: 0.13576486706733704\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.177079677581787 LOSS_Discriminator: 0.06830400228500366\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.312630653381348 LOSS_Discriminator: 0.1487520933151245\n",
            "ITERATION_NO.: 447 LOSS_Generator: 4.856965065002441 LOSS_Discriminator: 0.1251872181892395\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.377615928649902 LOSS_Discriminator: 0.04196561872959137\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.1600661277771 LOSS_Discriminator: 0.2733376622200012\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.417240142822266 LOSS_Discriminator: 0.13597898185253143\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.817105293273926 LOSS_Discriminator: 0.1746406853199005\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.487131118774414 LOSS_Discriminator: 0.1228765994310379\n",
            "ITERATION_NO.: 453 LOSS_Generator: 6.293211460113525 LOSS_Discriminator: 0.2564186155796051\n",
            "ITERATION_NO.: 454 LOSS_Generator: 6.4947028160095215 LOSS_Discriminator: 0.17481468617916107\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.384868621826172 LOSS_Discriminator: 0.1379503458738327\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.963359355926514 LOSS_Discriminator: 0.11258000135421753\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.608712196350098 LOSS_Discriminator: 0.07707719504833221\n",
            "ITERATION_NO.: 458 LOSS_Generator: 4.987520694732666 LOSS_Discriminator: 0.056960344314575195\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.4873738288879395 LOSS_Discriminator: 0.17493930459022522\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.427191734313965 LOSS_Discriminator: 0.1903161108493805\n",
            "ITERATION_NO.: 461 LOSS_Generator: 4.923549652099609 LOSS_Discriminator: 0.14051799476146698\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.340238094329834 LOSS_Discriminator: 0.16507881879806519\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.859490394592285 LOSS_Discriminator: 0.05883598327636719\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.979464530944824 LOSS_Discriminator: 0.10574312508106232\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.741722583770752 LOSS_Discriminator: 0.1574113368988037\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.905009746551514 LOSS_Discriminator: 0.18155616521835327\n",
            "ITERATION_NO.: 467 LOSS_Generator: 6.184633255004883 LOSS_Discriminator: 0.1109374538064003\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.7148332595825195 LOSS_Discriminator: 0.0848841667175293\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.282233715057373 LOSS_Discriminator: 0.08982236683368683\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.530172348022461 LOSS_Discriminator: 0.11917978525161743\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.070094108581543 LOSS_Discriminator: 0.08934223651885986\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.960853099822998 LOSS_Discriminator: 0.13652025163173676\n",
            "ITERATION_NO.: 473 LOSS_Generator: 4.877939224243164 LOSS_Discriminator: 0.10180912911891937\n",
            "ITERATION_NO.: 474 LOSS_Generator: 4.832101345062256 LOSS_Discriminator: 0.0690159872174263\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.035851955413818 LOSS_Discriminator: 0.10293364524841309\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.303701400756836 LOSS_Discriminator: 0.09873868525028229\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.601594924926758 LOSS_Discriminator: 0.1320430040359497\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.638941764831543 LOSS_Discriminator: 0.16253799200057983\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.641204833984375 LOSS_Discriminator: 0.27214616537094116\n",
            "ITERATION_NO.: 480 LOSS_Generator: 6.416748046875 LOSS_Discriminator: 0.1797327995300293\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.339531421661377 LOSS_Discriminator: 0.11392936110496521\n",
            "ITERATION_NO.: 482 LOSS_Generator: 6.184398651123047 LOSS_Discriminator: 0.1523219645023346\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.772299766540527 LOSS_Discriminator: 0.08899986743927002\n",
            "ITERATION_NO.: 484 LOSS_Generator: 6.162579536437988 LOSS_Discriminator: 0.134622260928154\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.469666004180908 LOSS_Discriminator: 0.15683120489120483\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.069287300109863 LOSS_Discriminator: 0.11206457763910294\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.4306864738464355 LOSS_Discriminator: 0.21309098601341248\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.306928634643555 LOSS_Discriminator: 0.07013432681560516\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.536749362945557 LOSS_Discriminator: 0.12296296656131744\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.089878559112549 LOSS_Discriminator: 0.144735649228096\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.087930679321289 LOSS_Discriminator: 0.2742125391960144\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.503544807434082 LOSS_Discriminator: 0.16693615913391113\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.6925578117370605 LOSS_Discriminator: 0.0753784030675888\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.7602033615112305 LOSS_Discriminator: 0.17973485589027405\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.60245418548584 LOSS_Discriminator: 0.14157432317733765\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.435424327850342 LOSS_Discriminator: 0.15817025303840637\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.716480731964111 LOSS_Discriminator: 0.13352856040000916\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.770891189575195 LOSS_Discriminator: 0.0728517547249794\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.320042133331299 LOSS_Discriminator: 0.13669602572917938\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.461794853210449 LOSS_Discriminator: 0.1122504398226738\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.708086013793945 LOSS_Discriminator: 0.19273987412452698\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.4850287437438965 LOSS_Discriminator: 0.17350929975509644\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.610968589782715 LOSS_Discriminator: 0.17331090569496155\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.7652764320373535 LOSS_Discriminator: 0.1370827555656433\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.823706150054932 LOSS_Discriminator: 0.09096187353134155\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.457211494445801 LOSS_Discriminator: 0.1494414210319519\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.898103713989258 LOSS_Discriminator: 0.1111888736486435\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.614090442657471 LOSS_Discriminator: 0.07818040251731873\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.853737831115723 LOSS_Discriminator: 0.1904381513595581\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.714738368988037 LOSS_Discriminator: 0.1856980323791504\n",
            "ITERATION_NO.: 511 LOSS_Generator: 6.278563976287842 LOSS_Discriminator: 0.1229279488325119\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.633207321166992 LOSS_Discriminator: 0.13658373057842255\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.48750114440918 LOSS_Discriminator: 0.08504987508058548\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.9225544929504395 LOSS_Discriminator: 0.16063135862350464\n",
            "ITERATION_NO.: 515 LOSS_Generator: 4.662611484527588 LOSS_Discriminator: 0.17341254651546478\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.289182186126709 LOSS_Discriminator: 0.1808241307735443\n",
            "ITERATION_NO.: 517 LOSS_Generator: 4.849221706390381 LOSS_Discriminator: 0.09969232976436615\n",
            "ITERATION_NO.: 518 LOSS_Generator: 4.778989791870117 LOSS_Discriminator: 0.11597520858049393\n",
            "ITERATION_NO.: 519 LOSS_Generator: 4.98075008392334 LOSS_Discriminator: 0.17715810239315033\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.380444526672363 LOSS_Discriminator: 0.12437397241592407\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.241438388824463 LOSS_Discriminator: 0.06470631808042526\n",
            "ITERATION_NO.: 522 LOSS_Generator: 6.1313557624816895 LOSS_Discriminator: 0.19594088196754456\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.507158279418945 LOSS_Discriminator: 0.13930006325244904\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.564126491546631 LOSS_Discriminator: 0.16566094756126404\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.005839824676514 LOSS_Discriminator: 0.16322556138038635\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.251605987548828 LOSS_Discriminator: 0.17139467597007751\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.120730400085449 LOSS_Discriminator: 0.16947263479232788\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.280148506164551 LOSS_Discriminator: 0.10125303268432617\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.4363226890563965 LOSS_Discriminator: 0.1405821442604065\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.5029497146606445 LOSS_Discriminator: 0.18246763944625854\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.947233200073242 LOSS_Discriminator: 0.21101239323616028\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.533297061920166 LOSS_Discriminator: 0.14013543725013733\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.554611682891846 LOSS_Discriminator: 0.18265977501869202\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.56550407409668 LOSS_Discriminator: 0.19292807579040527\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.588214874267578 LOSS_Discriminator: 0.1572723388671875\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.466280460357666 LOSS_Discriminator: 0.1798480749130249\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.6749677658081055 LOSS_Discriminator: 0.08949120342731476\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.505572319030762 LOSS_Discriminator: 0.11711106449365616\n",
            "ITERATION_NO.: 539 LOSS_Generator: 6.130190372467041 LOSS_Discriminator: 0.11975421011447906\n",
            "ITERATION_NO.: 540 LOSS_Generator: 6.1275835037231445 LOSS_Discriminator: 0.15237069129943848\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.1931071281433105 LOSS_Discriminator: 0.13994376361370087\n",
            "ITERATION_NO.: 542 LOSS_Generator: 6.361486434936523 LOSS_Discriminator: 0.1867692470550537\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.207577705383301 LOSS_Discriminator: 0.20151668787002563\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.638143539428711 LOSS_Discriminator: 0.17679035663604736\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.102177619934082 LOSS_Discriminator: 0.21110498905181885\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.901966571807861 LOSS_Discriminator: 0.07138168811798096\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.659885406494141 LOSS_Discriminator: 0.06914190202951431\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.666468620300293 LOSS_Discriminator: 0.0763574093580246\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.251615524291992 LOSS_Discriminator: 0.14831870794296265\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.831549167633057 LOSS_Discriminator: 0.1579323559999466\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.990262508392334 LOSS_Discriminator: 0.09511281549930573\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.129415512084961 LOSS_Discriminator: 0.10743100941181183\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.557148456573486 LOSS_Discriminator: 0.17962422966957092\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.305195331573486 LOSS_Discriminator: 0.10789496451616287\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.664900302886963 LOSS_Discriminator: 0.2013695240020752\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.2869744300842285 LOSS_Discriminator: 0.09907806664705276\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.5471601486206055 LOSS_Discriminator: 0.3165603578090668\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.154580593109131 LOSS_Discriminator: 0.14849551022052765\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.322577476501465 LOSS_Discriminator: 0.10918485373258591\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.23973274230957 LOSS_Discriminator: 0.2142428457736969\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.2758708000183105 LOSS_Discriminator: 0.1547505408525467\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.268026351928711 LOSS_Discriminator: 0.08671431243419647\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.040746688842773 LOSS_Discriminator: 0.0961853563785553\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.856070518493652 LOSS_Discriminator: 0.14890694618225098\n",
            "ITERATION_NO.: 565 LOSS_Generator: 6.547173023223877 LOSS_Discriminator: 0.15725579857826233\n",
            "ITERATION_NO.: 566 LOSS_Generator: 6.446023941040039 LOSS_Discriminator: 0.20984771847724915\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.8578290939331055 LOSS_Discriminator: 0.07782311737537384\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.835269927978516 LOSS_Discriminator: 0.14266201853752136\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.777523040771484 LOSS_Discriminator: 0.10698153078556061\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.1528191566467285 LOSS_Discriminator: 0.09020790457725525\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.233743667602539 LOSS_Discriminator: 0.1465521603822708\n",
            "ITERATION_NO.: 572 LOSS_Generator: 4.717241287231445 LOSS_Discriminator: 0.17237138748168945\n",
            "ITERATION_NO.: 573 LOSS_Generator: 4.84710693359375 LOSS_Discriminator: 0.12972614169120789\n",
            "ITERATION_NO.: 574 LOSS_Generator: 4.875426769256592 LOSS_Discriminator: 0.17947304248809814\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.0563578605651855 LOSS_Discriminator: 0.16830570995807648\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.585480213165283 LOSS_Discriminator: 0.14298628270626068\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.003233432769775 LOSS_Discriminator: 0.12987303733825684\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.949663162231445 LOSS_Discriminator: 0.16674605011940002\n",
            "ITERATION_NO.: 579 LOSS_Generator: 6.34603214263916 LOSS_Discriminator: 0.08774498105049133\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.686891555786133 LOSS_Discriminator: 0.1147705614566803\n",
            "ITERATION_NO.: 581 LOSS_Generator: 4.98027229309082 LOSS_Discriminator: 0.12841741740703583\n",
            "ITERATION_NO.: 582 LOSS_Generator: 4.916013717651367 LOSS_Discriminator: 0.20096424221992493\n",
            "ITERATION_NO.: 583 LOSS_Generator: 4.709559440612793 LOSS_Discriminator: 0.13095718622207642\n",
            "ITERATION_NO.: 584 LOSS_Generator: 5.642703056335449 LOSS_Discriminator: 0.14214636385440826\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.114833831787109 LOSS_Discriminator: 0.10270768404006958\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.0920329093933105 LOSS_Discriminator: 0.20918114483356476\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.553124904632568 LOSS_Discriminator: 0.18903911113739014\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.515263080596924 LOSS_Discriminator: 0.13977691531181335\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.968580722808838 LOSS_Discriminator: 0.049555711448192596\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.162893772125244 LOSS_Discriminator: 0.21452848613262177\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.213315010070801 LOSS_Discriminator: 0.09186480194330215\n",
            "ITERATION_NO.: 592 LOSS_Generator: 6.275047779083252 LOSS_Discriminator: 0.12079652398824692\n",
            "ITERATION_NO.: 593 LOSS_Generator: 6.200106143951416 LOSS_Discriminator: 0.13806356489658356\n",
            "ITERATION_NO.: 594 LOSS_Generator: 6.104989051818848 LOSS_Discriminator: 0.12339525669813156\n",
            "ITERATION_NO.: 595 LOSS_Generator: 6.063960075378418 LOSS_Discriminator: 0.18577441573143005\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.148559093475342 LOSS_Discriminator: 0.285274863243103\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.573581695556641 LOSS_Discriminator: 0.2021365612745285\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.492974758148193 LOSS_Discriminator: 0.12223544716835022\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.218536853790283 LOSS_Discriminator: 0.07352003455162048\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.450321197509766 LOSS_Discriminator: 0.15824344754219055\n",
            "EPOCH OVER: 4\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.6787872314453125 LOSS_Discriminator: 0.13992026448249817\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.8748860359191895 LOSS_Discriminator: 0.1541101038455963\n",
            "ITERATION_NO.: 3 LOSS_Generator: 6.106136322021484 LOSS_Discriminator: 0.12770766019821167\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.872657299041748 LOSS_Discriminator: 0.10572945326566696\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.156797885894775 LOSS_Discriminator: 0.09686426818370819\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.808148384094238 LOSS_Discriminator: 0.1198781430721283\n",
            "ITERATION_NO.: 7 LOSS_Generator: 6.3218512535095215 LOSS_Discriminator: 0.11993304640054703\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.560605525970459 LOSS_Discriminator: 0.15141677856445312\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.584494113922119 LOSS_Discriminator: 0.041041240096092224\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.586141586303711 LOSS_Discriminator: 0.17169693112373352\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.7717790603637695 LOSS_Discriminator: 0.10700131952762604\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.536093711853027 LOSS_Discriminator: 0.0878545269370079\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.298323154449463 LOSS_Discriminator: 0.0887925773859024\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.639430999755859 LOSS_Discriminator: 0.18587040901184082\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.8114728927612305 LOSS_Discriminator: 0.11567972600460052\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.504391670227051 LOSS_Discriminator: 0.12919366359710693\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.679327964782715 LOSS_Discriminator: 0.05021131783723831\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.770208835601807 LOSS_Discriminator: 0.13445624709129333\n",
            "ITERATION_NO.: 19 LOSS_Generator: 6.345384120941162 LOSS_Discriminator: 0.14773085713386536\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.81849479675293 LOSS_Discriminator: 0.03977629542350769\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.109847545623779 LOSS_Discriminator: 0.14077714085578918\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.758944034576416 LOSS_Discriminator: 0.09932269155979156\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.641908645629883 LOSS_Discriminator: 0.14156898856163025\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.564680576324463 LOSS_Discriminator: 0.15437051653862\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.4520263671875 LOSS_Discriminator: 0.21914042532444\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.092606067657471 LOSS_Discriminator: 0.14178085327148438\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.3029022216796875 LOSS_Discriminator: 0.06304249167442322\n",
            "ITERATION_NO.: 28 LOSS_Generator: 4.9162492752075195 LOSS_Discriminator: 0.08859243243932724\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.911599159240723 LOSS_Discriminator: 0.13451150059700012\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.519398212432861 LOSS_Discriminator: 0.07245030999183655\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.778390407562256 LOSS_Discriminator: 0.15383636951446533\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.776495933532715 LOSS_Discriminator: 0.1601879596710205\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.948344707489014 LOSS_Discriminator: 0.18743494153022766\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.490906238555908 LOSS_Discriminator: 0.06677724421024323\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.700545787811279 LOSS_Discriminator: 0.17870718240737915\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.408404350280762 LOSS_Discriminator: 0.17086325585842133\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.122691631317139 LOSS_Discriminator: 0.17099791765213013\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.006575107574463 LOSS_Discriminator: 0.22523188591003418\n",
            "ITERATION_NO.: 39 LOSS_Generator: 4.927374839782715 LOSS_Discriminator: 0.13967925310134888\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.371410369873047 LOSS_Discriminator: 0.2322598397731781\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.735088348388672 LOSS_Discriminator: 0.11109930276870728\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.886822700500488 LOSS_Discriminator: 0.14460159838199615\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.799562931060791 LOSS_Discriminator: 0.053044624626636505\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.443564414978027 LOSS_Discriminator: 0.10944029688835144\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.282797813415527 LOSS_Discriminator: 0.15783542394638062\n",
            "ITERATION_NO.: 46 LOSS_Generator: 6.281946182250977 LOSS_Discriminator: 0.10827067494392395\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.893825054168701 LOSS_Discriminator: 0.0951068252325058\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.798407077789307 LOSS_Discriminator: 0.10812557488679886\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.895245552062988 LOSS_Discriminator: 0.11073286831378937\n",
            "ITERATION_NO.: 50 LOSS_Generator: 6.161520004272461 LOSS_Discriminator: 0.11670941114425659\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.706350326538086 LOSS_Discriminator: 0.18665604293346405\n",
            "ITERATION_NO.: 52 LOSS_Generator: 5.734787464141846 LOSS_Discriminator: 0.22335916757583618\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.411690711975098 LOSS_Discriminator: 0.11802762746810913\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.517592906951904 LOSS_Discriminator: 0.15010854601860046\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.835514068603516 LOSS_Discriminator: 0.16182632744312286\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.383642196655273 LOSS_Discriminator: 0.17416858673095703\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.485702037811279 LOSS_Discriminator: 0.1091301217675209\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.1546406745910645 LOSS_Discriminator: 0.13460798561573029\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.396129131317139 LOSS_Discriminator: 0.24899375438690186\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.260249137878418 LOSS_Discriminator: 0.25770753622055054\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.200937271118164 LOSS_Discriminator: 0.13282302021980286\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.038459300994873 LOSS_Discriminator: 0.12764838337898254\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.926001071929932 LOSS_Discriminator: 0.2054348587989807\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.818516731262207 LOSS_Discriminator: 0.12195334583520889\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.759900093078613 LOSS_Discriminator: 0.17142634093761444\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.227684497833252 LOSS_Discriminator: 0.051797300577163696\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.1836419105529785 LOSS_Discriminator: 0.11286306381225586\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.621420860290527 LOSS_Discriminator: 0.16135002672672272\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.404193878173828 LOSS_Discriminator: 0.09893591701984406\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.583953380584717 LOSS_Discriminator: 0.11522818356752396\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.921779155731201 LOSS_Discriminator: 0.15597519278526306\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.593713283538818 LOSS_Discriminator: 0.11387702077627182\n",
            "ITERATION_NO.: 73 LOSS_Generator: 6.1524224281311035 LOSS_Discriminator: 0.08848817646503448\n",
            "ITERATION_NO.: 74 LOSS_Generator: 6.080011367797852 LOSS_Discriminator: 0.13409122824668884\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.981244087219238 LOSS_Discriminator: 0.08639591932296753\n",
            "ITERATION_NO.: 76 LOSS_Generator: 6.039670944213867 LOSS_Discriminator: 0.061504412442445755\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.893936634063721 LOSS_Discriminator: 0.10055521875619888\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.866180896759033 LOSS_Discriminator: 0.09328949451446533\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.186708450317383 LOSS_Discriminator: 0.1432420164346695\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.665826320648193 LOSS_Discriminator: 0.11729441583156586\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.2316975593566895 LOSS_Discriminator: 0.11425122618675232\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.350646495819092 LOSS_Discriminator: 0.20473089814186096\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.423447132110596 LOSS_Discriminator: 0.10062393546104431\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.055356502532959 LOSS_Discriminator: 0.19112731516361237\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.543145656585693 LOSS_Discriminator: 0.12269917130470276\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.338477611541748 LOSS_Discriminator: 0.1305161714553833\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.2867255210876465 LOSS_Discriminator: 0.1455821990966797\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.7761430740356445 LOSS_Discriminator: 0.22153624892234802\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.482211112976074 LOSS_Discriminator: 0.07131852209568024\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.588596820831299 LOSS_Discriminator: 0.1625663787126541\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.590265274047852 LOSS_Discriminator: 0.09579354524612427\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.618636608123779 LOSS_Discriminator: 0.1462235152721405\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.370697021484375 LOSS_Discriminator: 0.13233280181884766\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.998231887817383 LOSS_Discriminator: 0.14858150482177734\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.651959419250488 LOSS_Discriminator: 0.1642458587884903\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.387048244476318 LOSS_Discriminator: 0.12271808087825775\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.819730758666992 LOSS_Discriminator: 0.0519292838871479\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.3885579109191895 LOSS_Discriminator: 0.09960584342479706\n",
            "ITERATION_NO.: 99 LOSS_Generator: 5.533151626586914 LOSS_Discriminator: 0.20155435800552368\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.18919563293457 LOSS_Discriminator: 0.0920468419790268\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.69755744934082 LOSS_Discriminator: 0.11471115797758102\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.310585975646973 LOSS_Discriminator: 0.16659311950206757\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.75656795501709 LOSS_Discriminator: 0.1397700309753418\n",
            "ITERATION_NO.: 104 LOSS_Generator: 6.0582594871521 LOSS_Discriminator: 0.14394184947013855\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.774094104766846 LOSS_Discriminator: 0.21921184659004211\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.737683296203613 LOSS_Discriminator: 0.10892932116985321\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.497308254241943 LOSS_Discriminator: 0.15719863772392273\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.228265285491943 LOSS_Discriminator: 0.17262466251850128\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.553606986999512 LOSS_Discriminator: 0.11867177486419678\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.397387504577637 LOSS_Discriminator: 0.27857914566993713\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.142850399017334 LOSS_Discriminator: 0.13053227961063385\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.219507217407227 LOSS_Discriminator: 0.0796402171254158\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.3048996925354 LOSS_Discriminator: 0.16117356717586517\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.328077793121338 LOSS_Discriminator: 0.12471121549606323\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.713842868804932 LOSS_Discriminator: 0.12024219334125519\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.508792877197266 LOSS_Discriminator: 0.0694071352481842\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.937351703643799 LOSS_Discriminator: 0.1481773853302002\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.720579147338867 LOSS_Discriminator: 0.13883660733699799\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.372069835662842 LOSS_Discriminator: 0.1529313027858734\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.402566432952881 LOSS_Discriminator: 0.18189752101898193\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.336491584777832 LOSS_Discriminator: 0.14096933603286743\n",
            "ITERATION_NO.: 122 LOSS_Generator: 4.946028232574463 LOSS_Discriminator: 0.1201021820306778\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.232336044311523 LOSS_Discriminator: 0.10599274933338165\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.495686054229736 LOSS_Discriminator: 0.15877370536327362\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.6300129890441895 LOSS_Discriminator: 0.14126992225646973\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.991632461547852 LOSS_Discriminator: 0.19766557216644287\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.98543643951416 LOSS_Discriminator: 0.10797004401683807\n",
            "ITERATION_NO.: 128 LOSS_Generator: 6.1962738037109375 LOSS_Discriminator: 0.13765183091163635\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.799348831176758 LOSS_Discriminator: 0.2401406615972519\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.6261186599731445 LOSS_Discriminator: 0.16369447112083435\n",
            "ITERATION_NO.: 131 LOSS_Generator: 5.558104991912842 LOSS_Discriminator: 0.13702258467674255\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.973676681518555 LOSS_Discriminator: 0.1927441954612732\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.297031402587891 LOSS_Discriminator: 0.13357096910476685\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.249143123626709 LOSS_Discriminator: 0.14917367696762085\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.5422868728637695 LOSS_Discriminator: 0.1249413937330246\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.393188953399658 LOSS_Discriminator: 0.10789737105369568\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.5967912673950195 LOSS_Discriminator: 0.25307512283325195\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.3657145500183105 LOSS_Discriminator: 0.1175706684589386\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.225080490112305 LOSS_Discriminator: 0.15924613177776337\n",
            "ITERATION_NO.: 140 LOSS_Generator: 6.153525352478027 LOSS_Discriminator: 0.19604244828224182\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.328376293182373 LOSS_Discriminator: 0.10554501414299011\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.639179706573486 LOSS_Discriminator: 0.1225598081946373\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.74625301361084 LOSS_Discriminator: 0.14378295838832855\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.40187931060791 LOSS_Discriminator: 0.17123880982398987\n",
            "ITERATION_NO.: 145 LOSS_Generator: 4.99420690536499 LOSS_Discriminator: 0.26078128814697266\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.016383647918701 LOSS_Discriminator: 0.1428954154253006\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.394961357116699 LOSS_Discriminator: 0.17363114655017853\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.1022210121154785 LOSS_Discriminator: 0.15881258249282837\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.690951347351074 LOSS_Discriminator: 0.14302390813827515\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.963437080383301 LOSS_Discriminator: 0.2101791948080063\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.476290225982666 LOSS_Discriminator: 0.17658692598342896\n",
            "ITERATION_NO.: 152 LOSS_Generator: 6.816896915435791 LOSS_Discriminator: 0.19350308179855347\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.999567031860352 LOSS_Discriminator: 0.11715149879455566\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.374752044677734 LOSS_Discriminator: 0.29911544919013977\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.539463996887207 LOSS_Discriminator: 0.10568651556968689\n",
            "ITERATION_NO.: 156 LOSS_Generator: 5.0553789138793945 LOSS_Discriminator: 0.12513943016529083\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.225205898284912 LOSS_Discriminator: 0.16856232285499573\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.393895149230957 LOSS_Discriminator: 0.16854867339134216\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.3115620613098145 LOSS_Discriminator: 0.0906059518456459\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.085026741027832 LOSS_Discriminator: 0.10061053186655045\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.873015880584717 LOSS_Discriminator: 0.1323552429676056\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.99412727355957 LOSS_Discriminator: 0.11026248335838318\n",
            "ITERATION_NO.: 163 LOSS_Generator: 6.070691108703613 LOSS_Discriminator: 0.19265682995319366\n",
            "ITERATION_NO.: 164 LOSS_Generator: 6.057865619659424 LOSS_Discriminator: 0.2286635935306549\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.479404449462891 LOSS_Discriminator: 0.1746901571750641\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.014581203460693 LOSS_Discriminator: 0.07690967619419098\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.2034149169921875 LOSS_Discriminator: 0.14632058143615723\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.284358978271484 LOSS_Discriminator: 0.2074771374464035\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.1267781257629395 LOSS_Discriminator: 0.19606055319309235\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.2966718673706055 LOSS_Discriminator: 0.15645548701286316\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.4011549949646 LOSS_Discriminator: 0.1353135108947754\n",
            "ITERATION_NO.: 172 LOSS_Generator: 6.078127384185791 LOSS_Discriminator: 0.18490637838840485\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.987786293029785 LOSS_Discriminator: 0.17291313409805298\n",
            "ITERATION_NO.: 174 LOSS_Generator: 6.241340160369873 LOSS_Discriminator: 0.16809335350990295\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.118093967437744 LOSS_Discriminator: 0.19490401446819305\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.18797492980957 LOSS_Discriminator: 0.20421630144119263\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.52291202545166 LOSS_Discriminator: 0.12473464012145996\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.50764274597168 LOSS_Discriminator: 0.10361722111701965\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.186952590942383 LOSS_Discriminator: 0.1110055148601532\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.4956135749816895 LOSS_Discriminator: 0.09011752158403397\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.515369892120361 LOSS_Discriminator: 0.2111293226480484\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.239256381988525 LOSS_Discriminator: 0.10518179088830948\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.003610610961914 LOSS_Discriminator: 0.1521587371826172\n",
            "ITERATION_NO.: 184 LOSS_Generator: 4.971828460693359 LOSS_Discriminator: 0.08393856137990952\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.692354202270508 LOSS_Discriminator: 0.18130651116371155\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.8583221435546875 LOSS_Discriminator: 0.05693406984210014\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.779008388519287 LOSS_Discriminator: 0.09885469824075699\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.920411586761475 LOSS_Discriminator: 0.17777106165885925\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.562553882598877 LOSS_Discriminator: 0.14887964725494385\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.389486789703369 LOSS_Discriminator: 0.16130331158638\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.1723713874816895 LOSS_Discriminator: 0.12734347581863403\n",
            "ITERATION_NO.: 192 LOSS_Generator: 4.851497173309326 LOSS_Discriminator: 0.2224319577217102\n",
            "ITERATION_NO.: 193 LOSS_Generator: 4.8406453132629395 LOSS_Discriminator: 0.13839837908744812\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.075923919677734 LOSS_Discriminator: 0.14033156633377075\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.4285888671875 LOSS_Discriminator: 0.1435796320438385\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.991621494293213 LOSS_Discriminator: 0.17270231246948242\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.8617448806762695 LOSS_Discriminator: 0.15127430856227875\n",
            "ITERATION_NO.: 198 LOSS_Generator: 6.606255531311035 LOSS_Discriminator: 0.15443621575832367\n",
            "ITERATION_NO.: 199 LOSS_Generator: 6.114840507507324 LOSS_Discriminator: 0.11134826391935349\n",
            "ITERATION_NO.: 200 LOSS_Generator: 6.273138999938965 LOSS_Discriminator: 0.1645222157239914\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.6652445793151855 LOSS_Discriminator: 0.18221496045589447\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.197998523712158 LOSS_Discriminator: 0.14347337186336517\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.1239166259765625 LOSS_Discriminator: 0.18265292048454285\n",
            "ITERATION_NO.: 204 LOSS_Generator: 4.609619617462158 LOSS_Discriminator: 0.09265998005867004\n",
            "ITERATION_NO.: 205 LOSS_Generator: 4.679149627685547 LOSS_Discriminator: 0.2210289090871811\n",
            "ITERATION_NO.: 206 LOSS_Generator: 4.640305995941162 LOSS_Discriminator: 0.11466097086668015\n",
            "ITERATION_NO.: 207 LOSS_Generator: 4.773552894592285 LOSS_Discriminator: 0.23153182864189148\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.442403793334961 LOSS_Discriminator: 0.13417182862758636\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.750288486480713 LOSS_Discriminator: 0.08289323002099991\n",
            "ITERATION_NO.: 210 LOSS_Generator: 6.289361000061035 LOSS_Discriminator: 0.11428907513618469\n",
            "ITERATION_NO.: 211 LOSS_Generator: 6.881997585296631 LOSS_Discriminator: 0.07104340940713882\n",
            "ITERATION_NO.: 212 LOSS_Generator: 6.700250625610352 LOSS_Discriminator: 0.18324090540409088\n",
            "ITERATION_NO.: 213 LOSS_Generator: 6.516422748565674 LOSS_Discriminator: 0.24560260772705078\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.739750862121582 LOSS_Discriminator: 0.19196179509162903\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.028861999511719 LOSS_Discriminator: 0.07949428260326385\n",
            "ITERATION_NO.: 216 LOSS_Generator: 4.124249458312988 LOSS_Discriminator: 0.0965559333562851\n",
            "ITERATION_NO.: 217 LOSS_Generator: 4.6024627685546875 LOSS_Discriminator: 0.1430778205394745\n",
            "ITERATION_NO.: 218 LOSS_Generator: 4.4254536628723145 LOSS_Discriminator: 0.1270754188299179\n",
            "ITERATION_NO.: 219 LOSS_Generator: 4.673227787017822 LOSS_Discriminator: 0.13780641555786133\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.06433629989624 LOSS_Discriminator: 0.1672089397907257\n",
            "ITERATION_NO.: 221 LOSS_Generator: 6.071569442749023 LOSS_Discriminator: 0.1320030391216278\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.277358531951904 LOSS_Discriminator: 0.10536772012710571\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.709221839904785 LOSS_Discriminator: 0.10593512654304504\n",
            "ITERATION_NO.: 224 LOSS_Generator: 6.085224151611328 LOSS_Discriminator: 0.2120623141527176\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.760323524475098 LOSS_Discriminator: 0.12323819100856781\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.730719566345215 LOSS_Discriminator: 0.2136484533548355\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.733227729797363 LOSS_Discriminator: 0.06978444755077362\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.129739284515381 LOSS_Discriminator: 0.12091786414384842\n",
            "ITERATION_NO.: 229 LOSS_Generator: 4.866081714630127 LOSS_Discriminator: 0.07513167709112167\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.088867664337158 LOSS_Discriminator: 0.20095160603523254\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.1844282150268555 LOSS_Discriminator: 0.12056921422481537\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.337808609008789 LOSS_Discriminator: 0.10238926112651825\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.203958034515381 LOSS_Discriminator: 0.15876275300979614\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.518887519836426 LOSS_Discriminator: 0.11722995340824127\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.6773457527160645 LOSS_Discriminator: 0.11356435716152191\n",
            "ITERATION_NO.: 236 LOSS_Generator: 6.024744987487793 LOSS_Discriminator: 0.16551247239112854\n",
            "ITERATION_NO.: 237 LOSS_Generator: 6.107416152954102 LOSS_Discriminator: 0.18674889206886292\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.397992134094238 LOSS_Discriminator: 0.09265367686748505\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.955355167388916 LOSS_Discriminator: 0.12237389385700226\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.716855049133301 LOSS_Discriminator: 0.1618291437625885\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.000703811645508 LOSS_Discriminator: 0.12033624202013016\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.898817539215088 LOSS_Discriminator: 0.10907882452011108\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.6448540687561035 LOSS_Discriminator: 0.09640548378229141\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.01152229309082 LOSS_Discriminator: 0.12423359602689743\n",
            "ITERATION_NO.: 245 LOSS_Generator: 5.119570255279541 LOSS_Discriminator: 0.17519262433052063\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.205388069152832 LOSS_Discriminator: 0.07495881617069244\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.2993316650390625 LOSS_Discriminator: 0.185452401638031\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.263331413269043 LOSS_Discriminator: 0.28175902366638184\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.729820728302002 LOSS_Discriminator: 0.10028700530529022\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.4846086502075195 LOSS_Discriminator: 0.23948869109153748\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.64539909362793 LOSS_Discriminator: 0.19444577395915985\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.885847091674805 LOSS_Discriminator: 0.09001920372247696\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.059135913848877 LOSS_Discriminator: 0.07945544272661209\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.652548789978027 LOSS_Discriminator: 0.11617366969585419\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.731385707855225 LOSS_Discriminator: 0.09201249480247498\n",
            "ITERATION_NO.: 256 LOSS_Generator: 5.247063159942627 LOSS_Discriminator: 0.21065828204154968\n",
            "ITERATION_NO.: 257 LOSS_Generator: 5.733967304229736 LOSS_Discriminator: 0.10623134672641754\n",
            "ITERATION_NO.: 258 LOSS_Generator: 5.62261962890625 LOSS_Discriminator: 0.11052915453910828\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.756646156311035 LOSS_Discriminator: 0.12653358280658722\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.6424360275268555 LOSS_Discriminator: 0.04897220805287361\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.575389862060547 LOSS_Discriminator: 0.09212376177310944\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.683626174926758 LOSS_Discriminator: 0.1791941225528717\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.568324089050293 LOSS_Discriminator: 0.07644911855459213\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.597024440765381 LOSS_Discriminator: 0.12653805315494537\n",
            "ITERATION_NO.: 265 LOSS_Generator: 5.287275314331055 LOSS_Discriminator: 0.14304587244987488\n",
            "ITERATION_NO.: 266 LOSS_Generator: 5.630272388458252 LOSS_Discriminator: 0.12842723727226257\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.904195785522461 LOSS_Discriminator: 0.08389750868082047\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.428325176239014 LOSS_Discriminator: 0.11114224046468735\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.883843898773193 LOSS_Discriminator: 0.14865580201148987\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.877493381500244 LOSS_Discriminator: 0.11192408204078674\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.566552639007568 LOSS_Discriminator: 0.11822572350502014\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.357458114624023 LOSS_Discriminator: 0.11800188571214676\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.678233623504639 LOSS_Discriminator: 0.10063807666301727\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.59354305267334 LOSS_Discriminator: 0.292057067155838\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.154809951782227 LOSS_Discriminator: 0.18635748326778412\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.549853324890137 LOSS_Discriminator: 0.11373762786388397\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.888920783996582 LOSS_Discriminator: 0.060561686754226685\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.831737518310547 LOSS_Discriminator: 0.09841513633728027\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.554469108581543 LOSS_Discriminator: 0.13470861315727234\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.7509446144104 LOSS_Discriminator: 0.12422063946723938\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.881442070007324 LOSS_Discriminator: 0.10610531270503998\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.575202465057373 LOSS_Discriminator: 0.19724540412425995\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.750673770904541 LOSS_Discriminator: 0.09785950183868408\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.457358360290527 LOSS_Discriminator: 0.10171222686767578\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.766873836517334 LOSS_Discriminator: 0.1658993512392044\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.754369735717773 LOSS_Discriminator: 0.15724971890449524\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.063697338104248 LOSS_Discriminator: 0.27607113122940063\n",
            "ITERATION_NO.: 288 LOSS_Generator: 6.1236090660095215 LOSS_Discriminator: 0.0754210501909256\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.944828987121582 LOSS_Discriminator: 0.12139014154672623\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.591120719909668 LOSS_Discriminator: 0.2579208314418793\n",
            "ITERATION_NO.: 291 LOSS_Generator: 6.1266398429870605 LOSS_Discriminator: 0.10079164803028107\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.742492198944092 LOSS_Discriminator: 0.1026834100484848\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.529117584228516 LOSS_Discriminator: 0.19919660687446594\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.675461292266846 LOSS_Discriminator: 0.09590021520853043\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.630977630615234 LOSS_Discriminator: 0.1088516041636467\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.371296405792236 LOSS_Discriminator: 0.06918281316757202\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.3419599533081055 LOSS_Discriminator: 0.0836208313703537\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.588195323944092 LOSS_Discriminator: 0.10192584991455078\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.729130744934082 LOSS_Discriminator: 0.1532946228981018\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.7974958419799805 LOSS_Discriminator: 0.15805479884147644\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.578319072723389 LOSS_Discriminator: 0.19722889363765717\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.900056838989258 LOSS_Discriminator: 0.11801545321941376\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.292529106140137 LOSS_Discriminator: 0.07833194732666016\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.0489068031311035 LOSS_Discriminator: 0.11353934556245804\n",
            "ITERATION_NO.: 305 LOSS_Generator: 6.111711025238037 LOSS_Discriminator: 0.17079919576644897\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.865147590637207 LOSS_Discriminator: 0.11698807775974274\n",
            "ITERATION_NO.: 307 LOSS_Generator: 6.261051654815674 LOSS_Discriminator: 0.1239982545375824\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.172895431518555 LOSS_Discriminator: 0.13438406586647034\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.614760875701904 LOSS_Discriminator: 0.1077149510383606\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.918579578399658 LOSS_Discriminator: 0.1876872479915619\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.336783409118652 LOSS_Discriminator: 0.07782769203186035\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.391130447387695 LOSS_Discriminator: 0.12342573702335358\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.353151321411133 LOSS_Discriminator: 0.1715536266565323\n",
            "ITERATION_NO.: 314 LOSS_Generator: 4.68391752243042 LOSS_Discriminator: 0.1375652402639389\n",
            "ITERATION_NO.: 315 LOSS_Generator: 4.868871688842773 LOSS_Discriminator: 0.12193126976490021\n",
            "ITERATION_NO.: 316 LOSS_Generator: 5.170566558837891 LOSS_Discriminator: 0.12024381756782532\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.8497395515441895 LOSS_Discriminator: 0.12567618489265442\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.5132341384887695 LOSS_Discriminator: 0.14892210066318512\n",
            "ITERATION_NO.: 319 LOSS_Generator: 6.229724884033203 LOSS_Discriminator: 0.19253577291965485\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.751099109649658 LOSS_Discriminator: 0.17184005677700043\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.739343166351318 LOSS_Discriminator: 0.10188335925340652\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.171731472015381 LOSS_Discriminator: 0.11635522544384003\n",
            "ITERATION_NO.: 323 LOSS_Generator: 4.916984558105469 LOSS_Discriminator: 0.12174777686595917\n",
            "ITERATION_NO.: 324 LOSS_Generator: 4.85951566696167 LOSS_Discriminator: 0.08241751790046692\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.280828475952148 LOSS_Discriminator: 0.1491001546382904\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.335331916809082 LOSS_Discriminator: 0.11199808120727539\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.558528423309326 LOSS_Discriminator: 0.19778792560100555\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.6536359786987305 LOSS_Discriminator: 0.17489156126976013\n",
            "ITERATION_NO.: 329 LOSS_Generator: 6.156422138214111 LOSS_Discriminator: 0.17445003986358643\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.962284564971924 LOSS_Discriminator: 0.20099115371704102\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.770857334136963 LOSS_Discriminator: 0.15061578154563904\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.70146369934082 LOSS_Discriminator: 0.12280722707509995\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.648699760437012 LOSS_Discriminator: 0.2763422727584839\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.6884050369262695 LOSS_Discriminator: 0.10303352773189545\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.595199108123779 LOSS_Discriminator: 0.07188977301120758\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.341363906860352 LOSS_Discriminator: 0.29879945516586304\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.610864162445068 LOSS_Discriminator: 0.06658309698104858\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.623907566070557 LOSS_Discriminator: 0.2693147659301758\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.758991718292236 LOSS_Discriminator: 0.11102361977100372\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.753217697143555 LOSS_Discriminator: 0.17034903168678284\n",
            "ITERATION_NO.: 341 LOSS_Generator: 6.078302383422852 LOSS_Discriminator: 0.2176804542541504\n",
            "ITERATION_NO.: 342 LOSS_Generator: 6.175817966461182 LOSS_Discriminator: 0.10559248924255371\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.867321014404297 LOSS_Discriminator: 0.04241957515478134\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.812143325805664 LOSS_Discriminator: 0.10615552961826324\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.0347700119018555 LOSS_Discriminator: 0.1767977476119995\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.509530067443848 LOSS_Discriminator: 0.185492604970932\n",
            "ITERATION_NO.: 347 LOSS_Generator: 4.864655017852783 LOSS_Discriminator: 0.07365021854639053\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.311368465423584 LOSS_Discriminator: 0.23828336596488953\n",
            "ITERATION_NO.: 349 LOSS_Generator: 4.934638977050781 LOSS_Discriminator: 0.11047999560832977\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.4075093269348145 LOSS_Discriminator: 0.09839917719364166\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.386861801147461 LOSS_Discriminator: 0.1576879620552063\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.7007832527160645 LOSS_Discriminator: 0.13425247371196747\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.762787342071533 LOSS_Discriminator: 0.18009403347969055\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.737199783325195 LOSS_Discriminator: 0.12043625861406326\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.9203572273254395 LOSS_Discriminator: 0.09236793965101242\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.6870527267456055 LOSS_Discriminator: 0.06965374201536179\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.517687797546387 LOSS_Discriminator: 0.12043549120426178\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.60507345199585 LOSS_Discriminator: 0.10607725381851196\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.7005486488342285 LOSS_Discriminator: 0.13913044333457947\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.310210704803467 LOSS_Discriminator: 0.17785197496414185\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.356762886047363 LOSS_Discriminator: 0.0828888788819313\n",
            "ITERATION_NO.: 362 LOSS_Generator: 4.85357666015625 LOSS_Discriminator: 0.2349015176296234\n",
            "ITERATION_NO.: 363 LOSS_Generator: 5.013906955718994 LOSS_Discriminator: 0.16458357870578766\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.696785926818848 LOSS_Discriminator: 0.08492305129766464\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.94487190246582 LOSS_Discriminator: 0.13782301545143127\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.895547866821289 LOSS_Discriminator: 0.2014901041984558\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.795156478881836 LOSS_Discriminator: 0.07748569548130035\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.729062557220459 LOSS_Discriminator: 0.329600065946579\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.457919120788574 LOSS_Discriminator: 0.14138281345367432\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.67637825012207 LOSS_Discriminator: 0.10670657455921173\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.5623016357421875 LOSS_Discriminator: 0.17000263929367065\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.816941738128662 LOSS_Discriminator: 0.146602064371109\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.6374335289001465 LOSS_Discriminator: 0.1189044862985611\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.409893989562988 LOSS_Discriminator: 0.13277527689933777\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.073018550872803 LOSS_Discriminator: 0.14981138706207275\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.584936141967773 LOSS_Discriminator: 0.12053405493497849\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.600015640258789 LOSS_Discriminator: 0.12187857925891876\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.893228054046631 LOSS_Discriminator: 0.29610878229141235\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.731162071228027 LOSS_Discriminator: 0.1304062157869339\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.819941520690918 LOSS_Discriminator: 0.1486583650112152\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.098523139953613 LOSS_Discriminator: 0.16954976320266724\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.002162456512451 LOSS_Discriminator: 0.21670204401016235\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.0337653160095215 LOSS_Discriminator: 0.08179581165313721\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.435297966003418 LOSS_Discriminator: 0.16544918715953827\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.268840789794922 LOSS_Discriminator: 0.22206845879554749\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.797847747802734 LOSS_Discriminator: 0.1673671007156372\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.847510814666748 LOSS_Discriminator: 0.13354849815368652\n",
            "ITERATION_NO.: 388 LOSS_Generator: 6.369684219360352 LOSS_Discriminator: 0.18565243482589722\n",
            "ITERATION_NO.: 389 LOSS_Generator: 6.367593288421631 LOSS_Discriminator: 0.1919565200805664\n",
            "ITERATION_NO.: 390 LOSS_Generator: 6.154919624328613 LOSS_Discriminator: 0.10199946165084839\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.298213005065918 LOSS_Discriminator: 0.18139418959617615\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.879159927368164 LOSS_Discriminator: 0.14566755294799805\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.569475173950195 LOSS_Discriminator: 0.17408952116966248\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.679113864898682 LOSS_Discriminator: 0.20728006958961487\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.46107292175293 LOSS_Discriminator: 0.1964925229549408\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.479186534881592 LOSS_Discriminator: 0.14302562177181244\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.701688766479492 LOSS_Discriminator: 0.16444669663906097\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.4539690017700195 LOSS_Discriminator: 0.18178504705429077\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.47615909576416 LOSS_Discriminator: 0.14818628132343292\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.9467949867248535 LOSS_Discriminator: 0.0749552771449089\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.945722579956055 LOSS_Discriminator: 0.15588802099227905\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.933416843414307 LOSS_Discriminator: 0.22701334953308105\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.18480110168457 LOSS_Discriminator: 0.09436183422803879\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.7578535079956055 LOSS_Discriminator: 0.08075742423534393\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.806285858154297 LOSS_Discriminator: 0.08954378962516785\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.7801690101623535 LOSS_Discriminator: 0.13386040925979614\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.790716648101807 LOSS_Discriminator: 0.07093433290719986\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.603942394256592 LOSS_Discriminator: 0.21121817827224731\n",
            "ITERATION_NO.: 409 LOSS_Generator: 4.985730171203613 LOSS_Discriminator: 0.19434131681919098\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.33371114730835 LOSS_Discriminator: 0.32877206802368164\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.114295959472656 LOSS_Discriminator: 0.13812154531478882\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.0251288414001465 LOSS_Discriminator: 0.15552783012390137\n",
            "ITERATION_NO.: 413 LOSS_Generator: 4.983243942260742 LOSS_Discriminator: 0.1580098420381546\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.1888604164123535 LOSS_Discriminator: 0.08220133185386658\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.557743549346924 LOSS_Discriminator: 0.09476269036531448\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.382380485534668 LOSS_Discriminator: 0.17747598886489868\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.850959300994873 LOSS_Discriminator: 0.16689205169677734\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.930853843688965 LOSS_Discriminator: 0.12333865463733673\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.585758209228516 LOSS_Discriminator: 0.21307741105556488\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.8049845695495605 LOSS_Discriminator: 0.11562441289424896\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.147475719451904 LOSS_Discriminator: 0.10939304530620575\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.339853286743164 LOSS_Discriminator: 0.11835181713104248\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.586719512939453 LOSS_Discriminator: 0.19995392858982086\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.094342231750488 LOSS_Discriminator: 0.07452341914176941\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.4349164962768555 LOSS_Discriminator: 0.09297853708267212\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.608900547027588 LOSS_Discriminator: 0.1455688625574112\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.481280326843262 LOSS_Discriminator: 0.1469998061656952\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.138062953948975 LOSS_Discriminator: 0.1700533777475357\n",
            "ITERATION_NO.: 429 LOSS_Generator: 6.0979695320129395 LOSS_Discriminator: 0.1017535924911499\n",
            "ITERATION_NO.: 430 LOSS_Generator: 6.31510066986084 LOSS_Discriminator: 0.11528264731168747\n",
            "ITERATION_NO.: 431 LOSS_Generator: 6.083224296569824 LOSS_Discriminator: 0.12398837506771088\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.87204647064209 LOSS_Discriminator: 0.15504685044288635\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.452237606048584 LOSS_Discriminator: 0.271837055683136\n",
            "ITERATION_NO.: 434 LOSS_Generator: 5.681551933288574 LOSS_Discriminator: 0.169906347990036\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.221080303192139 LOSS_Discriminator: 0.1652999073266983\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.649888515472412 LOSS_Discriminator: 0.2522708773612976\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.180412769317627 LOSS_Discriminator: 0.11808595061302185\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.284414768218994 LOSS_Discriminator: 0.09741078317165375\n",
            "ITERATION_NO.: 439 LOSS_Generator: 5.818570137023926 LOSS_Discriminator: 0.12042216211557388\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.305213451385498 LOSS_Discriminator: 0.11931795626878738\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.38719367980957 LOSS_Discriminator: 0.14659807085990906\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.982675552368164 LOSS_Discriminator: 0.13560357689857483\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.456292629241943 LOSS_Discriminator: 0.15912209451198578\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.422918319702148 LOSS_Discriminator: 0.24670599400997162\n",
            "ITERATION_NO.: 445 LOSS_Generator: 4.819121837615967 LOSS_Discriminator: 0.14337563514709473\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.491011142730713 LOSS_Discriminator: 0.10069279372692108\n",
            "ITERATION_NO.: 447 LOSS_Generator: 6.012725830078125 LOSS_Discriminator: 0.10649905353784561\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.9637041091918945 LOSS_Discriminator: 0.13394927978515625\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.924083232879639 LOSS_Discriminator: 0.17943692207336426\n",
            "ITERATION_NO.: 450 LOSS_Generator: 6.041628360748291 LOSS_Discriminator: 0.15109357237815857\n",
            "ITERATION_NO.: 451 LOSS_Generator: 6.302623271942139 LOSS_Discriminator: 0.13654476404190063\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.028972148895264 LOSS_Discriminator: 0.0949660986661911\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.800064563751221 LOSS_Discriminator: 0.11284235119819641\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.672816276550293 LOSS_Discriminator: 0.04593413323163986\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.529557704925537 LOSS_Discriminator: 0.1110256239771843\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.875967025756836 LOSS_Discriminator: 0.09218014031648636\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.7533745765686035 LOSS_Discriminator: 0.12308315932750702\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.735292911529541 LOSS_Discriminator: 0.08237607777118683\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.386803150177002 LOSS_Discriminator: 0.24086809158325195\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.97048282623291 LOSS_Discriminator: 0.08257586508989334\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.979272365570068 LOSS_Discriminator: 0.1904032677412033\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.955251693725586 LOSS_Discriminator: 0.15884849429130554\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.790345668792725 LOSS_Discriminator: 0.12020613253116608\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.579085826873779 LOSS_Discriminator: 0.2249147593975067\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.18604850769043 LOSS_Discriminator: 0.2682281732559204\n",
            "ITERATION_NO.: 466 LOSS_Generator: 4.942848205566406 LOSS_Discriminator: 0.22027257084846497\n",
            "ITERATION_NO.: 467 LOSS_Generator: 4.7852396965026855 LOSS_Discriminator: 0.1412144899368286\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.085148811340332 LOSS_Discriminator: 0.20647968351840973\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.6816182136535645 LOSS_Discriminator: 0.13360676169395447\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.930926322937012 LOSS_Discriminator: 0.08646465837955475\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.54811954498291 LOSS_Discriminator: 0.0630427747964859\n",
            "ITERATION_NO.: 472 LOSS_Generator: 6.643314361572266 LOSS_Discriminator: 0.1294313222169876\n",
            "ITERATION_NO.: 473 LOSS_Generator: 6.365419864654541 LOSS_Discriminator: 0.20797088742256165\n",
            "ITERATION_NO.: 474 LOSS_Generator: 6.152734756469727 LOSS_Discriminator: 0.10898587852716446\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.684074878692627 LOSS_Discriminator: 0.19140508770942688\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.778438091278076 LOSS_Discriminator: 0.16499358415603638\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.244284629821777 LOSS_Discriminator: 0.2499220222234726\n",
            "ITERATION_NO.: 478 LOSS_Generator: 4.965147018432617 LOSS_Discriminator: 0.15152132511138916\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.234955310821533 LOSS_Discriminator: 0.1222192645072937\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.060344219207764 LOSS_Discriminator: 0.18613077700138092\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.207707405090332 LOSS_Discriminator: 0.12549704313278198\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.5383381843566895 LOSS_Discriminator: 0.14528197050094604\n",
            "ITERATION_NO.: 483 LOSS_Generator: 6.278660297393799 LOSS_Discriminator: 0.10604913532733917\n",
            "ITERATION_NO.: 484 LOSS_Generator: 6.678281784057617 LOSS_Discriminator: 0.12480311095714569\n",
            "ITERATION_NO.: 485 LOSS_Generator: 6.9354071617126465 LOSS_Discriminator: 0.1490241140127182\n",
            "ITERATION_NO.: 486 LOSS_Generator: 6.7761077880859375 LOSS_Discriminator: 0.18047435581684113\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.458079814910889 LOSS_Discriminator: 0.15977779030799866\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.131017684936523 LOSS_Discriminator: 0.18325722217559814\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.682538986206055 LOSS_Discriminator: 0.17516803741455078\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.397383213043213 LOSS_Discriminator: 0.10440290719270706\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.429434299468994 LOSS_Discriminator: 0.11416983604431152\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.150889873504639 LOSS_Discriminator: 0.07318148016929626\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.0858001708984375 LOSS_Discriminator: 0.2996170222759247\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.137213706970215 LOSS_Discriminator: 0.11696610599756241\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.230781078338623 LOSS_Discriminator: 0.2764398455619812\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.9935455322265625 LOSS_Discriminator: 0.15432359278202057\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.800346851348877 LOSS_Discriminator: 0.1746802031993866\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.635312080383301 LOSS_Discriminator: 0.13134655356407166\n",
            "ITERATION_NO.: 499 LOSS_Generator: 6.177438735961914 LOSS_Discriminator: 0.12104124575853348\n",
            "ITERATION_NO.: 500 LOSS_Generator: 6.172393798828125 LOSS_Discriminator: 0.270679771900177\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.611779689788818 LOSS_Discriminator: 0.14729511737823486\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.151883602142334 LOSS_Discriminator: 0.0898139551281929\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.359570503234863 LOSS_Discriminator: 0.15508681535720825\n",
            "ITERATION_NO.: 504 LOSS_Generator: 4.947329998016357 LOSS_Discriminator: 0.10469742119312286\n",
            "ITERATION_NO.: 505 LOSS_Generator: 4.8720808029174805 LOSS_Discriminator: 0.10343246161937714\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.372370719909668 LOSS_Discriminator: 0.09782883524894714\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.4864983558654785 LOSS_Discriminator: 0.18042004108428955\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.12864875793457 LOSS_Discriminator: 0.053343597799539566\n",
            "ITERATION_NO.: 509 LOSS_Generator: 6.179990291595459 LOSS_Discriminator: 0.02840634062886238\n",
            "ITERATION_NO.: 510 LOSS_Generator: 6.232912540435791 LOSS_Discriminator: 0.276563823223114\n",
            "ITERATION_NO.: 511 LOSS_Generator: 6.103593826293945 LOSS_Discriminator: 0.15113013982772827\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.88755989074707 LOSS_Discriminator: 0.0792183130979538\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.878176689147949 LOSS_Discriminator: 0.1943548023700714\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.269433498382568 LOSS_Discriminator: 0.17638763785362244\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.338901519775391 LOSS_Discriminator: 0.20530377328395844\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.004938125610352 LOSS_Discriminator: 0.17774537205696106\n",
            "ITERATION_NO.: 517 LOSS_Generator: 4.71602725982666 LOSS_Discriminator: 0.14401262998580933\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.063558578491211 LOSS_Discriminator: 0.11874164640903473\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.40273904800415 LOSS_Discriminator: 0.10788269340991974\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.65671443939209 LOSS_Discriminator: 0.21217487752437592\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.670915126800537 LOSS_Discriminator: 0.1339360922574997\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.875554084777832 LOSS_Discriminator: 0.16742795705795288\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.41774845123291 LOSS_Discriminator: 0.1537962108850479\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.4088358879089355 LOSS_Discriminator: 0.12152998149394989\n",
            "ITERATION_NO.: 525 LOSS_Generator: 4.9575276374816895 LOSS_Discriminator: 0.1724380999803543\n",
            "ITERATION_NO.: 526 LOSS_Generator: 4.9386396408081055 LOSS_Discriminator: 0.14777624607086182\n",
            "ITERATION_NO.: 527 LOSS_Generator: 4.867570400238037 LOSS_Discriminator: 0.11607266962528229\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.3898468017578125 LOSS_Discriminator: 0.14814457297325134\n",
            "ITERATION_NO.: 529 LOSS_Generator: 4.9427714347839355 LOSS_Discriminator: 0.13445734977722168\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.245624542236328 LOSS_Discriminator: 0.09303228557109833\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.418509006500244 LOSS_Discriminator: 0.09779155254364014\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.741436958312988 LOSS_Discriminator: 0.21856442093849182\n",
            "ITERATION_NO.: 533 LOSS_Generator: 6.140613555908203 LOSS_Discriminator: 0.11927186697721481\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.562827110290527 LOSS_Discriminator: 0.15908217430114746\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.8117876052856445 LOSS_Discriminator: 0.11221711337566376\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.918342113494873 LOSS_Discriminator: 0.13805817067623138\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.696887016296387 LOSS_Discriminator: 0.09166499227285385\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.432857036590576 LOSS_Discriminator: 0.11876299977302551\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.132180213928223 LOSS_Discriminator: 0.0974903404712677\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.173372268676758 LOSS_Discriminator: 0.11606258153915405\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.276016712188721 LOSS_Discriminator: 0.0870809257030487\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.273739814758301 LOSS_Discriminator: 0.0671217143535614\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.30059814453125 LOSS_Discriminator: 0.24539662897586823\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.342158794403076 LOSS_Discriminator: 0.19078493118286133\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.929203033447266 LOSS_Discriminator: 0.08220896869897842\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.994291305541992 LOSS_Discriminator: 0.21435420215129852\n",
            "ITERATION_NO.: 547 LOSS_Generator: 6.1851911544799805 LOSS_Discriminator: 0.12581941485404968\n",
            "ITERATION_NO.: 548 LOSS_Generator: 6.317618370056152 LOSS_Discriminator: 0.11914829164743423\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.772380828857422 LOSS_Discriminator: 0.09899412095546722\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.788893222808838 LOSS_Discriminator: 0.11625239998102188\n",
            "ITERATION_NO.: 551 LOSS_Generator: 6.179516792297363 LOSS_Discriminator: 0.09729571640491486\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.965362548828125 LOSS_Discriminator: 0.15408942103385925\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.444460868835449 LOSS_Discriminator: 0.043348006904125214\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.739118576049805 LOSS_Discriminator: 0.1532919853925705\n",
            "ITERATION_NO.: 555 LOSS_Generator: 6.320810317993164 LOSS_Discriminator: 0.08952769637107849\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.957210063934326 LOSS_Discriminator: 0.1908062994480133\n",
            "ITERATION_NO.: 557 LOSS_Generator: 6.234978199005127 LOSS_Discriminator: 0.13831058144569397\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.327744960784912 LOSS_Discriminator: 0.1119760274887085\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.812170028686523 LOSS_Discriminator: 0.19980211555957794\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.389834403991699 LOSS_Discriminator: 0.16132932901382446\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.580750942230225 LOSS_Discriminator: 0.1736220121383667\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.215827465057373 LOSS_Discriminator: 0.13004189729690552\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.248204231262207 LOSS_Discriminator: 0.15869589149951935\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.374714374542236 LOSS_Discriminator: 0.14308233559131622\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.634484767913818 LOSS_Discriminator: 0.1611206978559494\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.628644943237305 LOSS_Discriminator: 0.11960073560476303\n",
            "ITERATION_NO.: 567 LOSS_Generator: 6.122740268707275 LOSS_Discriminator: 0.11887743324041367\n",
            "ITERATION_NO.: 568 LOSS_Generator: 6.509020805358887 LOSS_Discriminator: 0.14066891372203827\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.628271102905273 LOSS_Discriminator: 0.06075001507997513\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.599318027496338 LOSS_Discriminator: 0.3151392936706543\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.359011650085449 LOSS_Discriminator: 0.21858836710453033\n",
            "ITERATION_NO.: 572 LOSS_Generator: 6.810981273651123 LOSS_Discriminator: 0.08554044365882874\n",
            "ITERATION_NO.: 573 LOSS_Generator: 6.021104335784912 LOSS_Discriminator: 0.27544867992401123\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.975992202758789 LOSS_Discriminator: 0.12115977704524994\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.337986469268799 LOSS_Discriminator: 0.13201391696929932\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.744024276733398 LOSS_Discriminator: 0.19105860590934753\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.773129940032959 LOSS_Discriminator: 0.08189642429351807\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.619812488555908 LOSS_Discriminator: 0.21955257654190063\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.362673282623291 LOSS_Discriminator: 0.11803720891475677\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.0638933181762695 LOSS_Discriminator: 0.17125438153743744\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.418076038360596 LOSS_Discriminator: 0.09093707799911499\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.701418399810791 LOSS_Discriminator: 0.12411782145500183\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.589537143707275 LOSS_Discriminator: 0.07449954003095627\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.375363826751709 LOSS_Discriminator: 0.14670836925506592\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.340380668640137 LOSS_Discriminator: 0.061925940215587616\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.882363319396973 LOSS_Discriminator: 0.12865661084651947\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.193522930145264 LOSS_Discriminator: 0.1865490823984146\n",
            "ITERATION_NO.: 588 LOSS_Generator: 6.1083784103393555 LOSS_Discriminator: 0.15104934573173523\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.7426862716674805 LOSS_Discriminator: 0.10871856659650803\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.760167121887207 LOSS_Discriminator: 0.19192002713680267\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.535032749176025 LOSS_Discriminator: 0.06430976837873459\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.424820423126221 LOSS_Discriminator: 0.11599202454090118\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.1722307205200195 LOSS_Discriminator: 0.12359336018562317\n",
            "ITERATION_NO.: 594 LOSS_Generator: 5.123363494873047 LOSS_Discriminator: 0.23186102509498596\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.1053571701049805 LOSS_Discriminator: 0.14318354427814484\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.413833141326904 LOSS_Discriminator: 0.07083239406347275\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.203375339508057 LOSS_Discriminator: 0.10971782356500626\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.307321071624756 LOSS_Discriminator: 0.20932075381278992\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.721889019012451 LOSS_Discriminator: 0.09905262291431427\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.585098743438721 LOSS_Discriminator: 0.07834118604660034\n",
            "EPOCH OVER: 5\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.956651210784912 LOSS_Discriminator: 0.061800092458724976\n",
            "ITERATION_NO.: 2 LOSS_Generator: 6.226370811462402 LOSS_Discriminator: 0.07103918492794037\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.849686145782471 LOSS_Discriminator: 0.12673863768577576\n",
            "ITERATION_NO.: 4 LOSS_Generator: 6.134183883666992 LOSS_Discriminator: 0.14356347918510437\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.500196933746338 LOSS_Discriminator: 0.206425279378891\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.166658878326416 LOSS_Discriminator: 0.1868278980255127\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.126827239990234 LOSS_Discriminator: 0.17478187382221222\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.380573272705078 LOSS_Discriminator: 0.14679697155952454\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.038163661956787 LOSS_Discriminator: 0.1516692042350769\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.497791290283203 LOSS_Discriminator: 0.2667032480239868\n",
            "ITERATION_NO.: 11 LOSS_Generator: 6.093564033508301 LOSS_Discriminator: 0.2793436646461487\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.836536407470703 LOSS_Discriminator: 0.07898268103599548\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.171505928039551 LOSS_Discriminator: 0.15086126327514648\n",
            "ITERATION_NO.: 14 LOSS_Generator: 6.340341567993164 LOSS_Discriminator: 0.24537423253059387\n",
            "ITERATION_NO.: 15 LOSS_Generator: 6.019559383392334 LOSS_Discriminator: 0.15709367394447327\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.785766124725342 LOSS_Discriminator: 0.16792058944702148\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.413875579833984 LOSS_Discriminator: 0.07985421270132065\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.246286392211914 LOSS_Discriminator: 0.1430097222328186\n",
            "ITERATION_NO.: 19 LOSS_Generator: 4.756711483001709 LOSS_Discriminator: 0.07796293497085571\n",
            "ITERATION_NO.: 20 LOSS_Generator: 4.815343379974365 LOSS_Discriminator: 0.16749508678913116\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.034420013427734 LOSS_Discriminator: 0.1870856136083603\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.302768707275391 LOSS_Discriminator: 0.1508919596672058\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.806993007659912 LOSS_Discriminator: 0.22032377123832703\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.565072059631348 LOSS_Discriminator: 0.10019738972187042\n",
            "ITERATION_NO.: 25 LOSS_Generator: 6.467339038848877 LOSS_Discriminator: 0.09332694113254547\n",
            "ITERATION_NO.: 26 LOSS_Generator: 6.255222797393799 LOSS_Discriminator: 0.11584218591451645\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.788193225860596 LOSS_Discriminator: 0.1889623999595642\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.3371500968933105 LOSS_Discriminator: 0.12781581282615662\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.07017707824707 LOSS_Discriminator: 0.16336658596992493\n",
            "ITERATION_NO.: 30 LOSS_Generator: 4.7178826332092285 LOSS_Discriminator: 0.06079888343811035\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.035501003265381 LOSS_Discriminator: 0.16176573932170868\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.319076061248779 LOSS_Discriminator: 0.12983956933021545\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.343745231628418 LOSS_Discriminator: 0.3070998787879944\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.598063945770264 LOSS_Discriminator: 0.14187082648277283\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.923084259033203 LOSS_Discriminator: 0.24978941679000854\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.004664897918701 LOSS_Discriminator: 0.12749922275543213\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.068169593811035 LOSS_Discriminator: 0.19061672687530518\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.761947154998779 LOSS_Discriminator: 0.19133561849594116\n",
            "ITERATION_NO.: 39 LOSS_Generator: 5.3940911293029785 LOSS_Discriminator: 0.22432205080986023\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.605266571044922 LOSS_Discriminator: 0.12095870077610016\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.503594398498535 LOSS_Discriminator: 0.15337663888931274\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.594966411590576 LOSS_Discriminator: 0.08229152858257294\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.5581865310668945 LOSS_Discriminator: 0.07585357129573822\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.592363357543945 LOSS_Discriminator: 0.23541754484176636\n",
            "ITERATION_NO.: 45 LOSS_Generator: 5.458990573883057 LOSS_Discriminator: 0.12278619408607483\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.317319869995117 LOSS_Discriminator: 0.09039036929607391\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.4558234214782715 LOSS_Discriminator: 0.1735672503709793\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.308425426483154 LOSS_Discriminator: 0.13356903195381165\n",
            "ITERATION_NO.: 49 LOSS_Generator: 4.62650728225708 LOSS_Discriminator: 0.18220148980617523\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.148470878601074 LOSS_Discriminator: 0.2741597592830658\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.276329517364502 LOSS_Discriminator: 0.08735597878694534\n",
            "ITERATION_NO.: 52 LOSS_Generator: 5.250342845916748 LOSS_Discriminator: 0.16939939558506012\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.4330220222473145 LOSS_Discriminator: 0.23547527194023132\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.348452091217041 LOSS_Discriminator: 0.127238467335701\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.836551666259766 LOSS_Discriminator: 0.09559035301208496\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.744704723358154 LOSS_Discriminator: 0.10220317542552948\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.735895156860352 LOSS_Discriminator: 0.11911635845899582\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.544986724853516 LOSS_Discriminator: 0.10916838049888611\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.394780158996582 LOSS_Discriminator: 0.15426203608512878\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.223928451538086 LOSS_Discriminator: 0.13767243921756744\n",
            "ITERATION_NO.: 61 LOSS_Generator: 5.385308265686035 LOSS_Discriminator: 0.10509158670902252\n",
            "ITERATION_NO.: 62 LOSS_Generator: 5.339271068572998 LOSS_Discriminator: 0.1075751781463623\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.601140022277832 LOSS_Discriminator: 0.06433624774217606\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.583265781402588 LOSS_Discriminator: 0.33837854862213135\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.276047229766846 LOSS_Discriminator: 0.1999758630990982\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.754139423370361 LOSS_Discriminator: 0.13570448756217957\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.51370096206665 LOSS_Discriminator: 0.10913871228694916\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.402230262756348 LOSS_Discriminator: 0.145802304148674\n",
            "ITERATION_NO.: 69 LOSS_Generator: 6.059820175170898 LOSS_Discriminator: 0.0981578677892685\n",
            "ITERATION_NO.: 70 LOSS_Generator: 6.486501693725586 LOSS_Discriminator: 0.12516357004642487\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.8502373695373535 LOSS_Discriminator: 0.08853591978549957\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.696976184844971 LOSS_Discriminator: 0.2801825702190399\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.124324321746826 LOSS_Discriminator: 0.2076144963502884\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.265802383422852 LOSS_Discriminator: 0.2636660635471344\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.517638683319092 LOSS_Discriminator: 0.22029046714305878\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.781535625457764 LOSS_Discriminator: 0.19727396965026855\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.194238185882568 LOSS_Discriminator: 0.16759884357452393\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.496026515960693 LOSS_Discriminator: 0.1483512967824936\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.977024555206299 LOSS_Discriminator: 0.19746756553649902\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.883091926574707 LOSS_Discriminator: 0.1090429350733757\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.558190822601318 LOSS_Discriminator: 0.21501363813877106\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.973637104034424 LOSS_Discriminator: 0.15950241684913635\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.724420547485352 LOSS_Discriminator: 0.16057994961738586\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.358967304229736 LOSS_Discriminator: 0.10315023362636566\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.220704555511475 LOSS_Discriminator: 0.06523197144269943\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.083142280578613 LOSS_Discriminator: 0.09339462220668793\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.233132362365723 LOSS_Discriminator: 0.2538878917694092\n",
            "ITERATION_NO.: 88 LOSS_Generator: 4.742260932922363 LOSS_Discriminator: 0.0945928692817688\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.410367965698242 LOSS_Discriminator: 0.07437168061733246\n",
            "ITERATION_NO.: 90 LOSS_Generator: 6.1200690269470215 LOSS_Discriminator: 0.1688731610774994\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.987008094787598 LOSS_Discriminator: 0.1484706699848175\n",
            "ITERATION_NO.: 92 LOSS_Generator: 6.151347637176514 LOSS_Discriminator: 0.22454361617565155\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.1123480796813965 LOSS_Discriminator: 0.3079007565975189\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.9460248947143555 LOSS_Discriminator: 0.11301854252815247\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.963252067565918 LOSS_Discriminator: 0.10984481871128082\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.4484124183654785 LOSS_Discriminator: 0.147390678524971\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.7653727531433105 LOSS_Discriminator: 0.09158569574356079\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.453606128692627 LOSS_Discriminator: 0.13543936610221863\n",
            "ITERATION_NO.: 99 LOSS_Generator: 5.756657123565674 LOSS_Discriminator: 0.15335744619369507\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.929881572723389 LOSS_Discriminator: 0.09702259302139282\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.724251747131348 LOSS_Discriminator: 0.14225898683071136\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.76104736328125 LOSS_Discriminator: 0.19242817163467407\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.608742713928223 LOSS_Discriminator: 0.22959476709365845\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.178261756896973 LOSS_Discriminator: 0.15936653316020966\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.063239574432373 LOSS_Discriminator: 0.12031034380197525\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.462482452392578 LOSS_Discriminator: 0.11953496187925339\n",
            "ITERATION_NO.: 107 LOSS_Generator: 4.828869819641113 LOSS_Discriminator: 0.08132180571556091\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.274368762969971 LOSS_Discriminator: 0.121253103017807\n",
            "ITERATION_NO.: 109 LOSS_Generator: 5.52115535736084 LOSS_Discriminator: 0.14091609418392181\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.091119766235352 LOSS_Discriminator: 0.08705485612154007\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.344606876373291 LOSS_Discriminator: 0.18991681933403015\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.132933139801025 LOSS_Discriminator: 0.18815749883651733\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.508962631225586 LOSS_Discriminator: 0.13159005343914032\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.493606090545654 LOSS_Discriminator: 0.16890154778957367\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.859566688537598 LOSS_Discriminator: 0.14761964976787567\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.747013092041016 LOSS_Discriminator: 0.17117688059806824\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.662161827087402 LOSS_Discriminator: 0.16969600319862366\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.641562461853027 LOSS_Discriminator: 0.14373064041137695\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.271730422973633 LOSS_Discriminator: 0.1277538239955902\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.097265243530273 LOSS_Discriminator: 0.3004177212715149\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.176183700561523 LOSS_Discriminator: 0.17895060777664185\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.207663059234619 LOSS_Discriminator: 0.15578961372375488\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.746313571929932 LOSS_Discriminator: 0.06193514168262482\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.421245574951172 LOSS_Discriminator: 0.1547464281320572\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.024995803833008 LOSS_Discriminator: 0.15097680687904358\n",
            "ITERATION_NO.: 126 LOSS_Generator: 6.1587090492248535 LOSS_Discriminator: 0.283133864402771\n",
            "ITERATION_NO.: 127 LOSS_Generator: 6.297468662261963 LOSS_Discriminator: 0.12466421723365784\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.968012809753418 LOSS_Discriminator: 0.25646644830703735\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.538527011871338 LOSS_Discriminator: 0.11375349760055542\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.083712577819824 LOSS_Discriminator: 0.10945597290992737\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.838594436645508 LOSS_Discriminator: 0.06198304891586304\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.6893720626831055 LOSS_Discriminator: 0.13586807250976562\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.2424774169921875 LOSS_Discriminator: 0.2408311665058136\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.340875625610352 LOSS_Discriminator: 0.15829232335090637\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.189458847045898 LOSS_Discriminator: 0.11760690063238144\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.846193790435791 LOSS_Discriminator: 0.20539817214012146\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.6995930671691895 LOSS_Discriminator: 0.21646754443645477\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.738384246826172 LOSS_Discriminator: 0.14653930068016052\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.66529655456543 LOSS_Discriminator: 0.12409666180610657\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.238191604614258 LOSS_Discriminator: 0.1182556301355362\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.158860683441162 LOSS_Discriminator: 0.15423382818698883\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.0549492835998535 LOSS_Discriminator: 0.16148605942726135\n",
            "ITERATION_NO.: 143 LOSS_Generator: 4.977124214172363 LOSS_Discriminator: 0.08700157701969147\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.425561904907227 LOSS_Discriminator: 0.16935059428215027\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.569403648376465 LOSS_Discriminator: 0.13975757360458374\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.639977931976318 LOSS_Discriminator: 0.20393556356430054\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.503897190093994 LOSS_Discriminator: 0.051911383867263794\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.052871227264404 LOSS_Discriminator: 0.16838350892066956\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.588344097137451 LOSS_Discriminator: 0.13451774418354034\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.850588798522949 LOSS_Discriminator: 0.0736985057592392\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.558145046234131 LOSS_Discriminator: 0.08916881680488586\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.532675266265869 LOSS_Discriminator: 0.1417357176542282\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.0322771072387695 LOSS_Discriminator: 0.07056710869073868\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.199543476104736 LOSS_Discriminator: 0.11126482486724854\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.576622486114502 LOSS_Discriminator: 0.18987229466438293\n",
            "ITERATION_NO.: 156 LOSS_Generator: 4.943514823913574 LOSS_Discriminator: 0.05943470820784569\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.383508682250977 LOSS_Discriminator: 0.19104287028312683\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.454523086547852 LOSS_Discriminator: 0.1817690134048462\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.444058418273926 LOSS_Discriminator: 0.13151612877845764\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.193744659423828 LOSS_Discriminator: 0.06975793093442917\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.157151699066162 LOSS_Discriminator: 0.10031218081712723\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.432286262512207 LOSS_Discriminator: 0.12377925217151642\n",
            "ITERATION_NO.: 163 LOSS_Generator: 6.013523101806641 LOSS_Discriminator: 0.12226928025484085\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.07829475402832 LOSS_Discriminator: 0.09061983972787857\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.208223819732666 LOSS_Discriminator: 0.11305844038724899\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.282663345336914 LOSS_Discriminator: 0.17037799954414368\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.378350257873535 LOSS_Discriminator: 0.1513470709323883\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.45315408706665 LOSS_Discriminator: 0.1309422254562378\n",
            "ITERATION_NO.: 169 LOSS_Generator: 4.976080417633057 LOSS_Discriminator: 0.12903225421905518\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.869441032409668 LOSS_Discriminator: 0.09934365749359131\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.48845100402832 LOSS_Discriminator: 0.2672489881515503\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.7822699546813965 LOSS_Discriminator: 0.12465988099575043\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.905661106109619 LOSS_Discriminator: 0.09872384369373322\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.758767127990723 LOSS_Discriminator: 0.08097762614488602\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.574973106384277 LOSS_Discriminator: 0.25821375846862793\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.589587211608887 LOSS_Discriminator: 0.09068402647972107\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.408687591552734 LOSS_Discriminator: 0.217368483543396\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.1116533279418945 LOSS_Discriminator: 0.17250192165374756\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.286603927612305 LOSS_Discriminator: 0.1026245653629303\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.6436967849731445 LOSS_Discriminator: 0.2097577154636383\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.607074737548828 LOSS_Discriminator: 0.12493699043989182\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.511500358581543 LOSS_Discriminator: 0.12226695567369461\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.9262261390686035 LOSS_Discriminator: 0.16033482551574707\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.550318717956543 LOSS_Discriminator: 0.08931557834148407\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.713670253753662 LOSS_Discriminator: 0.1681211292743683\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.3762102127075195 LOSS_Discriminator: 0.11804424226284027\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.971263885498047 LOSS_Discriminator: 0.16609051823616028\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.818625450134277 LOSS_Discriminator: 0.13306227326393127\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.852290153503418 LOSS_Discriminator: 0.12459305673837662\n",
            "ITERATION_NO.: 190 LOSS_Generator: 6.237524032592773 LOSS_Discriminator: 0.15036797523498535\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.71682071685791 LOSS_Discriminator: 0.11404627561569214\n",
            "ITERATION_NO.: 192 LOSS_Generator: 5.837834358215332 LOSS_Discriminator: 0.19244694709777832\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.513054847717285 LOSS_Discriminator: 0.133107528090477\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.021890163421631 LOSS_Discriminator: 0.14427316188812256\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.775989532470703 LOSS_Discriminator: 0.13773366808891296\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.994438648223877 LOSS_Discriminator: 0.10342472791671753\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.824460029602051 LOSS_Discriminator: 0.20608851313591003\n",
            "ITERATION_NO.: 198 LOSS_Generator: 6.037121772766113 LOSS_Discriminator: 0.043526627123355865\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.7945027351379395 LOSS_Discriminator: 0.1154758632183075\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.598273754119873 LOSS_Discriminator: 0.17260953783988953\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.422607898712158 LOSS_Discriminator: 0.09135173261165619\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.505779266357422 LOSS_Discriminator: 0.09884700179100037\n",
            "ITERATION_NO.: 203 LOSS_Generator: 6.071274280548096 LOSS_Discriminator: 0.09034818410873413\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.6997270584106445 LOSS_Discriminator: 0.13799959421157837\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.641071319580078 LOSS_Discriminator: 0.07252015918493271\n",
            "ITERATION_NO.: 206 LOSS_Generator: 6.0659613609313965 LOSS_Discriminator: 0.10066849738359451\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.986108303070068 LOSS_Discriminator: 0.12344393134117126\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.591502666473389 LOSS_Discriminator: 0.08190692961215973\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.759420394897461 LOSS_Discriminator: 0.20536735653877258\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.331081390380859 LOSS_Discriminator: 0.123092882335186\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.256153583526611 LOSS_Discriminator: 0.19391286373138428\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.325727939605713 LOSS_Discriminator: 0.12265825271606445\n",
            "ITERATION_NO.: 213 LOSS_Generator: 4.89925479888916 LOSS_Discriminator: 0.18244360387325287\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.020690441131592 LOSS_Discriminator: 0.1374020129442215\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.677525043487549 LOSS_Discriminator: 0.17382630705833435\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.5166215896606445 LOSS_Discriminator: 0.10636227577924728\n",
            "ITERATION_NO.: 217 LOSS_Generator: 5.500627040863037 LOSS_Discriminator: 0.1948712170124054\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.2226881980896 LOSS_Discriminator: 0.12198641896247864\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.187458515167236 LOSS_Discriminator: 0.11451873928308487\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.77029275894165 LOSS_Discriminator: 0.17969441413879395\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.470162391662598 LOSS_Discriminator: 0.20627433061599731\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.544462203979492 LOSS_Discriminator: 0.15554818511009216\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.098241806030273 LOSS_Discriminator: 0.0879957526922226\n",
            "ITERATION_NO.: 224 LOSS_Generator: 5.466380596160889 LOSS_Discriminator: 0.12784221768379211\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.170948028564453 LOSS_Discriminator: 0.19327832758426666\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.8414130210876465 LOSS_Discriminator: 0.08101046830415726\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.011436939239502 LOSS_Discriminator: 0.08624143153429031\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.538731098175049 LOSS_Discriminator: 0.2822771668434143\n",
            "ITERATION_NO.: 229 LOSS_Generator: 6.070539951324463 LOSS_Discriminator: 0.25929126143455505\n",
            "ITERATION_NO.: 230 LOSS_Generator: 6.359410285949707 LOSS_Discriminator: 0.16960372030735016\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.938314914703369 LOSS_Discriminator: 0.15236486494541168\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.7927374839782715 LOSS_Discriminator: 0.10962195694446564\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.612117767333984 LOSS_Discriminator: 0.07911346107721329\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.591529369354248 LOSS_Discriminator: 0.19695261120796204\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.0029520988464355 LOSS_Discriminator: 0.13369399309158325\n",
            "ITERATION_NO.: 236 LOSS_Generator: 4.761845588684082 LOSS_Discriminator: 0.14738662540912628\n",
            "ITERATION_NO.: 237 LOSS_Generator: 5.402890682220459 LOSS_Discriminator: 0.13254110515117645\n",
            "ITERATION_NO.: 238 LOSS_Generator: 5.6091837882995605 LOSS_Discriminator: 0.24973690509796143\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.948105335235596 LOSS_Discriminator: 0.18178674578666687\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.491663932800293 LOSS_Discriminator: 0.3575359582901001\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.872015953063965 LOSS_Discriminator: 0.23499862849712372\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.109969615936279 LOSS_Discriminator: 0.1430811583995819\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.413743495941162 LOSS_Discriminator: 0.1481122225522995\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.0574727058410645 LOSS_Discriminator: 0.17470061779022217\n",
            "ITERATION_NO.: 245 LOSS_Generator: 5.1904988288879395 LOSS_Discriminator: 0.19935865700244904\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.229127407073975 LOSS_Discriminator: 0.181736558675766\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.191677093505859 LOSS_Discriminator: 0.09631186723709106\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.299374580383301 LOSS_Discriminator: 0.06343016773462296\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.803445816040039 LOSS_Discriminator: 0.16774581372737885\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.7732415199279785 LOSS_Discriminator: 0.2491040825843811\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.864005088806152 LOSS_Discriminator: 0.09953808039426804\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.1708292961120605 LOSS_Discriminator: 0.19868898391723633\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.922206878662109 LOSS_Discriminator: 0.20368406176567078\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.964072227478027 LOSS_Discriminator: 0.10976959019899368\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.73482608795166 LOSS_Discriminator: 0.1237870454788208\n",
            "ITERATION_NO.: 256 LOSS_Generator: 5.288825511932373 LOSS_Discriminator: 0.07685375213623047\n",
            "ITERATION_NO.: 257 LOSS_Generator: 5.31951904296875 LOSS_Discriminator: 0.14641492068767548\n",
            "ITERATION_NO.: 258 LOSS_Generator: 5.073617935180664 LOSS_Discriminator: 0.08737446367740631\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.532690525054932 LOSS_Discriminator: 0.22215336561203003\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.872935771942139 LOSS_Discriminator: 0.08712241798639297\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.265604019165039 LOSS_Discriminator: 0.19026005268096924\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.11311149597168 LOSS_Discriminator: 0.24059435725212097\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.733519077301025 LOSS_Discriminator: 0.1471419483423233\n",
            "ITERATION_NO.: 264 LOSS_Generator: 6.12164306640625 LOSS_Discriminator: 0.16064269840717316\n",
            "ITERATION_NO.: 265 LOSS_Generator: 6.085692882537842 LOSS_Discriminator: 0.12375696003437042\n",
            "ITERATION_NO.: 266 LOSS_Generator: 5.574876308441162 LOSS_Discriminator: 0.30095234513282776\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.277533054351807 LOSS_Discriminator: 0.13467654585838318\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.012326240539551 LOSS_Discriminator: 0.1338229477405548\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.420944690704346 LOSS_Discriminator: 0.17532917857170105\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.499975204467773 LOSS_Discriminator: 0.09640754014253616\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.919461727142334 LOSS_Discriminator: 0.16376857459545135\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.935974597930908 LOSS_Discriminator: 0.06160370260477066\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.9682207107543945 LOSS_Discriminator: 0.08999507129192352\n",
            "ITERATION_NO.: 274 LOSS_Generator: 6.162905693054199 LOSS_Discriminator: 0.13978293538093567\n",
            "ITERATION_NO.: 275 LOSS_Generator: 6.069807529449463 LOSS_Discriminator: 0.0971369743347168\n",
            "ITERATION_NO.: 276 LOSS_Generator: 6.301449775695801 LOSS_Discriminator: 0.1256096065044403\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.334639072418213 LOSS_Discriminator: 0.14170295000076294\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.735225677490234 LOSS_Discriminator: 0.0909683033823967\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.208879470825195 LOSS_Discriminator: 0.19788850843906403\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.578891754150391 LOSS_Discriminator: 0.07089956104755402\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.314507007598877 LOSS_Discriminator: 0.10335350781679153\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.026144981384277 LOSS_Discriminator: 0.16865608096122742\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.062865734100342 LOSS_Discriminator: 0.09882622957229614\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.449648857116699 LOSS_Discriminator: 0.08953885734081268\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.7101030349731445 LOSS_Discriminator: 0.1732281744480133\n",
            "ITERATION_NO.: 286 LOSS_Generator: 6.060573577880859 LOSS_Discriminator: 0.12815935909748077\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.371891021728516 LOSS_Discriminator: 0.0860043615102768\n",
            "ITERATION_NO.: 288 LOSS_Generator: 6.042678356170654 LOSS_Discriminator: 0.19460752606391907\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.609476089477539 LOSS_Discriminator: 0.2615518271923065\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.820544242858887 LOSS_Discriminator: 0.18685157597064972\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.3182878494262695 LOSS_Discriminator: 0.14296293258666992\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.203752517700195 LOSS_Discriminator: 0.1376577615737915\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.265325546264648 LOSS_Discriminator: 0.12865175306797028\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.4410481452941895 LOSS_Discriminator: 0.1893751472234726\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.245007514953613 LOSS_Discriminator: 0.14937439560890198\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.982999801635742 LOSS_Discriminator: 0.20874950289726257\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.611027717590332 LOSS_Discriminator: 0.09932497888803482\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.482375621795654 LOSS_Discriminator: 0.3295944035053253\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.5267558097839355 LOSS_Discriminator: 0.08477513492107391\n",
            "ITERATION_NO.: 300 LOSS_Generator: 4.974110126495361 LOSS_Discriminator: 0.2894132137298584\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.650850296020508 LOSS_Discriminator: 0.16274623572826385\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.803525924682617 LOSS_Discriminator: 0.2384350597858429\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.1995038986206055 LOSS_Discriminator: 0.23950855433940887\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.599884510040283 LOSS_Discriminator: 0.14920008182525635\n",
            "ITERATION_NO.: 305 LOSS_Generator: 6.054279327392578 LOSS_Discriminator: 0.2421763837337494\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.546096324920654 LOSS_Discriminator: 0.15052714943885803\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.913034439086914 LOSS_Discriminator: 0.21436291933059692\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.6822943687438965 LOSS_Discriminator: 0.2451065182685852\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.862043857574463 LOSS_Discriminator: 0.16049520671367645\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.390006065368652 LOSS_Discriminator: 0.12991367280483246\n",
            "ITERATION_NO.: 311 LOSS_Generator: 4.583679676055908 LOSS_Discriminator: 0.24759697914123535\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.403011322021484 LOSS_Discriminator: 0.22485384345054626\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.690306186676025 LOSS_Discriminator: 0.12494997680187225\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.907536029815674 LOSS_Discriminator: 0.14958909153938293\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.1800618171691895 LOSS_Discriminator: 0.17170247435569763\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.311749458312988 LOSS_Discriminator: 0.08871784806251526\n",
            "ITERATION_NO.: 317 LOSS_Generator: 6.185152530670166 LOSS_Discriminator: 0.1818133294582367\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.795541286468506 LOSS_Discriminator: 0.062012165784835815\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.8738250732421875 LOSS_Discriminator: 0.14095920324325562\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.476658821105957 LOSS_Discriminator: 0.15342871844768524\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.4689154624938965 LOSS_Discriminator: 0.13628293573856354\n",
            "ITERATION_NO.: 322 LOSS_Generator: 5.320435047149658 LOSS_Discriminator: 0.14070117473602295\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.208776473999023 LOSS_Discriminator: 0.09493745863437653\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.374533176422119 LOSS_Discriminator: 0.17488667368888855\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.464231967926025 LOSS_Discriminator: 0.165581613779068\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.385044574737549 LOSS_Discriminator: 0.1093854233622551\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.294644832611084 LOSS_Discriminator: 0.1322489082813263\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.296959400177002 LOSS_Discriminator: 0.20684814453125\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.295379638671875 LOSS_Discriminator: 0.2910146713256836\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.006443023681641 LOSS_Discriminator: 0.16465118527412415\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.354314804077148 LOSS_Discriminator: 0.0681622326374054\n",
            "ITERATION_NO.: 332 LOSS_Generator: 4.876916885375977 LOSS_Discriminator: 0.15409670770168304\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.285919189453125 LOSS_Discriminator: 0.1904623806476593\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.551657676696777 LOSS_Discriminator: 0.14967116713523865\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.652454853057861 LOSS_Discriminator: 0.18044224381446838\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.382850170135498 LOSS_Discriminator: 0.1672174334526062\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.957476615905762 LOSS_Discriminator: 0.12146405875682831\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.570066452026367 LOSS_Discriminator: 0.09584298729896545\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.377419471740723 LOSS_Discriminator: 0.1170821487903595\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.415071487426758 LOSS_Discriminator: 0.13343091309070587\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.420472621917725 LOSS_Discriminator: 0.11135576665401459\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.418527126312256 LOSS_Discriminator: 0.1394389569759369\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.348620414733887 LOSS_Discriminator: 0.1467343121767044\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.574234485626221 LOSS_Discriminator: 0.07877572625875473\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.632877826690674 LOSS_Discriminator: 0.08957738429307938\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.344497203826904 LOSS_Discriminator: 0.16310833394527435\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.6603803634643555 LOSS_Discriminator: 0.16760537028312683\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.8257975578308105 LOSS_Discriminator: 0.11083950102329254\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.965887069702148 LOSS_Discriminator: 0.27464503049850464\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.782220840454102 LOSS_Discriminator: 0.254467636346817\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.189871788024902 LOSS_Discriminator: 0.09257746487855911\n",
            "ITERATION_NO.: 352 LOSS_Generator: 4.6438446044921875 LOSS_Discriminator: 0.12599578499794006\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.1532793045043945 LOSS_Discriminator: 0.13772156834602356\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.34310245513916 LOSS_Discriminator: 0.1122032105922699\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.44138240814209 LOSS_Discriminator: 0.0756349116563797\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.940159320831299 LOSS_Discriminator: 0.12041774392127991\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.790544509887695 LOSS_Discriminator: 0.15381456911563873\n",
            "ITERATION_NO.: 358 LOSS_Generator: 6.1790008544921875 LOSS_Discriminator: 0.1305343061685562\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.827233791351318 LOSS_Discriminator: 0.17594793438911438\n",
            "ITERATION_NO.: 360 LOSS_Generator: 6.1201019287109375 LOSS_Discriminator: 0.15772080421447754\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.151432037353516 LOSS_Discriminator: 0.1502140462398529\n",
            "ITERATION_NO.: 362 LOSS_Generator: 4.920437812805176 LOSS_Discriminator: 0.12664605677127838\n",
            "ITERATION_NO.: 363 LOSS_Generator: 4.904745101928711 LOSS_Discriminator: 0.13033491373062134\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.308131694793701 LOSS_Discriminator: 0.15960979461669922\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.1270222663879395 LOSS_Discriminator: 0.10815482586622238\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.520369052886963 LOSS_Discriminator: 0.14823776483535767\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.327693939208984 LOSS_Discriminator: 0.13667626678943634\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.770303726196289 LOSS_Discriminator: 0.04513522610068321\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.8837385177612305 LOSS_Discriminator: 0.1468191295862198\n",
            "ITERATION_NO.: 370 LOSS_Generator: 6.170126914978027 LOSS_Discriminator: 0.20338940620422363\n",
            "ITERATION_NO.: 371 LOSS_Generator: 6.229832649230957 LOSS_Discriminator: 0.10646915435791016\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.949041366577148 LOSS_Discriminator: 0.14521262049674988\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.726670742034912 LOSS_Discriminator: 0.1958288550376892\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.137753963470459 LOSS_Discriminator: 0.188558429479599\n",
            "ITERATION_NO.: 375 LOSS_Generator: 4.922685146331787 LOSS_Discriminator: 0.20136836171150208\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.105417251586914 LOSS_Discriminator: 0.16154544055461884\n",
            "ITERATION_NO.: 377 LOSS_Generator: 5.243358135223389 LOSS_Discriminator: 0.1294703483581543\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.39353084564209 LOSS_Discriminator: 0.170697882771492\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.332500457763672 LOSS_Discriminator: 0.09984126687049866\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.733781337738037 LOSS_Discriminator: 0.1719253659248352\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.543550968170166 LOSS_Discriminator: 0.08504702150821686\n",
            "ITERATION_NO.: 382 LOSS_Generator: 6.187658309936523 LOSS_Discriminator: 0.147688627243042\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.133649826049805 LOSS_Discriminator: 0.1429036259651184\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.946901321411133 LOSS_Discriminator: 0.14677250385284424\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.91032600402832 LOSS_Discriminator: 0.1213168054819107\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.817928314208984 LOSS_Discriminator: 0.17200613021850586\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.598661422729492 LOSS_Discriminator: 0.21118265390396118\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.338869571685791 LOSS_Discriminator: 0.12656787037849426\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.658222198486328 LOSS_Discriminator: 0.1399938315153122\n",
            "ITERATION_NO.: 390 LOSS_Generator: 4.875279903411865 LOSS_Discriminator: 0.12548214197158813\n",
            "ITERATION_NO.: 391 LOSS_Generator: 5.6654438972473145 LOSS_Discriminator: 0.2078520655632019\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.697061538696289 LOSS_Discriminator: 0.30349645018577576\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.504715442657471 LOSS_Discriminator: 0.17214076220989227\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.6477837562561035 LOSS_Discriminator: 0.23113982379436493\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.617877006530762 LOSS_Discriminator: 0.1747618019580841\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.227290630340576 LOSS_Discriminator: 0.13693034648895264\n",
            "ITERATION_NO.: 397 LOSS_Generator: 6.497280120849609 LOSS_Discriminator: 0.09008069336414337\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.818243980407715 LOSS_Discriminator: 0.07478412240743637\n",
            "ITERATION_NO.: 399 LOSS_Generator: 6.374825954437256 LOSS_Discriminator: 0.1151457279920578\n",
            "ITERATION_NO.: 400 LOSS_Generator: 6.029427528381348 LOSS_Discriminator: 0.1286933720111847\n",
            "ITERATION_NO.: 401 LOSS_Generator: 6.429768085479736 LOSS_Discriminator: 0.080879345536232\n",
            "ITERATION_NO.: 402 LOSS_Generator: 6.200236797332764 LOSS_Discriminator: 0.13604430854320526\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.764688014984131 LOSS_Discriminator: 0.17523010075092316\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.658023834228516 LOSS_Discriminator: 0.03438185900449753\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.063803672790527 LOSS_Discriminator: 0.08143773674964905\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.110744476318359 LOSS_Discriminator: 0.18449276685714722\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.385485649108887 LOSS_Discriminator: 0.08744629472494125\n",
            "ITERATION_NO.: 408 LOSS_Generator: 4.956811904907227 LOSS_Discriminator: 0.08529364317655563\n",
            "ITERATION_NO.: 409 LOSS_Generator: 5.4554033279418945 LOSS_Discriminator: 0.13107137382030487\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.737666130065918 LOSS_Discriminator: 0.1249723732471466\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.7464776039123535 LOSS_Discriminator: 0.14595015347003937\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.623358726501465 LOSS_Discriminator: 0.10421914607286453\n",
            "ITERATION_NO.: 413 LOSS_Generator: 6.2089524269104 LOSS_Discriminator: 0.058601152151823044\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.431118965148926 LOSS_Discriminator: 0.0576649084687233\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.3994245529174805 LOSS_Discriminator: 0.23038233816623688\n",
            "ITERATION_NO.: 416 LOSS_Generator: 6.630496025085449 LOSS_Discriminator: 0.09006427973508835\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.365642070770264 LOSS_Discriminator: 0.1237812489271164\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.496806621551514 LOSS_Discriminator: 0.17092835903167725\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.012845039367676 LOSS_Discriminator: 0.15936370193958282\n",
            "ITERATION_NO.: 420 LOSS_Generator: 4.569979667663574 LOSS_Discriminator: 0.2121562361717224\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.103555202484131 LOSS_Discriminator: 0.17384496331214905\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.217912673950195 LOSS_Discriminator: 0.21041491627693176\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.6425347328186035 LOSS_Discriminator: 0.13002163171768188\n",
            "ITERATION_NO.: 424 LOSS_Generator: 6.0472636222839355 LOSS_Discriminator: 0.15233419835567474\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.1181559562683105 LOSS_Discriminator: 0.14115455746650696\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.631985664367676 LOSS_Discriminator: 0.145173579454422\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.016146659851074 LOSS_Discriminator: 0.1757267862558365\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.59262752532959 LOSS_Discriminator: 0.1269209235906601\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.325809478759766 LOSS_Discriminator: 0.16957300901412964\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.0536274909973145 LOSS_Discriminator: 0.1661803424358368\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.159650802612305 LOSS_Discriminator: 0.2263263314962387\n",
            "ITERATION_NO.: 432 LOSS_Generator: 4.821011066436768 LOSS_Discriminator: 0.1780892014503479\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.5151519775390625 LOSS_Discriminator: 0.1506529450416565\n",
            "ITERATION_NO.: 434 LOSS_Generator: 5.711057186126709 LOSS_Discriminator: 0.21482713520526886\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.960233688354492 LOSS_Discriminator: 0.10277251899242401\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.138255596160889 LOSS_Discriminator: 0.08898340910673141\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.2627482414245605 LOSS_Discriminator: 0.0659329891204834\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.643251895904541 LOSS_Discriminator: 0.14426982402801514\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.54379940032959 LOSS_Discriminator: 0.08746304363012314\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.601332187652588 LOSS_Discriminator: 0.16128844022750854\n",
            "ITERATION_NO.: 441 LOSS_Generator: 5.579362869262695 LOSS_Discriminator: 0.2174215018749237\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.323398590087891 LOSS_Discriminator: 0.1577184498310089\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.696589946746826 LOSS_Discriminator: 0.15832318365573883\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.343294143676758 LOSS_Discriminator: 0.10029133409261703\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.160287380218506 LOSS_Discriminator: 0.25860100984573364\n",
            "ITERATION_NO.: 446 LOSS_Generator: 4.727358341217041 LOSS_Discriminator: 0.2301320731639862\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.1992573738098145 LOSS_Discriminator: 0.16604308784008026\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.291591167449951 LOSS_Discriminator: 0.10004907846450806\n",
            "ITERATION_NO.: 449 LOSS_Generator: 6.1937432289123535 LOSS_Discriminator: 0.1945778876543045\n",
            "ITERATION_NO.: 450 LOSS_Generator: 6.206467151641846 LOSS_Discriminator: 0.08960175514221191\n",
            "ITERATION_NO.: 451 LOSS_Generator: 6.245563983917236 LOSS_Discriminator: 0.176876962184906\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.122984409332275 LOSS_Discriminator: 0.17026513814926147\n",
            "ITERATION_NO.: 453 LOSS_Generator: 6.7693986892700195 LOSS_Discriminator: 0.15508350729942322\n",
            "ITERATION_NO.: 454 LOSS_Generator: 6.258343696594238 LOSS_Discriminator: 0.20435525476932526\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.304593563079834 LOSS_Discriminator: 0.16313980519771576\n",
            "ITERATION_NO.: 456 LOSS_Generator: 6.168853759765625 LOSS_Discriminator: 0.14209575951099396\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.670555591583252 LOSS_Discriminator: 0.11552615463733673\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.122791767120361 LOSS_Discriminator: 0.08996842056512833\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.14605712890625 LOSS_Discriminator: 0.08268837630748749\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.240719795227051 LOSS_Discriminator: 0.2554420828819275\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.240424156188965 LOSS_Discriminator: 0.15332798659801483\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.827863693237305 LOSS_Discriminator: 0.12927746772766113\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.907835006713867 LOSS_Discriminator: 0.12658734619617462\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.671596050262451 LOSS_Discriminator: 0.2071252018213272\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.580758571624756 LOSS_Discriminator: 0.13318240642547607\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.565609931945801 LOSS_Discriminator: 0.13931530714035034\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.748859405517578 LOSS_Discriminator: 0.17508098483085632\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.696316719055176 LOSS_Discriminator: 0.1457996964454651\n",
            "ITERATION_NO.: 469 LOSS_Generator: 4.602941513061523 LOSS_Discriminator: 0.23605504631996155\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.637534141540527 LOSS_Discriminator: 0.23827870190143585\n",
            "ITERATION_NO.: 471 LOSS_Generator: 4.978572845458984 LOSS_Discriminator: 0.2026439905166626\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.157315731048584 LOSS_Discriminator: 0.11128973215818405\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.683038234710693 LOSS_Discriminator: 0.24095410108566284\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.497757434844971 LOSS_Discriminator: 0.0877041220664978\n",
            "ITERATION_NO.: 475 LOSS_Generator: 6.318368434906006 LOSS_Discriminator: 0.12794317305088043\n",
            "ITERATION_NO.: 476 LOSS_Generator: 6.069341659545898 LOSS_Discriminator: 0.13529939949512482\n",
            "ITERATION_NO.: 477 LOSS_Generator: 6.29580020904541 LOSS_Discriminator: 0.09444523602724075\n",
            "ITERATION_NO.: 478 LOSS_Generator: 6.151373386383057 LOSS_Discriminator: 0.2524870038032532\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.119271755218506 LOSS_Discriminator: 0.14207106828689575\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.691755294799805 LOSS_Discriminator: 0.1540064662694931\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.142233371734619 LOSS_Discriminator: 0.10959164053201675\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.3674750328063965 LOSS_Discriminator: 0.08232580125331879\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.095170974731445 LOSS_Discriminator: 0.20991230010986328\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.346738815307617 LOSS_Discriminator: 0.08403605222702026\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.329814434051514 LOSS_Discriminator: 0.11897723376750946\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.519442558288574 LOSS_Discriminator: 0.13882969319820404\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.480200290679932 LOSS_Discriminator: 0.12517714500427246\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.104638576507568 LOSS_Discriminator: 0.0919952318072319\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.967428684234619 LOSS_Discriminator: 0.16832295060157776\n",
            "ITERATION_NO.: 490 LOSS_Generator: 6.775471210479736 LOSS_Discriminator: 0.05267462879419327\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.750889301300049 LOSS_Discriminator: 0.23843102157115936\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.751458644866943 LOSS_Discriminator: 0.07868388295173645\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.826160430908203 LOSS_Discriminator: 0.1387944221496582\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.297895431518555 LOSS_Discriminator: 0.11585452407598495\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.5294508934021 LOSS_Discriminator: 0.09774525463581085\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.4076247215271 LOSS_Discriminator: 0.1639799177646637\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.392934799194336 LOSS_Discriminator: 0.10254832357168198\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.627819061279297 LOSS_Discriminator: 0.16270765662193298\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.747626304626465 LOSS_Discriminator: 0.11689496040344238\n",
            "ITERATION_NO.: 500 LOSS_Generator: 6.013673305511475 LOSS_Discriminator: 0.14349550008773804\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.868979454040527 LOSS_Discriminator: 0.20651471614837646\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.458487033843994 LOSS_Discriminator: 0.08343246579170227\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.325505256652832 LOSS_Discriminator: 0.2326846718788147\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.254411697387695 LOSS_Discriminator: 0.10433890670537949\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.325925350189209 LOSS_Discriminator: 0.06738954782485962\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.294279098510742 LOSS_Discriminator: 0.12284592539072037\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.275628566741943 LOSS_Discriminator: 0.1542891412973404\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.353296279907227 LOSS_Discriminator: 0.13993290066719055\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.849803924560547 LOSS_Discriminator: 0.212042897939682\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.6694841384887695 LOSS_Discriminator: 0.235446959733963\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.881458282470703 LOSS_Discriminator: 0.05677973851561546\n",
            "ITERATION_NO.: 512 LOSS_Generator: 6.016762733459473 LOSS_Discriminator: 0.15246862173080444\n",
            "ITERATION_NO.: 513 LOSS_Generator: 6.012722969055176 LOSS_Discriminator: 0.2233622968196869\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.6770853996276855 LOSS_Discriminator: 0.16222922503948212\n",
            "ITERATION_NO.: 515 LOSS_Generator: 4.967220783233643 LOSS_Discriminator: 0.21185295283794403\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.047313690185547 LOSS_Discriminator: 0.2657596468925476\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.054325103759766 LOSS_Discriminator: 0.1537853479385376\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.182901382446289 LOSS_Discriminator: 0.2547374367713928\n",
            "ITERATION_NO.: 519 LOSS_Generator: 4.975550174713135 LOSS_Discriminator: 0.16574449837207794\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.14401388168335 LOSS_Discriminator: 0.1091730147600174\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.6949920654296875 LOSS_Discriminator: 0.21036610007286072\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.836205005645752 LOSS_Discriminator: 0.13705509901046753\n",
            "ITERATION_NO.: 523 LOSS_Generator: 6.052694797515869 LOSS_Discriminator: 0.17986682057380676\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.5827107429504395 LOSS_Discriminator: 0.13164296746253967\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.589534282684326 LOSS_Discriminator: 0.11506500095129013\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.466129779815674 LOSS_Discriminator: 0.14918574690818787\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.8126654624938965 LOSS_Discriminator: 0.1025039553642273\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.052958011627197 LOSS_Discriminator: 0.1309053599834442\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.3974223136901855 LOSS_Discriminator: 0.11041533946990967\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.526201248168945 LOSS_Discriminator: 0.1006234660744667\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.29299783706665 LOSS_Discriminator: 0.19421176612377167\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.764666557312012 LOSS_Discriminator: 0.1121109127998352\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.665175914764404 LOSS_Discriminator: 0.1327887326478958\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.625911235809326 LOSS_Discriminator: 0.12812775373458862\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.8925862312316895 LOSS_Discriminator: 0.20825767517089844\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.7214555740356445 LOSS_Discriminator: 0.18620304763317108\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.987605571746826 LOSS_Discriminator: 0.09051919728517532\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.2148261070251465 LOSS_Discriminator: 0.11491808295249939\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.347412109375 LOSS_Discriminator: 0.06965912133455276\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.134701728820801 LOSS_Discriminator: 0.10856515169143677\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.704592704772949 LOSS_Discriminator: 0.1911948025226593\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.453312873840332 LOSS_Discriminator: 0.12665212154388428\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.711972236633301 LOSS_Discriminator: 0.12943291664123535\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.777618408203125 LOSS_Discriminator: 0.08927571773529053\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.9078850746154785 LOSS_Discriminator: 0.2424258291721344\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.897180557250977 LOSS_Discriminator: 0.25713875889778137\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.9185004234313965 LOSS_Discriminator: 0.24514788389205933\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.951699256896973 LOSS_Discriminator: 0.08214685320854187\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.388088226318359 LOSS_Discriminator: 0.14450903236865997\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.948610305786133 LOSS_Discriminator: 0.17236889898777008\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.488591194152832 LOSS_Discriminator: 0.19257155060768127\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.123349666595459 LOSS_Discriminator: 0.1911124736070633\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.3931169509887695 LOSS_Discriminator: 0.16785019636154175\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.735141754150391 LOSS_Discriminator: 0.06773803383111954\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.3852739334106445 LOSS_Discriminator: 0.13338840007781982\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.426249980926514 LOSS_Discriminator: 0.2830906808376312\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.855607986450195 LOSS_Discriminator: 0.1085430160164833\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.714409828186035 LOSS_Discriminator: 0.14915227890014648\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.822934627532959 LOSS_Discriminator: 0.11936663091182709\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.359598159790039 LOSS_Discriminator: 0.15033112466335297\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.5023651123046875 LOSS_Discriminator: 0.25018852949142456\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.106083869934082 LOSS_Discriminator: 0.16602981090545654\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.582565784454346 LOSS_Discriminator: 0.18894514441490173\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.575432300567627 LOSS_Discriminator: 0.11795869469642639\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.645900249481201 LOSS_Discriminator: 0.19328996539115906\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.7819504737854 LOSS_Discriminator: 0.15739408135414124\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.724420547485352 LOSS_Discriminator: 0.0839669406414032\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.928342819213867 LOSS_Discriminator: 0.1565270870923996\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.468961715698242 LOSS_Discriminator: 0.1803743839263916\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.590181827545166 LOSS_Discriminator: 0.204854816198349\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.939278602600098 LOSS_Discriminator: 0.24842330813407898\n",
            "ITERATION_NO.: 572 LOSS_Generator: 6.358121871948242 LOSS_Discriminator: 0.10380186140537262\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.8902082443237305 LOSS_Discriminator: 0.17921224236488342\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.907815456390381 LOSS_Discriminator: 0.20144985616207123\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.180570125579834 LOSS_Discriminator: 0.08420491963624954\n",
            "ITERATION_NO.: 576 LOSS_Generator: 4.810198783874512 LOSS_Discriminator: 0.1493813693523407\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.012999534606934 LOSS_Discriminator: 0.12044354528188705\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.301151275634766 LOSS_Discriminator: 0.22753329575061798\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.412271022796631 LOSS_Discriminator: 0.11941714584827423\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.8438568115234375 LOSS_Discriminator: 0.12330304086208344\n",
            "ITERATION_NO.: 581 LOSS_Generator: 6.462456703186035 LOSS_Discriminator: 0.06506024301052094\n",
            "ITERATION_NO.: 582 LOSS_Generator: 6.558768272399902 LOSS_Discriminator: 0.21039021015167236\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.656494617462158 LOSS_Discriminator: 0.21573233604431152\n",
            "ITERATION_NO.: 584 LOSS_Generator: 5.850317478179932 LOSS_Discriminator: 0.2242887020111084\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.261602401733398 LOSS_Discriminator: 0.11796984076499939\n",
            "ITERATION_NO.: 586 LOSS_Generator: 4.884869575500488 LOSS_Discriminator: 0.14450907707214355\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.097602844238281 LOSS_Discriminator: 0.3279546797275543\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.218043327331543 LOSS_Discriminator: 0.12435714900493622\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.237126350402832 LOSS_Discriminator: 0.10442778468132019\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.711548328399658 LOSS_Discriminator: 0.17241264879703522\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.577305316925049 LOSS_Discriminator: 0.3311519920825958\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.870866298675537 LOSS_Discriminator: 0.2722979187965393\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.701743125915527 LOSS_Discriminator: 0.09146560728549957\n",
            "ITERATION_NO.: 594 LOSS_Generator: 5.524759292602539 LOSS_Discriminator: 0.08819276094436646\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.757216930389404 LOSS_Discriminator: 0.16868972778320312\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.721031665802002 LOSS_Discriminator: 0.09335242956876755\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.378681659698486 LOSS_Discriminator: 0.18370939791202545\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.722465991973877 LOSS_Discriminator: 0.1573612540960312\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.520369052886963 LOSS_Discriminator: 0.11701697111129761\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.815366268157959 LOSS_Discriminator: 0.13136690855026245\n",
            "EPOCH OVER: 6\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.5938029289245605 LOSS_Discriminator: 0.18308430910110474\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.76740837097168 LOSS_Discriminator: 0.07488641142845154\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.7152323722839355 LOSS_Discriminator: 0.11820664256811142\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.819442272186279 LOSS_Discriminator: 0.12393371760845184\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.913018226623535 LOSS_Discriminator: 0.12915967404842377\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.0797624588012695 LOSS_Discriminator: 0.14976437389850616\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.237133026123047 LOSS_Discriminator: 0.12184377759695053\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.581276893615723 LOSS_Discriminator: 0.07429476082324982\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.105651378631592 LOSS_Discriminator: 0.1662772297859192\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.452966690063477 LOSS_Discriminator: 0.10507439076900482\n",
            "ITERATION_NO.: 11 LOSS_Generator: 4.663827896118164 LOSS_Discriminator: 0.09819012135267258\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.663252353668213 LOSS_Discriminator: 0.09100425988435745\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.749006271362305 LOSS_Discriminator: 0.08940958976745605\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.552295684814453 LOSS_Discriminator: 0.1733075976371765\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.740390777587891 LOSS_Discriminator: 0.1463131159543991\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.408994674682617 LOSS_Discriminator: 0.06632392108440399\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.450006484985352 LOSS_Discriminator: 0.226149320602417\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.587566375732422 LOSS_Discriminator: 0.09643705189228058\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.341929912567139 LOSS_Discriminator: 0.15398696064949036\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.258350372314453 LOSS_Discriminator: 0.1710713654756546\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.053332328796387 LOSS_Discriminator: 0.12102332711219788\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.4941205978393555 LOSS_Discriminator: 0.08379395306110382\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.369723320007324 LOSS_Discriminator: 0.17528140544891357\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.621187686920166 LOSS_Discriminator: 0.12819214165210724\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.667150974273682 LOSS_Discriminator: 0.17097938060760498\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.370746612548828 LOSS_Discriminator: 0.18218037486076355\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.032059192657471 LOSS_Discriminator: 0.1089666485786438\n",
            "ITERATION_NO.: 28 LOSS_Generator: 4.391441822052002 LOSS_Discriminator: 0.11951050162315369\n",
            "ITERATION_NO.: 29 LOSS_Generator: 4.849127292633057 LOSS_Discriminator: 0.21612590551376343\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.154217720031738 LOSS_Discriminator: 0.08474047482013702\n",
            "ITERATION_NO.: 31 LOSS_Generator: 5.452268600463867 LOSS_Discriminator: 0.1970473825931549\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.65889835357666 LOSS_Discriminator: 0.1490410566329956\n",
            "ITERATION_NO.: 33 LOSS_Generator: 6.015152454376221 LOSS_Discriminator: 0.09783892333507538\n",
            "ITERATION_NO.: 34 LOSS_Generator: 6.087346076965332 LOSS_Discriminator: 0.19202928245067596\n",
            "ITERATION_NO.: 35 LOSS_Generator: 6.138399600982666 LOSS_Discriminator: 0.2906024754047394\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.959420680999756 LOSS_Discriminator: 0.12651678919792175\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.591517448425293 LOSS_Discriminator: 0.18521302938461304\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.198399066925049 LOSS_Discriminator: 0.1071278303861618\n",
            "ITERATION_NO.: 39 LOSS_Generator: 5.150381088256836 LOSS_Discriminator: 0.12803129851818085\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.286222457885742 LOSS_Discriminator: 0.1694703847169876\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.212186813354492 LOSS_Discriminator: 0.15447360277175903\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.9503493309021 LOSS_Discriminator: 0.18142153322696686\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.723474502563477 LOSS_Discriminator: 0.12147124111652374\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.087881565093994 LOSS_Discriminator: 0.11496064811944962\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.261470794677734 LOSS_Discriminator: 0.1445714831352234\n",
            "ITERATION_NO.: 46 LOSS_Generator: 6.198853969573975 LOSS_Discriminator: 0.08239670097827911\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.746830940246582 LOSS_Discriminator: 0.12548354268074036\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.643506050109863 LOSS_Discriminator: 0.1463123857975006\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.709226608276367 LOSS_Discriminator: 0.10552190244197845\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.266531467437744 LOSS_Discriminator: 0.19591878354549408\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.1887359619140625 LOSS_Discriminator: 0.11846265196800232\n",
            "ITERATION_NO.: 52 LOSS_Generator: 4.88759708404541 LOSS_Discriminator: 0.08803584426641464\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.773849964141846 LOSS_Discriminator: 0.1349370777606964\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.6240668296813965 LOSS_Discriminator: 0.09268592298030853\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.6676025390625 LOSS_Discriminator: 0.11569955199956894\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.760986328125 LOSS_Discriminator: 0.0558033287525177\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.725902557373047 LOSS_Discriminator: 0.1879768669605255\n",
            "ITERATION_NO.: 58 LOSS_Generator: 6.03857421875 LOSS_Discriminator: 0.06065017357468605\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.554347038269043 LOSS_Discriminator: 0.13532522320747375\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.7423858642578125 LOSS_Discriminator: 0.08225316554307938\n",
            "ITERATION_NO.: 61 LOSS_Generator: 5.396369457244873 LOSS_Discriminator: 0.2236664891242981\n",
            "ITERATION_NO.: 62 LOSS_Generator: 4.913172721862793 LOSS_Discriminator: 0.08277963101863861\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.188323974609375 LOSS_Discriminator: 0.12636759877204895\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.545663833618164 LOSS_Discriminator: 0.15243546664714813\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.536407947540283 LOSS_Discriminator: 0.21315264701843262\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.127801895141602 LOSS_Discriminator: 0.14917495846748352\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.534100532531738 LOSS_Discriminator: 0.10190749913454056\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.743767261505127 LOSS_Discriminator: 0.16144101321697235\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.670601844787598 LOSS_Discriminator: 0.18313473463058472\n",
            "ITERATION_NO.: 70 LOSS_Generator: 5.594186782836914 LOSS_Discriminator: 0.20124898850917816\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.389336109161377 LOSS_Discriminator: 0.16006222367286682\n",
            "ITERATION_NO.: 72 LOSS_Generator: 4.867859840393066 LOSS_Discriminator: 0.102987140417099\n",
            "ITERATION_NO.: 73 LOSS_Generator: 4.957173824310303 LOSS_Discriminator: 0.10464797168970108\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.4536356925964355 LOSS_Discriminator: 0.10511597990989685\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.809627532958984 LOSS_Discriminator: 0.24970728158950806\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.3929901123046875 LOSS_Discriminator: 0.10487005114555359\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.416489124298096 LOSS_Discriminator: 0.07442204654216766\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.543557167053223 LOSS_Discriminator: 0.1728520542383194\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.591032028198242 LOSS_Discriminator: 0.1229836642742157\n",
            "ITERATION_NO.: 80 LOSS_Generator: 4.880202770233154 LOSS_Discriminator: 0.2550744414329529\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.339766025543213 LOSS_Discriminator: 0.26452285051345825\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.252453804016113 LOSS_Discriminator: 0.1538475900888443\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.240525722503662 LOSS_Discriminator: 0.12580949068069458\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.08836030960083 LOSS_Discriminator: 0.11760366708040237\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.440600395202637 LOSS_Discriminator: 0.1607149839401245\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.799787998199463 LOSS_Discriminator: 0.1862463355064392\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.737436294555664 LOSS_Discriminator: 0.14566898345947266\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.909014701843262 LOSS_Discriminator: 0.11157875508069992\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.162496566772461 LOSS_Discriminator: 0.25143396854400635\n",
            "ITERATION_NO.: 90 LOSS_Generator: 6.156393527984619 LOSS_Discriminator: 0.12166033685207367\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.734695911407471 LOSS_Discriminator: 0.13289278745651245\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.4447174072265625 LOSS_Discriminator: 0.289614737033844\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.612369537353516 LOSS_Discriminator: 0.19609174132347107\n",
            "ITERATION_NO.: 94 LOSS_Generator: 6.054473876953125 LOSS_Discriminator: 0.15653468668460846\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.540812969207764 LOSS_Discriminator: 0.13948547840118408\n",
            "ITERATION_NO.: 96 LOSS_Generator: 6.063132286071777 LOSS_Discriminator: 0.1168554276227951\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.423252105712891 LOSS_Discriminator: 0.1779472529888153\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.91075325012207 LOSS_Discriminator: 0.11071161925792694\n",
            "ITERATION_NO.: 99 LOSS_Generator: 5.718077182769775 LOSS_Discriminator: 0.23263435065746307\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.438837051391602 LOSS_Discriminator: 0.141980841755867\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.056116104125977 LOSS_Discriminator: 0.13121646642684937\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.019026279449463 LOSS_Discriminator: 0.12543903291225433\n",
            "ITERATION_NO.: 103 LOSS_Generator: 4.551761627197266 LOSS_Discriminator: 0.13803072273731232\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.181295871734619 LOSS_Discriminator: 0.22414831817150116\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.453803062438965 LOSS_Discriminator: 0.18404552340507507\n",
            "ITERATION_NO.: 106 LOSS_Generator: 6.022785663604736 LOSS_Discriminator: 0.09496438503265381\n",
            "ITERATION_NO.: 107 LOSS_Generator: 6.203432559967041 LOSS_Discriminator: 0.10087651014328003\n",
            "ITERATION_NO.: 108 LOSS_Generator: 6.5798420906066895 LOSS_Discriminator: 0.12948371469974518\n",
            "ITERATION_NO.: 109 LOSS_Generator: 6.284757137298584 LOSS_Discriminator: 0.2415587306022644\n",
            "ITERATION_NO.: 110 LOSS_Generator: 6.154477119445801 LOSS_Discriminator: 0.15666286647319794\n",
            "ITERATION_NO.: 111 LOSS_Generator: 5.644205093383789 LOSS_Discriminator: 0.07879638671875\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.422586441040039 LOSS_Discriminator: 0.10843950510025024\n",
            "ITERATION_NO.: 113 LOSS_Generator: 4.835784912109375 LOSS_Discriminator: 0.09693825989961624\n",
            "ITERATION_NO.: 114 LOSS_Generator: 4.944567680358887 LOSS_Discriminator: 0.10972268879413605\n",
            "ITERATION_NO.: 115 LOSS_Generator: 4.910467147827148 LOSS_Discriminator: 0.14253316819667816\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.024162769317627 LOSS_Discriminator: 0.14505842328071594\n",
            "ITERATION_NO.: 117 LOSS_Generator: 4.887996673583984 LOSS_Discriminator: 0.12014658749103546\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.059073448181152 LOSS_Discriminator: 0.1760907918214798\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.1479878425598145 LOSS_Discriminator: 0.1429498940706253\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.413476467132568 LOSS_Discriminator: 0.09270729124546051\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.663339614868164 LOSS_Discriminator: 0.17875254154205322\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.241828441619873 LOSS_Discriminator: 0.14572830498218536\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.679279327392578 LOSS_Discriminator: 0.0632118433713913\n",
            "ITERATION_NO.: 124 LOSS_Generator: 5.910971164703369 LOSS_Discriminator: 0.16229036450386047\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.624517440795898 LOSS_Discriminator: 0.11126238107681274\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.403545379638672 LOSS_Discriminator: 0.33738768100738525\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.224405288696289 LOSS_Discriminator: 0.07527399063110352\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.02757453918457 LOSS_Discriminator: 0.12619541585445404\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.427403450012207 LOSS_Discriminator: 0.16998344659805298\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.659278392791748 LOSS_Discriminator: 0.2232891023159027\n",
            "ITERATION_NO.: 131 LOSS_Generator: 5.179715633392334 LOSS_Discriminator: 0.0746038407087326\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.581259727478027 LOSS_Discriminator: 0.07686752825975418\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.335450649261475 LOSS_Discriminator: 0.21337684988975525\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.501386642456055 LOSS_Discriminator: 0.16084438562393188\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.41855525970459 LOSS_Discriminator: 0.21971675753593445\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.337930679321289 LOSS_Discriminator: 0.1704452484846115\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.8262224197387695 LOSS_Discriminator: 0.12458789348602295\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.8397746086120605 LOSS_Discriminator: 0.18580986559391022\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.9110870361328125 LOSS_Discriminator: 0.16192814707756042\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.439644813537598 LOSS_Discriminator: 0.18128781020641327\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.767167568206787 LOSS_Discriminator: 0.10176537930965424\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.452500820159912 LOSS_Discriminator: 0.1633433699607849\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.222225189208984 LOSS_Discriminator: 0.18095988035202026\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.47882080078125 LOSS_Discriminator: 0.10906501114368439\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.369083881378174 LOSS_Discriminator: 0.20715926587581635\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.4578962326049805 LOSS_Discriminator: 0.11055827140808105\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.588323593139648 LOSS_Discriminator: 0.12251688539981842\n",
            "ITERATION_NO.: 148 LOSS_Generator: 5.481727123260498 LOSS_Discriminator: 0.178293377161026\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.9014201164245605 LOSS_Discriminator: 0.103719562292099\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.949358940124512 LOSS_Discriminator: 0.12877106666564941\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.622544765472412 LOSS_Discriminator: 0.0854094922542572\n",
            "ITERATION_NO.: 152 LOSS_Generator: 6.17697286605835 LOSS_Discriminator: 0.16871178150177002\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.8130879402160645 LOSS_Discriminator: 0.11253252625465393\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.815771102905273 LOSS_Discriminator: 0.10460199415683746\n",
            "ITERATION_NO.: 155 LOSS_Generator: 5.482794761657715 LOSS_Discriminator: 0.07277555018663406\n",
            "ITERATION_NO.: 156 LOSS_Generator: 5.701508045196533 LOSS_Discriminator: 0.10206277668476105\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.555770397186279 LOSS_Discriminator: 0.10856972634792328\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.367884635925293 LOSS_Discriminator: 0.054741159081459045\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.774550914764404 LOSS_Discriminator: 0.08416534960269928\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.727889537811279 LOSS_Discriminator: 0.06345292180776596\n",
            "ITERATION_NO.: 161 LOSS_Generator: 6.261443614959717 LOSS_Discriminator: 0.0713580995798111\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.924983501434326 LOSS_Discriminator: 0.19159887731075287\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.875477313995361 LOSS_Discriminator: 0.12672576308250427\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.687487602233887 LOSS_Discriminator: 0.09480365365743637\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.641815185546875 LOSS_Discriminator: 0.15494182705879211\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.804447174072266 LOSS_Discriminator: 0.12684431672096252\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.6490397453308105 LOSS_Discriminator: 0.09763945639133453\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.3666582107543945 LOSS_Discriminator: 0.10347513854503632\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.563989162445068 LOSS_Discriminator: 0.09162386506795883\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.688208103179932 LOSS_Discriminator: 0.14280353486537933\n",
            "ITERATION_NO.: 171 LOSS_Generator: 6.100956916809082 LOSS_Discriminator: 0.17250069975852966\n",
            "ITERATION_NO.: 172 LOSS_Generator: 6.117497444152832 LOSS_Discriminator: 0.044184036552906036\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.5314130783081055 LOSS_Discriminator: 0.14804384112358093\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.980099678039551 LOSS_Discriminator: 0.06534511595964432\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.065584659576416 LOSS_Discriminator: 0.14876335859298706\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.586658477783203 LOSS_Discriminator: 0.11500442028045654\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.249822616577148 LOSS_Discriminator: 0.1496528685092926\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.647752285003662 LOSS_Discriminator: 0.038387566804885864\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.6392502784729 LOSS_Discriminator: 0.14308470487594604\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.482538223266602 LOSS_Discriminator: 0.08444885164499283\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.955721378326416 LOSS_Discriminator: 0.14605839550495148\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.691268444061279 LOSS_Discriminator: 0.1005212739109993\n",
            "ITERATION_NO.: 183 LOSS_Generator: 6.31423807144165 LOSS_Discriminator: 0.07557716965675354\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.927910804748535 LOSS_Discriminator: 0.230032280087471\n",
            "ITERATION_NO.: 185 LOSS_Generator: 6.016726016998291 LOSS_Discriminator: 0.19184991717338562\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.84476375579834 LOSS_Discriminator: 0.09888260066509247\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.984288215637207 LOSS_Discriminator: 0.15257041156291962\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.830588340759277 LOSS_Discriminator: 0.17926087975502014\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.974027633666992 LOSS_Discriminator: 0.08891268074512482\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.414618015289307 LOSS_Discriminator: 0.15969623625278473\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.742398738861084 LOSS_Discriminator: 0.21804264187812805\n",
            "ITERATION_NO.: 192 LOSS_Generator: 5.5536675453186035 LOSS_Discriminator: 0.20741617679595947\n",
            "ITERATION_NO.: 193 LOSS_Generator: 5.5215253829956055 LOSS_Discriminator: 0.2000822126865387\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.369943618774414 LOSS_Discriminator: 0.26359280943870544\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.651797294616699 LOSS_Discriminator: 0.20116809010505676\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.754084587097168 LOSS_Discriminator: 0.2197132706642151\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.478957653045654 LOSS_Discriminator: 0.07878445088863373\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.711855888366699 LOSS_Discriminator: 0.21151134371757507\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.7054123878479 LOSS_Discriminator: 0.08667217940092087\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.578333854675293 LOSS_Discriminator: 0.3169258236885071\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.065017223358154 LOSS_Discriminator: 0.17260131239891052\n",
            "ITERATION_NO.: 202 LOSS_Generator: 4.595437049865723 LOSS_Discriminator: 0.15366396307945251\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.403650283813477 LOSS_Discriminator: 0.08975868672132492\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.529327392578125 LOSS_Discriminator: 0.17756229639053345\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.772305965423584 LOSS_Discriminator: 0.12889425456523895\n",
            "ITERATION_NO.: 206 LOSS_Generator: 6.422479629516602 LOSS_Discriminator: 0.11347030848264694\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.831607818603516 LOSS_Discriminator: 0.14436931908130646\n",
            "ITERATION_NO.: 208 LOSS_Generator: 6.127896785736084 LOSS_Discriminator: 0.17008933424949646\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.92631721496582 LOSS_Discriminator: 0.20509546995162964\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.682311534881592 LOSS_Discriminator: 0.17815664410591125\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.163485527038574 LOSS_Discriminator: 0.08416073024272919\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.426497936248779 LOSS_Discriminator: 0.15020230412483215\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.365155220031738 LOSS_Discriminator: 0.12107844650745392\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.8449177742004395 LOSS_Discriminator: 0.23865295946598053\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.887603282928467 LOSS_Discriminator: 0.1461193561553955\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.37705135345459 LOSS_Discriminator: 0.15477511286735535\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.6412224769592285 LOSS_Discriminator: 0.20364701747894287\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.429117202758789 LOSS_Discriminator: 0.058575425297021866\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.188487529754639 LOSS_Discriminator: 0.06721021980047226\n",
            "ITERATION_NO.: 220 LOSS_Generator: 6.14632511138916 LOSS_Discriminator: 0.24657675623893738\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.890406608581543 LOSS_Discriminator: 0.22186745703220367\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.697931289672852 LOSS_Discriminator: 0.29857975244522095\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.017827987670898 LOSS_Discriminator: 0.1052599847316742\n",
            "ITERATION_NO.: 224 LOSS_Generator: 4.649389743804932 LOSS_Discriminator: 0.10025911033153534\n",
            "ITERATION_NO.: 225 LOSS_Generator: 4.858773708343506 LOSS_Discriminator: 0.15396684408187866\n",
            "ITERATION_NO.: 226 LOSS_Generator: 4.548079967498779 LOSS_Discriminator: 0.22531405091285706\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.378108978271484 LOSS_Discriminator: 0.16535057127475739\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.624578952789307 LOSS_Discriminator: 0.06874962151050568\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.689849376678467 LOSS_Discriminator: 0.17937813699245453\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.861184120178223 LOSS_Discriminator: 0.20383232831954956\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.8030571937561035 LOSS_Discriminator: 0.2585446238517761\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.444880962371826 LOSS_Discriminator: 0.11242000758647919\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.729114532470703 LOSS_Discriminator: 0.19088631868362427\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.1888651847839355 LOSS_Discriminator: 0.1687593162059784\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.5670647621154785 LOSS_Discriminator: 0.24150145053863525\n",
            "ITERATION_NO.: 236 LOSS_Generator: 5.370287895202637 LOSS_Discriminator: 0.1862422525882721\n",
            "ITERATION_NO.: 237 LOSS_Generator: 5.3045878410339355 LOSS_Discriminator: 0.20173433423042297\n",
            "ITERATION_NO.: 238 LOSS_Generator: 5.241309642791748 LOSS_Discriminator: 0.15432660281658173\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.823941707611084 LOSS_Discriminator: 0.25921350717544556\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.714110851287842 LOSS_Discriminator: 0.15022151172161102\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.027372360229492 LOSS_Discriminator: 0.13747593760490417\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.32876443862915 LOSS_Discriminator: 0.16307324171066284\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.149874210357666 LOSS_Discriminator: 0.07548537850379944\n",
            "ITERATION_NO.: 244 LOSS_Generator: 6.013128280639648 LOSS_Discriminator: 0.045427706092596054\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.333282470703125 LOSS_Discriminator: 0.07418803870677948\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.968802452087402 LOSS_Discriminator: 0.11370499432086945\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.6885247230529785 LOSS_Discriminator: 0.11225080490112305\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.428024768829346 LOSS_Discriminator: 0.1451580673456192\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.113173961639404 LOSS_Discriminator: 0.10557825863361359\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.138287544250488 LOSS_Discriminator: 0.1084437370300293\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.160378932952881 LOSS_Discriminator: 0.19334650039672852\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.443937301635742 LOSS_Discriminator: 0.10901093482971191\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.834385871887207 LOSS_Discriminator: 0.05066812038421631\n",
            "ITERATION_NO.: 254 LOSS_Generator: 5.801376819610596 LOSS_Discriminator: 0.13620661199092865\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.7470879554748535 LOSS_Discriminator: 0.0870247483253479\n",
            "ITERATION_NO.: 256 LOSS_Generator: 6.73318338394165 LOSS_Discriminator: 0.20751643180847168\n",
            "ITERATION_NO.: 257 LOSS_Generator: 6.495702743530273 LOSS_Discriminator: 0.12246860563755035\n",
            "ITERATION_NO.: 258 LOSS_Generator: 6.00766658782959 LOSS_Discriminator: 0.1338585466146469\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.941925525665283 LOSS_Discriminator: 0.15614312887191772\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.775247097015381 LOSS_Discriminator: 0.19629362225532532\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.511096954345703 LOSS_Discriminator: 0.12139594554901123\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.446774482727051 LOSS_Discriminator: 0.11028806120157242\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.558281898498535 LOSS_Discriminator: 0.11585739254951477\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.2852864265441895 LOSS_Discriminator: 0.11355054378509521\n",
            "ITERATION_NO.: 265 LOSS_Generator: 5.650362968444824 LOSS_Discriminator: 0.1284393072128296\n",
            "ITERATION_NO.: 266 LOSS_Generator: 5.557460308074951 LOSS_Discriminator: 0.10436563938856125\n",
            "ITERATION_NO.: 267 LOSS_Generator: 6.219398021697998 LOSS_Discriminator: 0.12430354207754135\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.4981560707092285 LOSS_Discriminator: 0.11370895802974701\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.833995342254639 LOSS_Discriminator: 0.1609213501214981\n",
            "ITERATION_NO.: 270 LOSS_Generator: 6.060678482055664 LOSS_Discriminator: 0.15186147391796112\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.907237529754639 LOSS_Discriminator: 0.08665510267019272\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.812377452850342 LOSS_Discriminator: 0.1323014497756958\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.407533645629883 LOSS_Discriminator: 0.1585499495267868\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.477920055389404 LOSS_Discriminator: 0.16948677599430084\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.009528160095215 LOSS_Discriminator: 0.22784720361232758\n",
            "ITERATION_NO.: 276 LOSS_Generator: 4.739625453948975 LOSS_Discriminator: 0.1927255392074585\n",
            "ITERATION_NO.: 277 LOSS_Generator: 4.770212173461914 LOSS_Discriminator: 0.08878704160451889\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.0961713790893555 LOSS_Discriminator: 0.1518571674823761\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.382789134979248 LOSS_Discriminator: 0.10179759562015533\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.834278106689453 LOSS_Discriminator: 0.12309344112873077\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.274827480316162 LOSS_Discriminator: 0.29080554842948914\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.788811206817627 LOSS_Discriminator: 0.1581999957561493\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.855643272399902 LOSS_Discriminator: 0.12572328746318817\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.591348171234131 LOSS_Discriminator: 0.16888919472694397\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.627774715423584 LOSS_Discriminator: 0.09143932163715363\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.279602527618408 LOSS_Discriminator: 0.09745179116725922\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.476816177368164 LOSS_Discriminator: 0.17027467489242554\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.027070045471191 LOSS_Discriminator: 0.11217924952507019\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.254708290100098 LOSS_Discriminator: 0.3179262578487396\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.3336501121521 LOSS_Discriminator: 0.11784079670906067\n",
            "ITERATION_NO.: 291 LOSS_Generator: 4.849010467529297 LOSS_Discriminator: 0.21992716193199158\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.104520320892334 LOSS_Discriminator: 0.11419714987277985\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.118341445922852 LOSS_Discriminator: 0.19387921690940857\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.796909332275391 LOSS_Discriminator: 0.08850406855344772\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.5406389236450195 LOSS_Discriminator: 0.19037653505802155\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.646209239959717 LOSS_Discriminator: 0.04479692503809929\n",
            "ITERATION_NO.: 297 LOSS_Generator: 6.241081714630127 LOSS_Discriminator: 0.16693903505802155\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.22519063949585 LOSS_Discriminator: 0.1364002823829651\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.545995712280273 LOSS_Discriminator: 0.10574397444725037\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.472263336181641 LOSS_Discriminator: 0.1832488477230072\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.305005073547363 LOSS_Discriminator: 0.14073967933654785\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.682414531707764 LOSS_Discriminator: 0.17157308757305145\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.644351959228516 LOSS_Discriminator: 0.15028099715709686\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.721035957336426 LOSS_Discriminator: 0.06963296234607697\n",
            "ITERATION_NO.: 305 LOSS_Generator: 5.604855537414551 LOSS_Discriminator: 0.13569937646389008\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.433859825134277 LOSS_Discriminator: 0.08246294409036636\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.538823127746582 LOSS_Discriminator: 0.07213307917118073\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.551400661468506 LOSS_Discriminator: 0.06639023870229721\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.774773120880127 LOSS_Discriminator: 0.12256969511508942\n",
            "ITERATION_NO.: 310 LOSS_Generator: 5.258635997772217 LOSS_Discriminator: 0.08199213445186615\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.7971906661987305 LOSS_Discriminator: 0.10794743150472641\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.727602958679199 LOSS_Discriminator: 0.1029791384935379\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.463618278503418 LOSS_Discriminator: 0.14933714270591736\n",
            "ITERATION_NO.: 314 LOSS_Generator: 5.657026767730713 LOSS_Discriminator: 0.15454387664794922\n",
            "ITERATION_NO.: 315 LOSS_Generator: 5.3394646644592285 LOSS_Discriminator: 0.0714191198348999\n",
            "ITERATION_NO.: 316 LOSS_Generator: 5.235048294067383 LOSS_Discriminator: 0.11381004005670547\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.482190132141113 LOSS_Discriminator: 0.1303035020828247\n",
            "ITERATION_NO.: 318 LOSS_Generator: 4.9535064697265625 LOSS_Discriminator: 0.1697041392326355\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.230009078979492 LOSS_Discriminator: 0.06723934412002563\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.18126106262207 LOSS_Discriminator: 0.1111123114824295\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.481593132019043 LOSS_Discriminator: 0.15631985664367676\n",
            "ITERATION_NO.: 322 LOSS_Generator: 4.951577663421631 LOSS_Discriminator: 0.1442887783050537\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.465957164764404 LOSS_Discriminator: 0.13483181595802307\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.371175765991211 LOSS_Discriminator: 0.12696102261543274\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.326948642730713 LOSS_Discriminator: 0.06387244164943695\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.836015701293945 LOSS_Discriminator: 0.058517541736364365\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.793959140777588 LOSS_Discriminator: 0.10333028435707092\n",
            "ITERATION_NO.: 328 LOSS_Generator: 6.146737575531006 LOSS_Discriminator: 0.10915966331958771\n",
            "ITERATION_NO.: 329 LOSS_Generator: 6.001765251159668 LOSS_Discriminator: 0.1546512246131897\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.910154819488525 LOSS_Discriminator: 0.11044644564390182\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.708456993103027 LOSS_Discriminator: 0.06003187596797943\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.262851715087891 LOSS_Discriminator: 0.18623849749565125\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.183115005493164 LOSS_Discriminator: 0.12996970117092133\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.168982028961182 LOSS_Discriminator: 0.20613369345664978\n",
            "ITERATION_NO.: 335 LOSS_Generator: 4.763032913208008 LOSS_Discriminator: 0.12350523471832275\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.472885608673096 LOSS_Discriminator: 0.14019377529621124\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.651315689086914 LOSS_Discriminator: 0.1302182823419571\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.896499156951904 LOSS_Discriminator: 0.09740471839904785\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.72773551940918 LOSS_Discriminator: 0.15321752429008484\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.80626106262207 LOSS_Discriminator: 0.16905444860458374\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.891485214233398 LOSS_Discriminator: 0.08941199630498886\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.983677864074707 LOSS_Discriminator: 0.1985616683959961\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.0783491134643555 LOSS_Discriminator: 0.10907971113920212\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.422415733337402 LOSS_Discriminator: 0.11086978018283844\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.78428840637207 LOSS_Discriminator: 0.09013127535581589\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.3219804763793945 LOSS_Discriminator: 0.1438755989074707\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.332564830780029 LOSS_Discriminator: 0.2083997130393982\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.726998329162598 LOSS_Discriminator: 0.15060417354106903\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.292241096496582 LOSS_Discriminator: 0.07785382866859436\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.777263164520264 LOSS_Discriminator: 0.24348944425582886\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.697968006134033 LOSS_Discriminator: 0.14014042913913727\n",
            "ITERATION_NO.: 352 LOSS_Generator: 5.572411060333252 LOSS_Discriminator: 0.1864604502916336\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.860825061798096 LOSS_Discriminator: 0.12328538298606873\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.664971351623535 LOSS_Discriminator: 0.1421700268983841\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.909147262573242 LOSS_Discriminator: 0.13929390907287598\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.940713882446289 LOSS_Discriminator: 0.10772552341222763\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.063318729400635 LOSS_Discriminator: 0.10121849179267883\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.624237537384033 LOSS_Discriminator: 0.15439960360527039\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.72184944152832 LOSS_Discriminator: 0.18349024653434753\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.81296443939209 LOSS_Discriminator: 0.34113988280296326\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.729161262512207 LOSS_Discriminator: 0.12840306758880615\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.961958408355713 LOSS_Discriminator: 0.1359645426273346\n",
            "ITERATION_NO.: 363 LOSS_Generator: 6.042302131652832 LOSS_Discriminator: 0.23868854343891144\n",
            "ITERATION_NO.: 364 LOSS_Generator: 6.071625232696533 LOSS_Discriminator: 0.14905494451522827\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.581052780151367 LOSS_Discriminator: 0.22030219435691833\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.704687118530273 LOSS_Discriminator: 0.20210035145282745\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.609468460083008 LOSS_Discriminator: 0.20178166031837463\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.1779866218566895 LOSS_Discriminator: 0.29028478264808655\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.40103816986084 LOSS_Discriminator: 0.17771713435649872\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.2243123054504395 LOSS_Discriminator: 0.1480359435081482\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.2565460205078125 LOSS_Discriminator: 0.22560659050941467\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.438703536987305 LOSS_Discriminator: 0.16016113758087158\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.976752758026123 LOSS_Discriminator: 0.15534651279449463\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.856698513031006 LOSS_Discriminator: 0.16035756468772888\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.1851887702941895 LOSS_Discriminator: 0.028945256024599075\n",
            "ITERATION_NO.: 376 LOSS_Generator: 6.407021999359131 LOSS_Discriminator: 0.1325870305299759\n",
            "ITERATION_NO.: 377 LOSS_Generator: 6.441743850708008 LOSS_Discriminator: 0.10988038778305054\n",
            "ITERATION_NO.: 378 LOSS_Generator: 6.679140567779541 LOSS_Discriminator: 0.3225754499435425\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.810935020446777 LOSS_Discriminator: 0.0942998081445694\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.785758018493652 LOSS_Discriminator: 0.14341595768928528\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.664304256439209 LOSS_Discriminator: 0.1579637974500656\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.935670375823975 LOSS_Discriminator: 0.13398006558418274\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.231238842010498 LOSS_Discriminator: 0.22725090384483337\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.01237154006958 LOSS_Discriminator: 0.15610355138778687\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.27718448638916 LOSS_Discriminator: 0.0677952840924263\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.004900932312012 LOSS_Discriminator: 0.1257440447807312\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.898289203643799 LOSS_Discriminator: 0.1983388513326645\n",
            "ITERATION_NO.: 388 LOSS_Generator: 6.148818492889404 LOSS_Discriminator: 0.17431625723838806\n",
            "ITERATION_NO.: 389 LOSS_Generator: 6.067373752593994 LOSS_Discriminator: 0.22786836326122284\n",
            "ITERATION_NO.: 390 LOSS_Generator: 6.6061692237854 LOSS_Discriminator: 0.2629387676715851\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.360158920288086 LOSS_Discriminator: 0.24610523879528046\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.991849422454834 LOSS_Discriminator: 0.16781112551689148\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.729918003082275 LOSS_Discriminator: 0.11085028946399689\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.58730936050415 LOSS_Discriminator: 0.11192144453525543\n",
            "ITERATION_NO.: 395 LOSS_Generator: 4.948999881744385 LOSS_Discriminator: 0.11569917947053909\n",
            "ITERATION_NO.: 396 LOSS_Generator: 4.857439041137695 LOSS_Discriminator: 0.15717476606369019\n",
            "ITERATION_NO.: 397 LOSS_Generator: 4.894598484039307 LOSS_Discriminator: 0.15600907802581787\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.090269565582275 LOSS_Discriminator: 0.25211799144744873\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.421812534332275 LOSS_Discriminator: 0.13851289451122284\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.848977565765381 LOSS_Discriminator: 0.23062078654766083\n",
            "ITERATION_NO.: 401 LOSS_Generator: 6.012536525726318 LOSS_Discriminator: 0.09928735345602036\n",
            "ITERATION_NO.: 402 LOSS_Generator: 6.360112190246582 LOSS_Discriminator: 0.14697441458702087\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.807407855987549 LOSS_Discriminator: 0.06328513473272324\n",
            "ITERATION_NO.: 404 LOSS_Generator: 6.226250171661377 LOSS_Discriminator: 0.1300426721572876\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.856713771820068 LOSS_Discriminator: 0.07503916323184967\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.6076765060424805 LOSS_Discriminator: 0.15610358119010925\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.480024337768555 LOSS_Discriminator: 0.13585472106933594\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.300624370574951 LOSS_Discriminator: 0.07943163067102432\n",
            "ITERATION_NO.: 409 LOSS_Generator: 5.606873035430908 LOSS_Discriminator: 0.11089622974395752\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.513917922973633 LOSS_Discriminator: 0.12759239971637726\n",
            "ITERATION_NO.: 411 LOSS_Generator: 4.754165172576904 LOSS_Discriminator: 0.1308070421218872\n",
            "ITERATION_NO.: 412 LOSS_Generator: 4.5172200202941895 LOSS_Discriminator: 0.20080867409706116\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.104503154754639 LOSS_Discriminator: 0.20115536451339722\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.558671951293945 LOSS_Discriminator: 0.1357368528842926\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.455085277557373 LOSS_Discriminator: 0.20306667685508728\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.6136627197265625 LOSS_Discriminator: 0.1464357227087021\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.527890205383301 LOSS_Discriminator: 0.09950194507837296\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.335475444793701 LOSS_Discriminator: 0.20836125314235687\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.353415012359619 LOSS_Discriminator: 0.1190742552280426\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.121547222137451 LOSS_Discriminator: 0.1526726335287094\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.0497941970825195 LOSS_Discriminator: 0.27140456438064575\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.19532585144043 LOSS_Discriminator: 0.23160365223884583\n",
            "ITERATION_NO.: 423 LOSS_Generator: 5.466293811798096 LOSS_Discriminator: 0.17281711101531982\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.440983295440674 LOSS_Discriminator: 0.0944780707359314\n",
            "ITERATION_NO.: 425 LOSS_Generator: 5.7259345054626465 LOSS_Discriminator: 0.08086313307285309\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.976909160614014 LOSS_Discriminator: 0.15153768658638\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.357881546020508 LOSS_Discriminator: 0.19829699397087097\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.698829174041748 LOSS_Discriminator: 0.11022549867630005\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.830002307891846 LOSS_Discriminator: 0.09197695553302765\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.499518394470215 LOSS_Discriminator: 0.17603838443756104\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.591836452484131 LOSS_Discriminator: 0.11888889223337173\n",
            "ITERATION_NO.: 432 LOSS_Generator: 5.765364170074463 LOSS_Discriminator: 0.23432186245918274\n",
            "ITERATION_NO.: 433 LOSS_Generator: 4.694454193115234 LOSS_Discriminator: 0.13997668027877808\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.9356184005737305 LOSS_Discriminator: 0.1007203757762909\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.106863498687744 LOSS_Discriminator: 0.19689130783081055\n",
            "ITERATION_NO.: 436 LOSS_Generator: 4.869794845581055 LOSS_Discriminator: 0.22897198796272278\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.981959342956543 LOSS_Discriminator: 0.1703641712665558\n",
            "ITERATION_NO.: 438 LOSS_Generator: 5.833806037902832 LOSS_Discriminator: 0.09145081043243408\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.1607818603515625 LOSS_Discriminator: 0.21539872884750366\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.670102596282959 LOSS_Discriminator: 0.18450741469860077\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.198925971984863 LOSS_Discriminator: 0.2182685136795044\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.060830116271973 LOSS_Discriminator: 0.1938849240541458\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.4067463874816895 LOSS_Discriminator: 0.1994575560092926\n",
            "ITERATION_NO.: 444 LOSS_Generator: 4.911253452301025 LOSS_Discriminator: 0.183974027633667\n",
            "ITERATION_NO.: 445 LOSS_Generator: 4.852606296539307 LOSS_Discriminator: 0.07699290663003922\n",
            "ITERATION_NO.: 446 LOSS_Generator: 4.648345470428467 LOSS_Discriminator: 0.26438209414482117\n",
            "ITERATION_NO.: 447 LOSS_Generator: 4.428840160369873 LOSS_Discriminator: 0.1440395563840866\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.158816337585449 LOSS_Discriminator: 0.14240477979183197\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.4077534675598145 LOSS_Discriminator: 0.24837589263916016\n",
            "ITERATION_NO.: 450 LOSS_Generator: 6.604177474975586 LOSS_Discriminator: 0.09109076112508774\n",
            "ITERATION_NO.: 451 LOSS_Generator: 6.34158182144165 LOSS_Discriminator: 0.27862170338630676\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.378962516784668 LOSS_Discriminator: 0.1142389178276062\n",
            "ITERATION_NO.: 453 LOSS_Generator: 6.122375965118408 LOSS_Discriminator: 0.0946962907910347\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.877645492553711 LOSS_Discriminator: 0.15148311853408813\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.6790642738342285 LOSS_Discriminator: 0.22800685465335846\n",
            "ITERATION_NO.: 456 LOSS_Generator: 4.951067924499512 LOSS_Discriminator: 0.18005050718784332\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.584323883056641 LOSS_Discriminator: 0.1481601595878601\n",
            "ITERATION_NO.: 458 LOSS_Generator: 4.970198154449463 LOSS_Discriminator: 0.17683197557926178\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.022889614105225 LOSS_Discriminator: 0.08696965128183365\n",
            "ITERATION_NO.: 460 LOSS_Generator: 5.123939037322998 LOSS_Discriminator: 0.17380763590335846\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.178894519805908 LOSS_Discriminator: 0.1988925337791443\n",
            "ITERATION_NO.: 462 LOSS_Generator: 4.996577262878418 LOSS_Discriminator: 0.135994553565979\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.336299896240234 LOSS_Discriminator: 0.16727052628993988\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.313101291656494 LOSS_Discriminator: 0.24599280953407288\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.751791954040527 LOSS_Discriminator: 0.077644482254982\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.113709926605225 LOSS_Discriminator: 0.11618158221244812\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.807185649871826 LOSS_Discriminator: 0.22124305367469788\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.924708366394043 LOSS_Discriminator: 0.17436665296554565\n",
            "ITERATION_NO.: 469 LOSS_Generator: 6.356224536895752 LOSS_Discriminator: 0.06922110170125961\n",
            "ITERATION_NO.: 470 LOSS_Generator: 6.232020378112793 LOSS_Discriminator: 0.15162445604801178\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.026060104370117 LOSS_Discriminator: 0.07858465611934662\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.8278045654296875 LOSS_Discriminator: 0.13476435840129852\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.360541820526123 LOSS_Discriminator: 0.05655109882354736\n",
            "ITERATION_NO.: 474 LOSS_Generator: 5.3489484786987305 LOSS_Discriminator: 0.1455533504486084\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.130380630493164 LOSS_Discriminator: 0.14094844460487366\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.448095321655273 LOSS_Discriminator: 0.12515555322170258\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.442598819732666 LOSS_Discriminator: 0.1572105884552002\n",
            "ITERATION_NO.: 478 LOSS_Generator: 6.006648063659668 LOSS_Discriminator: 0.1493268460035324\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.49432373046875 LOSS_Discriminator: 0.19219401478767395\n",
            "ITERATION_NO.: 480 LOSS_Generator: 6.446078300476074 LOSS_Discriminator: 0.12124648690223694\n",
            "ITERATION_NO.: 481 LOSS_Generator: 7.273288726806641 LOSS_Discriminator: 0.16098269820213318\n",
            "ITERATION_NO.: 482 LOSS_Generator: 6.773512363433838 LOSS_Discriminator: 0.2247334122657776\n",
            "ITERATION_NO.: 483 LOSS_Generator: 6.462871074676514 LOSS_Discriminator: 0.09367093443870544\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.95876932144165 LOSS_Discriminator: 0.2503911256790161\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.2686767578125 LOSS_Discriminator: 0.13664841651916504\n",
            "ITERATION_NO.: 486 LOSS_Generator: 4.527842044830322 LOSS_Discriminator: 0.13626618683338165\n",
            "ITERATION_NO.: 487 LOSS_Generator: 3.966198205947876 LOSS_Discriminator: 0.16475696861743927\n",
            "ITERATION_NO.: 488 LOSS_Generator: 4.6363701820373535 LOSS_Discriminator: 0.2137528955936432\n",
            "ITERATION_NO.: 489 LOSS_Generator: 4.8505048751831055 LOSS_Discriminator: 0.15414468944072723\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.1569132804870605 LOSS_Discriminator: 0.1740681231021881\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.602817535400391 LOSS_Discriminator: 0.13945965468883514\n",
            "ITERATION_NO.: 492 LOSS_Generator: 6.087918758392334 LOSS_Discriminator: 0.12978510558605194\n",
            "ITERATION_NO.: 493 LOSS_Generator: 6.452194690704346 LOSS_Discriminator: 0.1482837200164795\n",
            "ITERATION_NO.: 494 LOSS_Generator: 6.072834014892578 LOSS_Discriminator: 0.20952123403549194\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.133900165557861 LOSS_Discriminator: 0.10088225454092026\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.600934982299805 LOSS_Discriminator: 0.16506029665470123\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.3186421394348145 LOSS_Discriminator: 0.16632816195487976\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.381382942199707 LOSS_Discriminator: 0.19247224926948547\n",
            "ITERATION_NO.: 499 LOSS_Generator: 4.616846084594727 LOSS_Discriminator: 0.15108782052993774\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.244722366333008 LOSS_Discriminator: 0.18112367391586304\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.263765335083008 LOSS_Discriminator: 0.1509714275598526\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.7940192222595215 LOSS_Discriminator: 0.17782753705978394\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.715336799621582 LOSS_Discriminator: 0.13472865521907806\n",
            "ITERATION_NO.: 504 LOSS_Generator: 6.026880264282227 LOSS_Discriminator: 0.17192135751247406\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.965630054473877 LOSS_Discriminator: 0.1808227151632309\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.9316864013671875 LOSS_Discriminator: 0.11811617016792297\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.951876163482666 LOSS_Discriminator: 0.13459640741348267\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.745362758636475 LOSS_Discriminator: 0.1844741404056549\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.485291004180908 LOSS_Discriminator: 0.11885683238506317\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.814352989196777 LOSS_Discriminator: 0.1907801330089569\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.079036712646484 LOSS_Discriminator: 0.22298313677310944\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.105349540710449 LOSS_Discriminator: 0.19133107364177704\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.621057033538818 LOSS_Discriminator: 0.1478242129087448\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.505488395690918 LOSS_Discriminator: 0.09843983501195908\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.354351043701172 LOSS_Discriminator: 0.11535020172595978\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.35334587097168 LOSS_Discriminator: 0.16097979247570038\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.352787494659424 LOSS_Discriminator: 0.18064457178115845\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.208505153656006 LOSS_Discriminator: 0.22193069756031036\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.119243621826172 LOSS_Discriminator: 0.09727243334054947\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.5698933601379395 LOSS_Discriminator: 0.17614413797855377\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.414494037628174 LOSS_Discriminator: 0.06555722653865814\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.823353290557861 LOSS_Discriminator: 0.19851495325565338\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.697932720184326 LOSS_Discriminator: 0.11934752762317657\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.4053425788879395 LOSS_Discriminator: 0.16899389028549194\n",
            "ITERATION_NO.: 525 LOSS_Generator: 6.033025741577148 LOSS_Discriminator: 0.0708150640130043\n",
            "ITERATION_NO.: 526 LOSS_Generator: 6.377373218536377 LOSS_Discriminator: 0.1279691755771637\n",
            "ITERATION_NO.: 527 LOSS_Generator: 6.388030529022217 LOSS_Discriminator: 0.1584041863679886\n",
            "ITERATION_NO.: 528 LOSS_Generator: 6.118943691253662 LOSS_Discriminator: 0.2623140513896942\n",
            "ITERATION_NO.: 529 LOSS_Generator: 5.863283634185791 LOSS_Discriminator: 0.1771504282951355\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.4591546058654785 LOSS_Discriminator: 0.2018241286277771\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.575006484985352 LOSS_Discriminator: 0.18980136513710022\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.196444511413574 LOSS_Discriminator: 0.10280750691890717\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.169256687164307 LOSS_Discriminator: 0.13924118876457214\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.328283309936523 LOSS_Discriminator: 0.24595452845096588\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.8494462966918945 LOSS_Discriminator: 0.15081056952476501\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.559420585632324 LOSS_Discriminator: 0.2026108205318451\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.222292900085449 LOSS_Discriminator: 0.1673019826412201\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.409584999084473 LOSS_Discriminator: 0.16260100901126862\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.626785755157471 LOSS_Discriminator: 0.17457953095436096\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.383111476898193 LOSS_Discriminator: 0.13990044593811035\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.570343494415283 LOSS_Discriminator: 0.08000245690345764\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.420114517211914 LOSS_Discriminator: 0.09068845212459564\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.519089221954346 LOSS_Discriminator: 0.16490350663661957\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.4280805587768555 LOSS_Discriminator: 0.12902212142944336\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.33990478515625 LOSS_Discriminator: 0.1394304633140564\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.108691215515137 LOSS_Discriminator: 0.16551834344863892\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.238906383514404 LOSS_Discriminator: 0.2693295478820801\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.547106742858887 LOSS_Discriminator: 0.15763966739177704\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.878095626831055 LOSS_Discriminator: 0.09212668240070343\n",
            "ITERATION_NO.: 550 LOSS_Generator: 5.921925067901611 LOSS_Discriminator: 0.09308172762393951\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.573087215423584 LOSS_Discriminator: 0.09731106460094452\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.215346813201904 LOSS_Discriminator: 0.12575414776802063\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.528080463409424 LOSS_Discriminator: 0.1337357759475708\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.023443222045898 LOSS_Discriminator: 0.1330062448978424\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.205893516540527 LOSS_Discriminator: 0.14090046286582947\n",
            "ITERATION_NO.: 556 LOSS_Generator: 5.202348709106445 LOSS_Discriminator: 0.1164843738079071\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.391168594360352 LOSS_Discriminator: 0.17436036467552185\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.0973310470581055 LOSS_Discriminator: 0.09530383348464966\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.2643656730651855 LOSS_Discriminator: 0.06770213693380356\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.688149452209473 LOSS_Discriminator: 0.17378170788288116\n",
            "ITERATION_NO.: 561 LOSS_Generator: 4.939262390136719 LOSS_Discriminator: 0.10114948451519012\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.3704657554626465 LOSS_Discriminator: 0.16541913151741028\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.664655685424805 LOSS_Discriminator: 0.1509634405374527\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.687820911407471 LOSS_Discriminator: 0.11680208891630173\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.5960564613342285 LOSS_Discriminator: 0.11303700506687164\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.43693733215332 LOSS_Discriminator: 0.1859838217496872\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.167386531829834 LOSS_Discriminator: 0.15971457958221436\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.348211765289307 LOSS_Discriminator: 0.14090630412101746\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.24481201171875 LOSS_Discriminator: 0.19736966490745544\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.345318794250488 LOSS_Discriminator: 0.20757326483726501\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.291174411773682 LOSS_Discriminator: 0.2296961545944214\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.4452948570251465 LOSS_Discriminator: 0.13477939367294312\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.451084136962891 LOSS_Discriminator: 0.1003553569316864\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.471484184265137 LOSS_Discriminator: 0.23582927882671356\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.2850165367126465 LOSS_Discriminator: 0.12524358928203583\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.284274101257324 LOSS_Discriminator: 0.11224087327718735\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.226996421813965 LOSS_Discriminator: 0.11680089682340622\n",
            "ITERATION_NO.: 578 LOSS_Generator: 4.84639310836792 LOSS_Discriminator: 0.10138408839702606\n",
            "ITERATION_NO.: 579 LOSS_Generator: 4.955065727233887 LOSS_Discriminator: 0.1785377413034439\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.162456512451172 LOSS_Discriminator: 0.20771187543869019\n",
            "ITERATION_NO.: 581 LOSS_Generator: 4.948025703430176 LOSS_Discriminator: 0.07954975217580795\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.356928825378418 LOSS_Discriminator: 0.16307158768177032\n",
            "ITERATION_NO.: 583 LOSS_Generator: 5.254751205444336 LOSS_Discriminator: 0.06187086179852486\n",
            "ITERATION_NO.: 584 LOSS_Generator: 5.580512523651123 LOSS_Discriminator: 0.2434171438217163\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.762456893920898 LOSS_Discriminator: 0.20313209295272827\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.505429744720459 LOSS_Discriminator: 0.1207130029797554\n",
            "ITERATION_NO.: 587 LOSS_Generator: 4.960960865020752 LOSS_Discriminator: 0.16360309720039368\n",
            "ITERATION_NO.: 588 LOSS_Generator: 4.972663879394531 LOSS_Discriminator: 0.09391336888074875\n",
            "ITERATION_NO.: 589 LOSS_Generator: 4.988325595855713 LOSS_Discriminator: 0.16545327007770538\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.319756031036377 LOSS_Discriminator: 0.0897093415260315\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.206535816192627 LOSS_Discriminator: 0.13492318987846375\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.552129745483398 LOSS_Discriminator: 0.06311332434415817\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.905807971954346 LOSS_Discriminator: 0.08847734332084656\n",
            "ITERATION_NO.: 594 LOSS_Generator: 6.128961563110352 LOSS_Discriminator: 0.15946269035339355\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.912990570068359 LOSS_Discriminator: 0.12953828275203705\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.480691909790039 LOSS_Discriminator: 0.23468303680419922\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.247578144073486 LOSS_Discriminator: 0.09899573773145676\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.0013747215271 LOSS_Discriminator: 0.08403180539608002\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.830677032470703 LOSS_Discriminator: 0.10778265446424484\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.636439800262451 LOSS_Discriminator: 0.15278297662734985\n",
            "EPOCH OVER: 7\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.319665431976318 LOSS_Discriminator: 0.1788463294506073\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.470188140869141 LOSS_Discriminator: 0.10428109765052795\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.6884236335754395 LOSS_Discriminator: 0.12216977775096893\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.161534309387207 LOSS_Discriminator: 0.1346718668937683\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.5849738121032715 LOSS_Discriminator: 0.22563177347183228\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.363065719604492 LOSS_Discriminator: 0.14883625507354736\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.5907673835754395 LOSS_Discriminator: 0.17019863426685333\n",
            "ITERATION_NO.: 8 LOSS_Generator: 5.456075668334961 LOSS_Discriminator: 0.14787474274635315\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.689123630523682 LOSS_Discriminator: 0.22326412796974182\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.487847328186035 LOSS_Discriminator: 0.16839934885501862\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.1223978996276855 LOSS_Discriminator: 0.1960991472005844\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.369339466094971 LOSS_Discriminator: 0.10151498019695282\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.4139533042907715 LOSS_Discriminator: 0.11151155829429626\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.070100784301758 LOSS_Discriminator: 0.22206968069076538\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.524186611175537 LOSS_Discriminator: 0.13782911002635956\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.696808338165283 LOSS_Discriminator: 0.11173009872436523\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.669239044189453 LOSS_Discriminator: 0.08342326432466507\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.930696487426758 LOSS_Discriminator: 0.09018559008836746\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.623213291168213 LOSS_Discriminator: 0.148567333817482\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.696508407592773 LOSS_Discriminator: 0.06384740024805069\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.655184745788574 LOSS_Discriminator: 0.1290997564792633\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.680823802947998 LOSS_Discriminator: 0.13064977526664734\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.719696044921875 LOSS_Discriminator: 0.06978194415569305\n",
            "ITERATION_NO.: 24 LOSS_Generator: 5.670497417449951 LOSS_Discriminator: 0.13356003165245056\n",
            "ITERATION_NO.: 25 LOSS_Generator: 5.359142303466797 LOSS_Discriminator: 0.12350913137197495\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.488215923309326 LOSS_Discriminator: 0.1596483439207077\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.645355224609375 LOSS_Discriminator: 0.14075998961925507\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.572745323181152 LOSS_Discriminator: 0.16281825304031372\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.5800933837890625 LOSS_Discriminator: 0.1691264808177948\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.222653865814209 LOSS_Discriminator: 0.21089878678321838\n",
            "ITERATION_NO.: 31 LOSS_Generator: 4.639485836029053 LOSS_Discriminator: 0.1385813057422638\n",
            "ITERATION_NO.: 32 LOSS_Generator: 4.5831074714660645 LOSS_Discriminator: 0.23197264969348907\n",
            "ITERATION_NO.: 33 LOSS_Generator: 5.240891933441162 LOSS_Discriminator: 0.23494821786880493\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.787778854370117 LOSS_Discriminator: 0.17950519919395447\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.662829399108887 LOSS_Discriminator: 0.18995751440525055\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.494502544403076 LOSS_Discriminator: 0.08330226689577103\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.111110210418701 LOSS_Discriminator: 0.19121836125850677\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.171030044555664 LOSS_Discriminator: 0.135997474193573\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.14511775970459 LOSS_Discriminator: 0.15791629254817963\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.277152061462402 LOSS_Discriminator: 0.15851819515228271\n",
            "ITERATION_NO.: 41 LOSS_Generator: 5.7790069580078125 LOSS_Discriminator: 0.14634180068969727\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.0445685386657715 LOSS_Discriminator: 0.2210606187582016\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.392196178436279 LOSS_Discriminator: 0.13279618322849274\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.137572765350342 LOSS_Discriminator: 0.21152877807617188\n",
            "ITERATION_NO.: 45 LOSS_Generator: 4.952322483062744 LOSS_Discriminator: 0.17453280091285706\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.736875057220459 LOSS_Discriminator: 0.18497002124786377\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.699313163757324 LOSS_Discriminator: 0.17721879482269287\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.527977466583252 LOSS_Discriminator: 0.08345985412597656\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.950892925262451 LOSS_Discriminator: 0.12158742547035217\n",
            "ITERATION_NO.: 50 LOSS_Generator: 6.455449104309082 LOSS_Discriminator: 0.16649779677391052\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.001216888427734 LOSS_Discriminator: 0.11267711222171783\n",
            "ITERATION_NO.: 52 LOSS_Generator: 5.8229827880859375 LOSS_Discriminator: 0.1328805387020111\n",
            "ITERATION_NO.: 53 LOSS_Generator: 5.858680248260498 LOSS_Discriminator: 0.19172894954681396\n",
            "ITERATION_NO.: 54 LOSS_Generator: 5.60700798034668 LOSS_Discriminator: 0.07558142393827438\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.852984428405762 LOSS_Discriminator: 0.11770686507225037\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.190810680389404 LOSS_Discriminator: 0.24353399872779846\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.065558433532715 LOSS_Discriminator: 0.19436362385749817\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.275986194610596 LOSS_Discriminator: 0.07896105200052261\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.950230121612549 LOSS_Discriminator: 0.17883436381816864\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.813432216644287 LOSS_Discriminator: 0.13213618099689484\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.159031391143799 LOSS_Discriminator: 0.1647263914346695\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.017854690551758 LOSS_Discriminator: 0.23016881942749023\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.872941493988037 LOSS_Discriminator: 0.19964881241321564\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.920759677886963 LOSS_Discriminator: 0.12941046059131622\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.690397262573242 LOSS_Discriminator: 0.07168368250131607\n",
            "ITERATION_NO.: 66 LOSS_Generator: 5.371264457702637 LOSS_Discriminator: 0.12505826354026794\n",
            "ITERATION_NO.: 67 LOSS_Generator: 5.546677112579346 LOSS_Discriminator: 0.1094132512807846\n",
            "ITERATION_NO.: 68 LOSS_Generator: 5.0489068031311035 LOSS_Discriminator: 0.21352139115333557\n",
            "ITERATION_NO.: 69 LOSS_Generator: 5.276684761047363 LOSS_Discriminator: 0.06890813261270523\n",
            "ITERATION_NO.: 70 LOSS_Generator: 4.80680513381958 LOSS_Discriminator: 0.16262605786323547\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.2856316566467285 LOSS_Discriminator: 0.15303131937980652\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.183063507080078 LOSS_Discriminator: 0.1262245625257492\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.765415191650391 LOSS_Discriminator: 0.1212405115365982\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.693112850189209 LOSS_Discriminator: 0.10819761455059052\n",
            "ITERATION_NO.: 75 LOSS_Generator: 6.240047454833984 LOSS_Discriminator: 0.17090356349945068\n",
            "ITERATION_NO.: 76 LOSS_Generator: 6.121712684631348 LOSS_Discriminator: 0.11091884970664978\n",
            "ITERATION_NO.: 77 LOSS_Generator: 6.431654453277588 LOSS_Discriminator: 0.17762771248817444\n",
            "ITERATION_NO.: 78 LOSS_Generator: 6.1084747314453125 LOSS_Discriminator: 0.10559460520744324\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.8436150550842285 LOSS_Discriminator: 0.20859745144844055\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.5775980949401855 LOSS_Discriminator: 0.18044620752334595\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.373370170593262 LOSS_Discriminator: 0.2629256844520569\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.589976787567139 LOSS_Discriminator: 0.15489144623279572\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.670356273651123 LOSS_Discriminator: 0.13004589080810547\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.340185165405273 LOSS_Discriminator: 0.11034324765205383\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.715185165405273 LOSS_Discriminator: 0.12441321462392807\n",
            "ITERATION_NO.: 86 LOSS_Generator: 6.188526153564453 LOSS_Discriminator: 0.1147749125957489\n",
            "ITERATION_NO.: 87 LOSS_Generator: 6.113956928253174 LOSS_Discriminator: 0.046880386769771576\n",
            "ITERATION_NO.: 88 LOSS_Generator: 6.0822906494140625 LOSS_Discriminator: 0.13919103145599365\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.142068386077881 LOSS_Discriminator: 0.17751798033714294\n",
            "ITERATION_NO.: 90 LOSS_Generator: 6.021468639373779 LOSS_Discriminator: 0.11696004122495651\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.308600425720215 LOSS_Discriminator: 0.1635608673095703\n",
            "ITERATION_NO.: 92 LOSS_Generator: 4.3966546058654785 LOSS_Discriminator: 0.2195393592119217\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.037326812744141 LOSS_Discriminator: 0.13318294286727905\n",
            "ITERATION_NO.: 94 LOSS_Generator: 4.572292804718018 LOSS_Discriminator: 0.1517399698495865\n",
            "ITERATION_NO.: 95 LOSS_Generator: 4.634469509124756 LOSS_Discriminator: 0.24336926639080048\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.223538398742676 LOSS_Discriminator: 0.22540557384490967\n",
            "ITERATION_NO.: 97 LOSS_Generator: 6.0885138511657715 LOSS_Discriminator: 0.14802852272987366\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.916416168212891 LOSS_Discriminator: 0.19751307368278503\n",
            "ITERATION_NO.: 99 LOSS_Generator: 6.201277256011963 LOSS_Discriminator: 0.19569522142410278\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.303701400756836 LOSS_Discriminator: 0.20058052241802216\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.704349994659424 LOSS_Discriminator: 0.16649731993675232\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.632149696350098 LOSS_Discriminator: 0.08800867199897766\n",
            "ITERATION_NO.: 103 LOSS_Generator: 4.850438594818115 LOSS_Discriminator: 0.11808312684297562\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.10216760635376 LOSS_Discriminator: 0.15639454126358032\n",
            "ITERATION_NO.: 105 LOSS_Generator: 4.895906925201416 LOSS_Discriminator: 0.13047002255916595\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.033927917480469 LOSS_Discriminator: 0.06655870378017426\n",
            "ITERATION_NO.: 107 LOSS_Generator: 5.351738452911377 LOSS_Discriminator: 0.10164622962474823\n",
            "ITERATION_NO.: 108 LOSS_Generator: 5.313007354736328 LOSS_Discriminator: 0.1452968716621399\n",
            "ITERATION_NO.: 109 LOSS_Generator: 6.14364767074585 LOSS_Discriminator: 0.10613346099853516\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.833209037780762 LOSS_Discriminator: 0.1462370902299881\n",
            "ITERATION_NO.: 111 LOSS_Generator: 6.846085071563721 LOSS_Discriminator: 0.08394934982061386\n",
            "ITERATION_NO.: 112 LOSS_Generator: 6.9553985595703125 LOSS_Discriminator: 0.24376611411571503\n",
            "ITERATION_NO.: 113 LOSS_Generator: 6.836658954620361 LOSS_Discriminator: 0.12126173079013824\n",
            "ITERATION_NO.: 114 LOSS_Generator: 6.136557102203369 LOSS_Discriminator: 0.25064706802368164\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.781341552734375 LOSS_Discriminator: 0.2322576642036438\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.4427714347839355 LOSS_Discriminator: 0.10673612356185913\n",
            "ITERATION_NO.: 117 LOSS_Generator: 4.868801116943359 LOSS_Discriminator: 0.13654804229736328\n",
            "ITERATION_NO.: 118 LOSS_Generator: 4.729079723358154 LOSS_Discriminator: 0.15814748406410217\n",
            "ITERATION_NO.: 119 LOSS_Generator: 5.5348968505859375 LOSS_Discriminator: 0.2697488069534302\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.493897914886475 LOSS_Discriminator: 0.11864733695983887\n",
            "ITERATION_NO.: 121 LOSS_Generator: 6.040349960327148 LOSS_Discriminator: 0.207574725151062\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.799627304077148 LOSS_Discriminator: 0.13754448294639587\n",
            "ITERATION_NO.: 123 LOSS_Generator: 6.239046096801758 LOSS_Discriminator: 0.11363617330789566\n",
            "ITERATION_NO.: 124 LOSS_Generator: 6.398238658905029 LOSS_Discriminator: 0.16836802661418915\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.23587703704834 LOSS_Discriminator: 0.24288123846054077\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.811382293701172 LOSS_Discriminator: 0.1269635111093521\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.201582431793213 LOSS_Discriminator: 0.14714771509170532\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.401528358459473 LOSS_Discriminator: 0.28184574842453003\n",
            "ITERATION_NO.: 129 LOSS_Generator: 4.776485443115234 LOSS_Discriminator: 0.16903343796730042\n",
            "ITERATION_NO.: 130 LOSS_Generator: 4.592357635498047 LOSS_Discriminator: 0.18323339521884918\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.289140224456787 LOSS_Discriminator: 0.16876451671123505\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.063851833343506 LOSS_Discriminator: 0.23985566198825836\n",
            "ITERATION_NO.: 133 LOSS_Generator: 5.746935844421387 LOSS_Discriminator: 0.18828406929969788\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.717838287353516 LOSS_Discriminator: 0.18821501731872559\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.921512603759766 LOSS_Discriminator: 0.142526775598526\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.704401969909668 LOSS_Discriminator: 0.19346138834953308\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.667628765106201 LOSS_Discriminator: 0.23258045315742493\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.356717586517334 LOSS_Discriminator: 0.12147578597068787\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.639519214630127 LOSS_Discriminator: 0.09671352803707123\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.349437236785889 LOSS_Discriminator: 0.14426741003990173\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.5417680740356445 LOSS_Discriminator: 0.175071120262146\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.283705234527588 LOSS_Discriminator: 0.18785613775253296\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.237049579620361 LOSS_Discriminator: 0.19178706407546997\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.258108139038086 LOSS_Discriminator: 0.24747520685195923\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.349146842956543 LOSS_Discriminator: 0.15490558743476868\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.64179801940918 LOSS_Discriminator: 0.10664784163236618\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.9433512687683105 LOSS_Discriminator: 0.1462462842464447\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.3015828132629395 LOSS_Discriminator: 0.10971473157405853\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.084808349609375 LOSS_Discriminator: 0.11003652215003967\n",
            "ITERATION_NO.: 150 LOSS_Generator: 6.158663272857666 LOSS_Discriminator: 0.22839874029159546\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.967686653137207 LOSS_Discriminator: 0.133330836892128\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.1987690925598145 LOSS_Discriminator: 0.29308825731277466\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.402126312255859 LOSS_Discriminator: 0.06948291510343552\n",
            "ITERATION_NO.: 154 LOSS_Generator: 4.518906116485596 LOSS_Discriminator: 0.14429326355457306\n",
            "ITERATION_NO.: 155 LOSS_Generator: 4.5082244873046875 LOSS_Discriminator: 0.15104083716869354\n",
            "ITERATION_NO.: 156 LOSS_Generator: 4.8912200927734375 LOSS_Discriminator: 0.1411600261926651\n",
            "ITERATION_NO.: 157 LOSS_Generator: 4.913794040679932 LOSS_Discriminator: 0.18215298652648926\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.238712787628174 LOSS_Discriminator: 0.11220851540565491\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.536653518676758 LOSS_Discriminator: 0.270092248916626\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.621148586273193 LOSS_Discriminator: 0.23112225532531738\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.20273494720459 LOSS_Discriminator: 0.08611881732940674\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.501187324523926 LOSS_Discriminator: 0.1258011907339096\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.316643714904785 LOSS_Discriminator: 0.06663940846920013\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.456701278686523 LOSS_Discriminator: 0.15495914220809937\n",
            "ITERATION_NO.: 165 LOSS_Generator: 5.439039707183838 LOSS_Discriminator: 0.15441244840621948\n",
            "ITERATION_NO.: 166 LOSS_Generator: 5.36605978012085 LOSS_Discriminator: 0.10394567996263504\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.505645275115967 LOSS_Discriminator: 0.09600009024143219\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.347372055053711 LOSS_Discriminator: 0.14051184058189392\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.0752458572387695 LOSS_Discriminator: 0.1187795028090477\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.699569225311279 LOSS_Discriminator: 0.12651073932647705\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.057128429412842 LOSS_Discriminator: 0.08689553290605545\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.4826202392578125 LOSS_Discriminator: 0.15727323293685913\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.065122604370117 LOSS_Discriminator: 0.23639395833015442\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.070573806762695 LOSS_Discriminator: 0.10101614892482758\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.307083606719971 LOSS_Discriminator: 0.14993953704833984\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.079037666320801 LOSS_Discriminator: 0.16207081079483032\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.401893138885498 LOSS_Discriminator: 0.08611281961202621\n",
            "ITERATION_NO.: 178 LOSS_Generator: 6.07158088684082 LOSS_Discriminator: 0.09923234581947327\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.998522758483887 LOSS_Discriminator: 0.11766619235277176\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.971146106719971 LOSS_Discriminator: 0.1061951145529747\n",
            "ITERATION_NO.: 181 LOSS_Generator: 6.0964250564575195 LOSS_Discriminator: 0.11182631552219391\n",
            "ITERATION_NO.: 182 LOSS_Generator: 5.695217132568359 LOSS_Discriminator: 0.1993439793586731\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.397039413452148 LOSS_Discriminator: 0.12448292970657349\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.223521709442139 LOSS_Discriminator: 0.11884205043315887\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.156467437744141 LOSS_Discriminator: 0.17619982361793518\n",
            "ITERATION_NO.: 186 LOSS_Generator: 4.611409664154053 LOSS_Discriminator: 0.17790183424949646\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.4489240646362305 LOSS_Discriminator: 0.13110101222991943\n",
            "ITERATION_NO.: 188 LOSS_Generator: 5.123131275177002 LOSS_Discriminator: 0.14808757603168488\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.415576934814453 LOSS_Discriminator: 0.2278149425983429\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.267526626586914 LOSS_Discriminator: 0.1892363727092743\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.21188497543335 LOSS_Discriminator: 0.08024049550294876\n",
            "ITERATION_NO.: 192 LOSS_Generator: 5.4565911293029785 LOSS_Discriminator: 0.1842486560344696\n",
            "ITERATION_NO.: 193 LOSS_Generator: 4.883723735809326 LOSS_Discriminator: 0.13523933291435242\n",
            "ITERATION_NO.: 194 LOSS_Generator: 5.509984016418457 LOSS_Discriminator: 0.16561661660671234\n",
            "ITERATION_NO.: 195 LOSS_Generator: 5.242288112640381 LOSS_Discriminator: 0.13813763856887817\n",
            "ITERATION_NO.: 196 LOSS_Generator: 5.273457050323486 LOSS_Discriminator: 0.13982398808002472\n",
            "ITERATION_NO.: 197 LOSS_Generator: 5.586322784423828 LOSS_Discriminator: 0.18738672137260437\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.644715785980225 LOSS_Discriminator: 0.1981114149093628\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.955150604248047 LOSS_Discriminator: 0.0955115258693695\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.5004048347473145 LOSS_Discriminator: 0.20582696795463562\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.363680362701416 LOSS_Discriminator: 0.11656947433948517\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.367326736450195 LOSS_Discriminator: 0.11211299151182175\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.276548862457275 LOSS_Discriminator: 0.14288705587387085\n",
            "ITERATION_NO.: 204 LOSS_Generator: 4.954938888549805 LOSS_Discriminator: 0.0719563290476799\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.407827854156494 LOSS_Discriminator: 0.09983843564987183\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.303129196166992 LOSS_Discriminator: 0.18602553009986877\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.262148380279541 LOSS_Discriminator: 0.10002651810646057\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.872658729553223 LOSS_Discriminator: 0.19106361269950867\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.795400619506836 LOSS_Discriminator: 0.1358257234096527\n",
            "ITERATION_NO.: 210 LOSS_Generator: 6.044684410095215 LOSS_Discriminator: 0.12769244611263275\n",
            "ITERATION_NO.: 211 LOSS_Generator: 6.068743705749512 LOSS_Discriminator: 0.13765622675418854\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.965451717376709 LOSS_Discriminator: 0.1935930848121643\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.878551483154297 LOSS_Discriminator: 0.19566303491592407\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.230993270874023 LOSS_Discriminator: 0.20337200164794922\n",
            "ITERATION_NO.: 215 LOSS_Generator: 4.463893413543701 LOSS_Discriminator: 0.15235188603401184\n",
            "ITERATION_NO.: 216 LOSS_Generator: 5.063572883605957 LOSS_Discriminator: 0.15352946519851685\n",
            "ITERATION_NO.: 217 LOSS_Generator: 4.280374526977539 LOSS_Discriminator: 0.19410227239131927\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.3149895668029785 LOSS_Discriminator: 0.27570390701293945\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.697608470916748 LOSS_Discriminator: 0.11812573671340942\n",
            "ITERATION_NO.: 220 LOSS_Generator: 6.288090229034424 LOSS_Discriminator: 0.16699020564556122\n",
            "ITERATION_NO.: 221 LOSS_Generator: 6.445155143737793 LOSS_Discriminator: 0.22131747007369995\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.906318187713623 LOSS_Discriminator: 0.19110774993896484\n",
            "ITERATION_NO.: 223 LOSS_Generator: 6.512818813323975 LOSS_Discriminator: 0.17553161084651947\n",
            "ITERATION_NO.: 224 LOSS_Generator: 6.3272271156311035 LOSS_Discriminator: 0.2300378680229187\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.664993762969971 LOSS_Discriminator: 0.16433730721473694\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.402318000793457 LOSS_Discriminator: 0.15510796010494232\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.193235397338867 LOSS_Discriminator: 0.09025351703166962\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.15892219543457 LOSS_Discriminator: 0.11566782742738724\n",
            "ITERATION_NO.: 229 LOSS_Generator: 4.553587436676025 LOSS_Discriminator: 0.19810859858989716\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.1969709396362305 LOSS_Discriminator: 0.19096754491329193\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.255350112915039 LOSS_Discriminator: 0.24041002988815308\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.34114933013916 LOSS_Discriminator: 0.10449372231960297\n",
            "ITERATION_NO.: 233 LOSS_Generator: 6.127618312835693 LOSS_Discriminator: 0.18152017891407013\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.901033878326416 LOSS_Discriminator: 0.15487685799598694\n",
            "ITERATION_NO.: 235 LOSS_Generator: 6.109856605529785 LOSS_Discriminator: 0.1546781212091446\n",
            "ITERATION_NO.: 236 LOSS_Generator: 5.7076263427734375 LOSS_Discriminator: 0.11714696139097214\n",
            "ITERATION_NO.: 237 LOSS_Generator: 5.537628173828125 LOSS_Discriminator: 0.15909221768379211\n",
            "ITERATION_NO.: 238 LOSS_Generator: 5.4717698097229 LOSS_Discriminator: 0.17239946126937866\n",
            "ITERATION_NO.: 239 LOSS_Generator: 4.972311496734619 LOSS_Discriminator: 0.10422483086585999\n",
            "ITERATION_NO.: 240 LOSS_Generator: 4.840401649475098 LOSS_Discriminator: 0.1705126166343689\n",
            "ITERATION_NO.: 241 LOSS_Generator: 5.288853645324707 LOSS_Discriminator: 0.08445419371128082\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.423964023590088 LOSS_Discriminator: 0.10271298140287399\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.523836135864258 LOSS_Discriminator: 0.15755319595336914\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.969257831573486 LOSS_Discriminator: 0.21872076392173767\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.108212471008301 LOSS_Discriminator: 0.14111943542957306\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.811761379241943 LOSS_Discriminator: 0.17898043990135193\n",
            "ITERATION_NO.: 247 LOSS_Generator: 6.045091152191162 LOSS_Discriminator: 0.0748605728149414\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.889997959136963 LOSS_Discriminator: 0.19022125005722046\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.69193172454834 LOSS_Discriminator: 0.0958530604839325\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.283498764038086 LOSS_Discriminator: 0.16553658246994019\n",
            "ITERATION_NO.: 251 LOSS_Generator: 5.2389421463012695 LOSS_Discriminator: 0.09789913892745972\n",
            "ITERATION_NO.: 252 LOSS_Generator: 5.389643669128418 LOSS_Discriminator: 0.164858877658844\n",
            "ITERATION_NO.: 253 LOSS_Generator: 5.2442402839660645 LOSS_Discriminator: 0.11078636348247528\n",
            "ITERATION_NO.: 254 LOSS_Generator: 4.895483016967773 LOSS_Discriminator: 0.1392841637134552\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.133720874786377 LOSS_Discriminator: 0.17233221232891083\n",
            "ITERATION_NO.: 256 LOSS_Generator: 5.546970844268799 LOSS_Discriminator: 0.15695235133171082\n",
            "ITERATION_NO.: 257 LOSS_Generator: 5.335117340087891 LOSS_Discriminator: 0.16605284810066223\n",
            "ITERATION_NO.: 258 LOSS_Generator: 6.3881402015686035 LOSS_Discriminator: 0.09339584410190582\n",
            "ITERATION_NO.: 259 LOSS_Generator: 5.539623260498047 LOSS_Discriminator: 0.2662864625453949\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.527175426483154 LOSS_Discriminator: 0.22985625267028809\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.005379676818848 LOSS_Discriminator: 0.14143118262290955\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.221386909484863 LOSS_Discriminator: 0.06220974773168564\n",
            "ITERATION_NO.: 263 LOSS_Generator: 5.037187099456787 LOSS_Discriminator: 0.06172729656100273\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.0955610275268555 LOSS_Discriminator: 0.09924750030040741\n",
            "ITERATION_NO.: 265 LOSS_Generator: 4.836559772491455 LOSS_Discriminator: 0.17472632229328156\n",
            "ITERATION_NO.: 266 LOSS_Generator: 4.967334747314453 LOSS_Discriminator: 0.09758265316486359\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.164167404174805 LOSS_Discriminator: 0.10357632488012314\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.087835311889648 LOSS_Discriminator: 0.11797282099723816\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.783005237579346 LOSS_Discriminator: 0.09656757116317749\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.768373012542725 LOSS_Discriminator: 0.14921218156814575\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.691761016845703 LOSS_Discriminator: 0.21745678782463074\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.783530235290527 LOSS_Discriminator: 0.1899232566356659\n",
            "ITERATION_NO.: 273 LOSS_Generator: 6.014947891235352 LOSS_Discriminator: 0.17328296601772308\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.583149433135986 LOSS_Discriminator: 0.10334612429141998\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.4285478591918945 LOSS_Discriminator: 0.15217922627925873\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.034967422485352 LOSS_Discriminator: 0.20407573878765106\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.1851487159729 LOSS_Discriminator: 0.12976618111133575\n",
            "ITERATION_NO.: 278 LOSS_Generator: 4.92822265625 LOSS_Discriminator: 0.1780301183462143\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.291445255279541 LOSS_Discriminator: 0.1886434555053711\n",
            "ITERATION_NO.: 280 LOSS_Generator: 5.207670211791992 LOSS_Discriminator: 0.13108856976032257\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.017834186553955 LOSS_Discriminator: 0.14414486289024353\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.801880359649658 LOSS_Discriminator: 0.1382167637348175\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.7800445556640625 LOSS_Discriminator: 0.14625607430934906\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.495307445526123 LOSS_Discriminator: 0.10817845165729523\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.515203952789307 LOSS_Discriminator: 0.2350090742111206\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.13265323638916 LOSS_Discriminator: 0.22143587470054626\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.358487606048584 LOSS_Discriminator: 0.14278048276901245\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.448171615600586 LOSS_Discriminator: 0.09526969492435455\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.478937149047852 LOSS_Discriminator: 0.15632548928260803\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.499368667602539 LOSS_Discriminator: 0.05503653734922409\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.992265701293945 LOSS_Discriminator: 0.09546458721160889\n",
            "ITERATION_NO.: 292 LOSS_Generator: 6.040254592895508 LOSS_Discriminator: 0.1721002459526062\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.5966997146606445 LOSS_Discriminator: 0.173115074634552\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.4912943840026855 LOSS_Discriminator: 0.15768520534038544\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.108762741088867 LOSS_Discriminator: 0.20217086374759674\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.215976715087891 LOSS_Discriminator: 0.12569308280944824\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.283610820770264 LOSS_Discriminator: 0.122046560049057\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.141162872314453 LOSS_Discriminator: 0.1288556456565857\n",
            "ITERATION_NO.: 299 LOSS_Generator: 5.560491561889648 LOSS_Discriminator: 0.10604072362184525\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.693248748779297 LOSS_Discriminator: 0.20154395699501038\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.302730083465576 LOSS_Discriminator: 0.12311805784702301\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.222866058349609 LOSS_Discriminator: 0.10789782553911209\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.3162665367126465 LOSS_Discriminator: 0.06327124685049057\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.237290859222412 LOSS_Discriminator: 0.09591186046600342\n",
            "ITERATION_NO.: 305 LOSS_Generator: 5.480271816253662 LOSS_Discriminator: 0.13108894228935242\n",
            "ITERATION_NO.: 306 LOSS_Generator: 5.212278842926025 LOSS_Discriminator: 0.17853951454162598\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.1778669357299805 LOSS_Discriminator: 0.23041623830795288\n",
            "ITERATION_NO.: 308 LOSS_Generator: 5.573315620422363 LOSS_Discriminator: 0.1760338842868805\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.625136852264404 LOSS_Discriminator: 0.11037753522396088\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.352171421051025 LOSS_Discriminator: 0.10627806186676025\n",
            "ITERATION_NO.: 311 LOSS_Generator: 6.353747367858887 LOSS_Discriminator: 0.1952289193868637\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.74008846282959 LOSS_Discriminator: 0.2534668743610382\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.813581466674805 LOSS_Discriminator: 0.12343477457761765\n",
            "ITERATION_NO.: 314 LOSS_Generator: 4.7679524421691895 LOSS_Discriminator: 0.09757589548826218\n",
            "ITERATION_NO.: 315 LOSS_Generator: 5.2485809326171875 LOSS_Discriminator: 0.14582352340221405\n",
            "ITERATION_NO.: 316 LOSS_Generator: 4.837011337280273 LOSS_Discriminator: 0.18097908794879913\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.154107093811035 LOSS_Discriminator: 0.17094576358795166\n",
            "ITERATION_NO.: 318 LOSS_Generator: 4.954023361206055 LOSS_Discriminator: 0.2835651636123657\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.734312534332275 LOSS_Discriminator: 0.24484530091285706\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.846207141876221 LOSS_Discriminator: 0.17683115601539612\n",
            "ITERATION_NO.: 321 LOSS_Generator: 6.367682933807373 LOSS_Discriminator: 0.12113958597183228\n",
            "ITERATION_NO.: 322 LOSS_Generator: 6.5259246826171875 LOSS_Discriminator: 0.21789374947547913\n",
            "ITERATION_NO.: 323 LOSS_Generator: 6.033905506134033 LOSS_Discriminator: 0.07299502193927765\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.869790077209473 LOSS_Discriminator: 0.16146978735923767\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.897774696350098 LOSS_Discriminator: 0.11905980855226517\n",
            "ITERATION_NO.: 326 LOSS_Generator: 5.819911956787109 LOSS_Discriminator: 0.21129505336284637\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.712777137756348 LOSS_Discriminator: 0.08559516817331314\n",
            "ITERATION_NO.: 328 LOSS_Generator: 6.027581214904785 LOSS_Discriminator: 0.10936276614665985\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.8993401527404785 LOSS_Discriminator: 0.17693868279457092\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.494598865509033 LOSS_Discriminator: 0.19384340941905975\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.865267276763916 LOSS_Discriminator: 0.1297367662191391\n",
            "ITERATION_NO.: 332 LOSS_Generator: 6.003253936767578 LOSS_Discriminator: 0.14633771777153015\n",
            "ITERATION_NO.: 333 LOSS_Generator: 6.104272365570068 LOSS_Discriminator: 0.13686086237430573\n",
            "ITERATION_NO.: 334 LOSS_Generator: 6.2556023597717285 LOSS_Discriminator: 0.11183113604784012\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.799423694610596 LOSS_Discriminator: 0.1772639900445938\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.98345947265625 LOSS_Discriminator: 0.14871253073215485\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.366064071655273 LOSS_Discriminator: 0.16507059335708618\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.239190101623535 LOSS_Discriminator: 0.17297795414924622\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.088733673095703 LOSS_Discriminator: 0.13963155448436737\n",
            "ITERATION_NO.: 340 LOSS_Generator: 4.846843719482422 LOSS_Discriminator: 0.18297213315963745\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.110775947570801 LOSS_Discriminator: 0.13794264197349548\n",
            "ITERATION_NO.: 342 LOSS_Generator: 4.700857639312744 LOSS_Discriminator: 0.1080317497253418\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.404160976409912 LOSS_Discriminator: 0.10605454444885254\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.504536151885986 LOSS_Discriminator: 0.22084078192710876\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.504486083984375 LOSS_Discriminator: 0.2958218455314636\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.677367687225342 LOSS_Discriminator: 0.09357878565788269\n",
            "ITERATION_NO.: 347 LOSS_Generator: 6.146509170532227 LOSS_Discriminator: 0.08722761273384094\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.780904769897461 LOSS_Discriminator: 0.2671615183353424\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.382076263427734 LOSS_Discriminator: 0.23266959190368652\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.46204948425293 LOSS_Discriminator: 0.1823209971189499\n",
            "ITERATION_NO.: 351 LOSS_Generator: 5.029192924499512 LOSS_Discriminator: 0.15710364282131195\n",
            "ITERATION_NO.: 352 LOSS_Generator: 4.66035795211792 LOSS_Discriminator: 0.14198967814445496\n",
            "ITERATION_NO.: 353 LOSS_Generator: 4.506607532501221 LOSS_Discriminator: 0.17448866367340088\n",
            "ITERATION_NO.: 354 LOSS_Generator: 4.630721569061279 LOSS_Discriminator: 0.1476745754480362\n",
            "ITERATION_NO.: 355 LOSS_Generator: 5.128993034362793 LOSS_Discriminator: 0.25962555408477783\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.053389072418213 LOSS_Discriminator: 0.1636052131652832\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.172398567199707 LOSS_Discriminator: 0.15466316044330597\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.3869853019714355 LOSS_Discriminator: 0.10204961895942688\n",
            "ITERATION_NO.: 359 LOSS_Generator: 5.763463020324707 LOSS_Discriminator: 0.15822765231132507\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.7272114753723145 LOSS_Discriminator: 0.16057422757148743\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.434998035430908 LOSS_Discriminator: 0.1617431640625\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.4921441078186035 LOSS_Discriminator: 0.17873555421829224\n",
            "ITERATION_NO.: 363 LOSS_Generator: 5.173418045043945 LOSS_Discriminator: 0.1570618599653244\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.435552597045898 LOSS_Discriminator: 0.17980575561523438\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.422407627105713 LOSS_Discriminator: 0.07400467246770859\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.635020732879639 LOSS_Discriminator: 0.14366194605827332\n",
            "ITERATION_NO.: 367 LOSS_Generator: 5.475051403045654 LOSS_Discriminator: 0.175787091255188\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.69444465637207 LOSS_Discriminator: 0.0572415329515934\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.830615043640137 LOSS_Discriminator: 0.21661561727523804\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.611201286315918 LOSS_Discriminator: 0.2920674681663513\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.568159103393555 LOSS_Discriminator: 0.18568581342697144\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.513391494750977 LOSS_Discriminator: 0.19262759387493134\n",
            "ITERATION_NO.: 373 LOSS_Generator: 4.776271820068359 LOSS_Discriminator: 0.18394142389297485\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.052374362945557 LOSS_Discriminator: 0.2282363474369049\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.05333137512207 LOSS_Discriminator: 0.08990281820297241\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.239616870880127 LOSS_Discriminator: 0.20412302017211914\n",
            "ITERATION_NO.: 377 LOSS_Generator: 4.660715103149414 LOSS_Discriminator: 0.1495732069015503\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.343682289123535 LOSS_Discriminator: 0.16316358745098114\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.724503040313721 LOSS_Discriminator: 0.1729067862033844\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.8050103187561035 LOSS_Discriminator: 0.18377986550331116\n",
            "ITERATION_NO.: 381 LOSS_Generator: 6.1373443603515625 LOSS_Discriminator: 0.20321866869926453\n",
            "ITERATION_NO.: 382 LOSS_Generator: 6.2738165855407715 LOSS_Discriminator: 0.18937531113624573\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.151553153991699 LOSS_Discriminator: 0.13408450782299042\n",
            "ITERATION_NO.: 384 LOSS_Generator: 6.3842668533325195 LOSS_Discriminator: 0.1457064002752304\n",
            "ITERATION_NO.: 385 LOSS_Generator: 6.397826671600342 LOSS_Discriminator: 0.21089795231819153\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.888838291168213 LOSS_Discriminator: 0.1436401605606079\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.825476169586182 LOSS_Discriminator: 0.19467677175998688\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.290256977081299 LOSS_Discriminator: 0.184291273355484\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.385184288024902 LOSS_Discriminator: 0.13224217295646667\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.374121189117432 LOSS_Discriminator: 0.1381581425666809\n",
            "ITERATION_NO.: 391 LOSS_Generator: 5.362900257110596 LOSS_Discriminator: 0.17388686537742615\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.342422008514404 LOSS_Discriminator: 0.1639273762702942\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.346551895141602 LOSS_Discriminator: 0.07270614802837372\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.373369216918945 LOSS_Discriminator: 0.10471855103969574\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.6045451164245605 LOSS_Discriminator: 0.17219729721546173\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.029260158538818 LOSS_Discriminator: 0.17333896458148956\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.655086517333984 LOSS_Discriminator: 0.1523130089044571\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.583972930908203 LOSS_Discriminator: 0.12846019864082336\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.78530216217041 LOSS_Discriminator: 0.23992159962654114\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.188368320465088 LOSS_Discriminator: 0.1884925812482834\n",
            "ITERATION_NO.: 401 LOSS_Generator: 4.901064395904541 LOSS_Discriminator: 0.17315974831581116\n",
            "ITERATION_NO.: 402 LOSS_Generator: 4.712558269500732 LOSS_Discriminator: 0.18019521236419678\n",
            "ITERATION_NO.: 403 LOSS_Generator: 5.0573930740356445 LOSS_Discriminator: 0.17580069601535797\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.366156101226807 LOSS_Discriminator: 0.14541760087013245\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.4947004318237305 LOSS_Discriminator: 0.1858154684305191\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.0974860191345215 LOSS_Discriminator: 0.12550976872444153\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.747734546661377 LOSS_Discriminator: 0.17754718661308289\n",
            "ITERATION_NO.: 408 LOSS_Generator: 6.330554008483887 LOSS_Discriminator: 0.1568087637424469\n",
            "ITERATION_NO.: 409 LOSS_Generator: 6.140810966491699 LOSS_Discriminator: 0.23753537237644196\n",
            "ITERATION_NO.: 410 LOSS_Generator: 6.111931324005127 LOSS_Discriminator: 0.17720448970794678\n",
            "ITERATION_NO.: 411 LOSS_Generator: 5.2968621253967285 LOSS_Discriminator: 0.12761366367340088\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.397650241851807 LOSS_Discriminator: 0.08050628006458282\n",
            "ITERATION_NO.: 413 LOSS_Generator: 4.9614667892456055 LOSS_Discriminator: 0.16750890016555786\n",
            "ITERATION_NO.: 414 LOSS_Generator: 4.125431060791016 LOSS_Discriminator: 0.09812749922275543\n",
            "ITERATION_NO.: 415 LOSS_Generator: 4.406263828277588 LOSS_Discriminator: 0.1470397412776947\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.10042142868042 LOSS_Discriminator: 0.2445029467344284\n",
            "ITERATION_NO.: 417 LOSS_Generator: 5.3991804122924805 LOSS_Discriminator: 0.16446301341056824\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.694570541381836 LOSS_Discriminator: 0.08705838024616241\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.835732460021973 LOSS_Discriminator: 0.24473796784877777\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.756591320037842 LOSS_Discriminator: 0.09961366653442383\n",
            "ITERATION_NO.: 421 LOSS_Generator: 5.289091110229492 LOSS_Discriminator: 0.07363757491111755\n",
            "ITERATION_NO.: 422 LOSS_Generator: 5.287651538848877 LOSS_Discriminator: 0.19674856960773468\n",
            "ITERATION_NO.: 423 LOSS_Generator: 4.943807601928711 LOSS_Discriminator: 0.12686920166015625\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.365621566772461 LOSS_Discriminator: 0.11586825549602509\n",
            "ITERATION_NO.: 425 LOSS_Generator: 4.955758571624756 LOSS_Discriminator: 0.10017730295658112\n",
            "ITERATION_NO.: 426 LOSS_Generator: 5.212038040161133 LOSS_Discriminator: 0.045558713376522064\n",
            "ITERATION_NO.: 427 LOSS_Generator: 5.468442916870117 LOSS_Discriminator: 0.18212676048278809\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.111690044403076 LOSS_Discriminator: 0.31124648451805115\n",
            "ITERATION_NO.: 429 LOSS_Generator: 5.692593574523926 LOSS_Discriminator: 0.07648204267024994\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.801248073577881 LOSS_Discriminator: 0.162965789437294\n",
            "ITERATION_NO.: 431 LOSS_Generator: 6.622766017913818 LOSS_Discriminator: 0.09687061607837677\n",
            "ITERATION_NO.: 432 LOSS_Generator: 6.502925872802734 LOSS_Discriminator: 0.10628759860992432\n",
            "ITERATION_NO.: 433 LOSS_Generator: 5.993007183074951 LOSS_Discriminator: 0.15245483815670013\n",
            "ITERATION_NO.: 434 LOSS_Generator: 5.621220588684082 LOSS_Discriminator: 0.20030386745929718\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.042305946350098 LOSS_Discriminator: 0.0655195415019989\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.0478196144104 LOSS_Discriminator: 0.1502974033355713\n",
            "ITERATION_NO.: 437 LOSS_Generator: 4.631594657897949 LOSS_Discriminator: 0.1772421896457672\n",
            "ITERATION_NO.: 438 LOSS_Generator: 5.1365556716918945 LOSS_Discriminator: 0.2857704162597656\n",
            "ITERATION_NO.: 439 LOSS_Generator: 5.304599761962891 LOSS_Discriminator: 0.10627321898937225\n",
            "ITERATION_NO.: 440 LOSS_Generator: 5.021740913391113 LOSS_Discriminator: 0.12952888011932373\n",
            "ITERATION_NO.: 441 LOSS_Generator: 5.850124359130859 LOSS_Discriminator: 0.09152358770370483\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.728066444396973 LOSS_Discriminator: 0.06634166836738586\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.833788871765137 LOSS_Discriminator: 0.11501579731702805\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.676578521728516 LOSS_Discriminator: 0.11659305542707443\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.967154026031494 LOSS_Discriminator: 0.08117341250181198\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.445338249206543 LOSS_Discriminator: 0.2909895181655884\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.736934185028076 LOSS_Discriminator: 0.12498688697814941\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.489365100860596 LOSS_Discriminator: 0.21079587936401367\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.628924369812012 LOSS_Discriminator: 0.10893185436725616\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.8165154457092285 LOSS_Discriminator: 0.12167470157146454\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.029524326324463 LOSS_Discriminator: 0.1762334555387497\n",
            "ITERATION_NO.: 452 LOSS_Generator: 4.711378574371338 LOSS_Discriminator: 0.1289185732603073\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.4420552253723145 LOSS_Discriminator: 0.1625846028327942\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.181520938873291 LOSS_Discriminator: 0.1744179129600525\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.27414608001709 LOSS_Discriminator: 0.10831435769796371\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.3918304443359375 LOSS_Discriminator: 0.09044742584228516\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.714665412902832 LOSS_Discriminator: 0.17748568952083588\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.775406360626221 LOSS_Discriminator: 0.13131213188171387\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.292837142944336 LOSS_Discriminator: 0.1359555870294571\n",
            "ITERATION_NO.: 460 LOSS_Generator: 6.046468734741211 LOSS_Discriminator: 0.2325390875339508\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.862054824829102 LOSS_Discriminator: 0.06732384860515594\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.583086967468262 LOSS_Discriminator: 0.08126593381166458\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.469351291656494 LOSS_Discriminator: 0.13643985986709595\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.196237087249756 LOSS_Discriminator: 0.17057834565639496\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.211565971374512 LOSS_Discriminator: 0.08539711683988571\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.2301812171936035 LOSS_Discriminator: 0.2622159719467163\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.2327189445495605 LOSS_Discriminator: 0.1334436982870102\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.374083995819092 LOSS_Discriminator: 0.1858653724193573\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.226146697998047 LOSS_Discriminator: 0.11884535849094391\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.192709445953369 LOSS_Discriminator: 0.17397630214691162\n",
            "ITERATION_NO.: 471 LOSS_Generator: 5.336938858032227 LOSS_Discriminator: 0.1639263927936554\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.8562397956848145 LOSS_Discriminator: 0.07562814652919769\n",
            "ITERATION_NO.: 473 LOSS_Generator: 4.779758930206299 LOSS_Discriminator: 0.20728561282157898\n",
            "ITERATION_NO.: 474 LOSS_Generator: 4.87355899810791 LOSS_Discriminator: 0.2301664501428604\n",
            "ITERATION_NO.: 475 LOSS_Generator: 5.540387153625488 LOSS_Discriminator: 0.1292128711938858\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.811518669128418 LOSS_Discriminator: 0.1224367544054985\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.318394184112549 LOSS_Discriminator: 0.27234622836112976\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.449087619781494 LOSS_Discriminator: 0.16312813758850098\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.559375762939453 LOSS_Discriminator: 0.17043879628181458\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.363514423370361 LOSS_Discriminator: 0.1563566029071808\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.317282676696777 LOSS_Discriminator: 0.1364675760269165\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.274506092071533 LOSS_Discriminator: 0.10494253039360046\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.7115631103515625 LOSS_Discriminator: 0.0869351327419281\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.009592533111572 LOSS_Discriminator: 0.10331588238477707\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.602235317230225 LOSS_Discriminator: 0.17062675952911377\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.867321014404297 LOSS_Discriminator: 0.19086334109306335\n",
            "ITERATION_NO.: 487 LOSS_Generator: 5.8286662101745605 LOSS_Discriminator: 0.13515394926071167\n",
            "ITERATION_NO.: 488 LOSS_Generator: 5.7843546867370605 LOSS_Discriminator: 0.127084881067276\n",
            "ITERATION_NO.: 489 LOSS_Generator: 5.568117618560791 LOSS_Discriminator: 0.13497355580329895\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.373934745788574 LOSS_Discriminator: 0.127965047955513\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.087108135223389 LOSS_Discriminator: 0.15826743841171265\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.1418280601501465 LOSS_Discriminator: 0.10001607239246368\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.209578037261963 LOSS_Discriminator: 0.2262643575668335\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.3845648765563965 LOSS_Discriminator: 0.125575989484787\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.564690589904785 LOSS_Discriminator: 0.0952754020690918\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.1316399574279785 LOSS_Discriminator: 0.10703278332948685\n",
            "ITERATION_NO.: 497 LOSS_Generator: 6.10845947265625 LOSS_Discriminator: 0.11648321151733398\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.652022838592529 LOSS_Discriminator: 0.20393416285514832\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.823235034942627 LOSS_Discriminator: 0.1290961056947708\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.618474006652832 LOSS_Discriminator: 0.16565224528312683\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.38852071762085 LOSS_Discriminator: 0.15446579456329346\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.746265888214111 LOSS_Discriminator: 0.11853384226560593\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.250557899475098 LOSS_Discriminator: 0.2427593320608139\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.502969741821289 LOSS_Discriminator: 0.1699652075767517\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.5111188888549805 LOSS_Discriminator: 0.15271851420402527\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.113784313201904 LOSS_Discriminator: 0.07175670564174652\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.491023063659668 LOSS_Discriminator: 0.13926154375076294\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.649863243103027 LOSS_Discriminator: 0.08303754031658173\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.324617385864258 LOSS_Discriminator: 0.26559165120124817\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.553516864776611 LOSS_Discriminator: 0.11043564230203629\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.1581244468688965 LOSS_Discriminator: 0.15528284013271332\n",
            "ITERATION_NO.: 512 LOSS_Generator: 4.985753059387207 LOSS_Discriminator: 0.13461783528327942\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.257913112640381 LOSS_Discriminator: 0.09407868981361389\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.224073886871338 LOSS_Discriminator: 0.22393228113651276\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.540614128112793 LOSS_Discriminator: 0.18113136291503906\n",
            "ITERATION_NO.: 516 LOSS_Generator: 4.982840061187744 LOSS_Discriminator: 0.10745646059513092\n",
            "ITERATION_NO.: 517 LOSS_Generator: 4.859472751617432 LOSS_Discriminator: 0.18957069516181946\n",
            "ITERATION_NO.: 518 LOSS_Generator: 5.4624505043029785 LOSS_Discriminator: 0.16161701083183289\n",
            "ITERATION_NO.: 519 LOSS_Generator: 5.4825921058654785 LOSS_Discriminator: 0.1908346712589264\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.796162128448486 LOSS_Discriminator: 0.1275644600391388\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.798557281494141 LOSS_Discriminator: 0.22430716454982758\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.7388787269592285 LOSS_Discriminator: 0.11902812868356705\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.897663116455078 LOSS_Discriminator: 0.07564659416675568\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.588137626647949 LOSS_Discriminator: 0.15765826404094696\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.859819412231445 LOSS_Discriminator: 0.21443161368370056\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.799793243408203 LOSS_Discriminator: 0.22552365064620972\n",
            "ITERATION_NO.: 527 LOSS_Generator: 5.323710918426514 LOSS_Discriminator: 0.14124459028244019\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.283897876739502 LOSS_Discriminator: 0.18147176504135132\n",
            "ITERATION_NO.: 529 LOSS_Generator: 4.955694675445557 LOSS_Discriminator: 0.19145983457565308\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.054825305938721 LOSS_Discriminator: 0.2006765455007553\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.143998622894287 LOSS_Discriminator: 0.11416254192590714\n",
            "ITERATION_NO.: 532 LOSS_Generator: 4.948470592498779 LOSS_Discriminator: 0.12242421507835388\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.188448905944824 LOSS_Discriminator: 0.144412100315094\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.257352352142334 LOSS_Discriminator: 0.231174498796463\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.775015830993652 LOSS_Discriminator: 0.25349390506744385\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.058279037475586 LOSS_Discriminator: 0.17002278566360474\n",
            "ITERATION_NO.: 537 LOSS_Generator: 6.42518424987793 LOSS_Discriminator: 0.16506509482860565\n",
            "ITERATION_NO.: 538 LOSS_Generator: 6.211523532867432 LOSS_Discriminator: 0.1471467912197113\n",
            "ITERATION_NO.: 539 LOSS_Generator: 6.039059638977051 LOSS_Discriminator: 0.0909140408039093\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.84665060043335 LOSS_Discriminator: 0.15387216210365295\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.872729301452637 LOSS_Discriminator: 0.18553794920444489\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.900880336761475 LOSS_Discriminator: 0.14779096841812134\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.335221767425537 LOSS_Discriminator: 0.11514467000961304\n",
            "ITERATION_NO.: 544 LOSS_Generator: 4.760338306427002 LOSS_Discriminator: 0.2877057194709778\n",
            "ITERATION_NO.: 545 LOSS_Generator: 5.365820407867432 LOSS_Discriminator: 0.15561814606189728\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.044435501098633 LOSS_Discriminator: 0.18793641030788422\n",
            "ITERATION_NO.: 547 LOSS_Generator: 6.168982982635498 LOSS_Discriminator: 0.15217812359333038\n",
            "ITERATION_NO.: 548 LOSS_Generator: 5.8197479248046875 LOSS_Discriminator: 0.13948208093643188\n",
            "ITERATION_NO.: 549 LOSS_Generator: 6.166353225708008 LOSS_Discriminator: 0.0882733166217804\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.0496368408203125 LOSS_Discriminator: 0.07945418357849121\n",
            "ITERATION_NO.: 551 LOSS_Generator: 6.368242263793945 LOSS_Discriminator: 0.21837370097637177\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.549942493438721 LOSS_Discriminator: 0.07106052339076996\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.598313808441162 LOSS_Discriminator: 0.08819262683391571\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.704746246337891 LOSS_Discriminator: 0.1974177062511444\n",
            "ITERATION_NO.: 555 LOSS_Generator: 4.90200662612915 LOSS_Discriminator: 0.1440783441066742\n",
            "ITERATION_NO.: 556 LOSS_Generator: 4.743273735046387 LOSS_Discriminator: 0.2648531496524811\n",
            "ITERATION_NO.: 557 LOSS_Generator: 4.790904998779297 LOSS_Discriminator: 0.16041864454746246\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.511958599090576 LOSS_Discriminator: 0.10244563221931458\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.82192325592041 LOSS_Discriminator: 0.16120553016662598\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.319034576416016 LOSS_Discriminator: 0.0869184285402298\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.588355541229248 LOSS_Discriminator: 0.14344684779644012\n",
            "ITERATION_NO.: 562 LOSS_Generator: 6.070879936218262 LOSS_Discriminator: 0.16435185074806213\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.828368663787842 LOSS_Discriminator: 0.23860515654087067\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.907510280609131 LOSS_Discriminator: 0.11739829927682877\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.581325054168701 LOSS_Discriminator: 0.18185339868068695\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.414036750793457 LOSS_Discriminator: 0.17105133831501007\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.217111587524414 LOSS_Discriminator: 0.20900261402130127\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.023301124572754 LOSS_Discriminator: 0.15390506386756897\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.415576934814453 LOSS_Discriminator: 0.12066903710365295\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.727602958679199 LOSS_Discriminator: 0.13005070388317108\n",
            "ITERATION_NO.: 571 LOSS_Generator: 5.834766864776611 LOSS_Discriminator: 0.20529305934906006\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.889334678649902 LOSS_Discriminator: 0.13824431598186493\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.729383945465088 LOSS_Discriminator: 0.189239040017128\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.381048679351807 LOSS_Discriminator: 0.1578376442193985\n",
            "ITERATION_NO.: 575 LOSS_Generator: 5.3933796882629395 LOSS_Discriminator: 0.1898415982723236\n",
            "ITERATION_NO.: 576 LOSS_Generator: 5.411797046661377 LOSS_Discriminator: 0.24310410022735596\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.097766876220703 LOSS_Discriminator: 0.2373446226119995\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.3730926513671875 LOSS_Discriminator: 0.15979191660881042\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.016684055328369 LOSS_Discriminator: 0.20645996928215027\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.5112433433532715 LOSS_Discriminator: 0.14638161659240723\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.638875961303711 LOSS_Discriminator: 0.12565316259860992\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.342568397521973 LOSS_Discriminator: 0.11094978451728821\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.001894474029541 LOSS_Discriminator: 0.08308947086334229\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.439783096313477 LOSS_Discriminator: 0.2047208994626999\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.199946880340576 LOSS_Discriminator: 0.12102136015892029\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.401715278625488 LOSS_Discriminator: 0.1370139867067337\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.5726470947265625 LOSS_Discriminator: 0.20561109483242035\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.711026191711426 LOSS_Discriminator: 0.09023155272006989\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.066584587097168 LOSS_Discriminator: 0.12433432787656784\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.539369583129883 LOSS_Discriminator: 0.14826229214668274\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.371607780456543 LOSS_Discriminator: 0.13987016677856445\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.8008575439453125 LOSS_Discriminator: 0.16849036514759064\n",
            "ITERATION_NO.: 593 LOSS_Generator: 6.167489528656006 LOSS_Discriminator: 0.09310232847929001\n",
            "ITERATION_NO.: 594 LOSS_Generator: 5.801783561706543 LOSS_Discriminator: 0.11676538735628128\n",
            "ITERATION_NO.: 595 LOSS_Generator: 5.876754283905029 LOSS_Discriminator: 0.12744376063346863\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.879658222198486 LOSS_Discriminator: 0.23031872510910034\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.6379804611206055 LOSS_Discriminator: 0.18333260715007782\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.237682342529297 LOSS_Discriminator: 0.18169185519218445\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.036408424377441 LOSS_Discriminator: 0.12949775159358978\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.182249546051025 LOSS_Discriminator: 0.08723925799131393\n",
            "EPOCH OVER: 8\n",
            "ITERATION_NO.: 1 LOSS_Generator: 5.0389838218688965 LOSS_Discriminator: 0.16072703897953033\n",
            "ITERATION_NO.: 2 LOSS_Generator: 5.096654891967773 LOSS_Discriminator: 0.10783682763576508\n",
            "ITERATION_NO.: 3 LOSS_Generator: 5.382655620574951 LOSS_Discriminator: 0.12245476245880127\n",
            "ITERATION_NO.: 4 LOSS_Generator: 5.135210990905762 LOSS_Discriminator: 0.23814010620117188\n",
            "ITERATION_NO.: 5 LOSS_Generator: 5.5164642333984375 LOSS_Discriminator: 0.11431847512722015\n",
            "ITERATION_NO.: 6 LOSS_Generator: 5.524002552032471 LOSS_Discriminator: 0.10757871717214584\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.6509785652160645 LOSS_Discriminator: 0.1603013426065445\n",
            "ITERATION_NO.: 8 LOSS_Generator: 6.0249152183532715 LOSS_Discriminator: 0.11437614262104034\n",
            "ITERATION_NO.: 9 LOSS_Generator: 5.2322893142700195 LOSS_Discriminator: 0.16305457055568695\n",
            "ITERATION_NO.: 10 LOSS_Generator: 5.66120719909668 LOSS_Discriminator: 0.08098562061786652\n",
            "ITERATION_NO.: 11 LOSS_Generator: 5.693414211273193 LOSS_Discriminator: 0.18278905749320984\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.460776329040527 LOSS_Discriminator: 0.10354296118021011\n",
            "ITERATION_NO.: 13 LOSS_Generator: 5.136501312255859 LOSS_Discriminator: 0.1395505964756012\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.347043991088867 LOSS_Discriminator: 0.2186596393585205\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.551793098449707 LOSS_Discriminator: 0.19331330060958862\n",
            "ITERATION_NO.: 16 LOSS_Generator: 5.572425365447998 LOSS_Discriminator: 0.09069094061851501\n",
            "ITERATION_NO.: 17 LOSS_Generator: 5.9003119468688965 LOSS_Discriminator: 0.11703759431838989\n",
            "ITERATION_NO.: 18 LOSS_Generator: 5.665213108062744 LOSS_Discriminator: 0.16047562658786774\n",
            "ITERATION_NO.: 19 LOSS_Generator: 5.868200302124023 LOSS_Discriminator: 0.11966577917337418\n",
            "ITERATION_NO.: 20 LOSS_Generator: 5.697946071624756 LOSS_Discriminator: 0.12429486215114594\n",
            "ITERATION_NO.: 21 LOSS_Generator: 5.406937122344971 LOSS_Discriminator: 0.18743771314620972\n",
            "ITERATION_NO.: 22 LOSS_Generator: 5.605241298675537 LOSS_Discriminator: 0.21511419117450714\n",
            "ITERATION_NO.: 23 LOSS_Generator: 5.112900257110596 LOSS_Discriminator: 0.21785590052604675\n",
            "ITERATION_NO.: 24 LOSS_Generator: 4.940483570098877 LOSS_Discriminator: 0.1580749750137329\n",
            "ITERATION_NO.: 25 LOSS_Generator: 4.537421703338623 LOSS_Discriminator: 0.10026451200246811\n",
            "ITERATION_NO.: 26 LOSS_Generator: 5.247159957885742 LOSS_Discriminator: 0.1728740930557251\n",
            "ITERATION_NO.: 27 LOSS_Generator: 5.295820713043213 LOSS_Discriminator: 0.16303296387195587\n",
            "ITERATION_NO.: 28 LOSS_Generator: 5.721472263336182 LOSS_Discriminator: 0.10149629414081573\n",
            "ITERATION_NO.: 29 LOSS_Generator: 5.790980339050293 LOSS_Discriminator: 0.14082804322242737\n",
            "ITERATION_NO.: 30 LOSS_Generator: 5.998822212219238 LOSS_Discriminator: 0.21205192804336548\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.005465030670166 LOSS_Discriminator: 0.15874086320400238\n",
            "ITERATION_NO.: 32 LOSS_Generator: 5.799693584442139 LOSS_Discriminator: 0.15946979820728302\n",
            "ITERATION_NO.: 33 LOSS_Generator: 6.0992112159729 LOSS_Discriminator: 0.1487790197134018\n",
            "ITERATION_NO.: 34 LOSS_Generator: 5.374063014984131 LOSS_Discriminator: 0.13097460567951202\n",
            "ITERATION_NO.: 35 LOSS_Generator: 5.689152717590332 LOSS_Discriminator: 0.12383890151977539\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.723687171936035 LOSS_Discriminator: 0.11909766495227814\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.2841057777404785 LOSS_Discriminator: 0.10445456206798553\n",
            "ITERATION_NO.: 38 LOSS_Generator: 5.042257308959961 LOSS_Discriminator: 0.1331806182861328\n",
            "ITERATION_NO.: 39 LOSS_Generator: 5.446009635925293 LOSS_Discriminator: 0.1046493649482727\n",
            "ITERATION_NO.: 40 LOSS_Generator: 5.883917331695557 LOSS_Discriminator: 0.10358282178640366\n",
            "ITERATION_NO.: 41 LOSS_Generator: 6.009566783905029 LOSS_Discriminator: 0.18842194974422455\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.4129743576049805 LOSS_Discriminator: 0.13083171844482422\n",
            "ITERATION_NO.: 43 LOSS_Generator: 5.948065280914307 LOSS_Discriminator: 0.08976234495639801\n",
            "ITERATION_NO.: 44 LOSS_Generator: 5.7259745597839355 LOSS_Discriminator: 0.2270342856645584\n",
            "ITERATION_NO.: 45 LOSS_Generator: 5.708194732666016 LOSS_Discriminator: 0.19331228733062744\n",
            "ITERATION_NO.: 46 LOSS_Generator: 5.510831832885742 LOSS_Discriminator: 0.18698737025260925\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.4858198165893555 LOSS_Discriminator: 0.17917639017105103\n",
            "ITERATION_NO.: 48 LOSS_Generator: 5.174200057983398 LOSS_Discriminator: 0.20148764550685883\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.307134628295898 LOSS_Discriminator: 0.12152501195669174\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.5898613929748535 LOSS_Discriminator: 0.2048444151878357\n",
            "ITERATION_NO.: 51 LOSS_Generator: 5.8487982749938965 LOSS_Discriminator: 0.17371675372123718\n",
            "ITERATION_NO.: 52 LOSS_Generator: 5.762098789215088 LOSS_Discriminator: 0.12732303142547607\n",
            "ITERATION_NO.: 53 LOSS_Generator: 6.087660312652588 LOSS_Discriminator: 0.06738901138305664\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.078702449798584 LOSS_Discriminator: 0.25077110528945923\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.890256881713867 LOSS_Discriminator: 0.12612740695476532\n",
            "ITERATION_NO.: 56 LOSS_Generator: 5.962609767913818 LOSS_Discriminator: 0.2257806956768036\n",
            "ITERATION_NO.: 57 LOSS_Generator: 5.553819179534912 LOSS_Discriminator: 0.11352057754993439\n",
            "ITERATION_NO.: 58 LOSS_Generator: 5.128019332885742 LOSS_Discriminator: 0.10712359100580215\n",
            "ITERATION_NO.: 59 LOSS_Generator: 5.035434722900391 LOSS_Discriminator: 0.1502688080072403\n",
            "ITERATION_NO.: 60 LOSS_Generator: 4.979977607727051 LOSS_Discriminator: 0.1467445194721222\n",
            "ITERATION_NO.: 61 LOSS_Generator: 4.718181610107422 LOSS_Discriminator: 0.22561894357204437\n",
            "ITERATION_NO.: 62 LOSS_Generator: 5.26624870300293 LOSS_Discriminator: 0.13041964173316956\n",
            "ITERATION_NO.: 63 LOSS_Generator: 5.771424770355225 LOSS_Discriminator: 0.11509707570075989\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.596919536590576 LOSS_Discriminator: 0.13718049228191376\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.493774890899658 LOSS_Discriminator: 0.20447811484336853\n",
            "ITERATION_NO.: 66 LOSS_Generator: 6.248587608337402 LOSS_Discriminator: 0.09881375730037689\n",
            "ITERATION_NO.: 67 LOSS_Generator: 6.051167011260986 LOSS_Discriminator: 0.06962805986404419\n",
            "ITERATION_NO.: 68 LOSS_Generator: 6.616079807281494 LOSS_Discriminator: 0.15707877278327942\n",
            "ITERATION_NO.: 69 LOSS_Generator: 6.2744269371032715 LOSS_Discriminator: 0.09820722043514252\n",
            "ITERATION_NO.: 70 LOSS_Generator: 6.227397441864014 LOSS_Discriminator: 0.10531154274940491\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.72109317779541 LOSS_Discriminator: 0.15519660711288452\n",
            "ITERATION_NO.: 72 LOSS_Generator: 5.850600719451904 LOSS_Discriminator: 0.2432769387960434\n",
            "ITERATION_NO.: 73 LOSS_Generator: 5.281429290771484 LOSS_Discriminator: 0.1377331018447876\n",
            "ITERATION_NO.: 74 LOSS_Generator: 5.301136493682861 LOSS_Discriminator: 0.11318725347518921\n",
            "ITERATION_NO.: 75 LOSS_Generator: 5.150500297546387 LOSS_Discriminator: 0.30532917380332947\n",
            "ITERATION_NO.: 76 LOSS_Generator: 5.439754009246826 LOSS_Discriminator: 0.12085728347301483\n",
            "ITERATION_NO.: 77 LOSS_Generator: 5.76248025894165 LOSS_Discriminator: 0.14321428537368774\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.251936435699463 LOSS_Discriminator: 0.2376067042350769\n",
            "ITERATION_NO.: 79 LOSS_Generator: 5.594785213470459 LOSS_Discriminator: 0.18158280849456787\n",
            "ITERATION_NO.: 80 LOSS_Generator: 5.460210800170898 LOSS_Discriminator: 0.23047490417957306\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.483561992645264 LOSS_Discriminator: 0.14332215487957\n",
            "ITERATION_NO.: 82 LOSS_Generator: 5.012115955352783 LOSS_Discriminator: 0.08808031678199768\n",
            "ITERATION_NO.: 83 LOSS_Generator: 5.1913251876831055 LOSS_Discriminator: 0.07482307404279709\n",
            "ITERATION_NO.: 84 LOSS_Generator: 5.2902631759643555 LOSS_Discriminator: 0.1839487999677658\n",
            "ITERATION_NO.: 85 LOSS_Generator: 5.095221042633057 LOSS_Discriminator: 0.16739165782928467\n",
            "ITERATION_NO.: 86 LOSS_Generator: 5.50596809387207 LOSS_Discriminator: 0.28730127215385437\n",
            "ITERATION_NO.: 87 LOSS_Generator: 5.816895008087158 LOSS_Discriminator: 0.19447636604309082\n",
            "ITERATION_NO.: 88 LOSS_Generator: 5.4699811935424805 LOSS_Discriminator: 0.18701687455177307\n",
            "ITERATION_NO.: 89 LOSS_Generator: 5.720922946929932 LOSS_Discriminator: 0.13879646360874176\n",
            "ITERATION_NO.: 90 LOSS_Generator: 5.413123607635498 LOSS_Discriminator: 0.14791461825370789\n",
            "ITERATION_NO.: 91 LOSS_Generator: 5.859177112579346 LOSS_Discriminator: 0.1556679606437683\n",
            "ITERATION_NO.: 92 LOSS_Generator: 5.821280002593994 LOSS_Discriminator: 0.1398300975561142\n",
            "ITERATION_NO.: 93 LOSS_Generator: 5.682000637054443 LOSS_Discriminator: 0.10758154839277267\n",
            "ITERATION_NO.: 94 LOSS_Generator: 5.426961898803711 LOSS_Discriminator: 0.25354117155075073\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.881875991821289 LOSS_Discriminator: 0.1700725555419922\n",
            "ITERATION_NO.: 96 LOSS_Generator: 5.374296188354492 LOSS_Discriminator: 0.17883838713169098\n",
            "ITERATION_NO.: 97 LOSS_Generator: 4.652105808258057 LOSS_Discriminator: 0.19380243122577667\n",
            "ITERATION_NO.: 98 LOSS_Generator: 5.7352986335754395 LOSS_Discriminator: 0.1996854543685913\n",
            "ITERATION_NO.: 99 LOSS_Generator: 4.922988414764404 LOSS_Discriminator: 0.1484527736902237\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.3191728591918945 LOSS_Discriminator: 0.20888805389404297\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.603440761566162 LOSS_Discriminator: 0.17371714115142822\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.373378276824951 LOSS_Discriminator: 0.30328190326690674\n",
            "ITERATION_NO.: 103 LOSS_Generator: 5.927811145782471 LOSS_Discriminator: 0.18916276097297668\n",
            "ITERATION_NO.: 104 LOSS_Generator: 5.784476280212402 LOSS_Discriminator: 0.2379857897758484\n",
            "ITERATION_NO.: 105 LOSS_Generator: 5.106242656707764 LOSS_Discriminator: 0.260545015335083\n",
            "ITERATION_NO.: 106 LOSS_Generator: 5.114190101623535 LOSS_Discriminator: 0.1450786143541336\n",
            "ITERATION_NO.: 107 LOSS_Generator: 4.261013031005859 LOSS_Discriminator: 0.08140768110752106\n",
            "ITERATION_NO.: 108 LOSS_Generator: 4.612010478973389 LOSS_Discriminator: 0.25773584842681885\n",
            "ITERATION_NO.: 109 LOSS_Generator: 4.65568733215332 LOSS_Discriminator: 0.10640718787908554\n",
            "ITERATION_NO.: 110 LOSS_Generator: 4.5176520347595215 LOSS_Discriminator: 0.30579087138175964\n",
            "ITERATION_NO.: 111 LOSS_Generator: 4.972957611083984 LOSS_Discriminator: 0.1314166784286499\n",
            "ITERATION_NO.: 112 LOSS_Generator: 5.7978434562683105 LOSS_Discriminator: 0.13769643008708954\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.643033027648926 LOSS_Discriminator: 0.2184874564409256\n",
            "ITERATION_NO.: 114 LOSS_Generator: 5.639398574829102 LOSS_Discriminator: 0.1744585633277893\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.837228298187256 LOSS_Discriminator: 0.08867108821868896\n",
            "ITERATION_NO.: 116 LOSS_Generator: 5.658885478973389 LOSS_Discriminator: 0.19176167249679565\n",
            "ITERATION_NO.: 117 LOSS_Generator: 5.660863876342773 LOSS_Discriminator: 0.18273788690567017\n",
            "ITERATION_NO.: 118 LOSS_Generator: 5.427487850189209 LOSS_Discriminator: 0.18631982803344727\n",
            "ITERATION_NO.: 119 LOSS_Generator: 4.839986801147461 LOSS_Discriminator: 0.21545398235321045\n",
            "ITERATION_NO.: 120 LOSS_Generator: 4.720973491668701 LOSS_Discriminator: 0.1392340213060379\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.309840202331543 LOSS_Discriminator: 0.0901355966925621\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.4510955810546875 LOSS_Discriminator: 0.2604469656944275\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.625816822052002 LOSS_Discriminator: 0.1304059624671936\n",
            "ITERATION_NO.: 124 LOSS_Generator: 6.0984954833984375 LOSS_Discriminator: 0.23355704545974731\n",
            "ITERATION_NO.: 125 LOSS_Generator: 5.456248760223389 LOSS_Discriminator: 0.2578144669532776\n",
            "ITERATION_NO.: 126 LOSS_Generator: 5.6244635581970215 LOSS_Discriminator: 0.09123340249061584\n",
            "ITERATION_NO.: 127 LOSS_Generator: 5.176313400268555 LOSS_Discriminator: 0.06614167243242264\n",
            "ITERATION_NO.: 128 LOSS_Generator: 5.397798538208008 LOSS_Discriminator: 0.08061153441667557\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.250784873962402 LOSS_Discriminator: 0.14085939526557922\n",
            "ITERATION_NO.: 130 LOSS_Generator: 4.909644603729248 LOSS_Discriminator: 0.19527992606163025\n",
            "ITERATION_NO.: 131 LOSS_Generator: 4.65829610824585 LOSS_Discriminator: 0.14327934384346008\n",
            "ITERATION_NO.: 132 LOSS_Generator: 5.2825493812561035 LOSS_Discriminator: 0.11746424436569214\n",
            "ITERATION_NO.: 133 LOSS_Generator: 4.872976303100586 LOSS_Discriminator: 0.1719193160533905\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.339388370513916 LOSS_Discriminator: 0.14205333590507507\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.318769454956055 LOSS_Discriminator: 0.10388430953025818\n",
            "ITERATION_NO.: 136 LOSS_Generator: 5.596163749694824 LOSS_Discriminator: 0.12415358424186707\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.565552234649658 LOSS_Discriminator: 0.10065069049596786\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.248379707336426 LOSS_Discriminator: 0.1262531727552414\n",
            "ITERATION_NO.: 139 LOSS_Generator: 5.29576301574707 LOSS_Discriminator: 0.04246608912944794\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.273942947387695 LOSS_Discriminator: 0.08318936824798584\n",
            "ITERATION_NO.: 141 LOSS_Generator: 5.371673107147217 LOSS_Discriminator: 0.09657496958971024\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.244568347930908 LOSS_Discriminator: 0.0992521345615387\n",
            "ITERATION_NO.: 143 LOSS_Generator: 5.390744686126709 LOSS_Discriminator: 0.13181465864181519\n",
            "ITERATION_NO.: 144 LOSS_Generator: 5.4981369972229 LOSS_Discriminator: 0.10133875906467438\n",
            "ITERATION_NO.: 145 LOSS_Generator: 5.130937099456787 LOSS_Discriminator: 0.2010960578918457\n",
            "ITERATION_NO.: 146 LOSS_Generator: 5.008782863616943 LOSS_Discriminator: 0.11533346027135849\n",
            "ITERATION_NO.: 147 LOSS_Generator: 4.917920112609863 LOSS_Discriminator: 0.23469974100589752\n",
            "ITERATION_NO.: 148 LOSS_Generator: 4.983123302459717 LOSS_Discriminator: 0.15687856078147888\n",
            "ITERATION_NO.: 149 LOSS_Generator: 5.465173244476318 LOSS_Discriminator: 0.15334868431091309\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.809711933135986 LOSS_Discriminator: 0.1414063572883606\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.453927993774414 LOSS_Discriminator: 0.14880064129829407\n",
            "ITERATION_NO.: 152 LOSS_Generator: 5.213505744934082 LOSS_Discriminator: 0.3283720016479492\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.254641532897949 LOSS_Discriminator: 0.18328088521957397\n",
            "ITERATION_NO.: 154 LOSS_Generator: 4.951989650726318 LOSS_Discriminator: 0.25753962993621826\n",
            "ITERATION_NO.: 155 LOSS_Generator: 4.783827304840088 LOSS_Discriminator: 0.24289703369140625\n",
            "ITERATION_NO.: 156 LOSS_Generator: 4.800668716430664 LOSS_Discriminator: 0.13875049352645874\n",
            "ITERATION_NO.: 157 LOSS_Generator: 5.37265157699585 LOSS_Discriminator: 0.20288293063640594\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.531798839569092 LOSS_Discriminator: 0.10002847760915756\n",
            "ITERATION_NO.: 159 LOSS_Generator: 6.222833156585693 LOSS_Discriminator: 0.13477760553359985\n",
            "ITERATION_NO.: 160 LOSS_Generator: 6.117185592651367 LOSS_Discriminator: 0.30536532402038574\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.635828018188477 LOSS_Discriminator: 0.20068812370300293\n",
            "ITERATION_NO.: 162 LOSS_Generator: 5.3241705894470215 LOSS_Discriminator: 0.1650959998369217\n",
            "ITERATION_NO.: 163 LOSS_Generator: 5.321929931640625 LOSS_Discriminator: 0.156575545668602\n",
            "ITERATION_NO.: 164 LOSS_Generator: 5.0385422706604 LOSS_Discriminator: 0.09819605946540833\n",
            "ITERATION_NO.: 165 LOSS_Generator: 4.984542369842529 LOSS_Discriminator: 0.1013500839471817\n",
            "ITERATION_NO.: 166 LOSS_Generator: 4.734384536743164 LOSS_Discriminator: 0.07907746732234955\n",
            "ITERATION_NO.: 167 LOSS_Generator: 4.847497463226318 LOSS_Discriminator: 0.11109130084514618\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.587287425994873 LOSS_Discriminator: 0.16614344716072083\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.314630031585693 LOSS_Discriminator: 0.10011132061481476\n",
            "ITERATION_NO.: 170 LOSS_Generator: 5.336319446563721 LOSS_Discriminator: 0.0665401965379715\n",
            "ITERATION_NO.: 171 LOSS_Generator: 5.646828651428223 LOSS_Discriminator: 0.14727434515953064\n",
            "ITERATION_NO.: 172 LOSS_Generator: 5.627948760986328 LOSS_Discriminator: 0.11050601303577423\n",
            "ITERATION_NO.: 173 LOSS_Generator: 5.6581268310546875 LOSS_Discriminator: 0.12481583654880524\n",
            "ITERATION_NO.: 174 LOSS_Generator: 5.319283962249756 LOSS_Discriminator: 0.10892512649297714\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.557124614715576 LOSS_Discriminator: 0.05822731927037239\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.2689056396484375 LOSS_Discriminator: 0.05137074366211891\n",
            "ITERATION_NO.: 177 LOSS_Generator: 5.092382907867432 LOSS_Discriminator: 0.14299286901950836\n",
            "ITERATION_NO.: 178 LOSS_Generator: 5.335205554962158 LOSS_Discriminator: 0.1103636771440506\n",
            "ITERATION_NO.: 179 LOSS_Generator: 5.54982852935791 LOSS_Discriminator: 0.09173163026571274\n",
            "ITERATION_NO.: 180 LOSS_Generator: 5.575167179107666 LOSS_Discriminator: 0.1404266357421875\n",
            "ITERATION_NO.: 181 LOSS_Generator: 5.888320446014404 LOSS_Discriminator: 0.13867425918579102\n",
            "ITERATION_NO.: 182 LOSS_Generator: 6.18386697769165 LOSS_Discriminator: 0.19285848736763\n",
            "ITERATION_NO.: 183 LOSS_Generator: 5.883218288421631 LOSS_Discriminator: 0.19395104050636292\n",
            "ITERATION_NO.: 184 LOSS_Generator: 5.665146350860596 LOSS_Discriminator: 0.11319375038146973\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.729025840759277 LOSS_Discriminator: 0.10292945802211761\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.1667399406433105 LOSS_Discriminator: 0.15307220816612244\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.615020275115967 LOSS_Discriminator: 0.243561252951622\n",
            "ITERATION_NO.: 188 LOSS_Generator: 4.835293769836426 LOSS_Discriminator: 0.11163415014743805\n",
            "ITERATION_NO.: 189 LOSS_Generator: 5.48294734954834 LOSS_Discriminator: 0.17690922319889069\n",
            "ITERATION_NO.: 190 LOSS_Generator: 5.056552886962891 LOSS_Discriminator: 0.11928907036781311\n",
            "ITERATION_NO.: 191 LOSS_Generator: 5.254782199859619 LOSS_Discriminator: 0.18618467450141907\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.044412136077881 LOSS_Discriminator: 0.13653406500816345\n",
            "ITERATION_NO.: 193 LOSS_Generator: 6.315545082092285 LOSS_Discriminator: 0.1752610206604004\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.052706718444824 LOSS_Discriminator: 0.1508689820766449\n",
            "ITERATION_NO.: 195 LOSS_Generator: 6.128047466278076 LOSS_Discriminator: 0.18274280428886414\n",
            "ITERATION_NO.: 196 LOSS_Generator: 6.142930507659912 LOSS_Discriminator: 0.11088740080595016\n",
            "ITERATION_NO.: 197 LOSS_Generator: 6.212629318237305 LOSS_Discriminator: 0.12625542283058167\n",
            "ITERATION_NO.: 198 LOSS_Generator: 5.6465888023376465 LOSS_Discriminator: 0.1079740822315216\n",
            "ITERATION_NO.: 199 LOSS_Generator: 5.357608795166016 LOSS_Discriminator: 0.13692060112953186\n",
            "ITERATION_NO.: 200 LOSS_Generator: 5.118392467498779 LOSS_Discriminator: 0.1385122388601303\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.3181471824646 LOSS_Discriminator: 0.12192012369632721\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.373096466064453 LOSS_Discriminator: 0.10659591853618622\n",
            "ITERATION_NO.: 203 LOSS_Generator: 5.359147548675537 LOSS_Discriminator: 0.10486644506454468\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.566726207733154 LOSS_Discriminator: 0.11649689078330994\n",
            "ITERATION_NO.: 205 LOSS_Generator: 5.799903392791748 LOSS_Discriminator: 0.1200735867023468\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.482945442199707 LOSS_Discriminator: 0.16934886574745178\n",
            "ITERATION_NO.: 207 LOSS_Generator: 5.432943820953369 LOSS_Discriminator: 0.11423933506011963\n",
            "ITERATION_NO.: 208 LOSS_Generator: 5.626002788543701 LOSS_Discriminator: 0.20555248856544495\n",
            "ITERATION_NO.: 209 LOSS_Generator: 5.182166576385498 LOSS_Discriminator: 0.1433521807193756\n",
            "ITERATION_NO.: 210 LOSS_Generator: 5.741950511932373 LOSS_Discriminator: 0.08961599320173264\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.063809871673584 LOSS_Discriminator: 0.134755477309227\n",
            "ITERATION_NO.: 212 LOSS_Generator: 5.594000339508057 LOSS_Discriminator: 0.1613188236951828\n",
            "ITERATION_NO.: 213 LOSS_Generator: 5.514188289642334 LOSS_Discriminator: 0.09906519949436188\n",
            "ITERATION_NO.: 214 LOSS_Generator: 5.1735734939575195 LOSS_Discriminator: 0.11735830456018448\n",
            "ITERATION_NO.: 215 LOSS_Generator: 5.115153789520264 LOSS_Discriminator: 0.09349733591079712\n",
            "ITERATION_NO.: 216 LOSS_Generator: 4.848222732543945 LOSS_Discriminator: 0.1275235265493393\n",
            "ITERATION_NO.: 217 LOSS_Generator: 4.910153388977051 LOSS_Discriminator: 0.12073057889938354\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.179977893829346 LOSS_Discriminator: 0.20070210099220276\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.260136604309082 LOSS_Discriminator: 0.2752865254878998\n",
            "ITERATION_NO.: 220 LOSS_Generator: 5.07003116607666 LOSS_Discriminator: 0.11356940865516663\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.331155300140381 LOSS_Discriminator: 0.13168483972549438\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.591353893280029 LOSS_Discriminator: 0.192642480134964\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.569758415222168 LOSS_Discriminator: 0.2028709053993225\n",
            "ITERATION_NO.: 224 LOSS_Generator: 5.858861923217773 LOSS_Discriminator: 0.08927034586668015\n",
            "ITERATION_NO.: 225 LOSS_Generator: 5.841188430786133 LOSS_Discriminator: 0.10369028151035309\n",
            "ITERATION_NO.: 226 LOSS_Generator: 5.766656398773193 LOSS_Discriminator: 0.21399778127670288\n",
            "ITERATION_NO.: 227 LOSS_Generator: 5.78788948059082 LOSS_Discriminator: 0.0829659253358841\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.650854110717773 LOSS_Discriminator: 0.08180584013462067\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.6512956619262695 LOSS_Discriminator: 0.24449414014816284\n",
            "ITERATION_NO.: 230 LOSS_Generator: 5.474189281463623 LOSS_Discriminator: 0.17522092163562775\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.467721462249756 LOSS_Discriminator: 0.13057143986225128\n",
            "ITERATION_NO.: 232 LOSS_Generator: 5.524271011352539 LOSS_Discriminator: 0.14317166805267334\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.740972995758057 LOSS_Discriminator: 0.14492198824882507\n",
            "ITERATION_NO.: 234 LOSS_Generator: 5.1811652183532715 LOSS_Discriminator: 0.1205701231956482\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.7102179527282715 LOSS_Discriminator: 0.1530451625585556\n",
            "ITERATION_NO.: 236 LOSS_Generator: 5.6552414894104 LOSS_Discriminator: 0.0939161628484726\n",
            "ITERATION_NO.: 237 LOSS_Generator: 6.026890277862549 LOSS_Discriminator: 0.11834680289030075\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.057744979858398 LOSS_Discriminator: 0.11501546204090118\n",
            "ITERATION_NO.: 239 LOSS_Generator: 5.697253227233887 LOSS_Discriminator: 0.2183496057987213\n",
            "ITERATION_NO.: 240 LOSS_Generator: 5.987703323364258 LOSS_Discriminator: 0.16104868054389954\n",
            "ITERATION_NO.: 241 LOSS_Generator: 4.966965198516846 LOSS_Discriminator: 0.19920332729816437\n",
            "ITERATION_NO.: 242 LOSS_Generator: 5.689207077026367 LOSS_Discriminator: 0.10572132468223572\n",
            "ITERATION_NO.: 243 LOSS_Generator: 5.070471286773682 LOSS_Discriminator: 0.1231699138879776\n",
            "ITERATION_NO.: 244 LOSS_Generator: 5.659539699554443 LOSS_Discriminator: 0.13435782492160797\n",
            "ITERATION_NO.: 245 LOSS_Generator: 5.405532360076904 LOSS_Discriminator: 0.16443675756454468\n",
            "ITERATION_NO.: 246 LOSS_Generator: 5.052887916564941 LOSS_Discriminator: 0.22582751512527466\n",
            "ITERATION_NO.: 247 LOSS_Generator: 4.888006687164307 LOSS_Discriminator: 0.12795214354991913\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.292558670043945 LOSS_Discriminator: 0.2690000534057617\n",
            "ITERATION_NO.: 249 LOSS_Generator: 5.338566780090332 LOSS_Discriminator: 0.1498415768146515\n",
            "ITERATION_NO.: 250 LOSS_Generator: 5.724936008453369 LOSS_Discriminator: 0.13255202770233154\n",
            "ITERATION_NO.: 251 LOSS_Generator: 6.135732650756836 LOSS_Discriminator: 0.13169309496879578\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.289443492889404 LOSS_Discriminator: 0.19690567255020142\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.544305324554443 LOSS_Discriminator: 0.17418918013572693\n",
            "ITERATION_NO.: 254 LOSS_Generator: 6.251516819000244 LOSS_Discriminator: 0.2253035306930542\n",
            "ITERATION_NO.: 255 LOSS_Generator: 5.679627895355225 LOSS_Discriminator: 0.09495992213487625\n",
            "ITERATION_NO.: 256 LOSS_Generator: 5.468695640563965 LOSS_Discriminator: 0.08273904025554657\n",
            "ITERATION_NO.: 257 LOSS_Generator: 4.878790855407715 LOSS_Discriminator: 0.1400177925825119\n",
            "ITERATION_NO.: 258 LOSS_Generator: 4.857248783111572 LOSS_Discriminator: 0.11522972583770752\n",
            "ITERATION_NO.: 259 LOSS_Generator: 4.692641258239746 LOSS_Discriminator: 0.193574920296669\n",
            "ITERATION_NO.: 260 LOSS_Generator: 5.427061557769775 LOSS_Discriminator: 0.09348196536302567\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.469360828399658 LOSS_Discriminator: 0.09764186292886734\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.546561241149902 LOSS_Discriminator: 0.13249772787094116\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.560230731964111 LOSS_Discriminator: 0.0808769166469574\n",
            "ITERATION_NO.: 264 LOSS_Generator: 6.504124164581299 LOSS_Discriminator: 0.10372571647167206\n",
            "ITERATION_NO.: 265 LOSS_Generator: 6.1074604988098145 LOSS_Discriminator: 0.14725466072559357\n",
            "ITERATION_NO.: 266 LOSS_Generator: 6.317187309265137 LOSS_Discriminator: 0.16938240826129913\n",
            "ITERATION_NO.: 267 LOSS_Generator: 5.4778265953063965 LOSS_Discriminator: 0.11570495367050171\n",
            "ITERATION_NO.: 268 LOSS_Generator: 5.8595757484436035 LOSS_Discriminator: 0.2253425121307373\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.401036262512207 LOSS_Discriminator: 0.04209211468696594\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.097681522369385 LOSS_Discriminator: 0.07726278901100159\n",
            "ITERATION_NO.: 271 LOSS_Generator: 5.158750057220459 LOSS_Discriminator: 0.11918506026268005\n",
            "ITERATION_NO.: 272 LOSS_Generator: 5.213629722595215 LOSS_Discriminator: 0.05936330929398537\n",
            "ITERATION_NO.: 273 LOSS_Generator: 5.437946796417236 LOSS_Discriminator: 0.15934288501739502\n",
            "ITERATION_NO.: 274 LOSS_Generator: 5.747315883636475 LOSS_Discriminator: 0.1560145616531372\n",
            "ITERATION_NO.: 275 LOSS_Generator: 5.401541233062744 LOSS_Discriminator: 0.10853362083435059\n",
            "ITERATION_NO.: 276 LOSS_Generator: 5.797357082366943 LOSS_Discriminator: 0.07294900715351105\n",
            "ITERATION_NO.: 277 LOSS_Generator: 5.638628959655762 LOSS_Discriminator: 0.12994882464408875\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.634711265563965 LOSS_Discriminator: 0.09270010888576508\n",
            "ITERATION_NO.: 279 LOSS_Generator: 5.798086643218994 LOSS_Discriminator: 0.1990368664264679\n",
            "ITERATION_NO.: 280 LOSS_Generator: 6.025488376617432 LOSS_Discriminator: 0.0453699454665184\n",
            "ITERATION_NO.: 281 LOSS_Generator: 5.324127197265625 LOSS_Discriminator: 0.16385284066200256\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.817800521850586 LOSS_Discriminator: 0.09891313314437866\n",
            "ITERATION_NO.: 283 LOSS_Generator: 5.668737411499023 LOSS_Discriminator: 0.0531732439994812\n",
            "ITERATION_NO.: 284 LOSS_Generator: 5.835249423980713 LOSS_Discriminator: 0.12167219817638397\n",
            "ITERATION_NO.: 285 LOSS_Generator: 5.437567234039307 LOSS_Discriminator: 0.2623026669025421\n",
            "ITERATION_NO.: 286 LOSS_Generator: 5.574780464172363 LOSS_Discriminator: 0.09830303490161896\n",
            "ITERATION_NO.: 287 LOSS_Generator: 5.442273139953613 LOSS_Discriminator: 0.15695686638355255\n",
            "ITERATION_NO.: 288 LOSS_Generator: 5.975357532501221 LOSS_Discriminator: 0.3172956705093384\n",
            "ITERATION_NO.: 289 LOSS_Generator: 5.6650776863098145 LOSS_Discriminator: 0.252131849527359\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.282376766204834 LOSS_Discriminator: 0.14924949407577515\n",
            "ITERATION_NO.: 291 LOSS_Generator: 5.398710250854492 LOSS_Discriminator: 0.16933408379554749\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.222645282745361 LOSS_Discriminator: 0.25513598322868347\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.363642692565918 LOSS_Discriminator: 0.24902024865150452\n",
            "ITERATION_NO.: 294 LOSS_Generator: 5.726163387298584 LOSS_Discriminator: 0.14536432921886444\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.8137640953063965 LOSS_Discriminator: 0.1379387378692627\n",
            "ITERATION_NO.: 296 LOSS_Generator: 6.185619354248047 LOSS_Discriminator: 0.15106557309627533\n",
            "ITERATION_NO.: 297 LOSS_Generator: 6.100996494293213 LOSS_Discriminator: 0.11902822554111481\n",
            "ITERATION_NO.: 298 LOSS_Generator: 5.897885322570801 LOSS_Discriminator: 0.24384504556655884\n",
            "ITERATION_NO.: 299 LOSS_Generator: 6.257625102996826 LOSS_Discriminator: 0.19510339200496674\n",
            "ITERATION_NO.: 300 LOSS_Generator: 5.848712921142578 LOSS_Discriminator: 0.2216479778289795\n",
            "ITERATION_NO.: 301 LOSS_Generator: 5.110617160797119 LOSS_Discriminator: 0.1783086508512497\n",
            "ITERATION_NO.: 302 LOSS_Generator: 5.231090545654297 LOSS_Discriminator: 0.15134383738040924\n",
            "ITERATION_NO.: 303 LOSS_Generator: 5.3659539222717285 LOSS_Discriminator: 0.1915358155965805\n",
            "ITERATION_NO.: 304 LOSS_Generator: 5.752873420715332 LOSS_Discriminator: 0.197914719581604\n",
            "ITERATION_NO.: 305 LOSS_Generator: 5.697511196136475 LOSS_Discriminator: 0.12393918633460999\n",
            "ITERATION_NO.: 306 LOSS_Generator: 6.190309047698975 LOSS_Discriminator: 0.2808488607406616\n",
            "ITERATION_NO.: 307 LOSS_Generator: 6.2347869873046875 LOSS_Discriminator: 0.15793557465076447\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.033535957336426 LOSS_Discriminator: 0.08266055583953857\n",
            "ITERATION_NO.: 309 LOSS_Generator: 6.374180316925049 LOSS_Discriminator: 0.08987574279308319\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.60552978515625 LOSS_Discriminator: 0.25472021102905273\n",
            "ITERATION_NO.: 311 LOSS_Generator: 6.247157573699951 LOSS_Discriminator: 0.16177257895469666\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.817162036895752 LOSS_Discriminator: 0.2934614419937134\n",
            "ITERATION_NO.: 313 LOSS_Generator: 5.814662456512451 LOSS_Discriminator: 0.13370338082313538\n",
            "ITERATION_NO.: 314 LOSS_Generator: 4.974848747253418 LOSS_Discriminator: 0.078730508685112\n",
            "ITERATION_NO.: 315 LOSS_Generator: 5.186593055725098 LOSS_Discriminator: 0.15570318698883057\n",
            "ITERATION_NO.: 316 LOSS_Generator: 4.933150291442871 LOSS_Discriminator: 0.10687418282032013\n",
            "ITERATION_NO.: 317 LOSS_Generator: 5.293581008911133 LOSS_Discriminator: 0.19466489553451538\n",
            "ITERATION_NO.: 318 LOSS_Generator: 5.665994644165039 LOSS_Discriminator: 0.13193613290786743\n",
            "ITERATION_NO.: 319 LOSS_Generator: 5.70130729675293 LOSS_Discriminator: 0.15350273251533508\n",
            "ITERATION_NO.: 320 LOSS_Generator: 5.966979503631592 LOSS_Discriminator: 0.13177096843719482\n",
            "ITERATION_NO.: 321 LOSS_Generator: 5.909024238586426 LOSS_Discriminator: 0.10647924244403839\n",
            "ITERATION_NO.: 322 LOSS_Generator: 6.0305938720703125 LOSS_Discriminator: 0.1211719736456871\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.562994480133057 LOSS_Discriminator: 0.08718908578157425\n",
            "ITERATION_NO.: 324 LOSS_Generator: 5.4867706298828125 LOSS_Discriminator: 0.1620069146156311\n",
            "ITERATION_NO.: 325 LOSS_Generator: 5.072967052459717 LOSS_Discriminator: 0.21958304941654205\n",
            "ITERATION_NO.: 326 LOSS_Generator: 4.917612075805664 LOSS_Discriminator: 0.14714986085891724\n",
            "ITERATION_NO.: 327 LOSS_Generator: 5.064262866973877 LOSS_Discriminator: 0.15990319848060608\n",
            "ITERATION_NO.: 328 LOSS_Generator: 5.046273708343506 LOSS_Discriminator: 0.2032812535762787\n",
            "ITERATION_NO.: 329 LOSS_Generator: 5.2030534744262695 LOSS_Discriminator: 0.12908092141151428\n",
            "ITERATION_NO.: 330 LOSS_Generator: 5.100309371948242 LOSS_Discriminator: 0.11660943925380707\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.636417388916016 LOSS_Discriminator: 0.19235870242118835\n",
            "ITERATION_NO.: 332 LOSS_Generator: 6.021936416625977 LOSS_Discriminator: 0.16749031841754913\n",
            "ITERATION_NO.: 333 LOSS_Generator: 5.7049431800842285 LOSS_Discriminator: 0.17072850465774536\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.582064628601074 LOSS_Discriminator: 0.17439648509025574\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.285556793212891 LOSS_Discriminator: 0.10185158252716064\n",
            "ITERATION_NO.: 336 LOSS_Generator: 5.6293487548828125 LOSS_Discriminator: 0.21159279346466064\n",
            "ITERATION_NO.: 337 LOSS_Generator: 5.574404716491699 LOSS_Discriminator: 0.1403401643037796\n",
            "ITERATION_NO.: 338 LOSS_Generator: 5.641970634460449 LOSS_Discriminator: 0.13421368598937988\n",
            "ITERATION_NO.: 339 LOSS_Generator: 5.79995584487915 LOSS_Discriminator: 0.21047256886959076\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.604250907897949 LOSS_Discriminator: 0.1669909507036209\n",
            "ITERATION_NO.: 341 LOSS_Generator: 5.069211483001709 LOSS_Discriminator: 0.20875422656536102\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.165184497833252 LOSS_Discriminator: 0.12850844860076904\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.464076042175293 LOSS_Discriminator: 0.11994244158267975\n",
            "ITERATION_NO.: 344 LOSS_Generator: 5.170916557312012 LOSS_Discriminator: 0.1471489518880844\n",
            "ITERATION_NO.: 345 LOSS_Generator: 5.360723495483398 LOSS_Discriminator: 0.09754107147455215\n",
            "ITERATION_NO.: 346 LOSS_Generator: 5.216766834259033 LOSS_Discriminator: 0.09178657829761505\n",
            "ITERATION_NO.: 347 LOSS_Generator: 5.534277439117432 LOSS_Discriminator: 0.11743947118520737\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.430160999298096 LOSS_Discriminator: 0.1432916522026062\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.765789985656738 LOSS_Discriminator: 0.07347489148378372\n",
            "ITERATION_NO.: 350 LOSS_Generator: 5.7638373374938965 LOSS_Discriminator: 0.1792524755001068\n",
            "ITERATION_NO.: 351 LOSS_Generator: 6.188066482543945 LOSS_Discriminator: 0.15171805024147034\n",
            "ITERATION_NO.: 352 LOSS_Generator: 6.031980514526367 LOSS_Discriminator: 0.18068979680538177\n",
            "ITERATION_NO.: 353 LOSS_Generator: 5.973852634429932 LOSS_Discriminator: 0.1632622331380844\n",
            "ITERATION_NO.: 354 LOSS_Generator: 5.7949113845825195 LOSS_Discriminator: 0.16959358751773834\n",
            "ITERATION_NO.: 355 LOSS_Generator: 6.002229690551758 LOSS_Discriminator: 0.07819700986146927\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.841813564300537 LOSS_Discriminator: 0.2354447990655899\n",
            "ITERATION_NO.: 357 LOSS_Generator: 5.715451240539551 LOSS_Discriminator: 0.12264261394739151\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.371761322021484 LOSS_Discriminator: 0.16801515221595764\n",
            "ITERATION_NO.: 359 LOSS_Generator: 4.862354278564453 LOSS_Discriminator: 0.2066175639629364\n",
            "ITERATION_NO.: 360 LOSS_Generator: 5.467889785766602 LOSS_Discriminator: 0.13565272092819214\n",
            "ITERATION_NO.: 361 LOSS_Generator: 5.318309783935547 LOSS_Discriminator: 0.11831118166446686\n",
            "ITERATION_NO.: 362 LOSS_Generator: 5.522325038909912 LOSS_Discriminator: 0.15676124393939972\n",
            "ITERATION_NO.: 363 LOSS_Generator: 5.452700138092041 LOSS_Discriminator: 0.10204917937517166\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.7096710205078125 LOSS_Discriminator: 0.08050121366977692\n",
            "ITERATION_NO.: 365 LOSS_Generator: 6.002164363861084 LOSS_Discriminator: 0.12826521694660187\n",
            "ITERATION_NO.: 366 LOSS_Generator: 5.931812763214111 LOSS_Discriminator: 0.13638246059417725\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.203945159912109 LOSS_Discriminator: 0.23441612720489502\n",
            "ITERATION_NO.: 368 LOSS_Generator: 5.849714756011963 LOSS_Discriminator: 0.10785114765167236\n",
            "ITERATION_NO.: 369 LOSS_Generator: 5.484735488891602 LOSS_Discriminator: 0.1540563553571701\n",
            "ITERATION_NO.: 370 LOSS_Generator: 5.522956371307373 LOSS_Discriminator: 0.07304692268371582\n",
            "ITERATION_NO.: 371 LOSS_Generator: 5.869490146636963 LOSS_Discriminator: 0.16446246206760406\n",
            "ITERATION_NO.: 372 LOSS_Generator: 5.231833457946777 LOSS_Discriminator: 0.08988405764102936\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.52229118347168 LOSS_Discriminator: 0.15037086606025696\n",
            "ITERATION_NO.: 374 LOSS_Generator: 5.683567523956299 LOSS_Discriminator: 0.16202910244464874\n",
            "ITERATION_NO.: 375 LOSS_Generator: 5.7713470458984375 LOSS_Discriminator: 0.11596933752298355\n",
            "ITERATION_NO.: 376 LOSS_Generator: 5.728759765625 LOSS_Discriminator: 0.06022898107767105\n",
            "ITERATION_NO.: 377 LOSS_Generator: 6.107776641845703 LOSS_Discriminator: 0.12954792380332947\n",
            "ITERATION_NO.: 378 LOSS_Generator: 5.6022491455078125 LOSS_Discriminator: 0.13775332272052765\n",
            "ITERATION_NO.: 379 LOSS_Generator: 5.6642351150512695 LOSS_Discriminator: 0.10119786113500595\n",
            "ITERATION_NO.: 380 LOSS_Generator: 5.704573154449463 LOSS_Discriminator: 0.1549476981163025\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.607830047607422 LOSS_Discriminator: 0.14779901504516602\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.587247848510742 LOSS_Discriminator: 0.17025253176689148\n",
            "ITERATION_NO.: 383 LOSS_Generator: 5.366885185241699 LOSS_Discriminator: 0.0997021347284317\n",
            "ITERATION_NO.: 384 LOSS_Generator: 5.247064113616943 LOSS_Discriminator: 0.19194406270980835\n",
            "ITERATION_NO.: 385 LOSS_Generator: 5.398492813110352 LOSS_Discriminator: 0.10972212255001068\n",
            "ITERATION_NO.: 386 LOSS_Generator: 5.370304584503174 LOSS_Discriminator: 0.1321505904197693\n",
            "ITERATION_NO.: 387 LOSS_Generator: 5.2809929847717285 LOSS_Discriminator: 0.08081340789794922\n",
            "ITERATION_NO.: 388 LOSS_Generator: 5.5125412940979 LOSS_Discriminator: 0.1685284674167633\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.463686466217041 LOSS_Discriminator: 0.19232439994812012\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.8394927978515625 LOSS_Discriminator: 0.05199647322297096\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.036569118499756 LOSS_Discriminator: 0.2400464117527008\n",
            "ITERATION_NO.: 392 LOSS_Generator: 5.81428337097168 LOSS_Discriminator: 0.11023958027362823\n",
            "ITERATION_NO.: 393 LOSS_Generator: 5.783092498779297 LOSS_Discriminator: 0.15899792313575745\n",
            "ITERATION_NO.: 394 LOSS_Generator: 5.723163604736328 LOSS_Discriminator: 0.24031884968280792\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.5002336502075195 LOSS_Discriminator: 0.13988104462623596\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.215522289276123 LOSS_Discriminator: 0.09808364510536194\n",
            "ITERATION_NO.: 397 LOSS_Generator: 5.187623500823975 LOSS_Discriminator: 0.20022694766521454\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.009674072265625 LOSS_Discriminator: 0.18383580446243286\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.28277587890625 LOSS_Discriminator: 0.21003228425979614\n",
            "ITERATION_NO.: 400 LOSS_Generator: 5.678277492523193 LOSS_Discriminator: 0.10148996114730835\n",
            "ITERATION_NO.: 401 LOSS_Generator: 5.750158309936523 LOSS_Discriminator: 0.16367819905281067\n",
            "ITERATION_NO.: 402 LOSS_Generator: 5.961181640625 LOSS_Discriminator: 0.2454347014427185\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.186566352844238 LOSS_Discriminator: 0.1533709168434143\n",
            "ITERATION_NO.: 404 LOSS_Generator: 6.063981533050537 LOSS_Discriminator: 0.13700762391090393\n",
            "ITERATION_NO.: 405 LOSS_Generator: 5.422732353210449 LOSS_Discriminator: 0.21278122067451477\n",
            "ITERATION_NO.: 406 LOSS_Generator: 5.445305347442627 LOSS_Discriminator: 0.1394323855638504\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.048463821411133 LOSS_Discriminator: 0.11410261690616608\n",
            "ITERATION_NO.: 408 LOSS_Generator: 5.015900611877441 LOSS_Discriminator: 0.13715994358062744\n",
            "ITERATION_NO.: 409 LOSS_Generator: 5.259100437164307 LOSS_Discriminator: 0.12294938415288925\n",
            "ITERATION_NO.: 410 LOSS_Generator: 5.622974395751953 LOSS_Discriminator: 0.19771809875965118\n",
            "ITERATION_NO.: 411 LOSS_Generator: 4.975646495819092 LOSS_Discriminator: 0.10311971604824066\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.364185333251953 LOSS_Discriminator: 0.17204302549362183\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.8494486808776855 LOSS_Discriminator: 0.14160367846488953\n",
            "ITERATION_NO.: 414 LOSS_Generator: 5.753115653991699 LOSS_Discriminator: 0.08787399530410767\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.38067626953125 LOSS_Discriminator: 0.20075872540473938\n",
            "ITERATION_NO.: 416 LOSS_Generator: 6.208334922790527 LOSS_Discriminator: 0.2048042267560959\n",
            "ITERATION_NO.: 417 LOSS_Generator: 6.050804615020752 LOSS_Discriminator: 0.22495457530021667\n",
            "ITERATION_NO.: 418 LOSS_Generator: 5.826284408569336 LOSS_Discriminator: 0.13753533363342285\n",
            "ITERATION_NO.: 419 LOSS_Generator: 5.454893112182617 LOSS_Discriminator: 0.17409281432628632\n",
            "ITERATION_NO.: 420 LOSS_Generator: 4.985945224761963 LOSS_Discriminator: 0.1402319371700287\n",
            "ITERATION_NO.: 421 LOSS_Generator: 4.642171382904053 LOSS_Discriminator: 0.22303932905197144\n",
            "ITERATION_NO.: 422 LOSS_Generator: 4.564022064208984 LOSS_Discriminator: 0.22951020300388336\n",
            "ITERATION_NO.: 423 LOSS_Generator: 4.970020771026611 LOSS_Discriminator: 0.21719133853912354\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.172046661376953 LOSS_Discriminator: 0.35581958293914795\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.676919460296631 LOSS_Discriminator: 0.1025809496641159\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.508538246154785 LOSS_Discriminator: 0.16818664968013763\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.791497707366943 LOSS_Discriminator: 0.133013054728508\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.194406509399414 LOSS_Discriminator: 0.2933741807937622\n",
            "ITERATION_NO.: 429 LOSS_Generator: 6.0558648109436035 LOSS_Discriminator: 0.23534302413463593\n",
            "ITERATION_NO.: 430 LOSS_Generator: 5.710379600524902 LOSS_Discriminator: 0.14677318930625916\n",
            "ITERATION_NO.: 431 LOSS_Generator: 5.010282516479492 LOSS_Discriminator: 0.14625245332717896\n",
            "ITERATION_NO.: 432 LOSS_Generator: 4.58074426651001 LOSS_Discriminator: 0.20636872947216034\n",
            "ITERATION_NO.: 433 LOSS_Generator: 4.846921920776367 LOSS_Discriminator: 0.2510501742362976\n",
            "ITERATION_NO.: 434 LOSS_Generator: 4.753152847290039 LOSS_Discriminator: 0.1044047400355339\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.0323615074157715 LOSS_Discriminator: 0.15041975677013397\n",
            "ITERATION_NO.: 436 LOSS_Generator: 5.150780200958252 LOSS_Discriminator: 0.1935575008392334\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.781714916229248 LOSS_Discriminator: 0.23618397116661072\n",
            "ITERATION_NO.: 438 LOSS_Generator: 5.917203426361084 LOSS_Discriminator: 0.1611342430114746\n",
            "ITERATION_NO.: 439 LOSS_Generator: 5.806481838226318 LOSS_Discriminator: 0.13071982562541962\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.009399890899658 LOSS_Discriminator: 0.21587246656417847\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.360885143280029 LOSS_Discriminator: 0.13698604702949524\n",
            "ITERATION_NO.: 442 LOSS_Generator: 5.8108978271484375 LOSS_Discriminator: 0.2953794300556183\n",
            "ITERATION_NO.: 443 LOSS_Generator: 5.232258319854736 LOSS_Discriminator: 0.09296557307243347\n",
            "ITERATION_NO.: 444 LOSS_Generator: 5.3199357986450195 LOSS_Discriminator: 0.11797785758972168\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.133936882019043 LOSS_Discriminator: 0.14644873142242432\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.2447004318237305 LOSS_Discriminator: 0.1532481163740158\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.343525409698486 LOSS_Discriminator: 0.18321913480758667\n",
            "ITERATION_NO.: 448 LOSS_Generator: 4.7112627029418945 LOSS_Discriminator: 0.08549520373344421\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.479364395141602 LOSS_Discriminator: 0.07016550004482269\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.174175262451172 LOSS_Discriminator: 0.21302005648612976\n",
            "ITERATION_NO.: 451 LOSS_Generator: 5.2900710105896 LOSS_Discriminator: 0.13397768139839172\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.003322601318359 LOSS_Discriminator: 0.17723101377487183\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.949934005737305 LOSS_Discriminator: 0.14626699686050415\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.981624603271484 LOSS_Discriminator: 0.19104476273059845\n",
            "ITERATION_NO.: 455 LOSS_Generator: 5.9165239334106445 LOSS_Discriminator: 0.10158748179674149\n",
            "ITERATION_NO.: 456 LOSS_Generator: 5.929243087768555 LOSS_Discriminator: 0.08057113736867905\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.455235481262207 LOSS_Discriminator: 0.15493977069854736\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.3915696144104 LOSS_Discriminator: 0.19723396003246307\n",
            "ITERATION_NO.: 459 LOSS_Generator: 5.141737461090088 LOSS_Discriminator: 0.18567287921905518\n",
            "ITERATION_NO.: 460 LOSS_Generator: 4.713951110839844 LOSS_Discriminator: 0.16568145155906677\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.008091926574707 LOSS_Discriminator: 0.2143523395061493\n",
            "ITERATION_NO.: 462 LOSS_Generator: 4.803619384765625 LOSS_Discriminator: 0.16596601903438568\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.091547012329102 LOSS_Discriminator: 0.13022944331169128\n",
            "ITERATION_NO.: 464 LOSS_Generator: 5.028960227966309 LOSS_Discriminator: 0.20869563519954681\n",
            "ITERATION_NO.: 465 LOSS_Generator: 5.531801700592041 LOSS_Discriminator: 0.17150741815567017\n",
            "ITERATION_NO.: 466 LOSS_Generator: 5.4189453125 LOSS_Discriminator: 0.18908128142356873\n",
            "ITERATION_NO.: 467 LOSS_Generator: 5.614260196685791 LOSS_Discriminator: 0.18899577856063843\n",
            "ITERATION_NO.: 468 LOSS_Generator: 5.9255805015563965 LOSS_Discriminator: 0.20685473084449768\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.4918060302734375 LOSS_Discriminator: 0.07885823398828506\n",
            "ITERATION_NO.: 470 LOSS_Generator: 5.613613128662109 LOSS_Discriminator: 0.09019649028778076\n",
            "ITERATION_NO.: 471 LOSS_Generator: 4.968988418579102 LOSS_Discriminator: 0.15018710494041443\n",
            "ITERATION_NO.: 472 LOSS_Generator: 4.979181289672852 LOSS_Discriminator: 0.15540939569473267\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.123688220977783 LOSS_Discriminator: 0.11584368348121643\n",
            "ITERATION_NO.: 474 LOSS_Generator: 4.827105522155762 LOSS_Discriminator: 0.08456771075725555\n",
            "ITERATION_NO.: 475 LOSS_Generator: 4.630037784576416 LOSS_Discriminator: 0.16822463274002075\n",
            "ITERATION_NO.: 476 LOSS_Generator: 4.671777248382568 LOSS_Discriminator: 0.10252019017934799\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.096800804138184 LOSS_Discriminator: 0.17789626121520996\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.279852867126465 LOSS_Discriminator: 0.08950259536504745\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.735347270965576 LOSS_Discriminator: 0.22871920466423035\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.7790913581848145 LOSS_Discriminator: 0.2567305564880371\n",
            "ITERATION_NO.: 481 LOSS_Generator: 5.869361400604248 LOSS_Discriminator: 0.10677271336317062\n",
            "ITERATION_NO.: 482 LOSS_Generator: 6.157931327819824 LOSS_Discriminator: 0.11811425536870956\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.50926399230957 LOSS_Discriminator: 0.12743832170963287\n",
            "ITERATION_NO.: 484 LOSS_Generator: 5.60922384262085 LOSS_Discriminator: 0.12300232797861099\n",
            "ITERATION_NO.: 485 LOSS_Generator: 5.354743480682373 LOSS_Discriminator: 0.13022935390472412\n",
            "ITERATION_NO.: 486 LOSS_Generator: 4.7918291091918945 LOSS_Discriminator: 0.07607632130384445\n",
            "ITERATION_NO.: 487 LOSS_Generator: 4.269044399261475 LOSS_Discriminator: 0.18666256964206696\n",
            "ITERATION_NO.: 488 LOSS_Generator: 4.876917839050293 LOSS_Discriminator: 0.12221741676330566\n",
            "ITERATION_NO.: 489 LOSS_Generator: 4.848745822906494 LOSS_Discriminator: 0.10895056277513504\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.071816921234131 LOSS_Discriminator: 0.1427496075630188\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.398725986480713 LOSS_Discriminator: 0.2587822675704956\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.975114822387695 LOSS_Discriminator: 0.10781202465295792\n",
            "ITERATION_NO.: 493 LOSS_Generator: 5.926496505737305 LOSS_Discriminator: 0.05967521294951439\n",
            "ITERATION_NO.: 494 LOSS_Generator: 5.880795955657959 LOSS_Discriminator: 0.09148924052715302\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.905925750732422 LOSS_Discriminator: 0.19871553778648376\n",
            "ITERATION_NO.: 496 LOSS_Generator: 5.587486743927002 LOSS_Discriminator: 0.1495036780834198\n",
            "ITERATION_NO.: 497 LOSS_Generator: 5.41756534576416 LOSS_Discriminator: 0.16639599204063416\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.525813102722168 LOSS_Discriminator: 0.11551452428102493\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.138859748840332 LOSS_Discriminator: 0.11351543664932251\n",
            "ITERATION_NO.: 500 LOSS_Generator: 5.357908725738525 LOSS_Discriminator: 0.12028683722019196\n",
            "ITERATION_NO.: 501 LOSS_Generator: 5.3207221031188965 LOSS_Discriminator: 0.12495726346969604\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.090549468994141 LOSS_Discriminator: 0.2116428017616272\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.182320594787598 LOSS_Discriminator: 0.1615784913301468\n",
            "ITERATION_NO.: 504 LOSS_Generator: 5.175539970397949 LOSS_Discriminator: 0.13214066624641418\n",
            "ITERATION_NO.: 505 LOSS_Generator: 5.850900173187256 LOSS_Discriminator: 0.15542927384376526\n",
            "ITERATION_NO.: 506 LOSS_Generator: 5.883907318115234 LOSS_Discriminator: 0.17286066710948944\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.706625938415527 LOSS_Discriminator: 0.259532630443573\n",
            "ITERATION_NO.: 508 LOSS_Generator: 5.564742565155029 LOSS_Discriminator: 0.13986027240753174\n",
            "ITERATION_NO.: 509 LOSS_Generator: 5.371028900146484 LOSS_Discriminator: 0.1023581400513649\n",
            "ITERATION_NO.: 510 LOSS_Generator: 5.566646099090576 LOSS_Discriminator: 0.10808204114437103\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.357606887817383 LOSS_Discriminator: 0.08165926486253738\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.526610851287842 LOSS_Discriminator: 0.1859157383441925\n",
            "ITERATION_NO.: 513 LOSS_Generator: 5.015213489532471 LOSS_Discriminator: 0.12420938909053802\n",
            "ITERATION_NO.: 514 LOSS_Generator: 5.277624130249023 LOSS_Discriminator: 0.1312098205089569\n",
            "ITERATION_NO.: 515 LOSS_Generator: 5.710728645324707 LOSS_Discriminator: 0.23618243634700775\n",
            "ITERATION_NO.: 516 LOSS_Generator: 5.912686347961426 LOSS_Discriminator: 0.1676846146583557\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.756656646728516 LOSS_Discriminator: 0.12398280203342438\n",
            "ITERATION_NO.: 518 LOSS_Generator: 6.110808849334717 LOSS_Discriminator: 0.137839674949646\n",
            "ITERATION_NO.: 519 LOSS_Generator: 6.031019687652588 LOSS_Discriminator: 0.24936500191688538\n",
            "ITERATION_NO.: 520 LOSS_Generator: 6.135551929473877 LOSS_Discriminator: 0.0809006541967392\n",
            "ITERATION_NO.: 521 LOSS_Generator: 5.751079082489014 LOSS_Discriminator: 0.09737759828567505\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.472930908203125 LOSS_Discriminator: 0.08746825158596039\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.475040912628174 LOSS_Discriminator: 0.1513470709323883\n",
            "ITERATION_NO.: 524 LOSS_Generator: 5.225407600402832 LOSS_Discriminator: 0.12389927357435226\n",
            "ITERATION_NO.: 525 LOSS_Generator: 4.743143081665039 LOSS_Discriminator: 0.1592513918876648\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.191972732543945 LOSS_Discriminator: 0.11717879772186279\n",
            "ITERATION_NO.: 527 LOSS_Generator: 4.514151096343994 LOSS_Discriminator: 0.2685130536556244\n",
            "ITERATION_NO.: 528 LOSS_Generator: 4.816921234130859 LOSS_Discriminator: 0.13891775906085968\n",
            "ITERATION_NO.: 529 LOSS_Generator: 4.955316066741943 LOSS_Discriminator: 0.2175961136817932\n",
            "ITERATION_NO.: 530 LOSS_Generator: 5.5282979011535645 LOSS_Discriminator: 0.2162891924381256\n",
            "ITERATION_NO.: 531 LOSS_Generator: 5.886997699737549 LOSS_Discriminator: 0.169545978307724\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.116541862487793 LOSS_Discriminator: 0.24746422469615936\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.6815266609191895 LOSS_Discriminator: 0.3013074994087219\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.677766799926758 LOSS_Discriminator: 0.16049377620220184\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.248727798461914 LOSS_Discriminator: 0.27201104164123535\n",
            "ITERATION_NO.: 536 LOSS_Generator: 5.184843063354492 LOSS_Discriminator: 0.1869710087776184\n",
            "ITERATION_NO.: 537 LOSS_Generator: 5.201673984527588 LOSS_Discriminator: 0.13741075992584229\n",
            "ITERATION_NO.: 538 LOSS_Generator: 5.574519157409668 LOSS_Discriminator: 0.10338914394378662\n",
            "ITERATION_NO.: 539 LOSS_Generator: 5.384059429168701 LOSS_Discriminator: 0.15417805314064026\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.676729679107666 LOSS_Discriminator: 0.17164239287376404\n",
            "ITERATION_NO.: 541 LOSS_Generator: 5.8543701171875 LOSS_Discriminator: 0.12821871042251587\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.646168231964111 LOSS_Discriminator: 0.10409753769636154\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.044673919677734 LOSS_Discriminator: 0.14622437953948975\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.975222110748291 LOSS_Discriminator: 0.11336568742990494\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.102983474731445 LOSS_Discriminator: 0.1078546866774559\n",
            "ITERATION_NO.: 546 LOSS_Generator: 5.541049003601074 LOSS_Discriminator: 0.17579641938209534\n",
            "ITERATION_NO.: 547 LOSS_Generator: 5.3639068603515625 LOSS_Discriminator: 0.18245550990104675\n",
            "ITERATION_NO.: 548 LOSS_Generator: 4.720431327819824 LOSS_Discriminator: 0.18839767575263977\n",
            "ITERATION_NO.: 549 LOSS_Generator: 5.084524154663086 LOSS_Discriminator: 0.08963593095541\n",
            "ITERATION_NO.: 550 LOSS_Generator: 4.839261054992676 LOSS_Discriminator: 0.08923852443695068\n",
            "ITERATION_NO.: 551 LOSS_Generator: 5.585020065307617 LOSS_Discriminator: 0.1816750168800354\n",
            "ITERATION_NO.: 552 LOSS_Generator: 5.648312568664551 LOSS_Discriminator: 0.11796534061431885\n",
            "ITERATION_NO.: 553 LOSS_Generator: 5.6970672607421875 LOSS_Discriminator: 0.0828293114900589\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.075836181640625 LOSS_Discriminator: 0.09931846708059311\n",
            "ITERATION_NO.: 555 LOSS_Generator: 5.8041815757751465 LOSS_Discriminator: 0.17135024070739746\n",
            "ITERATION_NO.: 556 LOSS_Generator: 6.102973461151123 LOSS_Discriminator: 0.13617348670959473\n",
            "ITERATION_NO.: 557 LOSS_Generator: 5.919935703277588 LOSS_Discriminator: 0.10578784346580505\n",
            "ITERATION_NO.: 558 LOSS_Generator: 5.8755879402160645 LOSS_Discriminator: 0.08663448691368103\n",
            "ITERATION_NO.: 559 LOSS_Generator: 5.571040630340576 LOSS_Discriminator: 0.11847531795501709\n",
            "ITERATION_NO.: 560 LOSS_Generator: 5.4247260093688965 LOSS_Discriminator: 0.09978872537612915\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.050428867340088 LOSS_Discriminator: 0.13311362266540527\n",
            "ITERATION_NO.: 562 LOSS_Generator: 5.6703667640686035 LOSS_Discriminator: 0.06400743126869202\n",
            "ITERATION_NO.: 563 LOSS_Generator: 5.274425506591797 LOSS_Discriminator: 0.19001051783561707\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.477787971496582 LOSS_Discriminator: 0.09798449277877808\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.816277503967285 LOSS_Discriminator: 0.11756947636604309\n",
            "ITERATION_NO.: 566 LOSS_Generator: 6.010312080383301 LOSS_Discriminator: 0.20669487118721008\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.947378635406494 LOSS_Discriminator: 0.05592101067304611\n",
            "ITERATION_NO.: 568 LOSS_Generator: 6.8778157234191895 LOSS_Discriminator: 0.22238464653491974\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.99087381362915 LOSS_Discriminator: 0.1368543654680252\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.896032333374023 LOSS_Discriminator: 0.1822175830602646\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.379058361053467 LOSS_Discriminator: 0.24559541046619415\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.907832622528076 LOSS_Discriminator: 0.14653506875038147\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.541070938110352 LOSS_Discriminator: 0.10573767125606537\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.215106964111328 LOSS_Discriminator: 0.1487068235874176\n",
            "ITERATION_NO.: 575 LOSS_Generator: 4.931715488433838 LOSS_Discriminator: 0.143624410033226\n",
            "ITERATION_NO.: 576 LOSS_Generator: 4.843997478485107 LOSS_Discriminator: 0.2787540853023529\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.1281843185424805 LOSS_Discriminator: 0.1097356379032135\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.074087142944336 LOSS_Discriminator: 0.114077627658844\n",
            "ITERATION_NO.: 579 LOSS_Generator: 5.387005805969238 LOSS_Discriminator: 0.09303944557905197\n",
            "ITERATION_NO.: 580 LOSS_Generator: 5.455284595489502 LOSS_Discriminator: 0.0868200734257698\n",
            "ITERATION_NO.: 581 LOSS_Generator: 6.012721538543701 LOSS_Discriminator: 0.16419780254364014\n",
            "ITERATION_NO.: 582 LOSS_Generator: 5.887932777404785 LOSS_Discriminator: 0.19175900518894196\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.0445733070373535 LOSS_Discriminator: 0.12848684191703796\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.118196964263916 LOSS_Discriminator: 0.1185954287648201\n",
            "ITERATION_NO.: 585 LOSS_Generator: 5.4387407302856445 LOSS_Discriminator: 0.046191707253456116\n",
            "ITERATION_NO.: 586 LOSS_Generator: 5.669147968292236 LOSS_Discriminator: 0.07819908857345581\n",
            "ITERATION_NO.: 587 LOSS_Generator: 5.614879131317139 LOSS_Discriminator: 0.12950146198272705\n",
            "ITERATION_NO.: 588 LOSS_Generator: 5.413248062133789 LOSS_Discriminator: 0.09336218237876892\n",
            "ITERATION_NO.: 589 LOSS_Generator: 5.25681209564209 LOSS_Discriminator: 0.1057191789150238\n",
            "ITERATION_NO.: 590 LOSS_Generator: 5.522564888000488 LOSS_Discriminator: 0.14457133412361145\n",
            "ITERATION_NO.: 591 LOSS_Generator: 5.764845371246338 LOSS_Discriminator: 0.11208506673574448\n",
            "ITERATION_NO.: 592 LOSS_Generator: 5.735913276672363 LOSS_Discriminator: 0.22574609518051147\n",
            "ITERATION_NO.: 593 LOSS_Generator: 5.861365795135498 LOSS_Discriminator: 0.14105956256389618\n",
            "ITERATION_NO.: 594 LOSS_Generator: 6.143918037414551 LOSS_Discriminator: 0.09801296889781952\n",
            "ITERATION_NO.: 595 LOSS_Generator: 6.0648627281188965 LOSS_Discriminator: 0.09178999811410904\n",
            "ITERATION_NO.: 596 LOSS_Generator: 5.9471235275268555 LOSS_Discriminator: 0.15529018640518188\n",
            "ITERATION_NO.: 597 LOSS_Generator: 5.5262837409973145 LOSS_Discriminator: 0.14303196966648102\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.518598556518555 LOSS_Discriminator: 0.24408316612243652\n",
            "ITERATION_NO.: 599 LOSS_Generator: 5.406304359436035 LOSS_Discriminator: 0.07557046413421631\n",
            "ITERATION_NO.: 600 LOSS_Generator: 5.4012532234191895 LOSS_Discriminator: 0.18938177824020386\n",
            "EPOCH OVER: 9\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyEMIoeKiyEMIoeJ+A0QvTzwdhwtdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Image_no 1\n",
            "Image_no 2\n",
            "Image_no 3\n",
            "Image_no 4\n",
            "Image_no 5\n",
            "Image_no 6\n",
            "Image_no 7\n",
            "Image_no 8\n",
            "Image_no 9\n",
            "Image_no 10\n",
            "Image_no 11\n",
            "Image_no 12\n",
            "Image_no 13\n",
            "Image_no 14\n",
            "Image_no 15\n",
            "Image_no 16\n",
            "Image_no 17\n",
            "Image_no 18\n",
            "Image_no 19\n",
            "Image_no 20\n",
            "Image_no 21\n",
            "Image_no 22\n",
            "Image_no 23\n",
            "Image_no 24\n",
            "Image_no 25\n",
            "Image_no 26\n",
            "Image_no 27\n",
            "Image_no 28\n",
            "Image_no 29\n",
            "Image_no 30\n",
            "Image_no 31\n",
            "Image_no 32\n",
            "Image_no 33\n",
            "Image_no 34\n",
            "Image_no 35\n",
            "Image_no 36\n",
            "Image_no 37\n",
            "Image_no 38\n",
            "Image_no 39\n",
            "Image_no 40\n",
            "Image_no 41\n",
            "Image_no 42\n",
            "Image_no 43\n",
            "Image_no 44\n",
            "Image_no 45\n",
            "Image_no 46\n",
            "Image_no 47\n",
            "Image_no 48\n",
            "Image_no 49\n",
            "Image_no 50\n",
            "Image_no 51\n",
            "Image_no 52\n",
            "Image_no 53\n",
            "Image_no 54\n",
            "Image_no 55\n",
            "Image_no 56\n",
            "Image_no 57\n",
            "Image_no 58\n",
            "Image_no 59\n",
            "Image_no 60\n",
            "Image_no 61\n",
            "Image_no 62\n",
            "Image_no 63\n",
            "Image_no 64\n",
            "Image_no 65\n",
            "Image_no 66\n",
            "Image_no 67\n",
            "Image_no 68\n",
            "Image_no 69\n",
            "Image_no 70\n",
            "Image_no 71\n",
            "Image_no 72\n",
            "Image_no 73\n",
            "Image_no 74\n",
            "Image_no 75\n",
            "Image_no 76\n",
            "Image_no 77\n",
            "Image_no 78\n",
            "Image_no 79\n",
            "Image_no 80\n",
            "Image_no 81\n",
            "Image_no 82\n",
            "Image_no 83\n",
            "Image_no 84\n",
            "Image_no 85\n",
            "Image_no 86\n",
            "Image_no 87\n",
            "Image_no 88\n",
            "Image_no 89\n",
            "Image_no 90\n",
            "Image_no 91\n",
            "Image_no 92\n",
            "Image_no 93\n",
            "Image_no 94\n",
            "Image_no 95\n",
            "Image_no 96\n",
            "Image_no 97\n",
            "Image_no 98\n",
            "Image_no 99\n",
            "Image_no 100\n",
            "Image_no 101\n",
            "Image_no 102\n",
            "Image_no 103\n",
            "Image_no 104\n",
            "Image_no 105\n",
            "Image_no 106\n",
            "Image_no 107\n",
            "Image_no 108\n",
            "Image_no 109\n",
            "Image_no 110\n",
            "Image_no 111\n",
            "Image_no 112\n",
            "Image_no 113\n",
            "Image_no 114\n",
            "Image_no 115\n",
            "Image_no 116\n",
            "Image_no 117\n",
            "Image_no 118\n",
            "Image_no 119\n",
            "Image_no 120\n",
            "Image_no 121\n",
            "Image_no 122\n",
            "Image_no 123\n",
            "Image_no 124\n",
            "Image_no 125\n",
            "Image_no 126\n",
            "Image_no 127\n",
            "Image_no 128\n",
            "Image_no 129\n",
            "Image_no 130\n",
            "Image_no 131\n",
            "Image_no 132\n",
            "Image_no 133\n",
            "Image_no 134\n",
            "Image_no 135\n",
            "Image_no 136\n",
            "Image_no 137\n",
            "Image_no 138\n",
            "Image_no 139\n",
            "Image_no 140\n",
            "Image_no 141\n",
            "Image_no 142\n",
            "Image_no 143\n",
            "Image_no 144\n",
            "Image_no 145\n",
            "Image_no 146\n",
            "Image_no 147\n",
            "Image_no 148\n",
            "Image_no 149\n",
            "Image_no 150\n",
            "Image_no 151\n",
            "Image_no 152\n",
            "Image_no 153\n",
            "Image_no 154\n",
            "Image_no 155\n",
            "Image_no 156\n",
            "Image_no 157\n",
            "Image_no 158\n",
            "Image_no 159\n",
            "Image_no 160\n",
            "Image_no 161\n",
            "Image_no 162\n",
            "Image_no 163\n",
            "Image_no 164\n",
            "Image_no 165\n",
            "Image_no 166\n",
            "Image_no 167\n",
            "Image_no 168\n",
            "Image_no 169\n",
            "Image_no 170\n",
            "Image_no 171\n",
            "Image_no 172\n",
            "Image_no 173\n",
            "Image_no 174\n",
            "Image_no 175\n",
            "Image_no 176\n",
            "Image_no 177\n",
            "Image_no 178\n",
            "Image_no 179\n",
            "Image_no 180\n",
            "Image_no 181\n",
            "Image_no 182\n",
            "Image_no 183\n",
            "Image_no 184\n",
            "Image_no 185\n",
            "Image_no 186\n",
            "Image_no 187\n",
            "Image_no 188\n",
            "Image_no 189\n",
            "Image_no 190\n",
            "Image_no 191\n",
            "Image_no 192\n",
            "Image_no 193\n",
            "Image_no 194\n",
            "Image_no 195\n",
            "Image_no 196\n",
            "Image_no 197\n",
            "Image_no 198\n",
            "Image_no 199\n",
            "Image_no 200\n",
            "Image_no 201\n",
            "Image_no 202\n",
            "Image_no 203\n",
            "Image_no 204\n",
            "Image_no 205\n",
            "Image_no 206\n",
            "Image_no 207\n",
            "Image_no 208\n",
            "Image_no 209\n",
            "Image_no 210\n",
            "Image_no 211\n",
            "Image_no 212\n",
            "Image_no 213\n",
            "Image_no 214\n",
            "Image_no 215\n",
            "Image_no 216\n",
            "Image_no 217\n",
            "Image_no 218\n",
            "Image_no 219\n",
            "Image_no 220\n",
            "Image_no 221\n",
            "Image_no 222\n",
            "Image_no 223\n",
            "Image_no 224\n",
            "Image_no 225\n",
            "Image_no 226\n",
            "Image_no 227\n",
            "Image_no 228\n",
            "Image_no 229\n",
            "Image_no 230\n",
            "Image_no 231\n",
            "Image_no 232\n",
            "Image_no 233\n",
            "Image_no 234\n",
            "Image_no 235\n",
            "Image_no 236\n",
            "Image_no 237\n",
            "Image_no 238\n",
            "Image_no 239\n",
            "Image_no 240\n",
            "Image_no 241\n",
            "Image_no 242\n",
            "Image_no 243\n",
            "Image_no 244\n",
            "Image_no 245\n",
            "Image_no 246\n",
            "Image_no 247\n",
            "Image_no 248\n",
            "Image_no 249\n",
            "Image_no 250\n",
            "Image_no 251\n",
            "Image_no 252\n",
            "Image_no 253\n",
            "Image_no 254\n",
            "Image_no 255\n",
            "Image_no 256\n",
            "Image_no 257\n",
            "Image_no 258\n",
            "Image_no 259\n",
            "Image_no 260\n",
            "Image_no 261\n",
            "Image_no 262\n",
            "Image_no 263\n",
            "Image_no 264\n",
            "Image_no 265\n",
            "Image_no 266\n",
            "Image_no 267\n",
            "Image_no 268\n",
            "Image_no 269\n",
            "Image_no 270\n",
            "Image_no 271\n",
            "Image_no 272\n",
            "Image_no 273\n",
            "Image_no 274\n",
            "Image_no 275\n",
            "Image_no 276\n",
            "Image_no 277\n",
            "Image_no 278\n",
            "Image_no 279\n",
            "Image_no 280\n",
            "Image_no 281\n",
            "Image_no 282\n",
            "Image_no 283\n",
            "Image_no 284\n",
            "Image_no 285\n",
            "Image_no 286\n",
            "Image_no 287\n",
            "Image_no 288\n",
            "Image_no 289\n",
            "Image_no 290\n",
            "Image_no 291\n",
            "Image_no 292\n",
            "Image_no 293\n",
            "Image_no 294\n",
            "Image_no 295\n",
            "Image_no 296\n",
            "Image_no 297\n",
            "Image_no 298\n",
            "Image_no 299\n",
            "Image_no 300\n",
            "Image_no 301\n",
            "Image_no 302\n",
            "Image_no 303\n",
            "Image_no 304\n",
            "Image_no 305\n",
            "Image_no 306\n",
            "Image_no 307\n",
            "Image_no 308\n",
            "Image_no 309\n",
            "Image_no 310\n",
            "Image_no 311\n",
            "Image_no 312\n",
            "Image_no 313\n",
            "Image_no 314\n",
            "Image_no 315\n",
            "Image_no 316\n",
            "Image_no 317\n",
            "Image_no 318\n",
            "Image_no 319\n",
            "Image_no 320\n",
            "Image_no 321\n",
            "Image_no 322\n",
            "Image_no 323\n",
            "Image_no 324\n",
            "Image_no 325\n",
            "Image_no 326\n",
            "Image_no 327\n",
            "Image_no 328\n",
            "Image_no 329\n",
            "Image_no 330\n",
            "Image_no 331\n",
            "Image_no 332\n",
            "Image_no 333\n",
            "Image_no 334\n",
            "Image_no 335\n",
            "Image_no 336\n",
            "Image_no 337\n",
            "Image_no 338\n",
            "Image_no 339\n",
            "Image_no 340\n",
            "Image_no 341\n",
            "Image_no 342\n",
            "Image_no 343\n",
            "Image_no 344\n",
            "Image_no 345\n",
            "Image_no 346\n",
            "Image_no 347\n",
            "Image_no 348\n",
            "Image_no 349\n",
            "Image_no 350\n",
            "Image_no 351\n",
            "Image_no 352\n",
            "Image_no 353\n",
            "Image_no 354\n",
            "Image_no 355\n",
            "Image_no 356\n",
            "Image_no 357\n",
            "Image_no 358\n",
            "Image_no 359\n",
            "Image_no 360\n",
            "Image_no 361\n",
            "Image_no 362\n",
            "Image_no 363\n",
            "Image_no 364\n",
            "Image_no 365\n",
            "Image_no 366\n",
            "Image_no 367\n",
            "Image_no 368\n",
            "Image_no 369\n",
            "Image_no 370\n",
            "Image_no 371\n",
            "Image_no 372\n",
            "Image_no 373\n",
            "Image_no 374\n",
            "Image_no 375\n",
            "Image_no 376\n",
            "Image_no 377\n",
            "Image_no 378\n",
            "Image_no 379\n",
            "Image_no 380\n",
            "Image_no 381\n",
            "Image_no 382\n",
            "Image_no 383\n",
            "Image_no 384\n",
            "Image_no 385\n",
            "Image_no 386\n",
            "Image_no 387\n",
            "Image_no 388\n",
            "Image_no 389\n",
            "Image_no 390\n",
            "Image_no 391\n",
            "Image_no 392\n",
            "Image_no 393\n",
            "Image_no 394\n",
            "Image_no 395\n",
            "Image_no 396\n",
            "Image_no 397\n",
            "Image_no 398\n",
            "Image_no 399\n",
            "Image_no 400\n",
            "Image_no 401\n",
            "Image_no 402\n",
            "Image_no 403\n",
            "Image_no 404\n",
            "Image_no 405\n",
            "Image_no 406\n",
            "Image_no 407\n",
            "Image_no 408\n",
            "Image_no 409\n",
            "Image_no 410\n",
            "Image_no 411\n",
            "Image_no 412\n",
            "Image_no 413\n",
            "Image_no 414\n",
            "Image_no 415\n",
            "Image_no 416\n",
            "Image_no 417\n",
            "Image_no 418\n",
            "Image_no 419\n",
            "Image_no 420\n",
            "Image_no 421\n",
            "Image_no 422\n",
            "Image_no 423\n",
            "Image_no 424\n",
            "Image_no 425\n",
            "Image_no 426\n",
            "Image_no 427\n",
            "Image_no 428\n",
            "Image_no 429\n",
            "Image_no 430\n",
            "Image_no 431\n",
            "Image_no 432\n",
            "Image_no 433\n",
            "Image_no 434\n",
            "Image_no 435\n",
            "Image_no 436\n",
            "Image_no 437\n",
            "Image_no 438\n",
            "Image_no 439\n",
            "Image_no 440\n",
            "Image_no 441\n",
            "Image_no 442\n",
            "Image_no 443\n",
            "Image_no 444\n",
            "Image_no 445\n",
            "Image_no 446\n",
            "Image_no 447\n",
            "Image_no 448\n",
            "Image_no 449\n",
            "Image_no 450\n",
            "Image_no 451\n",
            "Image_no 452\n",
            "Image_no 453\n",
            "Image_no 454\n",
            "Image_no 455\n",
            "Image_no 456\n",
            "Image_no 457\n",
            "Image_no 458\n",
            "Image_no 459\n",
            "Image_no 460\n",
            "Image_no 461\n",
            "Image_no 462\n",
            "Image_no 463\n",
            "Image_no 464\n",
            "Image_no 465\n",
            "Image_no 466\n",
            "Image_no 467\n",
            "Image_no 468\n",
            "Image_no 469\n",
            "Image_no 470\n",
            "Image_no 471\n",
            "Image_no 472\n",
            "Image_no 473\n",
            "Image_no 474\n",
            "Image_no 475\n",
            "Image_no 476\n",
            "Image_no 477\n",
            "Image_no 478\n",
            "Image_no 479\n",
            "Image_no 480\n",
            "Image_no 481\n",
            "Image_no 482\n",
            "Image_no 483\n",
            "Image_no 484\n",
            "Image_no 485\n",
            "Image_no 486\n",
            "Image_no 487\n",
            "Image_no 488\n",
            "Image_no 489\n",
            "Image_no 490\n",
            "Image_no 491\n",
            "Image_no 492\n",
            "Image_no 493\n",
            "Image_no 494\n",
            "Image_no 495\n",
            "Image_no 496\n",
            "Image_no 497\n",
            "Image_no 498\n",
            "Image_no 499\n",
            "Image_no 500\n",
            "Image_no 501\n",
            "Image_no 502\n",
            "Image_no 503\n",
            "Image_no 504\n",
            "Image_no 505\n",
            "Image_no 506\n",
            "Image_no 507\n",
            "Image_no 508\n",
            "Image_no 509\n",
            "Image_no 510\n",
            "Image_no 511\n",
            "Image_no 512\n",
            "Image_no 513\n",
            "Image_no 514\n",
            "Image_no 515\n",
            "Image_no 516\n",
            "Image_no 517\n",
            "Image_no 518\n",
            "Image_no 519\n",
            "Image_no 520\n",
            "Image_no 521\n",
            "Image_no 522\n",
            "Image_no 523\n",
            "Image_no 524\n",
            "Image_no 525\n",
            "Image_no 526\n",
            "Image_no 527\n",
            "Image_no 528\n",
            "Image_no 529\n",
            "Image_no 530\n",
            "Image_no 531\n",
            "Image_no 532\n",
            "Image_no 533\n",
            "Image_no 534\n",
            "Image_no 535\n",
            "Image_no 536\n",
            "Image_no 537\n",
            "Image_no 538\n",
            "Image_no 539\n",
            "Image_no 540\n",
            "Image_no 541\n",
            "Image_no 542\n",
            "Image_no 543\n",
            "Image_no 544\n",
            "Image_no 545\n",
            "Image_no 546\n",
            "Image_no 547\n",
            "Image_no 548\n",
            "Image_no 549\n",
            "Image_no 550\n",
            "Image_no 551\n",
            "Image_no 552\n",
            "Image_no 553\n",
            "Image_no 554\n",
            "Image_no 555\n",
            "Image_no 556\n",
            "Image_no 557\n",
            "Image_no 558\n",
            "Image_no 559\n",
            "Image_no 560\n",
            "Image_no 561\n",
            "Image_no 562\n",
            "Image_no 563\n",
            "Image_no 564\n",
            "Image_no 565\n",
            "Image_no 566\n",
            "Image_no 567\n",
            "Image_no 568\n",
            "Image_no 569\n",
            "Image_no 570\n",
            "Image_no 571\n",
            "Image_no 572\n",
            "Image_no 573\n",
            "Image_no 574\n",
            "Image_no 575\n",
            "Image_no 576\n",
            "Image_no 577\n",
            "Image_no 578\n",
            "Image_no 579\n",
            "Image_no 580\n",
            "Image_no 581\n",
            "Image_no 582\n",
            "Image_no 583\n",
            "Image_no 584\n",
            "Image_no 585\n",
            "Image_no 586\n",
            "Image_no 587\n",
            "Image_no 588\n",
            "Image_no 589\n",
            "Image_no 590\n",
            "Image_no 591\n",
            "Image_no 592\n",
            "Image_no 593\n",
            "Image_no 594\n",
            "Image_no 595\n",
            "Image_no 596\n",
            "Image_no 597\n",
            "Image_no 598\n",
            "Image_no 599\n",
            "Image_no 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJa_mpGvAKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "3dc9acf1-d7b7-4513-cb47-c90f238cad59"
      },
      "source": [
        "\n",
        "#test image visualization\n",
        "img=cv2.imread('testing100.png')\n",
        "plt.imshow(img)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9f804a3b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAAD8CAYAAADuSp8SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQh9debltAVr",
        "colab_type": "text"
      },
      "source": [
        "WE CAN SEE THE MIXING OCCURNG IN THE LATENT SPACE AND ESPECIALLY BECAUSE OF LOW DIMENSIONALITY 522 THOUGH THE POINTS IN LATENT SPACE ARE DISCRETE AND KIND OF ONE TO ONE MAPPING BUT THEY STILL SEEM TO BE QUITE CONTINUOUS. TAKING A HIGHER DIMENSIONAL DATASET MAY SHOW THE SPARSITY IN THE LATENT IN A BETTER WAY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkKRbjiJ7AOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "f03a543b-5a0a-45c9-ccb1-63410d0ca69c"
      },
      "source": [
        "#layer3 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAHTCAYAAABiN8IeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQohCadIUQQohCaNIVQgghCvF/u4cB+OfPgQIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emKAhl5VTgyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iq1TfmdTg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer2 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer2_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vZmGsyTg3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer1 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HJvYuG-CYE",
        "colab_type": "text"
      },
      "source": [
        "SO IT SEEMS THAT THE LATENT SPACE IS VERY CONTINUOUS BUT IN REALITY IT IS DISCRETE IN CASE OF A NORMAL AUTOENCODER TO VISUALIZE IT WE NEED A LARGER DIMENSION DATASET TO PROOVE IT WE NEED TO PLOT THE LATENT SPACE BY CONVERTING IT INTO TWO DIMENSIONS USING PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvIql91TgwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WZqSH9vTgtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axzG2TQeOhNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}