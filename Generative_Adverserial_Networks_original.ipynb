{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Adverserial Networks_original",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSJB8X8mPIbYvYewHXflhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f65b570059541579a66699bcbdf130e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e78f3bfc371e42bcbf85465417895561",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_53cb04c132554364ae1cca54ac74d282",
              "IPY_MODEL_a966d09829f5491388886aa517659e92"
            ]
          }
        },
        "e78f3bfc371e42bcbf85465417895561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53cb04c132554364ae1cca54ac74d282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c019df90afe94673ba35939598308303",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6d2788b516742789dcc312c50046a6e"
          }
        },
        "a966d09829f5491388886aa517659e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_94bfbeff3af745ae8c34407b759c330d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:05&lt;00:00, 1680447.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f129368dced84128a876d9c96b716a7f"
          }
        },
        "c019df90afe94673ba35939598308303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6d2788b516742789dcc312c50046a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94bfbeff3af745ae8c34407b759c330d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f129368dced84128a876d9c96b716a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e47ccc244814a13857adb0ce1587bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d5d947b24b4d463680750cc8f1dc064d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c8c25a8a52d143bd999262a58ea3eed4",
              "IPY_MODEL_08f565c8803044e2bf0d26f83e9e0c60"
            ]
          }
        },
        "d5d947b24b4d463680750cc8f1dc064d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8c25a8a52d143bd999262a58ea3eed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f41edb6e37974ea0a4e0e800aaa9ec9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_844a808c0c6f474bb2aa3f53a656e2f4"
          }
        },
        "08f565c8803044e2bf0d26f83e9e0c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f3dc352dac584352ac21309023bd4325",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:00&lt;00:00, 44331.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40cc9076c46f4bb38637d5086d80d5ed"
          }
        },
        "f41edb6e37974ea0a4e0e800aaa9ec9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "844a808c0c6f474bb2aa3f53a656e2f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3dc352dac584352ac21309023bd4325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40cc9076c46f4bb38637d5086d80d5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1650958f2d248ce958a45b6e9303d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a69c3b20352440fc910cebf7a54e6c7a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48dfbe0f2120482980e6bf8a7cb3d765",
              "IPY_MODEL_3e9029cfe4064f5b96d3e7e298598287"
            ]
          }
        },
        "a69c3b20352440fc910cebf7a54e6c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48dfbe0f2120482980e6bf8a7cb3d765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f5791891d095497780018efdd42725f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8678f252ada4250a683dcd0ad1787f5"
          }
        },
        "3e9029cfe4064f5b96d3e7e298598287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8bf0f417c11541a5a3356b7c7e2a4f31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:02&lt;00:00, 640142.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf3a34bee5904b979eafa32c172bda40"
          }
        },
        "f5791891d095497780018efdd42725f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8678f252ada4250a683dcd0ad1787f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8bf0f417c11541a5a3356b7c7e2a4f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf3a34bee5904b979eafa32c172bda40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "906765be03d84a6ea3e6bce3ca01a5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ec00528a1594f1eb8715733bd2e320b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_55f7160ef21d4eacb1385cf88069ea3f",
              "IPY_MODEL_6753385db8574ae4a9531c1a5943db20"
            ]
          }
        },
        "9ec00528a1594f1eb8715733bd2e320b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55f7160ef21d4eacb1385cf88069ea3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_47c1f85845114d6e8e87fa3ec4c0b5dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b862a7082ed94006b47885e2f93e81a0"
          }
        },
        "6753385db8574ae4a9531c1a5943db20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb36e898207347ecaf41428c30f27454",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 12532.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f0693f40e1c439bacae8a9398e39290"
          }
        },
        "47c1f85845114d6e8e87fa3ec4c0b5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b862a7082ed94006b47885e2f93e81a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb36e898207347ecaf41428c30f27454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f0693f40e1c439bacae8a9398e39290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyakjain0112/Generative-models-papers-and-code/blob/master/Generative_Adverserial_Networks_original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryF6KNvfIwsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all requirements\n",
        "import torch as torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu as Relu\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3pi-2xlo2im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "6f65b570059541579a66699bcbdf130e",
            "e78f3bfc371e42bcbf85465417895561",
            "53cb04c132554364ae1cca54ac74d282",
            "a966d09829f5491388886aa517659e92",
            "c019df90afe94673ba35939598308303",
            "d6d2788b516742789dcc312c50046a6e",
            "94bfbeff3af745ae8c34407b759c330d",
            "f129368dced84128a876d9c96b716a7f",
            "1e47ccc244814a13857adb0ce1587bd1",
            "d5d947b24b4d463680750cc8f1dc064d",
            "c8c25a8a52d143bd999262a58ea3eed4",
            "08f565c8803044e2bf0d26f83e9e0c60",
            "f41edb6e37974ea0a4e0e800aaa9ec9b",
            "844a808c0c6f474bb2aa3f53a656e2f4",
            "f3dc352dac584352ac21309023bd4325",
            "40cc9076c46f4bb38637d5086d80d5ed",
            "e1650958f2d248ce958a45b6e9303d07",
            "a69c3b20352440fc910cebf7a54e6c7a",
            "48dfbe0f2120482980e6bf8a7cb3d765",
            "3e9029cfe4064f5b96d3e7e298598287",
            "f5791891d095497780018efdd42725f9",
            "b8678f252ada4250a683dcd0ad1787f5",
            "8bf0f417c11541a5a3356b7c7e2a4f31",
            "bf3a34bee5904b979eafa32c172bda40",
            "906765be03d84a6ea3e6bce3ca01a5de",
            "9ec00528a1594f1eb8715733bd2e320b",
            "55f7160ef21d4eacb1385cf88069ea3f",
            "6753385db8574ae4a9531c1a5943db20",
            "47c1f85845114d6e8e87fa3ec4c0b5dc",
            "b862a7082ed94006b47885e2f93e81a0",
            "bb36e898207347ecaf41428c30f27454",
            "3f0693f40e1c439bacae8a9398e39290"
          ]
        },
        "outputId": "e44f989b-70f1-42f5-8101-71d965d78cfc"
      },
      "source": [
        "#using the inbuilt Dataset class where all data is loaded into the cpu memory and using getitem we can get the data by just passing the corresponding index4\n",
        "\n",
        "#VARIABLE=DATASET\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f65b570059541579a66699bcbdf130e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e47ccc244814a13857adb0ce1587bd1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1650958f2d248ce958a45b6e9303d07",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "906765be03d84a6ea3e6bce3ca01a5de",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuEHh_4vpfh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading all the data using Dataloaader inbuilt class it loads all the data again into the cpu memory without making a copy because it uses iter for it and even if suffle examples is true then also no cpy is \n",
        "#required it just reshuffles the order of the iteraror in the list. \n",
        "#NOTE: only when we will load the data for training in the training class then the data will be needed to be loaded in the gpu memory\n",
        "batch_size=100\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO6OQlJluQy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb673b60-17c7-46a3-eb6f-fbee872826ce"
      },
      "source": [
        "\n",
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_generator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=100):\n",
        "    super(model_generator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.mean=100\n",
        "    self.std=100\n",
        "    self.g_input_dim=100\n",
        "    self.g_output_dim=self.filter_size*self.filter_size\n",
        "    \n",
        "    self.fc1 = nn.Linear(self.g_input_dim, 128)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, self.g_output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,x_input,latent_space=False,latent=None,marker=False,plot=False,batcher=1):\n",
        "    #taking x_input as input for coding simplicity but not using it in any way\n",
        "    if latent_space==True or marker==True or plot==True:\n",
        "      self.x=(torch.randn([batcher,self.g_input_dim]) ).cuda()\n",
        "    else:\n",
        "      self.x=(torch.randn(self.batch_size,self.g_input_dim)).cuda()\n",
        "\n",
        "    x = F.leaky_relu(self.fc1(self.x), 0.2)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    return torch.tanh(self.fc4(x))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atPKIhw3CY_M",
        "colab_type": "code",
        "outputId": "3c3403ee-a865-4ba4-a6cd-5396bcc69c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Usually in the model class we need to inherit from nn.Module and use super class code otherwise the forward will not run.\n",
        "\n",
        "#VARIABLE: SELF.NUM_LAYERS\n",
        "\n",
        "class model_discriminator(torch.nn.Module):\n",
        "  def __init__(self,batch_size=100):\n",
        "    super(model_discriminator,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.d_input_dim=self.filter_size*self.filter_size\n",
        "\n",
        "    self.fc1 = nn.Linear(self.d_input_dim, 1024)\n",
        "    self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "    self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "    self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "\n",
        "  def forward(self,x,latent_space=False,latent=None,marker=False,plot=False):\n",
        "      x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "      if plot==True:\n",
        "        return x\n",
        "      if latent_space==True:\n",
        "        return x\n",
        "      if marker==True:\n",
        "        x=latent\n",
        "      x = F.dropout(x, 0.3)\n",
        "      x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "      x = F.dropout(x, 0.3)\n",
        "      return torch.sigmoid(self.fc4(x))\n",
        "\"\"\"\n",
        "    if plot==True:\n",
        "      return x\n",
        "    if latent_space==True:\n",
        "      return x\n",
        "    if marker==True:\n",
        "      x=latent\n",
        "    x=self.layer3(x)\n",
        "    x=Relu(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.layer4(x)\n",
        "    x=Relu(x)\n",
        "    x=self.layer5(x)\n",
        "    x=torch.sigmoid(x)\n",
        "    #print(x.size())\n",
        "    #x=x.cpu().detach().numpy()\n",
        "    #x=np.array(np.amax(x,axis=1))\n",
        "    \n",
        "    \n",
        "    #print(x.size())\n",
        "\n",
        "    return x\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    if plot==True:\\n      return x\\n    if latent_space==True:\\n      return x\\n    if marker==True:\\n      x=latent\\n    x=self.layer3(x)\\n    x=Relu(x)\\n    x=torch.flatten(x,1)\\n    x=self.layer4(x)\\n    x=Relu(x)\\n    x=self.layer5(x)\\n    x=torch.sigmoid(x)\\n    #print(x.size())\\n    #x=x.cpu().detach().numpy()\\n    #x=np.array(np.amax(x,axis=1))\\n    \\n    \\n    #print(x.size())\\n\\n    return x\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zeJguY2gvG",
        "colab_type": "code",
        "outputId": "323df460-e2d9-40ec-ceea-f9d8badcecb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "class model_generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(model_generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class model_discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(model_discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))\n",
        "        \n",
        "\"\"\"\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nclass model_generator(nn.Module):\\n    def __init__(self, g_input_dim, g_output_dim):\\n        super(model_generator, self).__init__()       \\n        self.fc1 = nn.Linear(g_input_dim, 256)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\\n\\n    \\n    # forward method\\n    def forward(self, x): \\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        return torch.tanh(self.fc4(x))\\n    \\nclass model_discriminator(nn.Module):\\n    def __init__(self, d_input_dim):\\n        super(model_discriminator, self).__init__()\\n        self.fc1 = nn.Linear(d_input_dim, 1024)\\n        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\\n        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\\n        self.fc4 = nn.Linear(self.fc3.out_features, 1)\\n    \\n    # forward method\\n    def forward(self, x):\\n        x = F.leaky_relu(self.fc1(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc2(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        x = F.leaky_relu(self.fc3(x), 0.2)\\n        x = F.dropout(x, 0.3)\\n        return torch.sigmoid(self.fc4(x))\\n        \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lMPEo6izHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#latent space visualization\n",
        "class modify_space(object):\n",
        "  def __init__(self,importer,test_gen,label=1,label2=2,num_latents=10):\n",
        "    self.batch_size=1\n",
        "    self.label=label\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.label2=label2\n",
        "    self.test_gen=test_gen\n",
        "    self.num_latents=num_latents\n",
        "    self.net_generator=importer.net_generator\n",
        "    self.net_discriminator=importer.net_discriminator\n",
        "    self.generate()\n",
        "    self.latent()\n",
        "\n",
        "\n",
        "  def generate(self):\n",
        "    w=0\n",
        "    e=0\n",
        "    print(1)\n",
        "    for (image,label) in self.test_gen:\n",
        "      if w==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label:\n",
        "          self.image=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "\n",
        "          self.label=label[i].cuda()\n",
        "          w=1\n",
        "          break\n",
        "    for (image,label) in self.test_gen:\n",
        "      if e==1:\n",
        "        break\n",
        "      for i in range(len(label)):\n",
        "        if (label[i].item())==self.label2:\n",
        "          self.image2=np.reshape(image[i],[self.batch_size, self.num_layers*self.filter_size*self.filter_size]).cuda()\n",
        "          self.label2=label[i].cuda()\n",
        "          e=1\n",
        "          break\n",
        "\n",
        "  def latent(self):\n",
        "    latent1=self.net_generator(self.image,latent_space=True)\n",
        "    latent2=self.net_generator(self.image2,latent_space=True)\n",
        "    for count in range(self.num_latents+1):\n",
        "      input_latent=(latent1*(count/(self.num_latents))+latent2*(1-count/self.num_latents))\n",
        "      output=self.net_generator(self.image2,latent=input_latent,marker=True)\n",
        "      output=output.view(1, 1,self.filter_size ,self.filter_size ) \n",
        "      save_image(output,'latent_layer1_'+str(count)+'.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4mZAqyQgp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualizing the latent space points in two dimenions using principal component analysis\n",
        "a=0\n",
        "class plotter(object):\n",
        "  def __init__(self,render):\n",
        "    self.batch_size=1\n",
        "    self.test_gen=test_gen\n",
        "    self.net_generator=render.net_generator\n",
        "    self.net_discriminator=render.net_discriminator\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.latent=[]\n",
        "    self.latent_size=256\n",
        "    self.label_array=[]\n",
        "    self.visualize()\n",
        "   \n",
        "  def visualize(self):\n",
        "    for (image,label) in self.test_gen:\n",
        "      for i in range(len(label)):\n",
        "        latent=self.net_generator(np.reshape(image[i],[self.batch_size,self.num_layers*self.filter_size*self.filter_size]).cuda(),plot=True).cpu().detach().numpy()\n",
        "        latent_x=np.reshape(latent,(self.latent_size))\n",
        "        self.latent.append(latent_x)\n",
        "        self.label_array.append(label[i].item())\n",
        "        #feat_cols = ['feature'+str(i) for i in range(latent_x.shape[1])]\n",
        "        #latent=pd.DataFrame(latent_x,columns=feat_cols)\n",
        "        #print(latent.shape)\n",
        "        #print(latent)\n",
        "\n",
        "    pca_latent = PCA(n_components=2)\n",
        "  \n",
        "    twodim_pca = pca_latent.fit_transform(np.array(self.latent))\n",
        "    colormap = np.array(['r', 'g', 'b','pink','orange','lightblue','black','brown','yellow','white'])\n",
        "    for i in range(10):\n",
        "      temp1=[]\n",
        "      temp2=[]\n",
        "      \n",
        "      for j in range(len(self.label_array)):\n",
        "\n",
        "        if self.label_array[j]==i:\n",
        "          temp1.append(twodim_pca[j][0])\n",
        "          temp2.append(twodim_pca[j][1]) \n",
        "      plt.scatter(temp1,temp2,c=colormap[i])\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN5MeSz2w6n3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1894995c-9e10-4f44-afd3-c013a63a613f"
      },
      "source": [
        "#MAIN DO NOT TOUCH IT\n",
        "net_generator=model_generator().cuda()\n",
        "net_discriminator=model_discriminator().cuda()\n",
        "lr=0.00005\n",
        "opt_generator=torch.optim.Adam(net_generator.parameters(),lr)\n",
        "opt_discriminator=torch.optim.Adam(net_discriminator.parameters(),lr)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8JeXOyqY-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If we are building a class without using any functionality of any other class then object can be written it means that the class will return an object type dataset\n",
        "#However if we write like data.Datasets of Sampler then it means that it will have functionality of the superclass data.Datasets or Sampler however ultimately these super classes will also return an object.\n",
        "#so here in a way we are returning an object of the super class via the sub class.\n",
        "#BUILDING A SIMPLE AUTOENCODER WHICH JUST AIMS AT RECONSTRUCTION WITHOUT ANY STOCHASTICITY\n",
        "\n",
        "\n",
        "class training(object):\n",
        "\n",
        "  def __init__(self,epochs=50,train_g=train_gen,test_g=test_gen,generate_latent=False,plot=False,opt_generator=opt_generator,opt_discriminator=opt_discriminator):\n",
        "    self.epochs=epochs\n",
        "    self.lr=lr\n",
        "    self.train_gen=train_g\n",
        "    self.batch_size=100\n",
        "    self.test_gen=test_g\n",
        "    self.device='cuda'\n",
        "    self.num_layers=1\n",
        "    self.filter_size=28\n",
        "    self.net_generator=net_generator\n",
        "    self.net_discriminator=net_discriminator\n",
        "    self.loss=torch.nn.BCELoss()\n",
        "    self.opt_discriminator=opt_discriminator\n",
        "    self.opt_generator=opt_generator\n",
        "    self.generation_iters=1\n",
        "    self.d_iters=2\n",
        "    self.train()\n",
        "    \n",
        "    if generate_latent==True:\n",
        "      modify_space(self,test_gen)\n",
        "    if plot==True:\n",
        "      plotter(self)\n",
        "    self.test()\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    for epoch_no in range(self.epochs):\n",
        "      counter=0\n",
        "      for (image,label) in self.train_gen:\n",
        "        img=image.to(self.device)\n",
        "        lab_real=torch.from_numpy(np.ones([self.batch_size,1],np.float32)).to(self.device)\n",
        "        lab_fake=torch.from_numpy(np.zeros([self.batch_size,1],np.float32)).to(self.device)\n",
        "\n",
        "        #train_discriminator\n",
        "        #self.opt_generator.zero_grad()\n",
        "\n",
        "        self.opt_discriminator.zero_grad()\n",
        "        \"\"\"random_choice=np.random.choice([0,1])\n",
        "        if random_choice==0:\n",
        "          input_label=lab_fake\n",
        "          input_img=output_generator[:,:,:27,:27]\n",
        "          print(\"fake_image\",input_img.size())\n",
        "        else:\n",
        "          input_label=lab_real\n",
        "          input_img=img[:,:,:27,:27]\n",
        "          print(\"real_image\",input_img.size())\n",
        "          \"\"\"\n",
        "        loss_d=0\n",
        "        for i in range(self.d_iters):  \n",
        "          output_generator=self.net_generator(img).to(self.device)\n",
        "          logits_fake = self.net_discriminator(output_generator).to(self.device)\n",
        "          logits_real = self.net_discriminator(img.view(self.batch_size,self.num_layers*self.filter_size*self.filter_size)).to(self.device)\n",
        "          loss_discriminator_real = self.loss(logits_real,lab_real)\n",
        "          loss_discriminator_fake = self.loss(logits_fake,lab_fake)\n",
        "          loss_discriminator=loss_discriminator_real+loss_discriminator_fake\n",
        "          loss_discriminator.backward(retain_graph=True)\n",
        "          self.opt_discriminator.step()\n",
        "          loss_d+=loss_discriminator\n",
        "        #discriminator one iteration complete\n",
        "\n",
        "        #train_generator\n",
        "        tot_loss=0\n",
        "        for i in range(self.generation_iters):\n",
        "          img=0\n",
        "          self.opt_generator.zero_grad()\n",
        "          #self.opt_discriminator.zero_grad()\n",
        "          output_generator=self.net_generator(img).to(self.device)\n",
        "          #print(\"fake_image\",output_generator.size())\n",
        "          logit_disc = self.net_discriminator(output_generator).to(self.device)\n",
        "          loss_generator = self.loss(logit_disc,lab_real)\n",
        "          tot_loss+=loss_generator\n",
        "          loss_generator.backward()\n",
        "          self.opt_generator.step()\n",
        "        #One generator iteration completed\n",
        "        counter+=1\n",
        "        print(\"ITERATION_NO.:\",counter ,\"LOSS_Generator:\",tot_loss.item()/self.generation_iters ,\"LOSS_Discriminator:\",loss_d.item()/self.d_iters)\n",
        "      print(\"EPOCH OVER:\",epoch_no)\n",
        "   \n",
        "  def test(self):\n",
        "    count=0\n",
        "    for (image,label) in self.train_gen:\n",
        "      img=image[0].to(self.device)\n",
        "      lab=label.to(self.device)\n",
        "      outputs = self.net_generator(img).view(100,1,28,28)\n",
        "      count+=1\n",
        "      print(\"Image_no\",count)\n",
        "      if count%100==0:\n",
        "        save_image(outputs,'testing'+str(count)+'.png')\n",
        "       \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esUJezNR8OFw",
        "colab_type": "code",
        "outputId": "477ce272-8e2d-44b2-f8be-89a00d994c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True,generate_latent=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ITERATION_NO.: 412 LOSS_Generator: 8.83456802368164 LOSS_Discriminator: 0.058569010347127914\n",
            "ITERATION_NO.: 413 LOSS_Generator: 7.652017593383789 LOSS_Discriminator: 0.12736907601356506\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.338455677032471 LOSS_Discriminator: 0.07154305279254913\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.132780075073242 LOSS_Discriminator: 0.07116107642650604\n",
            "ITERATION_NO.: 416 LOSS_Generator: 5.610228061676025 LOSS_Discriminator: 0.07584220916032791\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.401226043701172 LOSS_Discriminator: 0.10120582580566406\n",
            "ITERATION_NO.: 418 LOSS_Generator: 8.432464599609375 LOSS_Discriminator: 0.09460318088531494\n",
            "ITERATION_NO.: 419 LOSS_Generator: 9.420670509338379 LOSS_Discriminator: 0.07411596924066544\n",
            "ITERATION_NO.: 420 LOSS_Generator: 9.164786338806152 LOSS_Discriminator: 0.06488986313343048\n",
            "ITERATION_NO.: 421 LOSS_Generator: 9.02035903930664 LOSS_Discriminator: 0.127090722322464\n",
            "ITERATION_NO.: 422 LOSS_Generator: 7.8737101554870605 LOSS_Discriminator: 0.03419240936636925\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.680202484130859 LOSS_Discriminator: 0.06371469795703888\n",
            "ITERATION_NO.: 424 LOSS_Generator: 6.637413501739502 LOSS_Discriminator: 0.06313127279281616\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.335259437561035 LOSS_Discriminator: 0.10670531541109085\n",
            "ITERATION_NO.: 426 LOSS_Generator: 7.304868698120117 LOSS_Discriminator: 0.08841350674629211\n",
            "ITERATION_NO.: 427 LOSS_Generator: 7.9768781661987305 LOSS_Discriminator: 0.06979559361934662\n",
            "ITERATION_NO.: 428 LOSS_Generator: 8.222196578979492 LOSS_Discriminator: 0.02834249474108219\n",
            "ITERATION_NO.: 429 LOSS_Generator: 8.199200630187988 LOSS_Discriminator: 0.03833270072937012\n",
            "ITERATION_NO.: 430 LOSS_Generator: 7.531152248382568 LOSS_Discriminator: 0.0850994884967804\n",
            "ITERATION_NO.: 431 LOSS_Generator: 6.940728664398193 LOSS_Discriminator: 0.04588593542575836\n",
            "ITERATION_NO.: 432 LOSS_Generator: 6.683144569396973 LOSS_Discriminator: 0.04771101474761963\n",
            "ITERATION_NO.: 433 LOSS_Generator: 6.807287693023682 LOSS_Discriminator: 0.07891058921813965\n",
            "ITERATION_NO.: 434 LOSS_Generator: 6.243868827819824 LOSS_Discriminator: 0.08738531172275543\n",
            "ITERATION_NO.: 435 LOSS_Generator: 6.566734790802002 LOSS_Discriminator: 0.08195322751998901\n",
            "ITERATION_NO.: 436 LOSS_Generator: 7.752993106842041 LOSS_Discriminator: 0.062062665820121765\n",
            "ITERATION_NO.: 437 LOSS_Generator: 7.696136951446533 LOSS_Discriminator: 0.09706106036901474\n",
            "ITERATION_NO.: 438 LOSS_Generator: 7.720582962036133 LOSS_Discriminator: 0.08518768846988678\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.318012714385986 LOSS_Discriminator: 0.026692964136600494\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.558149814605713 LOSS_Discriminator: 0.11458814144134521\n",
            "ITERATION_NO.: 441 LOSS_Generator: 5.985925197601318 LOSS_Discriminator: 0.04576416313648224\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.465427398681641 LOSS_Discriminator: 0.0805177390575409\n",
            "ITERATION_NO.: 443 LOSS_Generator: 6.427353382110596 LOSS_Discriminator: 0.09731775522232056\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.8000054359436035 LOSS_Discriminator: 0.04417615011334419\n",
            "ITERATION_NO.: 445 LOSS_Generator: 7.718926429748535 LOSS_Discriminator: 0.07350929826498032\n",
            "ITERATION_NO.: 446 LOSS_Generator: 7.635535717010498 LOSS_Discriminator: 0.11809238791465759\n",
            "ITERATION_NO.: 447 LOSS_Generator: 6.5957159996032715 LOSS_Discriminator: 0.032929547131061554\n",
            "ITERATION_NO.: 448 LOSS_Generator: 5.976492881774902 LOSS_Discriminator: 0.05768341198563576\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.843557834625244 LOSS_Discriminator: 0.09685267508029938\n",
            "ITERATION_NO.: 450 LOSS_Generator: 6.459592342376709 LOSS_Discriminator: 0.08087471127510071\n",
            "ITERATION_NO.: 451 LOSS_Generator: 6.921092987060547 LOSS_Discriminator: 0.054005011916160583\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.217579364776611 LOSS_Discriminator: 0.0836334154009819\n",
            "ITERATION_NO.: 453 LOSS_Generator: 7.695759296417236 LOSS_Discriminator: 0.06445040553808212\n",
            "ITERATION_NO.: 454 LOSS_Generator: 6.908070087432861 LOSS_Discriminator: 0.06320089101791382\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.514686107635498 LOSS_Discriminator: 0.038922034204006195\n",
            "ITERATION_NO.: 456 LOSS_Generator: 6.401822566986084 LOSS_Discriminator: 0.034311022609472275\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.816857814788818 LOSS_Discriminator: 0.03261810541152954\n",
            "ITERATION_NO.: 458 LOSS_Generator: 7.659902572631836 LOSS_Discriminator: 0.037434909492731094\n",
            "ITERATION_NO.: 459 LOSS_Generator: 7.705241680145264 LOSS_Discriminator: 0.05784677714109421\n",
            "ITERATION_NO.: 460 LOSS_Generator: 8.003734588623047 LOSS_Discriminator: 0.06292399764060974\n",
            "ITERATION_NO.: 461 LOSS_Generator: 7.573449611663818 LOSS_Discriminator: 0.04736017808318138\n",
            "ITERATION_NO.: 462 LOSS_Generator: 7.378154754638672 LOSS_Discriminator: 0.0661109983921051\n",
            "ITERATION_NO.: 463 LOSS_Generator: 6.473595142364502 LOSS_Discriminator: 0.05738155171275139\n",
            "ITERATION_NO.: 464 LOSS_Generator: 6.416131496429443 LOSS_Discriminator: 0.1140938401222229\n",
            "ITERATION_NO.: 465 LOSS_Generator: 7.257206916809082 LOSS_Discriminator: 0.06802856922149658\n",
            "ITERATION_NO.: 466 LOSS_Generator: 8.188634872436523 LOSS_Discriminator: 0.06911632418632507\n",
            "ITERATION_NO.: 467 LOSS_Generator: 8.15921401977539 LOSS_Discriminator: 0.01650877669453621\n",
            "ITERATION_NO.: 468 LOSS_Generator: 7.655255317687988 LOSS_Discriminator: 0.04686865210533142\n",
            "ITERATION_NO.: 469 LOSS_Generator: 7.137063980102539 LOSS_Discriminator: 0.20069724321365356\n",
            "ITERATION_NO.: 470 LOSS_Generator: 6.817529201507568 LOSS_Discriminator: 0.039040278643369675\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.435558319091797 LOSS_Discriminator: 0.11565874516963959\n",
            "ITERATION_NO.: 472 LOSS_Generator: 6.984689235687256 LOSS_Discriminator: 0.09805203974246979\n",
            "ITERATION_NO.: 473 LOSS_Generator: 7.1221022605896 LOSS_Discriminator: 0.06765207648277283\n",
            "ITERATION_NO.: 474 LOSS_Generator: 7.803553581237793 LOSS_Discriminator: 0.12421856075525284\n",
            "ITERATION_NO.: 475 LOSS_Generator: 7.377078056335449 LOSS_Discriminator: 0.22377897799015045\n",
            "ITERATION_NO.: 476 LOSS_Generator: 6.440191745758057 LOSS_Discriminator: 0.11697936058044434\n",
            "ITERATION_NO.: 477 LOSS_Generator: 5.941720485687256 LOSS_Discriminator: 0.0531909242272377\n",
            "ITERATION_NO.: 478 LOSS_Generator: 6.280696392059326 LOSS_Discriminator: 0.12765301764011383\n",
            "ITERATION_NO.: 479 LOSS_Generator: 7.173539638519287 LOSS_Discriminator: 0.07018206268548965\n",
            "ITERATION_NO.: 480 LOSS_Generator: 8.633553504943848 LOSS_Discriminator: 0.08833858370780945\n",
            "ITERATION_NO.: 481 LOSS_Generator: 9.0532865524292 LOSS_Discriminator: 0.06823962181806564\n",
            "ITERATION_NO.: 482 LOSS_Generator: 8.354391098022461 LOSS_Discriminator: 0.07916420698165894\n",
            "ITERATION_NO.: 483 LOSS_Generator: 7.324019908905029 LOSS_Discriminator: 0.09935901314020157\n",
            "ITERATION_NO.: 484 LOSS_Generator: 6.101980686187744 LOSS_Discriminator: 0.07013405859470367\n",
            "ITERATION_NO.: 485 LOSS_Generator: 6.010402202606201 LOSS_Discriminator: 0.1306082010269165\n",
            "ITERATION_NO.: 486 LOSS_Generator: 6.019134044647217 LOSS_Discriminator: 0.06635042279958725\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.759221076965332 LOSS_Discriminator: 0.08542422950267792\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.921866416931152 LOSS_Discriminator: 0.036510683596134186\n",
            "ITERATION_NO.: 489 LOSS_Generator: 7.702013969421387 LOSS_Discriminator: 0.040912926197052\n",
            "ITERATION_NO.: 490 LOSS_Generator: 7.480098724365234 LOSS_Discriminator: 0.10935911536216736\n",
            "ITERATION_NO.: 491 LOSS_Generator: 7.6749138832092285 LOSS_Discriminator: 0.06593261659145355\n",
            "ITERATION_NO.: 492 LOSS_Generator: 7.0855231285095215 LOSS_Discriminator: 0.015573425218462944\n",
            "ITERATION_NO.: 493 LOSS_Generator: 6.5904974937438965 LOSS_Discriminator: 0.14223536849021912\n",
            "ITERATION_NO.: 494 LOSS_Generator: 6.077944755554199 LOSS_Discriminator: 0.07879705727100372\n",
            "ITERATION_NO.: 495 LOSS_Generator: 5.877131938934326 LOSS_Discriminator: 0.06374868750572205\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.099792957305908 LOSS_Discriminator: 0.07072427868843079\n",
            "ITERATION_NO.: 497 LOSS_Generator: 7.390031337738037 LOSS_Discriminator: 0.11593186855316162\n",
            "ITERATION_NO.: 498 LOSS_Generator: 7.992947578430176 LOSS_Discriminator: 0.09175842255353928\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.745487213134766 LOSS_Discriminator: 0.05867781862616539\n",
            "ITERATION_NO.: 500 LOSS_Generator: 7.171436309814453 LOSS_Discriminator: 0.036367107182741165\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.4140448570251465 LOSS_Discriminator: 0.06265377253293991\n",
            "ITERATION_NO.: 502 LOSS_Generator: 6.477055549621582 LOSS_Discriminator: 0.07311762869358063\n",
            "ITERATION_NO.: 503 LOSS_Generator: 5.667316436767578 LOSS_Discriminator: 0.06730353087186813\n",
            "ITERATION_NO.: 504 LOSS_Generator: 6.033828258514404 LOSS_Discriminator: 0.12480778247117996\n",
            "ITERATION_NO.: 505 LOSS_Generator: 6.250495433807373 LOSS_Discriminator: 0.04449757561087608\n",
            "ITERATION_NO.: 506 LOSS_Generator: 7.157627105712891 LOSS_Discriminator: 0.05562932789325714\n",
            "ITERATION_NO.: 507 LOSS_Generator: 7.677753448486328 LOSS_Discriminator: 0.03506592661142349\n",
            "ITERATION_NO.: 508 LOSS_Generator: 8.35944652557373 LOSS_Discriminator: 0.054873645305633545\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.880856990814209 LOSS_Discriminator: 0.08193225413560867\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.181246757507324 LOSS_Discriminator: 0.09186562895774841\n",
            "ITERATION_NO.: 511 LOSS_Generator: 5.88568115234375 LOSS_Discriminator: 0.13229283690452576\n",
            "ITERATION_NO.: 512 LOSS_Generator: 5.512824058532715 LOSS_Discriminator: 0.06599141657352448\n",
            "ITERATION_NO.: 513 LOSS_Generator: 6.595727920532227 LOSS_Discriminator: 0.09409677237272263\n",
            "ITERATION_NO.: 514 LOSS_Generator: 7.547274589538574 LOSS_Discriminator: 0.043810077011585236\n",
            "ITERATION_NO.: 515 LOSS_Generator: 8.00429630279541 LOSS_Discriminator: 0.07138106226921082\n",
            "ITERATION_NO.: 516 LOSS_Generator: 8.1138277053833 LOSS_Discriminator: 0.04600637033581734\n",
            "ITERATION_NO.: 517 LOSS_Generator: 8.004034996032715 LOSS_Discriminator: 0.12135696411132812\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.471026420593262 LOSS_Discriminator: 0.06405565142631531\n",
            "ITERATION_NO.: 519 LOSS_Generator: 6.477752685546875 LOSS_Discriminator: 0.07468001544475555\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.804812431335449 LOSS_Discriminator: 0.10760462284088135\n",
            "ITERATION_NO.: 521 LOSS_Generator: 6.145434379577637 LOSS_Discriminator: 0.05141611397266388\n",
            "ITERATION_NO.: 522 LOSS_Generator: 6.470634937286377 LOSS_Discriminator: 0.08866959810256958\n",
            "ITERATION_NO.: 523 LOSS_Generator: 7.736143112182617 LOSS_Discriminator: 0.09370634704828262\n",
            "ITERATION_NO.: 524 LOSS_Generator: 7.882016658782959 LOSS_Discriminator: 0.04978685826063156\n",
            "ITERATION_NO.: 525 LOSS_Generator: 8.260722160339355 LOSS_Discriminator: 0.13722768425941467\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.361210823059082 LOSS_Discriminator: 0.05180070549249649\n",
            "ITERATION_NO.: 527 LOSS_Generator: 6.909890174865723 LOSS_Discriminator: 0.048368096351623535\n",
            "ITERATION_NO.: 528 LOSS_Generator: 5.81405782699585 LOSS_Discriminator: 0.08941352367401123\n",
            "ITERATION_NO.: 529 LOSS_Generator: 6.121573448181152 LOSS_Discriminator: 0.0886441320180893\n",
            "ITERATION_NO.: 530 LOSS_Generator: 7.635784149169922 LOSS_Discriminator: 0.10213160514831543\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.558619976043701 LOSS_Discriminator: 0.04193699359893799\n",
            "ITERATION_NO.: 532 LOSS_Generator: 8.205061912536621 LOSS_Discriminator: 0.0923210084438324\n",
            "ITERATION_NO.: 533 LOSS_Generator: 8.288758277893066 LOSS_Discriminator: 0.07701965421438217\n",
            "ITERATION_NO.: 534 LOSS_Generator: 7.383627414703369 LOSS_Discriminator: 0.08436905592679977\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.013953685760498 LOSS_Discriminator: 0.08150354027748108\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.800317287445068 LOSS_Discriminator: 0.03334584832191467\n",
            "ITERATION_NO.: 537 LOSS_Generator: 6.96297550201416 LOSS_Discriminator: 0.0849447026848793\n",
            "ITERATION_NO.: 538 LOSS_Generator: 6.956426620483398 LOSS_Discriminator: 0.047635145485401154\n",
            "ITERATION_NO.: 539 LOSS_Generator: 7.151440620422363 LOSS_Discriminator: 0.03413572534918785\n",
            "ITERATION_NO.: 540 LOSS_Generator: 7.309156894683838 LOSS_Discriminator: 0.08708976954221725\n",
            "ITERATION_NO.: 541 LOSS_Generator: 7.596581935882568 LOSS_Discriminator: 0.04052204266190529\n",
            "ITERATION_NO.: 542 LOSS_Generator: 7.230737209320068 LOSS_Discriminator: 0.04907703772187233\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.270083427429199 LOSS_Discriminator: 0.039396289736032486\n",
            "ITERATION_NO.: 544 LOSS_Generator: 6.2825140953063965 LOSS_Discriminator: 0.04599153622984886\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.12419319152832 LOSS_Discriminator: 0.037395089864730835\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.8129119873046875 LOSS_Discriminator: 0.06070273369550705\n",
            "ITERATION_NO.: 547 LOSS_Generator: 7.094436168670654 LOSS_Discriminator: 0.054428424686193466\n",
            "ITERATION_NO.: 548 LOSS_Generator: 8.51611042022705 LOSS_Discriminator: 0.038926102221012115\n",
            "ITERATION_NO.: 549 LOSS_Generator: 8.766570091247559 LOSS_Discriminator: 0.11744415014982224\n",
            "ITERATION_NO.: 550 LOSS_Generator: 8.147628784179688 LOSS_Discriminator: 0.1075473204255104\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.204012870788574 LOSS_Discriminator: 0.15937651693820953\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.393524169921875 LOSS_Discriminator: 0.0497589185833931\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.213595390319824 LOSS_Discriminator: 0.059818342328071594\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.185412406921387 LOSS_Discriminator: 0.06249023228883743\n",
            "ITERATION_NO.: 555 LOSS_Generator: 8.345290184020996 LOSS_Discriminator: 0.07594959437847137\n",
            "ITERATION_NO.: 556 LOSS_Generator: 8.951807975769043 LOSS_Discriminator: 0.04631445184350014\n",
            "ITERATION_NO.: 557 LOSS_Generator: 8.842876434326172 LOSS_Discriminator: 0.24973773956298828\n",
            "ITERATION_NO.: 558 LOSS_Generator: 7.880402088165283 LOSS_Discriminator: 0.12750011682510376\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.889347553253174 LOSS_Discriminator: 0.1398342102766037\n",
            "ITERATION_NO.: 560 LOSS_Generator: 4.689853668212891 LOSS_Discriminator: 0.13719135522842407\n",
            "ITERATION_NO.: 561 LOSS_Generator: 5.881552219390869 LOSS_Discriminator: 0.1402166187763214\n",
            "ITERATION_NO.: 562 LOSS_Generator: 7.127568244934082 LOSS_Discriminator: 0.09265412390232086\n",
            "ITERATION_NO.: 563 LOSS_Generator: 8.231401443481445 LOSS_Discriminator: 0.030653033405542374\n",
            "ITERATION_NO.: 564 LOSS_Generator: 9.08010196685791 LOSS_Discriminator: 0.043684668838977814\n",
            "ITERATION_NO.: 565 LOSS_Generator: 9.44135570526123 LOSS_Discriminator: 0.08319425582885742\n",
            "ITERATION_NO.: 566 LOSS_Generator: 8.479905128479004 LOSS_Discriminator: 0.21553227305412292\n",
            "ITERATION_NO.: 567 LOSS_Generator: 7.166930675506592 LOSS_Discriminator: 0.1259409487247467\n",
            "ITERATION_NO.: 568 LOSS_Generator: 6.330951690673828 LOSS_Discriminator: 0.034357041120529175\n",
            "ITERATION_NO.: 569 LOSS_Generator: 5.6578168869018555 LOSS_Discriminator: 0.09019675850868225\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.300905704498291 LOSS_Discriminator: 0.07708330452442169\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.5123724937438965 LOSS_Discriminator: 0.11537113785743713\n",
            "ITERATION_NO.: 572 LOSS_Generator: 6.513247013092041 LOSS_Discriminator: 0.09860554337501526\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.857751369476318 LOSS_Discriminator: 0.0884646475315094\n",
            "ITERATION_NO.: 574 LOSS_Generator: 7.85080099105835 LOSS_Discriminator: 0.08820168673992157\n",
            "ITERATION_NO.: 575 LOSS_Generator: 8.075287818908691 LOSS_Discriminator: 0.04789602756500244\n",
            "ITERATION_NO.: 576 LOSS_Generator: 7.297579288482666 LOSS_Discriminator: 0.07147616147994995\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.864094257354736 LOSS_Discriminator: 0.07535922527313232\n",
            "ITERATION_NO.: 578 LOSS_Generator: 5.866062164306641 LOSS_Discriminator: 0.06135449558496475\n",
            "ITERATION_NO.: 579 LOSS_Generator: 6.1848626136779785 LOSS_Discriminator: 0.13965901732444763\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.801328659057617 LOSS_Discriminator: 0.05776205286383629\n",
            "ITERATION_NO.: 581 LOSS_Generator: 8.029206275939941 LOSS_Discriminator: 0.05509822443127632\n",
            "ITERATION_NO.: 582 LOSS_Generator: 8.647948265075684 LOSS_Discriminator: 0.03319917619228363\n",
            "ITERATION_NO.: 583 LOSS_Generator: 8.88106632232666 LOSS_Discriminator: 0.03333461657166481\n",
            "ITERATION_NO.: 584 LOSS_Generator: 8.340921401977539 LOSS_Discriminator: 0.04330255091190338\n",
            "ITERATION_NO.: 585 LOSS_Generator: 8.12063980102539 LOSS_Discriminator: 0.02677031233906746\n",
            "ITERATION_NO.: 586 LOSS_Generator: 7.494160175323486 LOSS_Discriminator: 0.07458444684743881\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.4182610511779785 LOSS_Discriminator: 0.04286976531147957\n",
            "ITERATION_NO.: 588 LOSS_Generator: 6.343809127807617 LOSS_Discriminator: 0.08240672200918198\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.020566463470459 LOSS_Discriminator: 0.06640999019145966\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.4481201171875 LOSS_Discriminator: 0.045649491250514984\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.7223801612854 LOSS_Discriminator: 0.030011741444468498\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.3177080154418945 LOSS_Discriminator: 0.055761005729436874\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.555845737457275 LOSS_Discriminator: 0.05111335590481758\n",
            "ITERATION_NO.: 594 LOSS_Generator: 6.900069713592529 LOSS_Discriminator: 0.10110433399677277\n",
            "ITERATION_NO.: 595 LOSS_Generator: 6.311130523681641 LOSS_Discriminator: 0.06191309541463852\n",
            "ITERATION_NO.: 596 LOSS_Generator: 6.131497383117676 LOSS_Discriminator: 0.042848434299230576\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.186710834503174 LOSS_Discriminator: 0.07167062908411026\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.332391262054443 LOSS_Discriminator: 0.07881234586238861\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.549531936645508 LOSS_Discriminator: 0.06680478155612946\n",
            "ITERATION_NO.: 600 LOSS_Generator: 7.335285186767578 LOSS_Discriminator: 0.042946845293045044\n",
            "EPOCH OVER: 41\n",
            "ITERATION_NO.: 1 LOSS_Generator: 7.114030838012695 LOSS_Discriminator: 0.0511765256524086\n",
            "ITERATION_NO.: 2 LOSS_Generator: 7.039840221405029 LOSS_Discriminator: 0.05394451320171356\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.014601230621338 LOSS_Discriminator: 0.016430098563432693\n",
            "ITERATION_NO.: 4 LOSS_Generator: 6.470792770385742 LOSS_Discriminator: 0.07100988179445267\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.695827007293701 LOSS_Discriminator: 0.0542774423956871\n",
            "ITERATION_NO.: 6 LOSS_Generator: 7.02500057220459 LOSS_Discriminator: 0.0517716258764267\n",
            "ITERATION_NO.: 7 LOSS_Generator: 7.233627796173096 LOSS_Discriminator: 0.07154083251953125\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.094711780548096 LOSS_Discriminator: 0.07518330961465836\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.295687198638916 LOSS_Discriminator: 0.04157692566514015\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.370356559753418 LOSS_Discriminator: 0.05065317824482918\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.575498104095459 LOSS_Discriminator: 0.07454751431941986\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.557546615600586 LOSS_Discriminator: 0.041865356266498566\n",
            "ITERATION_NO.: 13 LOSS_Generator: 7.058059215545654 LOSS_Discriminator: 0.06048827990889549\n",
            "ITERATION_NO.: 14 LOSS_Generator: 7.0082597732543945 LOSS_Discriminator: 0.07081729173660278\n",
            "ITERATION_NO.: 15 LOSS_Generator: 6.394091606140137 LOSS_Discriminator: 0.05233384668827057\n",
            "ITERATION_NO.: 16 LOSS_Generator: 6.55177116394043 LOSS_Discriminator: 0.05790984630584717\n",
            "ITERATION_NO.: 17 LOSS_Generator: 6.590511322021484 LOSS_Discriminator: 0.05432959273457527\n",
            "ITERATION_NO.: 18 LOSS_Generator: 6.903494358062744 LOSS_Discriminator: 0.08334468305110931\n",
            "ITERATION_NO.: 19 LOSS_Generator: 6.736447334289551 LOSS_Discriminator: 0.028625018894672394\n",
            "ITERATION_NO.: 20 LOSS_Generator: 6.9938740730285645 LOSS_Discriminator: 0.05871006101369858\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.8175740242004395 LOSS_Discriminator: 0.02842220664024353\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.187376499176025 LOSS_Discriminator: 0.10705280303955078\n",
            "ITERATION_NO.: 23 LOSS_Generator: 6.94443416595459 LOSS_Discriminator: 0.10088224709033966\n",
            "ITERATION_NO.: 24 LOSS_Generator: 7.794673442840576 LOSS_Discriminator: 0.05191102996468544\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.854334831237793 LOSS_Discriminator: 0.04947946220636368\n",
            "ITERATION_NO.: 26 LOSS_Generator: 8.012713432312012 LOSS_Discriminator: 0.10537052154541016\n",
            "ITERATION_NO.: 27 LOSS_Generator: 6.756824970245361 LOSS_Discriminator: 0.04098284989595413\n",
            "ITERATION_NO.: 28 LOSS_Generator: 6.807846546173096 LOSS_Discriminator: 0.030959773808717728\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.223958969116211 LOSS_Discriminator: 0.12289168685674667\n",
            "ITERATION_NO.: 30 LOSS_Generator: 7.08900260925293 LOSS_Discriminator: 0.05772296339273453\n",
            "ITERATION_NO.: 31 LOSS_Generator: 7.833432674407959 LOSS_Discriminator: 0.03886222839355469\n",
            "ITERATION_NO.: 32 LOSS_Generator: 8.530085563659668 LOSS_Discriminator: 0.10194933414459229\n",
            "ITERATION_NO.: 33 LOSS_Generator: 9.297778129577637 LOSS_Discriminator: 0.0859166830778122\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.136401176452637 LOSS_Discriminator: 0.12177839875221252\n",
            "ITERATION_NO.: 35 LOSS_Generator: 7.223078727722168 LOSS_Discriminator: 0.044850751757621765\n",
            "ITERATION_NO.: 36 LOSS_Generator: 5.734100818634033 LOSS_Discriminator: 0.09207384288311005\n",
            "ITERATION_NO.: 37 LOSS_Generator: 5.955918788909912 LOSS_Discriminator: 0.1312749683856964\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.036106586456299 LOSS_Discriminator: 0.11106517165899277\n",
            "ITERATION_NO.: 39 LOSS_Generator: 7.2722649574279785 LOSS_Discriminator: 0.06905965507030487\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.8241753578186035 LOSS_Discriminator: 0.07860969007015228\n",
            "ITERATION_NO.: 41 LOSS_Generator: 8.430715560913086 LOSS_Discriminator: 0.10603129863739014\n",
            "ITERATION_NO.: 42 LOSS_Generator: 8.7085542678833 LOSS_Discriminator: 0.08662310987710953\n",
            "ITERATION_NO.: 43 LOSS_Generator: 8.065106391906738 LOSS_Discriminator: 0.0883944034576416\n",
            "ITERATION_NO.: 44 LOSS_Generator: 8.145793914794922 LOSS_Discriminator: 0.026051264256238937\n",
            "ITERATION_NO.: 45 LOSS_Generator: 7.474611759185791 LOSS_Discriminator: 0.06602516770362854\n",
            "ITERATION_NO.: 46 LOSS_Generator: 7.1254658699035645 LOSS_Discriminator: 0.08708219975233078\n",
            "ITERATION_NO.: 47 LOSS_Generator: 6.89444637298584 LOSS_Discriminator: 0.05586615949869156\n",
            "ITERATION_NO.: 48 LOSS_Generator: 7.052690505981445 LOSS_Discriminator: 0.09224850684404373\n",
            "ITERATION_NO.: 49 LOSS_Generator: 7.584018707275391 LOSS_Discriminator: 0.058732226490974426\n",
            "ITERATION_NO.: 50 LOSS_Generator: 7.809951305389404 LOSS_Discriminator: 0.04912739247083664\n",
            "ITERATION_NO.: 51 LOSS_Generator: 7.245244026184082 LOSS_Discriminator: 0.06015913933515549\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.057231426239014 LOSS_Discriminator: 0.045884378254413605\n",
            "ITERATION_NO.: 53 LOSS_Generator: 6.818258285522461 LOSS_Discriminator: 0.08212648332118988\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.809010028839111 LOSS_Discriminator: 0.0803084522485733\n",
            "ITERATION_NO.: 55 LOSS_Generator: 7.162125110626221 LOSS_Discriminator: 0.053266361355781555\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.7558417320251465 LOSS_Discriminator: 0.05591997504234314\n",
            "ITERATION_NO.: 57 LOSS_Generator: 7.551995754241943 LOSS_Discriminator: 0.06827515363693237\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.752565860748291 LOSS_Discriminator: 0.08253763616085052\n",
            "ITERATION_NO.: 59 LOSS_Generator: 6.8255157470703125 LOSS_Discriminator: 0.06195065379142761\n",
            "ITERATION_NO.: 60 LOSS_Generator: 7.310327053070068 LOSS_Discriminator: 0.03856129199266434\n",
            "ITERATION_NO.: 61 LOSS_Generator: 7.3153791427612305 LOSS_Discriminator: 0.0773952454328537\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.920975208282471 LOSS_Discriminator: 0.0547289177775383\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.535552978515625 LOSS_Discriminator: 0.11243642866611481\n",
            "ITERATION_NO.: 64 LOSS_Generator: 5.9235992431640625 LOSS_Discriminator: 0.07949250936508179\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.094608306884766 LOSS_Discriminator: 0.06669413298368454\n",
            "ITERATION_NO.: 66 LOSS_Generator: 6.955160140991211 LOSS_Discriminator: 0.04208630695939064\n",
            "ITERATION_NO.: 67 LOSS_Generator: 8.510120391845703 LOSS_Discriminator: 0.036856759339571\n",
            "ITERATION_NO.: 68 LOSS_Generator: 8.504383087158203 LOSS_Discriminator: 0.07838553935289383\n",
            "ITERATION_NO.: 69 LOSS_Generator: 8.14161491394043 LOSS_Discriminator: 0.09287482500076294\n",
            "ITERATION_NO.: 70 LOSS_Generator: 7.953962326049805 LOSS_Discriminator: 0.05497624725103378\n",
            "ITERATION_NO.: 71 LOSS_Generator: 6.705384731292725 LOSS_Discriminator: 0.08305114507675171\n",
            "ITERATION_NO.: 72 LOSS_Generator: 6.561946868896484 LOSS_Discriminator: 0.09485594183206558\n",
            "ITERATION_NO.: 73 LOSS_Generator: 6.945445537567139 LOSS_Discriminator: 0.0782245397567749\n",
            "ITERATION_NO.: 74 LOSS_Generator: 7.205753326416016 LOSS_Discriminator: 0.028575550764799118\n",
            "ITERATION_NO.: 75 LOSS_Generator: 8.158794403076172 LOSS_Discriminator: 0.030025897547602654\n",
            "ITERATION_NO.: 76 LOSS_Generator: 8.64688491821289 LOSS_Discriminator: 0.09441467374563217\n",
            "ITERATION_NO.: 77 LOSS_Generator: 8.932692527770996 LOSS_Discriminator: 0.07359430193901062\n",
            "ITERATION_NO.: 78 LOSS_Generator: 8.032391548156738 LOSS_Discriminator: 0.07547968626022339\n",
            "ITERATION_NO.: 79 LOSS_Generator: 6.786596775054932 LOSS_Discriminator: 0.07474003732204437\n",
            "ITERATION_NO.: 80 LOSS_Generator: 6.342352867126465 LOSS_Discriminator: 0.07775072753429413\n",
            "ITERATION_NO.: 81 LOSS_Generator: 5.911828517913818 LOSS_Discriminator: 0.12885458767414093\n",
            "ITERATION_NO.: 82 LOSS_Generator: 6.75225830078125 LOSS_Discriminator: 0.06549596041440964\n",
            "ITERATION_NO.: 83 LOSS_Generator: 7.977077484130859 LOSS_Discriminator: 0.04721827432513237\n",
            "ITERATION_NO.: 84 LOSS_Generator: 8.119680404663086 LOSS_Discriminator: 0.11061638593673706\n",
            "ITERATION_NO.: 85 LOSS_Generator: 7.941655158996582 LOSS_Discriminator: 0.08819606900215149\n",
            "ITERATION_NO.: 86 LOSS_Generator: 7.335786819458008 LOSS_Discriminator: 0.02055501565337181\n",
            "ITERATION_NO.: 87 LOSS_Generator: 7.462335586547852 LOSS_Discriminator: 0.08966735750436783\n",
            "ITERATION_NO.: 88 LOSS_Generator: 6.574114799499512 LOSS_Discriminator: 0.05114710330963135\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.156185150146484 LOSS_Discriminator: 0.11032017320394516\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.140425205230713 LOSS_Discriminator: 0.09989834576845169\n",
            "ITERATION_NO.: 91 LOSS_Generator: 7.241389751434326 LOSS_Discriminator: 0.060146212577819824\n",
            "ITERATION_NO.: 92 LOSS_Generator: 8.0452241897583 LOSS_Discriminator: 0.058300428092479706\n",
            "ITERATION_NO.: 93 LOSS_Generator: 7.45705509185791 LOSS_Discriminator: 0.11909842491149902\n",
            "ITERATION_NO.: 94 LOSS_Generator: 7.645230770111084 LOSS_Discriminator: 0.04590637981891632\n",
            "ITERATION_NO.: 95 LOSS_Generator: 7.021944522857666 LOSS_Discriminator: 0.12228799611330032\n",
            "ITERATION_NO.: 96 LOSS_Generator: 7.239506244659424 LOSS_Discriminator: 0.04259279742836952\n",
            "ITERATION_NO.: 97 LOSS_Generator: 7.1133551597595215 LOSS_Discriminator: 0.18578605353832245\n",
            "ITERATION_NO.: 98 LOSS_Generator: 7.509854793548584 LOSS_Discriminator: 0.03585904836654663\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.369802951812744 LOSS_Discriminator: 0.05705742537975311\n",
            "ITERATION_NO.: 100 LOSS_Generator: 7.7686238288879395 LOSS_Discriminator: 0.07096701860427856\n",
            "ITERATION_NO.: 101 LOSS_Generator: 7.634295463562012 LOSS_Discriminator: 0.07795371115207672\n",
            "ITERATION_NO.: 102 LOSS_Generator: 7.431535720825195 LOSS_Discriminator: 0.13498345017433167\n",
            "ITERATION_NO.: 103 LOSS_Generator: 7.183796405792236 LOSS_Discriminator: 0.03788520023226738\n",
            "ITERATION_NO.: 104 LOSS_Generator: 6.8913798332214355 LOSS_Discriminator: 0.07017703354358673\n",
            "ITERATION_NO.: 105 LOSS_Generator: 7.434717178344727 LOSS_Discriminator: 0.039183951914310455\n",
            "ITERATION_NO.: 106 LOSS_Generator: 7.076486587524414 LOSS_Discriminator: 0.039494480937719345\n",
            "ITERATION_NO.: 107 LOSS_Generator: 6.887944221496582 LOSS_Discriminator: 0.04500469192862511\n",
            "ITERATION_NO.: 108 LOSS_Generator: 7.2270894050598145 LOSS_Discriminator: 0.030944349244236946\n",
            "ITERATION_NO.: 109 LOSS_Generator: 7.423284530639648 LOSS_Discriminator: 0.02757020853459835\n",
            "ITERATION_NO.: 110 LOSS_Generator: 7.3787522315979 LOSS_Discriminator: 0.019673753529787064\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.55234432220459 LOSS_Discriminator: 0.059268318116664886\n",
            "ITERATION_NO.: 112 LOSS_Generator: 7.120575428009033 LOSS_Discriminator: 0.07038688659667969\n",
            "ITERATION_NO.: 113 LOSS_Generator: 6.640835762023926 LOSS_Discriminator: 0.04897238686680794\n",
            "ITERATION_NO.: 114 LOSS_Generator: 7.828130722045898 LOSS_Discriminator: 0.08184066414833069\n",
            "ITERATION_NO.: 115 LOSS_Generator: 7.13304328918457 LOSS_Discriminator: 0.08688713610172272\n",
            "ITERATION_NO.: 116 LOSS_Generator: 7.496798038482666 LOSS_Discriminator: 0.04316364973783493\n",
            "ITERATION_NO.: 117 LOSS_Generator: 7.1066107749938965 LOSS_Discriminator: 0.07558612525463104\n",
            "ITERATION_NO.: 118 LOSS_Generator: 7.305109977722168 LOSS_Discriminator: 0.05332199111580849\n",
            "ITERATION_NO.: 119 LOSS_Generator: 6.753773212432861 LOSS_Discriminator: 0.10057750344276428\n",
            "ITERATION_NO.: 120 LOSS_Generator: 6.684332847595215 LOSS_Discriminator: 0.05815459415316582\n",
            "ITERATION_NO.: 121 LOSS_Generator: 6.808255672454834 LOSS_Discriminator: 0.08465251326560974\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.728792190551758 LOSS_Discriminator: 0.049912385642528534\n",
            "ITERATION_NO.: 123 LOSS_Generator: 7.024783134460449 LOSS_Discriminator: 0.060537829995155334\n",
            "ITERATION_NO.: 124 LOSS_Generator: 7.6580424308776855 LOSS_Discriminator: 0.13276919722557068\n",
            "ITERATION_NO.: 125 LOSS_Generator: 7.819634914398193 LOSS_Discriminator: 0.08054031431674957\n",
            "ITERATION_NO.: 126 LOSS_Generator: 8.117578506469727 LOSS_Discriminator: 0.09062953293323517\n",
            "ITERATION_NO.: 127 LOSS_Generator: 8.167436599731445 LOSS_Discriminator: 0.11166711151599884\n",
            "ITERATION_NO.: 128 LOSS_Generator: 6.835626602172852 LOSS_Discriminator: 0.06452088803052902\n",
            "ITERATION_NO.: 129 LOSS_Generator: 5.597141742706299 LOSS_Discriminator: 0.0912494957447052\n",
            "ITERATION_NO.: 130 LOSS_Generator: 5.831070423126221 LOSS_Discriminator: 0.1275346279144287\n",
            "ITERATION_NO.: 131 LOSS_Generator: 7.015709400177002 LOSS_Discriminator: 0.10180991142988205\n",
            "ITERATION_NO.: 132 LOSS_Generator: 7.996973037719727 LOSS_Discriminator: 0.042761579155921936\n",
            "ITERATION_NO.: 133 LOSS_Generator: 8.245368003845215 LOSS_Discriminator: 0.09426721185445786\n",
            "ITERATION_NO.: 134 LOSS_Generator: 8.614246368408203 LOSS_Discriminator: 0.06705860048532486\n",
            "ITERATION_NO.: 135 LOSS_Generator: 7.957238674163818 LOSS_Discriminator: 0.07042805850505829\n",
            "ITERATION_NO.: 136 LOSS_Generator: 6.805339336395264 LOSS_Discriminator: 0.06378404796123505\n",
            "ITERATION_NO.: 137 LOSS_Generator: 5.6562113761901855 LOSS_Discriminator: 0.10973970592021942\n",
            "ITERATION_NO.: 138 LOSS_Generator: 5.7950968742370605 LOSS_Discriminator: 0.12222956120967865\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.974860668182373 LOSS_Discriminator: 0.057956185191869736\n",
            "ITERATION_NO.: 140 LOSS_Generator: 8.173086166381836 LOSS_Discriminator: 0.06659861654043198\n",
            "ITERATION_NO.: 141 LOSS_Generator: 8.497132301330566 LOSS_Discriminator: 0.04604277014732361\n",
            "ITERATION_NO.: 142 LOSS_Generator: 8.766862869262695 LOSS_Discriminator: 0.018514597788453102\n",
            "ITERATION_NO.: 143 LOSS_Generator: 8.759763717651367 LOSS_Discriminator: 0.06890946626663208\n",
            "ITERATION_NO.: 144 LOSS_Generator: 8.650191307067871 LOSS_Discriminator: 0.044698961079120636\n",
            "ITERATION_NO.: 145 LOSS_Generator: 7.574519157409668 LOSS_Discriminator: 0.06568076461553574\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.199347019195557 LOSS_Discriminator: 0.11959711462259293\n",
            "ITERATION_NO.: 147 LOSS_Generator: 5.613605976104736 LOSS_Discriminator: 0.06629393994808197\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.1472930908203125 LOSS_Discriminator: 0.11629261076450348\n",
            "ITERATION_NO.: 149 LOSS_Generator: 8.262234687805176 LOSS_Discriminator: 0.11668471992015839\n",
            "ITERATION_NO.: 150 LOSS_Generator: 9.89206600189209 LOSS_Discriminator: 0.1110491156578064\n",
            "ITERATION_NO.: 151 LOSS_Generator: 10.383134841918945 LOSS_Discriminator: 0.18967081606388092\n",
            "ITERATION_NO.: 152 LOSS_Generator: 9.076969146728516 LOSS_Discriminator: 0.044636502861976624\n",
            "ITERATION_NO.: 153 LOSS_Generator: 7.974541187286377 LOSS_Discriminator: 0.06022443249821663\n",
            "ITERATION_NO.: 154 LOSS_Generator: 7.195992469787598 LOSS_Discriminator: 0.036129117012023926\n",
            "ITERATION_NO.: 155 LOSS_Generator: 6.551100254058838 LOSS_Discriminator: 0.1660388708114624\n",
            "ITERATION_NO.: 156 LOSS_Generator: 6.484063625335693 LOSS_Discriminator: 0.05017882585525513\n",
            "ITERATION_NO.: 157 LOSS_Generator: 6.907590866088867 LOSS_Discriminator: 0.056860677897930145\n",
            "ITERATION_NO.: 158 LOSS_Generator: 6.981950759887695 LOSS_Discriminator: 0.04044688120484352\n",
            "ITERATION_NO.: 159 LOSS_Generator: 7.255258560180664 LOSS_Discriminator: 0.019724931567907333\n",
            "ITERATION_NO.: 160 LOSS_Generator: 7.9093918800354 LOSS_Discriminator: 0.04133588448166847\n",
            "ITERATION_NO.: 161 LOSS_Generator: 8.218823432922363 LOSS_Discriminator: 0.045583415776491165\n",
            "ITERATION_NO.: 162 LOSS_Generator: 8.25511646270752 LOSS_Discriminator: 0.02478630095720291\n",
            "ITERATION_NO.: 163 LOSS_Generator: 8.194731712341309 LOSS_Discriminator: 0.04843687266111374\n",
            "ITERATION_NO.: 164 LOSS_Generator: 7.601235866546631 LOSS_Discriminator: 0.024603217840194702\n",
            "ITERATION_NO.: 165 LOSS_Generator: 7.418818473815918 LOSS_Discriminator: 0.03925956040620804\n",
            "ITERATION_NO.: 166 LOSS_Generator: 6.7972731590271 LOSS_Discriminator: 0.04256746545433998\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.897030830383301 LOSS_Discriminator: 0.07441334426403046\n",
            "ITERATION_NO.: 168 LOSS_Generator: 6.544549942016602 LOSS_Discriminator: 0.05630049109458923\n",
            "ITERATION_NO.: 169 LOSS_Generator: 7.428316593170166 LOSS_Discriminator: 0.03249624744057655\n",
            "ITERATION_NO.: 170 LOSS_Generator: 7.3059797286987305 LOSS_Discriminator: 0.03827238082885742\n",
            "ITERATION_NO.: 171 LOSS_Generator: 7.78281307220459 LOSS_Discriminator: 0.06580685824155807\n",
            "ITERATION_NO.: 172 LOSS_Generator: 7.427516937255859 LOSS_Discriminator: 0.05932580307126045\n",
            "ITERATION_NO.: 173 LOSS_Generator: 7.732287406921387 LOSS_Discriminator: 0.07056128233671188\n",
            "ITERATION_NO.: 174 LOSS_Generator: 7.257445812225342 LOSS_Discriminator: 0.06007968634366989\n",
            "ITERATION_NO.: 175 LOSS_Generator: 7.1939544677734375 LOSS_Discriminator: 0.057772085070610046\n",
            "ITERATION_NO.: 176 LOSS_Generator: 7.6130805015563965 LOSS_Discriminator: 0.09692402184009552\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.885573863983154 LOSS_Discriminator: 0.024564215913414955\n",
            "ITERATION_NO.: 178 LOSS_Generator: 7.028387069702148 LOSS_Discriminator: 0.03810150548815727\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.040267467498779 LOSS_Discriminator: 0.05035177618265152\n",
            "ITERATION_NO.: 180 LOSS_Generator: 7.639369487762451 LOSS_Discriminator: 0.051003944128751755\n",
            "ITERATION_NO.: 181 LOSS_Generator: 6.583067417144775 LOSS_Discriminator: 0.08330120146274567\n",
            "ITERATION_NO.: 182 LOSS_Generator: 6.980429649353027 LOSS_Discriminator: 0.02674371935427189\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.721576690673828 LOSS_Discriminator: 0.07224695384502411\n",
            "ITERATION_NO.: 184 LOSS_Generator: 8.181914329528809 LOSS_Discriminator: 0.024305321276187897\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.391241550445557 LOSS_Discriminator: 0.1394326239824295\n",
            "ITERATION_NO.: 186 LOSS_Generator: 6.389936447143555 LOSS_Discriminator: 0.060451604425907135\n",
            "ITERATION_NO.: 187 LOSS_Generator: 5.8617353439331055 LOSS_Discriminator: 0.05047919601202011\n",
            "ITERATION_NO.: 188 LOSS_Generator: 6.434808731079102 LOSS_Discriminator: 0.11471571028232574\n",
            "ITERATION_NO.: 189 LOSS_Generator: 6.958505630493164 LOSS_Discriminator: 0.07837586104869843\n",
            "ITERATION_NO.: 190 LOSS_Generator: 7.075676918029785 LOSS_Discriminator: 0.10910701751708984\n",
            "ITERATION_NO.: 191 LOSS_Generator: 7.195687770843506 LOSS_Discriminator: 0.08303388953208923\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.6311492919921875 LOSS_Discriminator: 0.058735232800245285\n",
            "ITERATION_NO.: 193 LOSS_Generator: 6.87432861328125 LOSS_Discriminator: 0.07297258824110031\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.508884906768799 LOSS_Discriminator: 0.049028489738702774\n",
            "ITERATION_NO.: 195 LOSS_Generator: 7.036471366882324 LOSS_Discriminator: 0.041081495583057404\n",
            "ITERATION_NO.: 196 LOSS_Generator: 7.13031005859375 LOSS_Discriminator: 0.04127637669444084\n",
            "ITERATION_NO.: 197 LOSS_Generator: 7.355035305023193 LOSS_Discriminator: 0.06358762830495834\n",
            "ITERATION_NO.: 198 LOSS_Generator: 7.509517192840576 LOSS_Discriminator: 0.07713725417852402\n",
            "ITERATION_NO.: 199 LOSS_Generator: 7.440976619720459 LOSS_Discriminator: 0.059333860874176025\n",
            "ITERATION_NO.: 200 LOSS_Generator: 6.730281829833984 LOSS_Discriminator: 0.04860472306609154\n",
            "ITERATION_NO.: 201 LOSS_Generator: 6.328916072845459 LOSS_Discriminator: 0.07917455583810806\n",
            "ITERATION_NO.: 202 LOSS_Generator: 6.425968170166016 LOSS_Discriminator: 0.0866619125008583\n",
            "ITERATION_NO.: 203 LOSS_Generator: 6.51759147644043 LOSS_Discriminator: 0.06063919514417648\n",
            "ITERATION_NO.: 204 LOSS_Generator: 7.4209723472595215 LOSS_Discriminator: 0.13985633850097656\n",
            "ITERATION_NO.: 205 LOSS_Generator: 8.131775856018066 LOSS_Discriminator: 0.07499194145202637\n",
            "ITERATION_NO.: 206 LOSS_Generator: 8.155866622924805 LOSS_Discriminator: 0.08108627796173096\n",
            "ITERATION_NO.: 207 LOSS_Generator: 7.652404308319092 LOSS_Discriminator: 0.03491625562310219\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.207758903503418 LOSS_Discriminator: 0.08803146332502365\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.081196308135986 LOSS_Discriminator: 0.04687849432229996\n",
            "ITERATION_NO.: 210 LOSS_Generator: 7.329141616821289 LOSS_Discriminator: 0.03155824914574623\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.174374580383301 LOSS_Discriminator: 0.07985875755548477\n",
            "ITERATION_NO.: 212 LOSS_Generator: 7.531508922576904 LOSS_Discriminator: 0.06820473819971085\n",
            "ITERATION_NO.: 213 LOSS_Generator: 7.31436824798584 LOSS_Discriminator: 0.05235625058412552\n",
            "ITERATION_NO.: 214 LOSS_Generator: 6.622996807098389 LOSS_Discriminator: 0.06538359820842743\n",
            "ITERATION_NO.: 215 LOSS_Generator: 6.245794296264648 LOSS_Discriminator: 0.08775687217712402\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.699325084686279 LOSS_Discriminator: 0.07241292297840118\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.994605541229248 LOSS_Discriminator: 0.049838822335004807\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.805063247680664 LOSS_Discriminator: 0.05540400370955467\n",
            "ITERATION_NO.: 219 LOSS_Generator: 7.381437301635742 LOSS_Discriminator: 0.07355563342571259\n",
            "ITERATION_NO.: 220 LOSS_Generator: 7.535687446594238 LOSS_Discriminator: 0.02953299507498741\n",
            "ITERATION_NO.: 221 LOSS_Generator: 7.037776470184326 LOSS_Discriminator: 0.02743958868086338\n",
            "ITERATION_NO.: 222 LOSS_Generator: 7.881105422973633 LOSS_Discriminator: 0.06512971222400665\n",
            "ITERATION_NO.: 223 LOSS_Generator: 7.222753524780273 LOSS_Discriminator: 0.03978604078292847\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.264791965484619 LOSS_Discriminator: 0.08833672106266022\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.457064628601074 LOSS_Discriminator: 0.03235848993062973\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.056472301483154 LOSS_Discriminator: 0.12407971173524857\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.321561336517334 LOSS_Discriminator: 0.10221762210130692\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.622298717498779 LOSS_Discriminator: 0.08694539964199066\n",
            "ITERATION_NO.: 229 LOSS_Generator: 8.00268268585205 LOSS_Discriminator: 0.12760356068611145\n",
            "ITERATION_NO.: 230 LOSS_Generator: 8.630267143249512 LOSS_Discriminator: 0.008208762854337692\n",
            "ITERATION_NO.: 231 LOSS_Generator: 9.271915435791016 LOSS_Discriminator: 0.04842790216207504\n",
            "ITERATION_NO.: 232 LOSS_Generator: 8.421277046203613 LOSS_Discriminator: 0.21889261901378632\n",
            "ITERATION_NO.: 233 LOSS_Generator: 5.804103851318359 LOSS_Discriminator: 0.09136966615915298\n",
            "ITERATION_NO.: 234 LOSS_Generator: 6.266565322875977 LOSS_Discriminator: 0.0782088041305542\n",
            "ITERATION_NO.: 235 LOSS_Generator: 6.635636806488037 LOSS_Discriminator: 0.06163249909877777\n",
            "ITERATION_NO.: 236 LOSS_Generator: 7.071377754211426 LOSS_Discriminator: 0.061375949531793594\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.606728553771973 LOSS_Discriminator: 0.024618301540613174\n",
            "ITERATION_NO.: 238 LOSS_Generator: 7.794707775115967 LOSS_Discriminator: 0.07237488776445389\n",
            "ITERATION_NO.: 239 LOSS_Generator: 8.092638969421387 LOSS_Discriminator: 0.05441625788807869\n",
            "ITERATION_NO.: 240 LOSS_Generator: 7.4693522453308105 LOSS_Discriminator: 0.02246992662549019\n",
            "ITERATION_NO.: 241 LOSS_Generator: 7.234799385070801 LOSS_Discriminator: 0.0810931921005249\n",
            "ITERATION_NO.: 242 LOSS_Generator: 7.196601390838623 LOSS_Discriminator: 0.04840536788105965\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.85257625579834 LOSS_Discriminator: 0.0712495893239975\n",
            "ITERATION_NO.: 244 LOSS_Generator: 6.166134834289551 LOSS_Discriminator: 0.06668633222579956\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.8525614738464355 LOSS_Discriminator: 0.09651260823011398\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.661518096923828 LOSS_Discriminator: 0.04240958392620087\n",
            "ITERATION_NO.: 247 LOSS_Generator: 7.259234428405762 LOSS_Discriminator: 0.03519424796104431\n",
            "ITERATION_NO.: 248 LOSS_Generator: 7.199371814727783 LOSS_Discriminator: 0.026598583906888962\n",
            "ITERATION_NO.: 249 LOSS_Generator: 7.331209659576416 LOSS_Discriminator: 0.031232982873916626\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.393707752227783 LOSS_Discriminator: 0.060170531272888184\n",
            "ITERATION_NO.: 251 LOSS_Generator: 6.361865043640137 LOSS_Discriminator: 0.05900316685438156\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.682919502258301 LOSS_Discriminator: 0.05274764448404312\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.577047824859619 LOSS_Discriminator: 0.035291582345962524\n",
            "ITERATION_NO.: 254 LOSS_Generator: 7.562727451324463 LOSS_Discriminator: 0.041496433317661285\n",
            "ITERATION_NO.: 255 LOSS_Generator: 7.876118183135986 LOSS_Discriminator: 0.06390482932329178\n",
            "ITERATION_NO.: 256 LOSS_Generator: 8.444440841674805 LOSS_Discriminator: 0.049751028418540955\n",
            "ITERATION_NO.: 257 LOSS_Generator: 8.20006275177002 LOSS_Discriminator: 0.07134434580802917\n",
            "ITERATION_NO.: 258 LOSS_Generator: 7.507021903991699 LOSS_Discriminator: 0.04537395387887955\n",
            "ITERATION_NO.: 259 LOSS_Generator: 6.548448085784912 LOSS_Discriminator: 0.02963143214583397\n",
            "ITERATION_NO.: 260 LOSS_Generator: 6.184948921203613 LOSS_Discriminator: 0.07127919793128967\n",
            "ITERATION_NO.: 261 LOSS_Generator: 5.851815223693848 LOSS_Discriminator: 0.050293173640966415\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.286477565765381 LOSS_Discriminator: 0.06313835084438324\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.944270133972168 LOSS_Discriminator: 0.03195595741271973\n",
            "ITERATION_NO.: 264 LOSS_Generator: 7.97465705871582 LOSS_Discriminator: 0.04913271963596344\n",
            "ITERATION_NO.: 265 LOSS_Generator: 8.483602523803711 LOSS_Discriminator: 0.013180552050471306\n",
            "ITERATION_NO.: 266 LOSS_Generator: 8.974447250366211 LOSS_Discriminator: 0.06834769994020462\n",
            "ITERATION_NO.: 267 LOSS_Generator: 7.765774726867676 LOSS_Discriminator: 0.07006760686635971\n",
            "ITERATION_NO.: 268 LOSS_Generator: 6.5745086669921875 LOSS_Discriminator: 0.06954027712345123\n",
            "ITERATION_NO.: 269 LOSS_Generator: 5.8598551750183105 LOSS_Discriminator: 0.05452502891421318\n",
            "ITERATION_NO.: 270 LOSS_Generator: 5.908048629760742 LOSS_Discriminator: 0.13001835346221924\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.794749736785889 LOSS_Discriminator: 0.1105760931968689\n",
            "ITERATION_NO.: 272 LOSS_Generator: 8.388229370117188 LOSS_Discriminator: 0.09456168860197067\n",
            "ITERATION_NO.: 273 LOSS_Generator: 8.976629257202148 LOSS_Discriminator: 0.08667360246181488\n",
            "ITERATION_NO.: 274 LOSS_Generator: 8.924332618713379 LOSS_Discriminator: 0.12147602438926697\n",
            "ITERATION_NO.: 275 LOSS_Generator: 8.006519317626953 LOSS_Discriminator: 0.07003431022167206\n",
            "ITERATION_NO.: 276 LOSS_Generator: 7.567706108093262 LOSS_Discriminator: 0.06358733773231506\n",
            "ITERATION_NO.: 277 LOSS_Generator: 6.495456695556641 LOSS_Discriminator: 0.07386643439531326\n",
            "ITERATION_NO.: 278 LOSS_Generator: 6.628738880157471 LOSS_Discriminator: 0.05134378746151924\n",
            "ITERATION_NO.: 279 LOSS_Generator: 7.002651214599609 LOSS_Discriminator: 0.03900005668401718\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.222349643707275 LOSS_Discriminator: 0.035778772085905075\n",
            "ITERATION_NO.: 281 LOSS_Generator: 7.784090042114258 LOSS_Discriminator: 0.03512034937739372\n",
            "ITERATION_NO.: 282 LOSS_Generator: 7.5860209465026855 LOSS_Discriminator: 0.04830783233046532\n",
            "ITERATION_NO.: 283 LOSS_Generator: 7.045715808868408 LOSS_Discriminator: 0.0240500308573246\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.265544891357422 LOSS_Discriminator: 0.030045926570892334\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.143765449523926 LOSS_Discriminator: 0.04138270765542984\n",
            "ITERATION_NO.: 286 LOSS_Generator: 7.2601165771484375 LOSS_Discriminator: 0.051429376006126404\n",
            "ITERATION_NO.: 287 LOSS_Generator: 7.219619274139404 LOSS_Discriminator: 0.030880670994520187\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.365907669067383 LOSS_Discriminator: 0.02963448502123356\n",
            "ITERATION_NO.: 289 LOSS_Generator: 7.948793411254883 LOSS_Discriminator: 0.04671238362789154\n",
            "ITERATION_NO.: 290 LOSS_Generator: 7.912572860717773 LOSS_Discriminator: 0.07868875563144684\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.5831298828125 LOSS_Discriminator: 0.04227063059806824\n",
            "ITERATION_NO.: 292 LOSS_Generator: 7.283474922180176 LOSS_Discriminator: 0.07600487768650055\n",
            "ITERATION_NO.: 293 LOSS_Generator: 7.781416416168213 LOSS_Discriminator: 0.029163023456931114\n",
            "ITERATION_NO.: 294 LOSS_Generator: 7.387148380279541 LOSS_Discriminator: 0.029186293482780457\n",
            "ITERATION_NO.: 295 LOSS_Generator: 8.386261940002441 LOSS_Discriminator: 0.03268110752105713\n",
            "ITERATION_NO.: 296 LOSS_Generator: 7.732888221740723 LOSS_Discriminator: 0.14814332127571106\n",
            "ITERATION_NO.: 297 LOSS_Generator: 7.5123372077941895 LOSS_Discriminator: 0.1319194734096527\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.705599308013916 LOSS_Discriminator: 0.07023601233959198\n",
            "ITERATION_NO.: 299 LOSS_Generator: 6.42991828918457 LOSS_Discriminator: 0.15822884440422058\n",
            "ITERATION_NO.: 300 LOSS_Generator: 7.468989849090576 LOSS_Discriminator: 0.09906703233718872\n",
            "ITERATION_NO.: 301 LOSS_Generator: 6.829428195953369 LOSS_Discriminator: 0.0723242312669754\n",
            "ITERATION_NO.: 302 LOSS_Generator: 7.718219757080078 LOSS_Discriminator: 0.05718369036912918\n",
            "ITERATION_NO.: 303 LOSS_Generator: 8.090117454528809 LOSS_Discriminator: 0.0758126750588417\n",
            "ITERATION_NO.: 304 LOSS_Generator: 8.576387405395508 LOSS_Discriminator: 0.05318877473473549\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.834445953369141 LOSS_Discriminator: 0.14009350538253784\n",
            "ITERATION_NO.: 306 LOSS_Generator: 6.12828254699707 LOSS_Discriminator: 0.08802966773509979\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.406492233276367 LOSS_Discriminator: 0.1559467315673828\n",
            "ITERATION_NO.: 308 LOSS_Generator: 7.842953681945801 LOSS_Discriminator: 0.17652563750743866\n",
            "ITERATION_NO.: 309 LOSS_Generator: 8.840123176574707 LOSS_Discriminator: 0.042664483189582825\n",
            "ITERATION_NO.: 310 LOSS_Generator: 9.647769927978516 LOSS_Discriminator: 0.035331010818481445\n",
            "ITERATION_NO.: 311 LOSS_Generator: 9.372185707092285 LOSS_Discriminator: 0.13067442178726196\n",
            "ITERATION_NO.: 312 LOSS_Generator: 9.016446113586426 LOSS_Discriminator: 0.056751832365989685\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.976307392120361 LOSS_Discriminator: 0.10725418478250504\n",
            "ITERATION_NO.: 314 LOSS_Generator: 6.643145561218262 LOSS_Discriminator: 0.0675525963306427\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.455333709716797 LOSS_Discriminator: 0.12085441499948502\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.6472930908203125 LOSS_Discriminator: 0.09013862907886505\n",
            "ITERATION_NO.: 317 LOSS_Generator: 6.885845184326172 LOSS_Discriminator: 0.1386314332485199\n",
            "ITERATION_NO.: 318 LOSS_Generator: 8.218460083007812 LOSS_Discriminator: 0.06534440815448761\n",
            "ITERATION_NO.: 319 LOSS_Generator: 8.567888259887695 LOSS_Discriminator: 0.05095517262816429\n",
            "ITERATION_NO.: 320 LOSS_Generator: 8.478840827941895 LOSS_Discriminator: 0.047857608646154404\n",
            "ITERATION_NO.: 321 LOSS_Generator: 7.780086040496826 LOSS_Discriminator: 0.07377895712852478\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.43007755279541 LOSS_Discriminator: 0.04255932196974754\n",
            "ITERATION_NO.: 323 LOSS_Generator: 7.946959972381592 LOSS_Discriminator: 0.053167566657066345\n",
            "ITERATION_NO.: 324 LOSS_Generator: 7.776772975921631 LOSS_Discriminator: 0.07022199034690857\n",
            "ITERATION_NO.: 325 LOSS_Generator: 8.254103660583496 LOSS_Discriminator: 0.029526904225349426\n",
            "ITERATION_NO.: 326 LOSS_Generator: 8.243361473083496 LOSS_Discriminator: 0.04127078130841255\n",
            "ITERATION_NO.: 327 LOSS_Generator: 8.844121932983398 LOSS_Discriminator: 0.03821048140525818\n",
            "ITERATION_NO.: 328 LOSS_Generator: 8.241800308227539 LOSS_Discriminator: 0.027163196355104446\n",
            "ITERATION_NO.: 329 LOSS_Generator: 7.725949287414551 LOSS_Discriminator: 0.051298268139362335\n",
            "ITERATION_NO.: 330 LOSS_Generator: 7.194686412811279 LOSS_Discriminator: 0.05020624399185181\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.528240203857422 LOSS_Discriminator: 0.040125515311956406\n",
            "ITERATION_NO.: 332 LOSS_Generator: 7.2775373458862305 LOSS_Discriminator: 0.05027857422828674\n",
            "ITERATION_NO.: 333 LOSS_Generator: 6.724851131439209 LOSS_Discriminator: 0.07130885124206543\n",
            "ITERATION_NO.: 334 LOSS_Generator: 7.169580459594727 LOSS_Discriminator: 0.04371911287307739\n",
            "ITERATION_NO.: 335 LOSS_Generator: 6.918749809265137 LOSS_Discriminator: 0.051021985709667206\n",
            "ITERATION_NO.: 336 LOSS_Generator: 7.423128128051758 LOSS_Discriminator: 0.08022649586200714\n",
            "ITERATION_NO.: 337 LOSS_Generator: 8.631444931030273 LOSS_Discriminator: 0.04863938316702843\n",
            "ITERATION_NO.: 338 LOSS_Generator: 9.405172348022461 LOSS_Discriminator: 0.0367862805724144\n",
            "ITERATION_NO.: 339 LOSS_Generator: 9.083025932312012 LOSS_Discriminator: 0.08923229575157166\n",
            "ITERATION_NO.: 340 LOSS_Generator: 8.400651931762695 LOSS_Discriminator: 0.04236287996172905\n",
            "ITERATION_NO.: 341 LOSS_Generator: 6.929078578948975 LOSS_Discriminator: 0.06532036513090134\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.931474208831787 LOSS_Discriminator: 0.12140338867902756\n",
            "ITERATION_NO.: 343 LOSS_Generator: 5.345369338989258 LOSS_Discriminator: 0.08303767442703247\n",
            "ITERATION_NO.: 344 LOSS_Generator: 6.933432102203369 LOSS_Discriminator: 0.15787281095981598\n",
            "ITERATION_NO.: 345 LOSS_Generator: 8.318435668945312 LOSS_Discriminator: 0.07270348072052002\n",
            "ITERATION_NO.: 346 LOSS_Generator: 8.733877182006836 LOSS_Discriminator: 0.06353755295276642\n",
            "ITERATION_NO.: 347 LOSS_Generator: 9.435737609863281 LOSS_Discriminator: 0.21129955351352692\n",
            "ITERATION_NO.: 348 LOSS_Generator: 8.163691520690918 LOSS_Discriminator: 0.09022659808397293\n",
            "ITERATION_NO.: 349 LOSS_Generator: 7.419376850128174 LOSS_Discriminator: 0.08552692085504532\n",
            "ITERATION_NO.: 350 LOSS_Generator: 6.017394542694092 LOSS_Discriminator: 0.0187345240265131\n",
            "ITERATION_NO.: 351 LOSS_Generator: 6.669291973114014 LOSS_Discriminator: 0.11427262425422668\n",
            "ITERATION_NO.: 352 LOSS_Generator: 7.133442401885986 LOSS_Discriminator: 0.05144786089658737\n",
            "ITERATION_NO.: 353 LOSS_Generator: 7.536452770233154 LOSS_Discriminator: 0.017743639647960663\n",
            "ITERATION_NO.: 354 LOSS_Generator: 8.532859802246094 LOSS_Discriminator: 0.11282233893871307\n",
            "ITERATION_NO.: 355 LOSS_Generator: 8.300968170166016 LOSS_Discriminator: 0.1104656308889389\n",
            "ITERATION_NO.: 356 LOSS_Generator: 7.937925338745117 LOSS_Discriminator: 0.04580280929803848\n",
            "ITERATION_NO.: 357 LOSS_Generator: 6.8157429695129395 LOSS_Discriminator: 0.12953707575798035\n",
            "ITERATION_NO.: 358 LOSS_Generator: 5.870867729187012 LOSS_Discriminator: 0.0817415863275528\n",
            "ITERATION_NO.: 359 LOSS_Generator: 6.6903815269470215 LOSS_Discriminator: 0.1145600974559784\n",
            "ITERATION_NO.: 360 LOSS_Generator: 7.8192315101623535 LOSS_Discriminator: 0.04527661204338074\n",
            "ITERATION_NO.: 361 LOSS_Generator: 8.671651840209961 LOSS_Discriminator: 0.0858183354139328\n",
            "ITERATION_NO.: 362 LOSS_Generator: 8.881570816040039 LOSS_Discriminator: 0.05048801004886627\n",
            "ITERATION_NO.: 363 LOSS_Generator: 8.860433578491211 LOSS_Discriminator: 0.03759784996509552\n",
            "ITERATION_NO.: 364 LOSS_Generator: 8.657692909240723 LOSS_Discriminator: 0.08284861594438553\n",
            "ITERATION_NO.: 365 LOSS_Generator: 7.827012538909912 LOSS_Discriminator: 0.03846469148993492\n",
            "ITERATION_NO.: 366 LOSS_Generator: 7.640465259552002 LOSS_Discriminator: 0.032265737652778625\n",
            "ITERATION_NO.: 367 LOSS_Generator: 7.428372859954834 LOSS_Discriminator: 0.0688403844833374\n",
            "ITERATION_NO.: 368 LOSS_Generator: 7.695801258087158 LOSS_Discriminator: 0.11752479523420334\n",
            "ITERATION_NO.: 369 LOSS_Generator: 8.071399688720703 LOSS_Discriminator: 0.07461871951818466\n",
            "ITERATION_NO.: 370 LOSS_Generator: 8.190250396728516 LOSS_Discriminator: 0.03094170242547989\n",
            "ITERATION_NO.: 371 LOSS_Generator: 8.949479103088379 LOSS_Discriminator: 0.08676491677761078\n",
            "ITERATION_NO.: 372 LOSS_Generator: 7.624978065490723 LOSS_Discriminator: 0.1357915848493576\n",
            "ITERATION_NO.: 373 LOSS_Generator: 6.079395771026611 LOSS_Discriminator: 0.08149459958076477\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.404943943023682 LOSS_Discriminator: 0.13661108911037445\n",
            "ITERATION_NO.: 375 LOSS_Generator: 7.065275192260742 LOSS_Discriminator: 0.1538592129945755\n",
            "ITERATION_NO.: 376 LOSS_Generator: 7.90609073638916 LOSS_Discriminator: 0.05833641067147255\n",
            "ITERATION_NO.: 377 LOSS_Generator: 8.607781410217285 LOSS_Discriminator: 0.140431746840477\n",
            "ITERATION_NO.: 378 LOSS_Generator: 8.954081535339355 LOSS_Discriminator: 0.09669849276542664\n",
            "ITERATION_NO.: 379 LOSS_Generator: 8.379819869995117 LOSS_Discriminator: 0.10758163034915924\n",
            "ITERATION_NO.: 380 LOSS_Generator: 7.167679309844971 LOSS_Discriminator: 0.1800762116909027\n",
            "ITERATION_NO.: 381 LOSS_Generator: 5.707606315612793 LOSS_Discriminator: 0.11071879416704178\n",
            "ITERATION_NO.: 382 LOSS_Generator: 5.552546501159668 LOSS_Discriminator: 0.08455731719732285\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.584649085998535 LOSS_Discriminator: 0.09271799027919769\n",
            "ITERATION_NO.: 384 LOSS_Generator: 8.51134967803955 LOSS_Discriminator: 0.10947851836681366\n",
            "ITERATION_NO.: 385 LOSS_Generator: 9.14631175994873 LOSS_Discriminator: 0.0766100287437439\n",
            "ITERATION_NO.: 386 LOSS_Generator: 9.515869140625 LOSS_Discriminator: 0.23726516962051392\n",
            "ITERATION_NO.: 387 LOSS_Generator: 8.589406967163086 LOSS_Discriminator: 0.17749284207820892\n",
            "ITERATION_NO.: 388 LOSS_Generator: 6.680383682250977 LOSS_Discriminator: 0.11063589155673981\n",
            "ITERATION_NO.: 389 LOSS_Generator: 6.077628135681152 LOSS_Discriminator: 0.10386002063751221\n",
            "ITERATION_NO.: 390 LOSS_Generator: 6.146907806396484 LOSS_Discriminator: 0.19036871194839478\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.792953014373779 LOSS_Discriminator: 0.15979671478271484\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.55722713470459 LOSS_Discriminator: 0.10141654312610626\n",
            "ITERATION_NO.: 393 LOSS_Generator: 7.995930194854736 LOSS_Discriminator: 0.04599969834089279\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.143159866333008 LOSS_Discriminator: 0.1391637921333313\n",
            "ITERATION_NO.: 395 LOSS_Generator: 7.75663948059082 LOSS_Discriminator: 0.04049460217356682\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.762329578399658 LOSS_Discriminator: 0.05994107574224472\n",
            "ITERATION_NO.: 397 LOSS_Generator: 6.697065830230713 LOSS_Discriminator: 0.03731437027454376\n",
            "ITERATION_NO.: 398 LOSS_Generator: 6.907838344573975 LOSS_Discriminator: 0.04669388383626938\n",
            "ITERATION_NO.: 399 LOSS_Generator: 6.780569553375244 LOSS_Discriminator: 0.03057899698615074\n",
            "ITERATION_NO.: 400 LOSS_Generator: 6.971945285797119 LOSS_Discriminator: 0.05619393289089203\n",
            "ITERATION_NO.: 401 LOSS_Generator: 7.595415115356445 LOSS_Discriminator: 0.06086069345474243\n",
            "ITERATION_NO.: 402 LOSS_Generator: 7.563681602478027 LOSS_Discriminator: 0.05089018493890762\n",
            "ITERATION_NO.: 403 LOSS_Generator: 7.87666130065918 LOSS_Discriminator: 0.04410893842577934\n",
            "ITERATION_NO.: 404 LOSS_Generator: 7.608608245849609 LOSS_Discriminator: 0.10335338115692139\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.4690985679626465 LOSS_Discriminator: 0.06039636209607124\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.151975154876709 LOSS_Discriminator: 0.07395879924297333\n",
            "ITERATION_NO.: 407 LOSS_Generator: 5.544765472412109 LOSS_Discriminator: 0.09917031973600388\n",
            "ITERATION_NO.: 408 LOSS_Generator: 6.053486347198486 LOSS_Discriminator: 0.10113240778446198\n",
            "ITERATION_NO.: 409 LOSS_Generator: 7.250482082366943 LOSS_Discriminator: 0.03875817731022835\n",
            "ITERATION_NO.: 410 LOSS_Generator: 7.839283466339111 LOSS_Discriminator: 0.032419055700302124\n",
            "ITERATION_NO.: 411 LOSS_Generator: 8.337992668151855 LOSS_Discriminator: 0.06404858082532883\n",
            "ITERATION_NO.: 412 LOSS_Generator: 8.317731857299805 LOSS_Discriminator: 0.12259171903133392\n",
            "ITERATION_NO.: 413 LOSS_Generator: 8.033038139343262 LOSS_Discriminator: 0.08682358264923096\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.201425075531006 LOSS_Discriminator: 0.10078530758619308\n",
            "ITERATION_NO.: 415 LOSS_Generator: 5.041439056396484 LOSS_Discriminator: 0.08149214833974838\n",
            "ITERATION_NO.: 416 LOSS_Generator: 6.295078277587891 LOSS_Discriminator: 0.10764208436012268\n",
            "ITERATION_NO.: 417 LOSS_Generator: 6.603186130523682 LOSS_Discriminator: 0.08670219779014587\n",
            "ITERATION_NO.: 418 LOSS_Generator: 7.870927810668945 LOSS_Discriminator: 0.058588407933712006\n",
            "ITERATION_NO.: 419 LOSS_Generator: 8.57442569732666 LOSS_Discriminator: 0.01766117662191391\n",
            "ITERATION_NO.: 420 LOSS_Generator: 9.008299827575684 LOSS_Discriminator: 0.11704827845096588\n",
            "ITERATION_NO.: 421 LOSS_Generator: 8.713069915771484 LOSS_Discriminator: 0.10078772157430649\n",
            "ITERATION_NO.: 422 LOSS_Generator: 7.647808074951172 LOSS_Discriminator: 0.09596036374568939\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.690143585205078 LOSS_Discriminator: 0.06503615528345108\n",
            "ITERATION_NO.: 424 LOSS_Generator: 6.015801906585693 LOSS_Discriminator: 0.08777362108230591\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.3236894607543945 LOSS_Discriminator: 0.05064111575484276\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.4358038902282715 LOSS_Discriminator: 0.07125090062618256\n",
            "ITERATION_NO.: 427 LOSS_Generator: 7.106148719787598 LOSS_Discriminator: 0.04508807882666588\n",
            "ITERATION_NO.: 428 LOSS_Generator: 7.898317813873291 LOSS_Discriminator: 0.037807296961545944\n",
            "ITERATION_NO.: 429 LOSS_Generator: 7.910989284515381 LOSS_Discriminator: 0.08496139198541641\n",
            "ITERATION_NO.: 430 LOSS_Generator: 7.679096698760986 LOSS_Discriminator: 0.03522767499089241\n",
            "ITERATION_NO.: 431 LOSS_Generator: 7.897855281829834 LOSS_Discriminator: 0.08783097565174103\n",
            "ITERATION_NO.: 432 LOSS_Generator: 6.522048473358154 LOSS_Discriminator: 0.14194703102111816\n",
            "ITERATION_NO.: 433 LOSS_Generator: 6.496840953826904 LOSS_Discriminator: 0.05513056367635727\n",
            "ITERATION_NO.: 434 LOSS_Generator: 6.908916473388672 LOSS_Discriminator: 0.09801442921161652\n",
            "ITERATION_NO.: 435 LOSS_Generator: 7.583045482635498 LOSS_Discriminator: 0.055238205939531326\n",
            "ITERATION_NO.: 436 LOSS_Generator: 8.124284744262695 LOSS_Discriminator: 0.035618919879198074\n",
            "ITERATION_NO.: 437 LOSS_Generator: 8.205268859863281 LOSS_Discriminator: 0.09736943989992142\n",
            "ITERATION_NO.: 438 LOSS_Generator: 7.794182777404785 LOSS_Discriminator: 0.10051672160625458\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.358911037445068 LOSS_Discriminator: 0.035621367394924164\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.421860218048096 LOSS_Discriminator: 0.07924344390630722\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.549133777618408 LOSS_Discriminator: 0.09276293218135834\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.959385395050049 LOSS_Discriminator: 0.04311448335647583\n",
            "ITERATION_NO.: 443 LOSS_Generator: 7.802745819091797 LOSS_Discriminator: 0.07416316866874695\n",
            "ITERATION_NO.: 444 LOSS_Generator: 8.652048110961914 LOSS_Discriminator: 0.0739116296172142\n",
            "ITERATION_NO.: 445 LOSS_Generator: 8.958476066589355 LOSS_Discriminator: 0.030806919559836388\n",
            "ITERATION_NO.: 446 LOSS_Generator: 9.089472770690918 LOSS_Discriminator: 0.08162101358175278\n",
            "ITERATION_NO.: 447 LOSS_Generator: 9.125079154968262 LOSS_Discriminator: 0.11023956537246704\n",
            "ITERATION_NO.: 448 LOSS_Generator: 7.245200157165527 LOSS_Discriminator: 0.19931671023368835\n",
            "ITERATION_NO.: 449 LOSS_Generator: 5.771519184112549 LOSS_Discriminator: 0.08230569958686829\n",
            "ITERATION_NO.: 450 LOSS_Generator: 5.564576625823975 LOSS_Discriminator: 0.15955623984336853\n",
            "ITERATION_NO.: 451 LOSS_Generator: 6.451467514038086 LOSS_Discriminator: 0.09716001152992249\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.859849452972412 LOSS_Discriminator: 0.0434306263923645\n",
            "ITERATION_NO.: 453 LOSS_Generator: 8.801994323730469 LOSS_Discriminator: 0.09881184250116348\n",
            "ITERATION_NO.: 454 LOSS_Generator: 8.344688415527344 LOSS_Discriminator: 0.14091715216636658\n",
            "ITERATION_NO.: 455 LOSS_Generator: 7.779421806335449 LOSS_Discriminator: 0.14146777987480164\n",
            "ITERATION_NO.: 456 LOSS_Generator: 6.465676784515381 LOSS_Discriminator: 0.0563671737909317\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.336009502410889 LOSS_Discriminator: 0.05370020866394043\n",
            "ITERATION_NO.: 458 LOSS_Generator: 5.527001857757568 LOSS_Discriminator: 0.07461750507354736\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.531013011932373 LOSS_Discriminator: 0.12452016770839691\n",
            "ITERATION_NO.: 460 LOSS_Generator: 8.429510116577148 LOSS_Discriminator: 0.05405294895172119\n",
            "ITERATION_NO.: 461 LOSS_Generator: 9.089975357055664 LOSS_Discriminator: 0.05953746661543846\n",
            "ITERATION_NO.: 462 LOSS_Generator: 8.99856185913086 LOSS_Discriminator: 0.16229838132858276\n",
            "ITERATION_NO.: 463 LOSS_Generator: 8.302820205688477 LOSS_Discriminator: 0.021942228078842163\n",
            "ITERATION_NO.: 464 LOSS_Generator: 6.837696075439453 LOSS_Discriminator: 0.08399108052253723\n",
            "ITERATION_NO.: 465 LOSS_Generator: 6.819118022918701 LOSS_Discriminator: 0.04489069804549217\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.466069221496582 LOSS_Discriminator: 0.06537114083766937\n",
            "ITERATION_NO.: 467 LOSS_Generator: 6.776670455932617 LOSS_Discriminator: 0.07096442580223083\n",
            "ITERATION_NO.: 468 LOSS_Generator: 7.214601993560791 LOSS_Discriminator: 0.03758872672915459\n",
            "ITERATION_NO.: 469 LOSS_Generator: 7.559988975524902 LOSS_Discriminator: 0.0344487726688385\n",
            "ITERATION_NO.: 470 LOSS_Generator: 8.004037857055664 LOSS_Discriminator: 0.07462495565414429\n",
            "ITERATION_NO.: 471 LOSS_Generator: 7.9936442375183105 LOSS_Discriminator: 0.08393533527851105\n",
            "ITERATION_NO.: 472 LOSS_Generator: 7.166552543640137 LOSS_Discriminator: 0.028056630864739418\n",
            "ITERATION_NO.: 473 LOSS_Generator: 7.326610565185547 LOSS_Discriminator: 0.0878513976931572\n",
            "ITERATION_NO.: 474 LOSS_Generator: 7.002099514007568 LOSS_Discriminator: 0.08986762166023254\n",
            "ITERATION_NO.: 475 LOSS_Generator: 6.732122421264648 LOSS_Discriminator: 0.03797368332743645\n",
            "ITERATION_NO.: 476 LOSS_Generator: 7.5209760665893555 LOSS_Discriminator: 0.0406615324318409\n",
            "ITERATION_NO.: 477 LOSS_Generator: 7.893517971038818 LOSS_Discriminator: 0.09882517904043198\n",
            "ITERATION_NO.: 478 LOSS_Generator: 7.8638763427734375 LOSS_Discriminator: 0.11173665523529053\n",
            "ITERATION_NO.: 479 LOSS_Generator: 7.952195644378662 LOSS_Discriminator: 0.07318953424692154\n",
            "ITERATION_NO.: 480 LOSS_Generator: 8.055885314941406 LOSS_Discriminator: 0.028963372111320496\n",
            "ITERATION_NO.: 481 LOSS_Generator: 7.128934383392334 LOSS_Discriminator: 0.1409202218055725\n",
            "ITERATION_NO.: 482 LOSS_Generator: 6.9066691398620605 LOSS_Discriminator: 0.038986071944236755\n",
            "ITERATION_NO.: 483 LOSS_Generator: 6.3431715965271 LOSS_Discriminator: 0.034331537783145905\n",
            "ITERATION_NO.: 484 LOSS_Generator: 7.0663838386535645 LOSS_Discriminator: 0.09118928760290146\n",
            "ITERATION_NO.: 485 LOSS_Generator: 6.80429744720459 LOSS_Discriminator: 0.0560857318341732\n",
            "ITERATION_NO.: 486 LOSS_Generator: 7.997519493103027 LOSS_Discriminator: 0.09967342019081116\n",
            "ITERATION_NO.: 487 LOSS_Generator: 8.720979690551758 LOSS_Discriminator: 0.060534004122018814\n",
            "ITERATION_NO.: 488 LOSS_Generator: 9.230474472045898 LOSS_Discriminator: 0.07789397984743118\n",
            "ITERATION_NO.: 489 LOSS_Generator: 8.127062797546387 LOSS_Discriminator: 0.1531466841697693\n",
            "ITERATION_NO.: 490 LOSS_Generator: 6.999339580535889 LOSS_Discriminator: 0.05858592689037323\n",
            "ITERATION_NO.: 491 LOSS_Generator: 5.747714996337891 LOSS_Discriminator: 0.04070451483130455\n",
            "ITERATION_NO.: 492 LOSS_Generator: 5.384746074676514 LOSS_Discriminator: 0.054142601788043976\n",
            "ITERATION_NO.: 493 LOSS_Generator: 6.017374038696289 LOSS_Discriminator: 0.0779883861541748\n",
            "ITERATION_NO.: 494 LOSS_Generator: 6.614861965179443 LOSS_Discriminator: 0.11159920692443848\n",
            "ITERATION_NO.: 495 LOSS_Generator: 7.391448497772217 LOSS_Discriminator: 0.09183622896671295\n",
            "ITERATION_NO.: 496 LOSS_Generator: 8.75008487701416 LOSS_Discriminator: 0.03150667995214462\n",
            "ITERATION_NO.: 497 LOSS_Generator: 8.791482925415039 LOSS_Discriminator: 0.09741513431072235\n",
            "ITERATION_NO.: 498 LOSS_Generator: 8.372956275939941 LOSS_Discriminator: 0.136611670255661\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.483570575714111 LOSS_Discriminator: 0.05267646163702011\n",
            "ITERATION_NO.: 500 LOSS_Generator: 6.814742565155029 LOSS_Discriminator: 0.0299545731395483\n",
            "ITERATION_NO.: 501 LOSS_Generator: 6.457630634307861 LOSS_Discriminator: 0.06365079432725906\n",
            "ITERATION_NO.: 502 LOSS_Generator: 5.932636737823486 LOSS_Discriminator: 0.06075093150138855\n",
            "ITERATION_NO.: 503 LOSS_Generator: 6.3215460777282715 LOSS_Discriminator: 0.05805639922618866\n",
            "ITERATION_NO.: 504 LOSS_Generator: 7.15471076965332 LOSS_Discriminator: 0.033500269055366516\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.522428035736084 LOSS_Discriminator: 0.038195423781871796\n",
            "ITERATION_NO.: 506 LOSS_Generator: 7.715964317321777 LOSS_Discriminator: 0.09170787036418915\n",
            "ITERATION_NO.: 507 LOSS_Generator: 8.477843284606934 LOSS_Discriminator: 0.02527143806219101\n",
            "ITERATION_NO.: 508 LOSS_Generator: 7.554881572723389 LOSS_Discriminator: 0.056313656270504\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.3235931396484375 LOSS_Discriminator: 0.07975666224956512\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.510206699371338 LOSS_Discriminator: 0.013569562695920467\n",
            "ITERATION_NO.: 511 LOSS_Generator: 6.99921989440918 LOSS_Discriminator: 0.04814422130584717\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.037397861480713 LOSS_Discriminator: 0.0647062361240387\n",
            "ITERATION_NO.: 513 LOSS_Generator: 6.699239730834961 LOSS_Discriminator: 0.05060666427016258\n",
            "ITERATION_NO.: 514 LOSS_Generator: 6.733201503753662 LOSS_Discriminator: 0.03421321511268616\n",
            "ITERATION_NO.: 515 LOSS_Generator: 6.925175666809082 LOSS_Discriminator: 0.09174533933401108\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.689438343048096 LOSS_Discriminator: 0.03507448360323906\n",
            "ITERATION_NO.: 517 LOSS_Generator: 7.038797378540039 LOSS_Discriminator: 0.07224294543266296\n",
            "ITERATION_NO.: 518 LOSS_Generator: 8.071800231933594 LOSS_Discriminator: 0.05601589381694794\n",
            "ITERATION_NO.: 519 LOSS_Generator: 8.115279197692871 LOSS_Discriminator: 0.13893868029117584\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.420861721038818 LOSS_Discriminator: 0.10043071210384369\n",
            "ITERATION_NO.: 521 LOSS_Generator: 6.5743231773376465 LOSS_Discriminator: 0.029308108612895012\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.953738212585449 LOSS_Discriminator: 0.17169441282749176\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.425143241882324 LOSS_Discriminator: 0.10222192853689194\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.032012462615967 LOSS_Discriminator: 0.05350187420845032\n",
            "ITERATION_NO.: 525 LOSS_Generator: 6.396098613739014 LOSS_Discriminator: 0.07246185839176178\n",
            "ITERATION_NO.: 526 LOSS_Generator: 8.273209571838379 LOSS_Discriminator: 0.1256750226020813\n",
            "ITERATION_NO.: 527 LOSS_Generator: 8.82901382446289 LOSS_Discriminator: 0.04256429895758629\n",
            "ITERATION_NO.: 528 LOSS_Generator: 8.700841903686523 LOSS_Discriminator: 0.09597532451152802\n",
            "ITERATION_NO.: 529 LOSS_Generator: 7.587465763092041 LOSS_Discriminator: 0.17997358739376068\n",
            "ITERATION_NO.: 530 LOSS_Generator: 6.847421646118164 LOSS_Discriminator: 0.03653165325522423\n",
            "ITERATION_NO.: 531 LOSS_Generator: 6.057111740112305 LOSS_Discriminator: 0.07878260314464569\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.408576011657715 LOSS_Discriminator: 0.12077850848436356\n",
            "ITERATION_NO.: 533 LOSS_Generator: 7.505325794219971 LOSS_Discriminator: 0.06195543333888054\n",
            "ITERATION_NO.: 534 LOSS_Generator: 7.529836654663086 LOSS_Discriminator: 0.07831467688083649\n",
            "ITERATION_NO.: 535 LOSS_Generator: 8.023598670959473 LOSS_Discriminator: 0.05744810402393341\n",
            "ITERATION_NO.: 536 LOSS_Generator: 8.33415412902832 LOSS_Discriminator: 0.05913447216153145\n",
            "ITERATION_NO.: 537 LOSS_Generator: 8.408432960510254 LOSS_Discriminator: 0.0924142450094223\n",
            "ITERATION_NO.: 538 LOSS_Generator: 7.463076591491699 LOSS_Discriminator: 0.11064889281988144\n",
            "ITERATION_NO.: 539 LOSS_Generator: 6.477861404418945 LOSS_Discriminator: 0.04978959262371063\n",
            "ITERATION_NO.: 540 LOSS_Generator: 5.925152778625488 LOSS_Discriminator: 0.06079026311635971\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.34597635269165 LOSS_Discriminator: 0.06119786575436592\n",
            "ITERATION_NO.: 542 LOSS_Generator: 7.263747692108154 LOSS_Discriminator: 0.06847647577524185\n",
            "ITERATION_NO.: 543 LOSS_Generator: 7.273863315582275 LOSS_Discriminator: 0.07789242267608643\n",
            "ITERATION_NO.: 544 LOSS_Generator: 7.875344753265381 LOSS_Discriminator: 0.06749202311038971\n",
            "ITERATION_NO.: 545 LOSS_Generator: 8.191983222961426 LOSS_Discriminator: 0.0475616417825222\n",
            "ITERATION_NO.: 546 LOSS_Generator: 7.856639385223389 LOSS_Discriminator: 0.05887336656451225\n",
            "ITERATION_NO.: 547 LOSS_Generator: 7.990427017211914 LOSS_Discriminator: 0.058215752243995667\n",
            "ITERATION_NO.: 548 LOSS_Generator: 7.173845291137695 LOSS_Discriminator: 0.049140579998493195\n",
            "ITERATION_NO.: 549 LOSS_Generator: 7.205244541168213 LOSS_Discriminator: 0.02434610202908516\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.5943379402160645 LOSS_Discriminator: 0.05630272999405861\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.4443888664245605 LOSS_Discriminator: 0.042350128293037415\n",
            "ITERATION_NO.: 552 LOSS_Generator: 7.268110275268555 LOSS_Discriminator: 0.08599776774644852\n",
            "ITERATION_NO.: 553 LOSS_Generator: 8.306438446044922 LOSS_Discriminator: 0.04906532168388367\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.93863582611084 LOSS_Discriminator: 0.026177644729614258\n",
            "ITERATION_NO.: 555 LOSS_Generator: 7.760126113891602 LOSS_Discriminator: 0.04251126945018768\n",
            "ITERATION_NO.: 556 LOSS_Generator: 7.764963150024414 LOSS_Discriminator: 0.04320194572210312\n",
            "ITERATION_NO.: 557 LOSS_Generator: 7.220628261566162 LOSS_Discriminator: 0.042286064475774765\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.821976184844971 LOSS_Discriminator: 0.10257906466722488\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.284053802490234 LOSS_Discriminator: 0.09654855728149414\n",
            "ITERATION_NO.: 560 LOSS_Generator: 6.552250862121582 LOSS_Discriminator: 0.0549616776406765\n",
            "ITERATION_NO.: 561 LOSS_Generator: 6.919310092926025 LOSS_Discriminator: 0.02565491758286953\n",
            "ITERATION_NO.: 562 LOSS_Generator: 7.38601016998291 LOSS_Discriminator: 0.06910732388496399\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.7961344718933105 LOSS_Discriminator: 0.09292404353618622\n",
            "ITERATION_NO.: 564 LOSS_Generator: 6.749619007110596 LOSS_Discriminator: 0.07249446213245392\n",
            "ITERATION_NO.: 565 LOSS_Generator: 6.6522393226623535 LOSS_Discriminator: 0.06676141172647476\n",
            "ITERATION_NO.: 566 LOSS_Generator: 7.1949944496154785 LOSS_Discriminator: 0.08822351694107056\n",
            "ITERATION_NO.: 567 LOSS_Generator: 6.373013496398926 LOSS_Discriminator: 0.05796429514884949\n",
            "ITERATION_NO.: 568 LOSS_Generator: 7.252814769744873 LOSS_Discriminator: 0.04350673407316208\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.770948886871338 LOSS_Discriminator: 0.04675156623125076\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.999083042144775 LOSS_Discriminator: 0.06007268279790878\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.651524543762207 LOSS_Discriminator: 0.06636098027229309\n",
            "ITERATION_NO.: 572 LOSS_Generator: 7.2640252113342285 LOSS_Discriminator: 0.07240220904350281\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.329352855682373 LOSS_Discriminator: 0.06639453768730164\n",
            "ITERATION_NO.: 574 LOSS_Generator: 7.5217766761779785 LOSS_Discriminator: 0.14076514542102814\n",
            "ITERATION_NO.: 575 LOSS_Generator: 7.065966606140137 LOSS_Discriminator: 0.12297128885984421\n",
            "ITERATION_NO.: 576 LOSS_Generator: 6.472314357757568 LOSS_Discriminator: 0.10393145680427551\n",
            "ITERATION_NO.: 577 LOSS_Generator: 5.9676594734191895 LOSS_Discriminator: 0.06755033135414124\n",
            "ITERATION_NO.: 578 LOSS_Generator: 6.333219051361084 LOSS_Discriminator: 0.10836151242256165\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.396627902984619 LOSS_Discriminator: 0.08794200420379639\n",
            "ITERATION_NO.: 580 LOSS_Generator: 7.741950035095215 LOSS_Discriminator: 0.057886309921741486\n",
            "ITERATION_NO.: 581 LOSS_Generator: 8.751662254333496 LOSS_Discriminator: 0.07263610512018204\n",
            "ITERATION_NO.: 582 LOSS_Generator: 8.818181991577148 LOSS_Discriminator: 0.03256816044449806\n",
            "ITERATION_NO.: 583 LOSS_Generator: 8.351572036743164 LOSS_Discriminator: 0.07287812978029251\n",
            "ITERATION_NO.: 584 LOSS_Generator: 7.3899688720703125 LOSS_Discriminator: 0.12092021107673645\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.508248805999756 LOSS_Discriminator: 0.05735166743397713\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.217415809631348 LOSS_Discriminator: 0.07824891805648804\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.260975360870361 LOSS_Discriminator: 0.05567315220832825\n",
            "ITERATION_NO.: 588 LOSS_Generator: 7.033968448638916 LOSS_Discriminator: 0.08076322823762894\n",
            "ITERATION_NO.: 589 LOSS_Generator: 7.174678802490234 LOSS_Discriminator: 0.030966157093644142\n",
            "ITERATION_NO.: 590 LOSS_Generator: 7.533337593078613 LOSS_Discriminator: 0.044087812304496765\n",
            "ITERATION_NO.: 591 LOSS_Generator: 7.9269514083862305 LOSS_Discriminator: 0.06872057914733887\n",
            "ITERATION_NO.: 592 LOSS_Generator: 8.362643241882324 LOSS_Discriminator: 0.026159416884183884\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.776569843292236 LOSS_Discriminator: 0.030696725472807884\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.869211196899414 LOSS_Discriminator: 0.03153982758522034\n",
            "ITERATION_NO.: 595 LOSS_Generator: 7.594825267791748 LOSS_Discriminator: 0.026914391666650772\n",
            "ITERATION_NO.: 596 LOSS_Generator: 6.998103618621826 LOSS_Discriminator: 0.030046291649341583\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.916074752807617 LOSS_Discriminator: 0.038058843463659286\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.45252799987793 LOSS_Discriminator: 0.11966739594936371\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.252881050109863 LOSS_Discriminator: 0.047070086002349854\n",
            "ITERATION_NO.: 600 LOSS_Generator: 6.324337959289551 LOSS_Discriminator: 0.07225865125656128\n",
            "EPOCH OVER: 42\n",
            "ITERATION_NO.: 1 LOSS_Generator: 7.096315860748291 LOSS_Discriminator: 0.12742935121059418\n",
            "ITERATION_NO.: 2 LOSS_Generator: 7.656075954437256 LOSS_Discriminator: 0.08901447057723999\n",
            "ITERATION_NO.: 3 LOSS_Generator: 6.892228603363037 LOSS_Discriminator: 0.12260806560516357\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.543356895446777 LOSS_Discriminator: 0.049830369651317596\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.930273532867432 LOSS_Discriminator: 0.09212872385978699\n",
            "ITERATION_NO.: 6 LOSS_Generator: 6.371888637542725 LOSS_Discriminator: 0.1090422198176384\n",
            "ITERATION_NO.: 7 LOSS_Generator: 6.418961048126221 LOSS_Discriminator: 0.11921680718660355\n",
            "ITERATION_NO.: 8 LOSS_Generator: 6.337822437286377 LOSS_Discriminator: 0.05251935496926308\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.095283031463623 LOSS_Discriminator: 0.1431102305650711\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.070489883422852 LOSS_Discriminator: 0.05989821255207062\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.000192642211914 LOSS_Discriminator: 0.07551498711109161\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.354037284851074 LOSS_Discriminator: 0.08313862234354019\n",
            "ITERATION_NO.: 13 LOSS_Generator: 7.141629695892334 LOSS_Discriminator: 0.13395071029663086\n",
            "ITERATION_NO.: 14 LOSS_Generator: 6.592889308929443 LOSS_Discriminator: 0.10616070032119751\n",
            "ITERATION_NO.: 15 LOSS_Generator: 7.365426540374756 LOSS_Discriminator: 0.09058725833892822\n",
            "ITERATION_NO.: 16 LOSS_Generator: 7.789725303649902 LOSS_Discriminator: 0.0490763857960701\n",
            "ITERATION_NO.: 17 LOSS_Generator: 7.160130023956299 LOSS_Discriminator: 0.10304268449544907\n",
            "ITERATION_NO.: 18 LOSS_Generator: 7.402012348175049 LOSS_Discriminator: 0.09904597699642181\n",
            "ITERATION_NO.: 19 LOSS_Generator: 6.910645008087158 LOSS_Discriminator: 0.06359876692295074\n",
            "ITERATION_NO.: 20 LOSS_Generator: 6.564828872680664 LOSS_Discriminator: 0.0595676451921463\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.903158187866211 LOSS_Discriminator: 0.06062192842364311\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.275143623352051 LOSS_Discriminator: 0.07685604691505432\n",
            "ITERATION_NO.: 23 LOSS_Generator: 7.314410209655762 LOSS_Discriminator: 0.041413940489292145\n",
            "ITERATION_NO.: 24 LOSS_Generator: 7.845710277557373 LOSS_Discriminator: 0.040256571024656296\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.094948768615723 LOSS_Discriminator: 0.02018585428595543\n",
            "ITERATION_NO.: 26 LOSS_Generator: 7.660908222198486 LOSS_Discriminator: 0.033675238490104675\n",
            "ITERATION_NO.: 27 LOSS_Generator: 7.279040336608887 LOSS_Discriminator: 0.03207290172576904\n",
            "ITERATION_NO.: 28 LOSS_Generator: 6.874884605407715 LOSS_Discriminator: 0.01801467314362526\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.533421039581299 LOSS_Discriminator: 0.02172067016363144\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.675416469573975 LOSS_Discriminator: 0.021770335733890533\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.655522346496582 LOSS_Discriminator: 0.05922488868236542\n",
            "ITERATION_NO.: 32 LOSS_Generator: 6.968451499938965 LOSS_Discriminator: 0.07996518909931183\n",
            "ITERATION_NO.: 33 LOSS_Generator: 6.70518684387207 LOSS_Discriminator: 0.03176075220108032\n",
            "ITERATION_NO.: 34 LOSS_Generator: 7.175206184387207 LOSS_Discriminator: 0.02719949744641781\n",
            "ITERATION_NO.: 35 LOSS_Generator: 6.686178684234619 LOSS_Discriminator: 0.08606518805027008\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.215481758117676 LOSS_Discriminator: 0.03519488498568535\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.0924296379089355 LOSS_Discriminator: 0.0471770353615284\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.255484580993652 LOSS_Discriminator: 0.047245897352695465\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.284245491027832 LOSS_Discriminator: 0.07299362123012543\n",
            "ITERATION_NO.: 40 LOSS_Generator: 6.752795219421387 LOSS_Discriminator: 0.04259883612394333\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.644852161407471 LOSS_Discriminator: 0.03715312480926514\n",
            "ITERATION_NO.: 42 LOSS_Generator: 7.5860066413879395 LOSS_Discriminator: 0.03178415447473526\n",
            "ITERATION_NO.: 43 LOSS_Generator: 7.247528076171875 LOSS_Discriminator: 0.045382723212242126\n",
            "ITERATION_NO.: 44 LOSS_Generator: 7.5545854568481445 LOSS_Discriminator: 0.03147023171186447\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.456944465637207 LOSS_Discriminator: 0.07766960561275482\n",
            "ITERATION_NO.: 46 LOSS_Generator: 6.155956268310547 LOSS_Discriminator: 0.05020109936594963\n",
            "ITERATION_NO.: 47 LOSS_Generator: 5.517916202545166 LOSS_Discriminator: 0.05751785635948181\n",
            "ITERATION_NO.: 48 LOSS_Generator: 6.110279083251953 LOSS_Discriminator: 0.07476691901683807\n",
            "ITERATION_NO.: 49 LOSS_Generator: 6.843826770782471 LOSS_Discriminator: 0.0642545223236084\n",
            "ITERATION_NO.: 50 LOSS_Generator: 7.7715744972229 LOSS_Discriminator: 0.07245246320962906\n",
            "ITERATION_NO.: 51 LOSS_Generator: 7.605749607086182 LOSS_Discriminator: 0.05307425558567047\n",
            "ITERATION_NO.: 52 LOSS_Generator: 6.805181980133057 LOSS_Discriminator: 0.11002381145954132\n",
            "ITERATION_NO.: 53 LOSS_Generator: 6.783383846282959 LOSS_Discriminator: 0.06901472806930542\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.274196147918701 LOSS_Discriminator: 0.05282259359955788\n",
            "ITERATION_NO.: 55 LOSS_Generator: 5.904748916625977 LOSS_Discriminator: 0.05830591171979904\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.3852219581604 LOSS_Discriminator: 0.10142256319522858\n",
            "ITERATION_NO.: 57 LOSS_Generator: 6.942472457885742 LOSS_Discriminator: 0.07903066277503967\n",
            "ITERATION_NO.: 58 LOSS_Generator: 6.955516338348389 LOSS_Discriminator: 0.07152332365512848\n",
            "ITERATION_NO.: 59 LOSS_Generator: 7.634425163269043 LOSS_Discriminator: 0.05375449359416962\n",
            "ITERATION_NO.: 60 LOSS_Generator: 7.887049674987793 LOSS_Discriminator: 0.036042049527168274\n",
            "ITERATION_NO.: 61 LOSS_Generator: 7.721719741821289 LOSS_Discriminator: 0.031219344586133957\n",
            "ITERATION_NO.: 62 LOSS_Generator: 7.2382683753967285 LOSS_Discriminator: 0.031172115355730057\n",
            "ITERATION_NO.: 63 LOSS_Generator: 7.041895866394043 LOSS_Discriminator: 0.07342316210269928\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.07156229019165 LOSS_Discriminator: 0.06492096930742264\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.376016139984131 LOSS_Discriminator: 0.06689173728227615\n",
            "ITERATION_NO.: 66 LOSS_Generator: 6.796207904815674 LOSS_Discriminator: 0.09882496297359467\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.219271659851074 LOSS_Discriminator: 0.09132656455039978\n",
            "ITERATION_NO.: 68 LOSS_Generator: 7.856207847595215 LOSS_Discriminator: 0.11081482470035553\n",
            "ITERATION_NO.: 69 LOSS_Generator: 8.588258743286133 LOSS_Discriminator: 0.10425597429275513\n",
            "ITERATION_NO.: 70 LOSS_Generator: 8.453911781311035 LOSS_Discriminator: 0.055836714804172516\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.715020179748535 LOSS_Discriminator: 0.024061456322669983\n",
            "ITERATION_NO.: 72 LOSS_Generator: 7.271783351898193 LOSS_Discriminator: 0.04034658148884773\n",
            "ITERATION_NO.: 73 LOSS_Generator: 7.460761070251465 LOSS_Discriminator: 0.09343652427196503\n",
            "ITERATION_NO.: 74 LOSS_Generator: 8.263243675231934 LOSS_Discriminator: 0.06574608385562897\n",
            "ITERATION_NO.: 75 LOSS_Generator: 7.918073654174805 LOSS_Discriminator: 0.0336940623819828\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.803860664367676 LOSS_Discriminator: 0.09111282974481583\n",
            "ITERATION_NO.: 77 LOSS_Generator: 6.904443264007568 LOSS_Discriminator: 0.1350513994693756\n",
            "ITERATION_NO.: 78 LOSS_Generator: 6.6732001304626465 LOSS_Discriminator: 0.03410474956035614\n",
            "ITERATION_NO.: 79 LOSS_Generator: 6.532602310180664 LOSS_Discriminator: 0.07138125598430634\n",
            "ITERATION_NO.: 80 LOSS_Generator: 6.829843521118164 LOSS_Discriminator: 0.05928581953048706\n",
            "ITERATION_NO.: 81 LOSS_Generator: 7.212255477905273 LOSS_Discriminator: 0.10658799856901169\n",
            "ITERATION_NO.: 82 LOSS_Generator: 7.4420061111450195 LOSS_Discriminator: 0.039664458483457565\n",
            "ITERATION_NO.: 83 LOSS_Generator: 7.989953517913818 LOSS_Discriminator: 0.03691478818655014\n",
            "ITERATION_NO.: 84 LOSS_Generator: 7.6969218254089355 LOSS_Discriminator: 0.04898524656891823\n",
            "ITERATION_NO.: 85 LOSS_Generator: 7.652736186981201 LOSS_Discriminator: 0.03323601931333542\n",
            "ITERATION_NO.: 86 LOSS_Generator: 7.31706428527832 LOSS_Discriminator: 0.08717957139015198\n",
            "ITERATION_NO.: 87 LOSS_Generator: 6.217672824859619 LOSS_Discriminator: 0.05617201328277588\n",
            "ITERATION_NO.: 88 LOSS_Generator: 6.527190685272217 LOSS_Discriminator: 0.06838575005531311\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.520838737487793 LOSS_Discriminator: 0.07386043667793274\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.2256855964660645 LOSS_Discriminator: 0.07087346166372299\n",
            "ITERATION_NO.: 91 LOSS_Generator: 8.364981651306152 LOSS_Discriminator: 0.0324857123196125\n",
            "ITERATION_NO.: 92 LOSS_Generator: 8.362410545349121 LOSS_Discriminator: 0.05720590054988861\n",
            "ITERATION_NO.: 93 LOSS_Generator: 7.562716007232666 LOSS_Discriminator: 0.0990697592496872\n",
            "ITERATION_NO.: 94 LOSS_Generator: 7.234248638153076 LOSS_Discriminator: 0.08259323984384537\n",
            "ITERATION_NO.: 95 LOSS_Generator: 6.789604663848877 LOSS_Discriminator: 0.0487506240606308\n",
            "ITERATION_NO.: 96 LOSS_Generator: 7.6345343589782715 LOSS_Discriminator: 0.02680908888578415\n",
            "ITERATION_NO.: 97 LOSS_Generator: 7.640787124633789 LOSS_Discriminator: 0.02417099103331566\n",
            "ITERATION_NO.: 98 LOSS_Generator: 7.5342607498168945 LOSS_Discriminator: 0.045738477259874344\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.099124908447266 LOSS_Discriminator: 0.1417715847492218\n",
            "ITERATION_NO.: 100 LOSS_Generator: 5.927445888519287 LOSS_Discriminator: 0.06695802509784698\n",
            "ITERATION_NO.: 101 LOSS_Generator: 6.483994007110596 LOSS_Discriminator: 0.08760291337966919\n",
            "ITERATION_NO.: 102 LOSS_Generator: 7.524227142333984 LOSS_Discriminator: 0.026668833568692207\n",
            "ITERATION_NO.: 103 LOSS_Generator: 8.388875961303711 LOSS_Discriminator: 0.053350627422332764\n",
            "ITERATION_NO.: 104 LOSS_Generator: 9.180428504943848 LOSS_Discriminator: 0.08549400418996811\n",
            "ITERATION_NO.: 105 LOSS_Generator: 9.030795097351074 LOSS_Discriminator: 0.029729653149843216\n",
            "ITERATION_NO.: 106 LOSS_Generator: 8.465980529785156 LOSS_Discriminator: 0.11044882237911224\n",
            "ITERATION_NO.: 107 LOSS_Generator: 7.0197672843933105 LOSS_Discriminator: 0.03564441204071045\n",
            "ITERATION_NO.: 108 LOSS_Generator: 7.443942070007324 LOSS_Discriminator: 0.043111734092235565\n",
            "ITERATION_NO.: 109 LOSS_Generator: 7.574070930480957 LOSS_Discriminator: 0.0853082686662674\n",
            "ITERATION_NO.: 110 LOSS_Generator: 7.5161614418029785 LOSS_Discriminator: 0.061767205595970154\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.208193302154541 LOSS_Discriminator: 0.04099181294441223\n",
            "ITERATION_NO.: 112 LOSS_Generator: 7.994332790374756 LOSS_Discriminator: 0.023271631449460983\n",
            "ITERATION_NO.: 113 LOSS_Generator: 7.572666168212891 LOSS_Discriminator: 0.08220043778419495\n",
            "ITERATION_NO.: 114 LOSS_Generator: 7.177790641784668 LOSS_Discriminator: 0.047142088413238525\n",
            "ITERATION_NO.: 115 LOSS_Generator: 7.624727249145508 LOSS_Discriminator: 0.04455209895968437\n",
            "ITERATION_NO.: 116 LOSS_Generator: 7.294851303100586 LOSS_Discriminator: 0.03449779748916626\n",
            "ITERATION_NO.: 117 LOSS_Generator: 7.32163143157959 LOSS_Discriminator: 0.0826047956943512\n",
            "ITERATION_NO.: 118 LOSS_Generator: 7.620594024658203 LOSS_Discriminator: 0.08867904543876648\n",
            "ITERATION_NO.: 119 LOSS_Generator: 7.634271621704102 LOSS_Discriminator: 0.09252096712589264\n",
            "ITERATION_NO.: 120 LOSS_Generator: 6.883061408996582 LOSS_Discriminator: 0.07484128326177597\n",
            "ITERATION_NO.: 121 LOSS_Generator: 6.910544395446777 LOSS_Discriminator: 0.061694711446762085\n",
            "ITERATION_NO.: 122 LOSS_Generator: 6.878824234008789 LOSS_Discriminator: 0.06923232972621918\n",
            "ITERATION_NO.: 123 LOSS_Generator: 7.202769756317139 LOSS_Discriminator: 0.03524155169725418\n",
            "ITERATION_NO.: 124 LOSS_Generator: 7.679205894470215 LOSS_Discriminator: 0.08581924438476562\n",
            "ITERATION_NO.: 125 LOSS_Generator: 8.59931755065918 LOSS_Discriminator: 0.06407719105482101\n",
            "ITERATION_NO.: 126 LOSS_Generator: 8.188632011413574 LOSS_Discriminator: 0.057596996426582336\n",
            "ITERATION_NO.: 127 LOSS_Generator: 7.743341445922852 LOSS_Discriminator: 0.059240877628326416\n",
            "ITERATION_NO.: 128 LOSS_Generator: 7.672053337097168 LOSS_Discriminator: 0.050162866711616516\n",
            "ITERATION_NO.: 129 LOSS_Generator: 6.88987922668457 LOSS_Discriminator: 0.07416440546512604\n",
            "ITERATION_NO.: 130 LOSS_Generator: 6.823231220245361 LOSS_Discriminator: 0.08525361120700836\n",
            "ITERATION_NO.: 131 LOSS_Generator: 7.061039924621582 LOSS_Discriminator: 0.060741815716028214\n",
            "ITERATION_NO.: 132 LOSS_Generator: 7.455239295959473 LOSS_Discriminator: 0.06466257572174072\n",
            "ITERATION_NO.: 133 LOSS_Generator: 7.931756496429443 LOSS_Discriminator: 0.056220702826976776\n",
            "ITERATION_NO.: 134 LOSS_Generator: 8.884552955627441 LOSS_Discriminator: 0.031103111803531647\n",
            "ITERATION_NO.: 135 LOSS_Generator: 8.934724807739258 LOSS_Discriminator: 0.058613311499357224\n",
            "ITERATION_NO.: 136 LOSS_Generator: 7.603456497192383 LOSS_Discriminator: 0.034336961805820465\n",
            "ITERATION_NO.: 137 LOSS_Generator: 7.601721286773682 LOSS_Discriminator: 0.04738675802946091\n",
            "ITERATION_NO.: 138 LOSS_Generator: 7.041032791137695 LOSS_Discriminator: 0.04100608080625534\n",
            "ITERATION_NO.: 139 LOSS_Generator: 7.069857120513916 LOSS_Discriminator: 0.0569864921271801\n",
            "ITERATION_NO.: 140 LOSS_Generator: 7.899969577789307 LOSS_Discriminator: 0.04250960052013397\n",
            "ITERATION_NO.: 141 LOSS_Generator: 8.84811782836914 LOSS_Discriminator: 0.0831262469291687\n",
            "ITERATION_NO.: 142 LOSS_Generator: 8.735018730163574 LOSS_Discriminator: 0.10243192315101624\n",
            "ITERATION_NO.: 143 LOSS_Generator: 7.987531661987305 LOSS_Discriminator: 0.024600865319371223\n",
            "ITERATION_NO.: 144 LOSS_Generator: 7.006500720977783 LOSS_Discriminator: 0.06436492502689362\n",
            "ITERATION_NO.: 145 LOSS_Generator: 7.127452373504639 LOSS_Discriminator: 0.08729851245880127\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.965908050537109 LOSS_Discriminator: 0.04781363159418106\n",
            "ITERATION_NO.: 147 LOSS_Generator: 7.3208231925964355 LOSS_Discriminator: 0.09579844027757645\n",
            "ITERATION_NO.: 148 LOSS_Generator: 7.732766151428223 LOSS_Discriminator: 0.08652292191982269\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.60812520980835 LOSS_Discriminator: 0.08407949656248093\n",
            "ITERATION_NO.: 150 LOSS_Generator: 6.739105224609375 LOSS_Discriminator: 0.03995012864470482\n",
            "ITERATION_NO.: 151 LOSS_Generator: 7.209806442260742 LOSS_Discriminator: 0.04501313716173172\n",
            "ITERATION_NO.: 152 LOSS_Generator: 7.597452163696289 LOSS_Discriminator: 0.04279030114412308\n",
            "ITERATION_NO.: 153 LOSS_Generator: 7.514140605926514 LOSS_Discriminator: 0.052052561193704605\n",
            "ITERATION_NO.: 154 LOSS_Generator: 7.6973443031311035 LOSS_Discriminator: 0.08300510048866272\n",
            "ITERATION_NO.: 155 LOSS_Generator: 7.159513473510742 LOSS_Discriminator: 0.08726971596479416\n",
            "ITERATION_NO.: 156 LOSS_Generator: 6.816221714019775 LOSS_Discriminator: 0.034395597875118256\n",
            "ITERATION_NO.: 157 LOSS_Generator: 7.212688446044922 LOSS_Discriminator: 0.07484133541584015\n",
            "ITERATION_NO.: 158 LOSS_Generator: 8.597938537597656 LOSS_Discriminator: 0.050689779222011566\n",
            "ITERATION_NO.: 159 LOSS_Generator: 9.16137981414795 LOSS_Discriminator: 0.06322961300611496\n",
            "ITERATION_NO.: 160 LOSS_Generator: 8.951547622680664 LOSS_Discriminator: 0.024676840752363205\n",
            "ITERATION_NO.: 161 LOSS_Generator: 7.926407337188721 LOSS_Discriminator: 0.08239966630935669\n",
            "ITERATION_NO.: 162 LOSS_Generator: 6.602125644683838 LOSS_Discriminator: 0.12522362172603607\n",
            "ITERATION_NO.: 163 LOSS_Generator: 6.736310005187988 LOSS_Discriminator: 0.0760071724653244\n",
            "ITERATION_NO.: 164 LOSS_Generator: 7.300994396209717 LOSS_Discriminator: 0.09149450063705444\n",
            "ITERATION_NO.: 165 LOSS_Generator: 8.889535903930664 LOSS_Discriminator: 0.06184585392475128\n",
            "ITERATION_NO.: 166 LOSS_Generator: 8.972064018249512 LOSS_Discriminator: 0.0349210649728775\n",
            "ITERATION_NO.: 167 LOSS_Generator: 8.773588180541992 LOSS_Discriminator: 0.11787965893745422\n",
            "ITERATION_NO.: 168 LOSS_Generator: 7.4249773025512695 LOSS_Discriminator: 0.08116187155246735\n",
            "ITERATION_NO.: 169 LOSS_Generator: 6.361070156097412 LOSS_Discriminator: 0.1156744435429573\n",
            "ITERATION_NO.: 170 LOSS_Generator: 6.234682559967041 LOSS_Discriminator: 0.08553823828697205\n",
            "ITERATION_NO.: 171 LOSS_Generator: 7.481661319732666 LOSS_Discriminator: 0.09556195139884949\n",
            "ITERATION_NO.: 172 LOSS_Generator: 8.094293594360352 LOSS_Discriminator: 0.1665734350681305\n",
            "ITERATION_NO.: 173 LOSS_Generator: 9.568252563476562 LOSS_Discriminator: 0.04050161689519882\n",
            "ITERATION_NO.: 174 LOSS_Generator: 10.006113052368164 LOSS_Discriminator: 0.035373155027627945\n",
            "ITERATION_NO.: 175 LOSS_Generator: 9.42802619934082 LOSS_Discriminator: 0.1774599254131317\n",
            "ITERATION_NO.: 176 LOSS_Generator: 7.285976409912109 LOSS_Discriminator: 0.16251429915428162\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.276656627655029 LOSS_Discriminator: 0.0781797468662262\n",
            "ITERATION_NO.: 178 LOSS_Generator: 7.060103893280029 LOSS_Discriminator: 0.16534504294395447\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.546551704406738 LOSS_Discriminator: 0.06245661526918411\n",
            "ITERATION_NO.: 180 LOSS_Generator: 9.127492904663086 LOSS_Discriminator: 0.09425067901611328\n",
            "ITERATION_NO.: 181 LOSS_Generator: 8.624029159545898 LOSS_Discriminator: 0.08178068697452545\n",
            "ITERATION_NO.: 182 LOSS_Generator: 8.243590354919434 LOSS_Discriminator: 0.045969173312187195\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.6997904777526855 LOSS_Discriminator: 0.02696646749973297\n",
            "ITERATION_NO.: 184 LOSS_Generator: 7.589188098907471 LOSS_Discriminator: 0.01595308817923069\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.451500415802002 LOSS_Discriminator: 0.033373624086380005\n",
            "ITERATION_NO.: 186 LOSS_Generator: 7.904065132141113 LOSS_Discriminator: 0.07210938632488251\n",
            "ITERATION_NO.: 187 LOSS_Generator: 8.041508674621582 LOSS_Discriminator: 0.05977441743016243\n",
            "ITERATION_NO.: 188 LOSS_Generator: 8.592472076416016 LOSS_Discriminator: 0.06393851339817047\n",
            "ITERATION_NO.: 189 LOSS_Generator: 8.626885414123535 LOSS_Discriminator: 0.016993289813399315\n",
            "ITERATION_NO.: 190 LOSS_Generator: 7.653393745422363 LOSS_Discriminator: 0.09577003121376038\n",
            "ITERATION_NO.: 191 LOSS_Generator: 7.602208137512207 LOSS_Discriminator: 0.024961184710264206\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.960826396942139 LOSS_Discriminator: 0.07025650143623352\n",
            "ITERATION_NO.: 193 LOSS_Generator: 6.361276626586914 LOSS_Discriminator: 0.12101912498474121\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.860903739929199 LOSS_Discriminator: 0.07326583564281464\n",
            "ITERATION_NO.: 195 LOSS_Generator: 7.606500148773193 LOSS_Discriminator: 0.06357528269290924\n",
            "ITERATION_NO.: 196 LOSS_Generator: 7.818052768707275 LOSS_Discriminator: 0.03982479125261307\n",
            "ITERATION_NO.: 197 LOSS_Generator: 8.114180564880371 LOSS_Discriminator: 0.05943702906370163\n",
            "ITERATION_NO.: 198 LOSS_Generator: 8.275392532348633 LOSS_Discriminator: 0.07762432098388672\n",
            "ITERATION_NO.: 199 LOSS_Generator: 7.947744846343994 LOSS_Discriminator: 0.03844437748193741\n",
            "ITERATION_NO.: 200 LOSS_Generator: 7.5326924324035645 LOSS_Discriminator: 0.1474856436252594\n",
            "ITERATION_NO.: 201 LOSS_Generator: 7.335264205932617 LOSS_Discriminator: 0.06669332832098007\n",
            "ITERATION_NO.: 202 LOSS_Generator: 7.579029083251953 LOSS_Discriminator: 0.05618159845471382\n",
            "ITERATION_NO.: 203 LOSS_Generator: 7.57958984375 LOSS_Discriminator: 0.03826570883393288\n",
            "ITERATION_NO.: 204 LOSS_Generator: 7.589980602264404 LOSS_Discriminator: 0.06616769731044769\n",
            "ITERATION_NO.: 205 LOSS_Generator: 7.175802707672119 LOSS_Discriminator: 0.02845425345003605\n",
            "ITERATION_NO.: 206 LOSS_Generator: 7.1870856285095215 LOSS_Discriminator: 0.05837654694914818\n",
            "ITERATION_NO.: 207 LOSS_Generator: 6.717607021331787 LOSS_Discriminator: 0.09835736453533173\n",
            "ITERATION_NO.: 208 LOSS_Generator: 6.449802398681641 LOSS_Discriminator: 0.06669370830059052\n",
            "ITERATION_NO.: 209 LOSS_Generator: 6.778909683227539 LOSS_Discriminator: 0.07335200905799866\n",
            "ITERATION_NO.: 210 LOSS_Generator: 6.91171407699585 LOSS_Discriminator: 0.13199050724506378\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.895305156707764 LOSS_Discriminator: 0.03655382990837097\n",
            "ITERATION_NO.: 212 LOSS_Generator: 7.557534217834473 LOSS_Discriminator: 0.06158231198787689\n",
            "ITERATION_NO.: 213 LOSS_Generator: 7.292959213256836 LOSS_Discriminator: 0.032815299928188324\n",
            "ITERATION_NO.: 214 LOSS_Generator: 7.235029697418213 LOSS_Discriminator: 0.021456278860569\n",
            "ITERATION_NO.: 215 LOSS_Generator: 6.923846244812012 LOSS_Discriminator: 0.0963032990694046\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.9019775390625 LOSS_Discriminator: 0.09642991423606873\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.912833213806152 LOSS_Discriminator: 0.06873716413974762\n",
            "ITERATION_NO.: 218 LOSS_Generator: 6.492194175720215 LOSS_Discriminator: 0.061002738773822784\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.984124660491943 LOSS_Discriminator: 0.07054203748703003\n",
            "ITERATION_NO.: 220 LOSS_Generator: 7.670436382293701 LOSS_Discriminator: 0.06016543135046959\n",
            "ITERATION_NO.: 221 LOSS_Generator: 8.695823669433594 LOSS_Discriminator: 0.03423808515071869\n",
            "ITERATION_NO.: 222 LOSS_Generator: 8.572754859924316 LOSS_Discriminator: 0.0967828631401062\n",
            "ITERATION_NO.: 223 LOSS_Generator: 8.392477989196777 LOSS_Discriminator: 0.039619170129299164\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.638204574584961 LOSS_Discriminator: 0.0982295572757721\n",
            "ITERATION_NO.: 225 LOSS_Generator: 6.399441719055176 LOSS_Discriminator: 0.05205732583999634\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.598687171936035 LOSS_Discriminator: 0.08693375438451767\n",
            "ITERATION_NO.: 227 LOSS_Generator: 7.139395713806152 LOSS_Discriminator: 0.0947282463312149\n",
            "ITERATION_NO.: 228 LOSS_Generator: 7.71592903137207 LOSS_Discriminator: 0.06418877840042114\n",
            "ITERATION_NO.: 229 LOSS_Generator: 8.089888572692871 LOSS_Discriminator: 0.05879117548465729\n",
            "ITERATION_NO.: 230 LOSS_Generator: 7.875039100646973 LOSS_Discriminator: 0.12753133475780487\n",
            "ITERATION_NO.: 231 LOSS_Generator: 7.6111369132995605 LOSS_Discriminator: 0.07768326252698898\n",
            "ITERATION_NO.: 232 LOSS_Generator: 6.7823076248168945 LOSS_Discriminator: 0.07431305944919586\n",
            "ITERATION_NO.: 233 LOSS_Generator: 6.584603309631348 LOSS_Discriminator: 0.08545832335948944\n",
            "ITERATION_NO.: 234 LOSS_Generator: 6.0969438552856445 LOSS_Discriminator: 0.04688118398189545\n",
            "ITERATION_NO.: 235 LOSS_Generator: 6.868303298950195 LOSS_Discriminator: 0.0353245846927166\n",
            "ITERATION_NO.: 236 LOSS_Generator: 7.7640557289123535 LOSS_Discriminator: 0.029475141316652298\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.558151721954346 LOSS_Discriminator: 0.027310576289892197\n",
            "ITERATION_NO.: 238 LOSS_Generator: 8.060340881347656 LOSS_Discriminator: 0.06621997803449631\n",
            "ITERATION_NO.: 239 LOSS_Generator: 8.305159568786621 LOSS_Discriminator: 0.049933452159166336\n",
            "ITERATION_NO.: 240 LOSS_Generator: 7.346221446990967 LOSS_Discriminator: 0.055712658911943436\n",
            "ITERATION_NO.: 241 LOSS_Generator: 7.058351039886475 LOSS_Discriminator: 0.014828908257186413\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.933788299560547 LOSS_Discriminator: 0.03823167085647583\n",
            "ITERATION_NO.: 243 LOSS_Generator: 6.773641586303711 LOSS_Discriminator: 0.07014460861682892\n",
            "ITERATION_NO.: 244 LOSS_Generator: 7.057587146759033 LOSS_Discriminator: 0.0555184967815876\n",
            "ITERATION_NO.: 245 LOSS_Generator: 7.173642635345459 LOSS_Discriminator: 0.036038145422935486\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.825524806976318 LOSS_Discriminator: 0.1070929765701294\n",
            "ITERATION_NO.: 247 LOSS_Generator: 7.7266340255737305 LOSS_Discriminator: 0.04455271363258362\n",
            "ITERATION_NO.: 248 LOSS_Generator: 8.177569389343262 LOSS_Discriminator: 0.11036836355924606\n",
            "ITERATION_NO.: 249 LOSS_Generator: 8.035652160644531 LOSS_Discriminator: 0.041294075548648834\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.427224159240723 LOSS_Discriminator: 0.0627213716506958\n",
            "ITERATION_NO.: 251 LOSS_Generator: 6.843904495239258 LOSS_Discriminator: 0.0533885732293129\n",
            "ITERATION_NO.: 252 LOSS_Generator: 6.418151378631592 LOSS_Discriminator: 0.021930530667304993\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.8154168128967285 LOSS_Discriminator: 0.07225443422794342\n",
            "ITERATION_NO.: 254 LOSS_Generator: 6.996142387390137 LOSS_Discriminator: 0.0619417279958725\n",
            "ITERATION_NO.: 255 LOSS_Generator: 7.550326347351074 LOSS_Discriminator: 0.04185768961906433\n",
            "ITERATION_NO.: 256 LOSS_Generator: 8.509288787841797 LOSS_Discriminator: 0.04795731604099274\n",
            "ITERATION_NO.: 257 LOSS_Generator: 8.494071960449219 LOSS_Discriminator: 0.04907955229282379\n",
            "ITERATION_NO.: 258 LOSS_Generator: 9.170434951782227 LOSS_Discriminator: 0.07524894177913666\n",
            "ITERATION_NO.: 259 LOSS_Generator: 8.278705596923828 LOSS_Discriminator: 0.1509343981742859\n",
            "ITERATION_NO.: 260 LOSS_Generator: 7.022607326507568 LOSS_Discriminator: 0.06396669149398804\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.1876912117004395 LOSS_Discriminator: 0.10023266822099686\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.223309516906738 LOSS_Discriminator: 0.10171332955360413\n",
            "ITERATION_NO.: 263 LOSS_Generator: 7.321419715881348 LOSS_Discriminator: 0.07103338837623596\n",
            "ITERATION_NO.: 264 LOSS_Generator: 8.097919464111328 LOSS_Discriminator: 0.08561625331640244\n",
            "ITERATION_NO.: 265 LOSS_Generator: 8.527752876281738 LOSS_Discriminator: 0.09302056580781937\n",
            "ITERATION_NO.: 266 LOSS_Generator: 8.39139461517334 LOSS_Discriminator: 0.035604074597358704\n",
            "ITERATION_NO.: 267 LOSS_Generator: 8.579895973205566 LOSS_Discriminator: 0.0682189092040062\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.893554210662842 LOSS_Discriminator: 0.03785393387079239\n",
            "ITERATION_NO.: 269 LOSS_Generator: 6.827587127685547 LOSS_Discriminator: 0.10758061707019806\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.146722793579102 LOSS_Discriminator: 0.043609652668237686\n",
            "ITERATION_NO.: 271 LOSS_Generator: 6.601595401763916 LOSS_Discriminator: 0.045284390449523926\n",
            "ITERATION_NO.: 272 LOSS_Generator: 7.082863807678223 LOSS_Discriminator: 0.12389253824949265\n",
            "ITERATION_NO.: 273 LOSS_Generator: 8.109439849853516 LOSS_Discriminator: 0.04615107178688049\n",
            "ITERATION_NO.: 274 LOSS_Generator: 8.321002960205078 LOSS_Discriminator: 0.031089726835489273\n",
            "ITERATION_NO.: 275 LOSS_Generator: 8.767074584960938 LOSS_Discriminator: 0.07005579769611359\n",
            "ITERATION_NO.: 276 LOSS_Generator: 8.057003021240234 LOSS_Discriminator: 0.026697741821408272\n",
            "ITERATION_NO.: 277 LOSS_Generator: 7.553638458251953 LOSS_Discriminator: 0.08037217706441879\n",
            "ITERATION_NO.: 278 LOSS_Generator: 7.285801410675049 LOSS_Discriminator: 0.06806498765945435\n",
            "ITERATION_NO.: 279 LOSS_Generator: 6.226274490356445 LOSS_Discriminator: 0.06171978637576103\n",
            "ITERATION_NO.: 280 LOSS_Generator: 6.356113910675049 LOSS_Discriminator: 0.019062403589487076\n",
            "ITERATION_NO.: 281 LOSS_Generator: 6.181619167327881 LOSS_Discriminator: 0.06957370042800903\n",
            "ITERATION_NO.: 282 LOSS_Generator: 6.471714019775391 LOSS_Discriminator: 0.0912582278251648\n",
            "ITERATION_NO.: 283 LOSS_Generator: 7.207781791687012 LOSS_Discriminator: 0.05474203824996948\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.554103851318359 LOSS_Discriminator: 0.03771773725748062\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.942895412445068 LOSS_Discriminator: 0.08154329657554626\n",
            "ITERATION_NO.: 286 LOSS_Generator: 7.896196365356445 LOSS_Discriminator: 0.02853088080883026\n",
            "ITERATION_NO.: 287 LOSS_Generator: 8.134269714355469 LOSS_Discriminator: 0.0641256719827652\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.217154026031494 LOSS_Discriminator: 0.05762963742017746\n",
            "ITERATION_NO.: 289 LOSS_Generator: 6.441890716552734 LOSS_Discriminator: 0.04083510488271713\n",
            "ITERATION_NO.: 290 LOSS_Generator: 5.933748245239258 LOSS_Discriminator: 0.04734111949801445\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.047947883605957 LOSS_Discriminator: 0.09547518193721771\n",
            "ITERATION_NO.: 292 LOSS_Generator: 7.240948677062988 LOSS_Discriminator: 0.07112190127372742\n",
            "ITERATION_NO.: 293 LOSS_Generator: 7.211428165435791 LOSS_Discriminator: 0.08077558875083923\n",
            "ITERATION_NO.: 294 LOSS_Generator: 7.415054798126221 LOSS_Discriminator: 0.06616546958684921\n",
            "ITERATION_NO.: 295 LOSS_Generator: 7.088140487670898 LOSS_Discriminator: 0.022568538784980774\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.984788417816162 LOSS_Discriminator: 0.055123768746852875\n",
            "ITERATION_NO.: 297 LOSS_Generator: 6.875089168548584 LOSS_Discriminator: 0.04131343960762024\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.893365383148193 LOSS_Discriminator: 0.07394787669181824\n",
            "ITERATION_NO.: 299 LOSS_Generator: 6.679684638977051 LOSS_Discriminator: 0.04501183331012726\n",
            "ITERATION_NO.: 300 LOSS_Generator: 6.764716148376465 LOSS_Discriminator: 0.0710752010345459\n",
            "ITERATION_NO.: 301 LOSS_Generator: 7.57162618637085 LOSS_Discriminator: 0.09724276512861252\n",
            "ITERATION_NO.: 302 LOSS_Generator: 8.03186321258545 LOSS_Discriminator: 0.12975996732711792\n",
            "ITERATION_NO.: 303 LOSS_Generator: 8.595827102661133 LOSS_Discriminator: 0.07074534893035889\n",
            "ITERATION_NO.: 304 LOSS_Generator: 8.861242294311523 LOSS_Discriminator: 0.02033664658665657\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.927337646484375 LOSS_Discriminator: 0.14140275120735168\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.094501495361328 LOSS_Discriminator: 0.06482255458831787\n",
            "ITERATION_NO.: 307 LOSS_Generator: 5.5798115730285645 LOSS_Discriminator: 0.07298581302165985\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.102433681488037 LOSS_Discriminator: 0.06684144586324692\n",
            "ITERATION_NO.: 309 LOSS_Generator: 6.944697380065918 LOSS_Discriminator: 0.07754360139369965\n",
            "ITERATION_NO.: 310 LOSS_Generator: 8.029668807983398 LOSS_Discriminator: 0.04945001378655434\n",
            "ITERATION_NO.: 311 LOSS_Generator: 7.773092269897461 LOSS_Discriminator: 0.08728696405887604\n",
            "ITERATION_NO.: 312 LOSS_Generator: 8.158347129821777 LOSS_Discriminator: 0.07957564294338226\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.284689426422119 LOSS_Discriminator: 0.04597431421279907\n",
            "ITERATION_NO.: 314 LOSS_Generator: 7.225921154022217 LOSS_Discriminator: 0.016593627631664276\n",
            "ITERATION_NO.: 315 LOSS_Generator: 7.06696891784668 LOSS_Discriminator: 0.019921138882637024\n",
            "ITERATION_NO.: 316 LOSS_Generator: 7.097924709320068 LOSS_Discriminator: 0.03089330531656742\n",
            "ITERATION_NO.: 317 LOSS_Generator: 7.9672160148620605 LOSS_Discriminator: 0.06672221422195435\n",
            "ITERATION_NO.: 318 LOSS_Generator: 8.195043563842773 LOSS_Discriminator: 0.04453685134649277\n",
            "ITERATION_NO.: 319 LOSS_Generator: 8.559355735778809 LOSS_Discriminator: 0.05239307880401611\n",
            "ITERATION_NO.: 320 LOSS_Generator: 8.528623580932617 LOSS_Discriminator: 0.052491359412670135\n",
            "ITERATION_NO.: 321 LOSS_Generator: 8.093443870544434 LOSS_Discriminator: 0.013583189807832241\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.306950092315674 LOSS_Discriminator: 0.08407425880432129\n",
            "ITERATION_NO.: 323 LOSS_Generator: 6.069901943206787 LOSS_Discriminator: 0.09143094718456268\n",
            "ITERATION_NO.: 324 LOSS_Generator: 6.219015121459961 LOSS_Discriminator: 0.1407650113105774\n",
            "ITERATION_NO.: 325 LOSS_Generator: 7.26873779296875 LOSS_Discriminator: 0.07747900485992432\n",
            "ITERATION_NO.: 326 LOSS_Generator: 7.922926425933838 LOSS_Discriminator: 0.11570611596107483\n",
            "ITERATION_NO.: 327 LOSS_Generator: 7.980978965759277 LOSS_Discriminator: 0.11004403233528137\n",
            "ITERATION_NO.: 328 LOSS_Generator: 7.821139335632324 LOSS_Discriminator: 0.07222720980644226\n",
            "ITERATION_NO.: 329 LOSS_Generator: 8.411896705627441 LOSS_Discriminator: 0.03815693408250809\n",
            "ITERATION_NO.: 330 LOSS_Generator: 7.676275253295898 LOSS_Discriminator: 0.06103907525539398\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.245148181915283 LOSS_Discriminator: 0.04978981614112854\n",
            "ITERATION_NO.: 332 LOSS_Generator: 6.149787425994873 LOSS_Discriminator: 0.06348934769630432\n",
            "ITERATION_NO.: 333 LOSS_Generator: 6.683960437774658 LOSS_Discriminator: 0.05488156899809837\n",
            "ITERATION_NO.: 334 LOSS_Generator: 6.791894912719727 LOSS_Discriminator: 0.07764160633087158\n",
            "ITERATION_NO.: 335 LOSS_Generator: 6.97567892074585 LOSS_Discriminator: 0.061948902904987335\n",
            "ITERATION_NO.: 336 LOSS_Generator: 8.392218589782715 LOSS_Discriminator: 0.026390396058559418\n",
            "ITERATION_NO.: 337 LOSS_Generator: 8.069140434265137 LOSS_Discriminator: 0.055685821920633316\n",
            "ITERATION_NO.: 338 LOSS_Generator: 7.556210994720459 LOSS_Discriminator: 0.07406933605670929\n",
            "ITERATION_NO.: 339 LOSS_Generator: 6.378423690795898 LOSS_Discriminator: 0.055817313492298126\n",
            "ITERATION_NO.: 340 LOSS_Generator: 5.930576801300049 LOSS_Discriminator: 0.08244742453098297\n",
            "ITERATION_NO.: 341 LOSS_Generator: 6.524947643280029 LOSS_Discriminator: 0.0927794873714447\n",
            "ITERATION_NO.: 342 LOSS_Generator: 8.286419868469238 LOSS_Discriminator: 0.0743541270494461\n",
            "ITERATION_NO.: 343 LOSS_Generator: 8.581799507141113 LOSS_Discriminator: 0.01974480226635933\n",
            "ITERATION_NO.: 344 LOSS_Generator: 8.805769920349121 LOSS_Discriminator: 0.02323569729924202\n",
            "ITERATION_NO.: 345 LOSS_Generator: 9.468995094299316 LOSS_Discriminator: 0.06555713713169098\n",
            "ITERATION_NO.: 346 LOSS_Generator: 7.989099025726318 LOSS_Discriminator: 0.09849156439304352\n",
            "ITERATION_NO.: 347 LOSS_Generator: 6.624122142791748 LOSS_Discriminator: 0.16720333695411682\n",
            "ITERATION_NO.: 348 LOSS_Generator: 5.078752040863037 LOSS_Discriminator: 0.11400279402732849\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.884883880615234 LOSS_Discriminator: 0.18516738712787628\n",
            "ITERATION_NO.: 350 LOSS_Generator: 7.110497951507568 LOSS_Discriminator: 0.07149383425712585\n",
            "ITERATION_NO.: 351 LOSS_Generator: 7.717803478240967 LOSS_Discriminator: 0.07917101681232452\n",
            "ITERATION_NO.: 352 LOSS_Generator: 7.918755054473877 LOSS_Discriminator: 0.049208756536245346\n",
            "ITERATION_NO.: 353 LOSS_Generator: 8.113824844360352 LOSS_Discriminator: 0.066193126142025\n",
            "ITERATION_NO.: 354 LOSS_Generator: 7.630735874176025 LOSS_Discriminator: 0.060250651091337204\n",
            "ITERATION_NO.: 355 LOSS_Generator: 6.176587104797363 LOSS_Discriminator: 0.15486280620098114\n",
            "ITERATION_NO.: 356 LOSS_Generator: 5.737313270568848 LOSS_Discriminator: 0.05901259928941727\n",
            "ITERATION_NO.: 357 LOSS_Generator: 6.002252101898193 LOSS_Discriminator: 0.11815127730369568\n",
            "ITERATION_NO.: 358 LOSS_Generator: 7.065390586853027 LOSS_Discriminator: 0.03908120095729828\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.337696552276611 LOSS_Discriminator: 0.05526510253548622\n",
            "ITERATION_NO.: 360 LOSS_Generator: 7.8206257820129395 LOSS_Discriminator: 0.034649528563022614\n",
            "ITERATION_NO.: 361 LOSS_Generator: 8.200560569763184 LOSS_Discriminator: 0.03173701465129852\n",
            "ITERATION_NO.: 362 LOSS_Generator: 8.199767112731934 LOSS_Discriminator: 0.0292666032910347\n",
            "ITERATION_NO.: 363 LOSS_Generator: 8.563943862915039 LOSS_Discriminator: 0.023882117122411728\n",
            "ITERATION_NO.: 364 LOSS_Generator: 7.047961235046387 LOSS_Discriminator: 0.12899409234523773\n",
            "ITERATION_NO.: 365 LOSS_Generator: 6.555234432220459 LOSS_Discriminator: 0.08093349635601044\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.7418036460876465 LOSS_Discriminator: 0.11613110452890396\n",
            "ITERATION_NO.: 367 LOSS_Generator: 7.587017059326172 LOSS_Discriminator: 0.1066884994506836\n",
            "ITERATION_NO.: 368 LOSS_Generator: 8.499567031860352 LOSS_Discriminator: 0.04332171753048897\n",
            "ITERATION_NO.: 369 LOSS_Generator: 8.256270408630371 LOSS_Discriminator: 0.04749832674860954\n",
            "ITERATION_NO.: 370 LOSS_Generator: 8.226584434509277 LOSS_Discriminator: 0.12315214425325394\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.434909820556641 LOSS_Discriminator: 0.06561332941055298\n",
            "ITERATION_NO.: 372 LOSS_Generator: 6.1554155349731445 LOSS_Discriminator: 0.04858775436878204\n",
            "ITERATION_NO.: 373 LOSS_Generator: 5.9693450927734375 LOSS_Discriminator: 0.07693512737751007\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.355875015258789 LOSS_Discriminator: 0.05491212010383606\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.5533976554870605 LOSS_Discriminator: 0.0963447093963623\n",
            "ITERATION_NO.: 376 LOSS_Generator: 7.896531581878662 LOSS_Discriminator: 0.054510366171598434\n",
            "ITERATION_NO.: 377 LOSS_Generator: 8.960005760192871 LOSS_Discriminator: 0.08953975886106491\n",
            "ITERATION_NO.: 378 LOSS_Generator: 8.973031997680664 LOSS_Discriminator: 0.05095256119966507\n",
            "ITERATION_NO.: 379 LOSS_Generator: 7.61018180847168 LOSS_Discriminator: 0.045978207141160965\n",
            "ITERATION_NO.: 380 LOSS_Generator: 7.483316421508789 LOSS_Discriminator: 0.08060682564973831\n",
            "ITERATION_NO.: 381 LOSS_Generator: 6.831973075866699 LOSS_Discriminator: 0.04587602615356445\n",
            "ITERATION_NO.: 382 LOSS_Generator: 6.284210681915283 LOSS_Discriminator: 0.04404517263174057\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.698634147644043 LOSS_Discriminator: 0.05962235853075981\n",
            "ITERATION_NO.: 384 LOSS_Generator: 7.902538299560547 LOSS_Discriminator: 0.08247052133083344\n",
            "ITERATION_NO.: 385 LOSS_Generator: 8.614810943603516 LOSS_Discriminator: 0.03356383740901947\n",
            "ITERATION_NO.: 386 LOSS_Generator: 8.931671142578125 LOSS_Discriminator: 0.044539399445056915\n",
            "ITERATION_NO.: 387 LOSS_Generator: 7.9817023277282715 LOSS_Discriminator: 0.10115384310483932\n",
            "ITERATION_NO.: 388 LOSS_Generator: 7.172134876251221 LOSS_Discriminator: 0.04060245305299759\n",
            "ITERATION_NO.: 389 LOSS_Generator: 7.066796779632568 LOSS_Discriminator: 0.05518066883087158\n",
            "ITERATION_NO.: 390 LOSS_Generator: 6.734645366668701 LOSS_Discriminator: 0.09431886672973633\n",
            "ITERATION_NO.: 391 LOSS_Generator: 7.404426097869873 LOSS_Discriminator: 0.0423523485660553\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.369085311889648 LOSS_Discriminator: 0.0549742728471756\n",
            "ITERATION_NO.: 393 LOSS_Generator: 8.250029563903809 LOSS_Discriminator: 0.037488993257284164\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.301919937133789 LOSS_Discriminator: 0.048898845911026\n",
            "ITERATION_NO.: 395 LOSS_Generator: 8.325042724609375 LOSS_Discriminator: 0.05402345210313797\n",
            "ITERATION_NO.: 396 LOSS_Generator: 7.820835590362549 LOSS_Discriminator: 0.0483953095972538\n",
            "ITERATION_NO.: 397 LOSS_Generator: 7.025174140930176 LOSS_Discriminator: 0.10464497655630112\n",
            "ITERATION_NO.: 398 LOSS_Generator: 6.5073065757751465 LOSS_Discriminator: 0.045614320784807205\n",
            "ITERATION_NO.: 399 LOSS_Generator: 6.295904636383057 LOSS_Discriminator: 0.1139284074306488\n",
            "ITERATION_NO.: 400 LOSS_Generator: 6.958823204040527 LOSS_Discriminator: 0.10407169908285141\n",
            "ITERATION_NO.: 401 LOSS_Generator: 8.72652816772461 LOSS_Discriminator: 0.10232611000537872\n",
            "ITERATION_NO.: 402 LOSS_Generator: 8.686090469360352 LOSS_Discriminator: 0.11728553473949432\n",
            "ITERATION_NO.: 403 LOSS_Generator: 8.744606971740723 LOSS_Discriminator: 0.05995994433760643\n",
            "ITERATION_NO.: 404 LOSS_Generator: 7.923468112945557 LOSS_Discriminator: 0.03942784667015076\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.554684638977051 LOSS_Discriminator: 0.07066505402326584\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.027328968048096 LOSS_Discriminator: 0.050260260701179504\n",
            "ITERATION_NO.: 407 LOSS_Generator: 6.397630214691162 LOSS_Discriminator: 0.12748347222805023\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.186601161956787 LOSS_Discriminator: 0.03705597668886185\n",
            "ITERATION_NO.: 409 LOSS_Generator: 8.141948699951172 LOSS_Discriminator: 0.039263829588890076\n",
            "ITERATION_NO.: 410 LOSS_Generator: 8.13781452178955 LOSS_Discriminator: 0.06692206859588623\n",
            "ITERATION_NO.: 411 LOSS_Generator: 8.1884126663208 LOSS_Discriminator: 0.10772378742694855\n",
            "ITERATION_NO.: 412 LOSS_Generator: 7.4869537353515625 LOSS_Discriminator: 0.06437469273805618\n",
            "ITERATION_NO.: 413 LOSS_Generator: 7.735037803649902 LOSS_Discriminator: 0.021085374057292938\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.796608924865723 LOSS_Discriminator: 0.04178869351744652\n",
            "ITERATION_NO.: 415 LOSS_Generator: 7.023687362670898 LOSS_Discriminator: 0.06226503849029541\n",
            "ITERATION_NO.: 416 LOSS_Generator: 7.195721626281738 LOSS_Discriminator: 0.04231535643339157\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.298214912414551 LOSS_Discriminator: 0.12194155901670456\n",
            "ITERATION_NO.: 418 LOSS_Generator: 7.273721694946289 LOSS_Discriminator: 0.11453074216842651\n",
            "ITERATION_NO.: 419 LOSS_Generator: 7.432933330535889 LOSS_Discriminator: 0.1086253672838211\n",
            "ITERATION_NO.: 420 LOSS_Generator: 6.78565788269043 LOSS_Discriminator: 0.04885055869817734\n",
            "ITERATION_NO.: 421 LOSS_Generator: 6.655008316040039 LOSS_Discriminator: 0.056870024651288986\n",
            "ITERATION_NO.: 422 LOSS_Generator: 7.781136512756348 LOSS_Discriminator: 0.030739963054656982\n",
            "ITERATION_NO.: 423 LOSS_Generator: 7.0512871742248535 LOSS_Discriminator: 0.038067035377025604\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.134316444396973 LOSS_Discriminator: 0.04799322038888931\n",
            "ITERATION_NO.: 425 LOSS_Generator: 7.545635223388672 LOSS_Discriminator: 0.033447764813899994\n",
            "ITERATION_NO.: 426 LOSS_Generator: 7.936223030090332 LOSS_Discriminator: 0.016277579590678215\n",
            "ITERATION_NO.: 427 LOSS_Generator: 7.268520355224609 LOSS_Discriminator: 0.12676991522312164\n",
            "ITERATION_NO.: 428 LOSS_Generator: 7.079849720001221 LOSS_Discriminator: 0.051054198294878006\n",
            "ITERATION_NO.: 429 LOSS_Generator: 6.7774858474731445 LOSS_Discriminator: 0.04495993256568909\n",
            "ITERATION_NO.: 430 LOSS_Generator: 7.114901542663574 LOSS_Discriminator: 0.1088392361998558\n",
            "ITERATION_NO.: 431 LOSS_Generator: 7.16832160949707 LOSS_Discriminator: 0.026910413056612015\n",
            "ITERATION_NO.: 432 LOSS_Generator: 7.356445789337158 LOSS_Discriminator: 0.03986789286136627\n",
            "ITERATION_NO.: 433 LOSS_Generator: 8.472612380981445 LOSS_Discriminator: 0.015714429318904877\n",
            "ITERATION_NO.: 434 LOSS_Generator: 8.392394065856934 LOSS_Discriminator: 0.11594807356595993\n",
            "ITERATION_NO.: 435 LOSS_Generator: 7.3790130615234375 LOSS_Discriminator: 0.0850633978843689\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.633295059204102 LOSS_Discriminator: 0.06472138315439224\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.3568902015686035 LOSS_Discriminator: 0.0889965072274208\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.32005500793457 LOSS_Discriminator: 0.06193588301539421\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.8303704261779785 LOSS_Discriminator: 0.07237327843904495\n",
            "ITERATION_NO.: 440 LOSS_Generator: 8.568599700927734 LOSS_Discriminator: 0.028591036796569824\n",
            "ITERATION_NO.: 441 LOSS_Generator: 10.057741165161133 LOSS_Discriminator: 0.07887747138738632\n",
            "ITERATION_NO.: 442 LOSS_Generator: 9.108124732971191 LOSS_Discriminator: 0.09542110562324524\n",
            "ITERATION_NO.: 443 LOSS_Generator: 8.084488868713379 LOSS_Discriminator: 0.040428392589092255\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.015599250793457 LOSS_Discriminator: 0.10024400055408478\n",
            "ITERATION_NO.: 445 LOSS_Generator: 6.8677897453308105 LOSS_Discriminator: 0.022602086886763573\n",
            "ITERATION_NO.: 446 LOSS_Generator: 7.256112098693848 LOSS_Discriminator: 0.07456689327955246\n",
            "ITERATION_NO.: 447 LOSS_Generator: 8.032051086425781 LOSS_Discriminator: 0.04996344447135925\n",
            "ITERATION_NO.: 448 LOSS_Generator: 8.185794830322266 LOSS_Discriminator: 0.06488066166639328\n",
            "ITERATION_NO.: 449 LOSS_Generator: 8.576486587524414 LOSS_Discriminator: 0.0514305979013443\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.71678352355957 LOSS_Discriminator: 0.10110357403755188\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.745160102844238 LOSS_Discriminator: 0.04020221158862114\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.6721696853637695 LOSS_Discriminator: 0.07301844656467438\n",
            "ITERATION_NO.: 453 LOSS_Generator: 7.037291049957275 LOSS_Discriminator: 0.03645625710487366\n",
            "ITERATION_NO.: 454 LOSS_Generator: 7.388178825378418 LOSS_Discriminator: 0.06657317280769348\n",
            "ITERATION_NO.: 455 LOSS_Generator: 8.31788444519043 LOSS_Discriminator: 0.06658683717250824\n",
            "ITERATION_NO.: 456 LOSS_Generator: 8.974091529846191 LOSS_Discriminator: 0.11238936334848404\n",
            "ITERATION_NO.: 457 LOSS_Generator: 8.837474822998047 LOSS_Discriminator: 0.025995299220085144\n",
            "ITERATION_NO.: 458 LOSS_Generator: 9.0836763381958 LOSS_Discriminator: 0.15514323115348816\n",
            "ITERATION_NO.: 459 LOSS_Generator: 7.8364033699035645 LOSS_Discriminator: 0.05136515200138092\n",
            "ITERATION_NO.: 460 LOSS_Generator: 7.314882278442383 LOSS_Discriminator: 0.03469029441475868\n",
            "ITERATION_NO.: 461 LOSS_Generator: 6.005745887756348 LOSS_Discriminator: 0.1145438700914383\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.994408130645752 LOSS_Discriminator: 0.08041433990001678\n",
            "ITERATION_NO.: 463 LOSS_Generator: 7.159802436828613 LOSS_Discriminator: 0.1765238493680954\n",
            "ITERATION_NO.: 464 LOSS_Generator: 8.4527587890625 LOSS_Discriminator: 0.07048827409744263\n",
            "ITERATION_NO.: 465 LOSS_Generator: 8.496572494506836 LOSS_Discriminator: 0.040700316429138184\n",
            "ITERATION_NO.: 466 LOSS_Generator: 9.133255958557129 LOSS_Discriminator: 0.09946782886981964\n",
            "ITERATION_NO.: 467 LOSS_Generator: 7.993495464324951 LOSS_Discriminator: 0.17152491211891174\n",
            "ITERATION_NO.: 468 LOSS_Generator: 6.230915069580078 LOSS_Discriminator: 0.07167889922857285\n",
            "ITERATION_NO.: 469 LOSS_Generator: 5.607810020446777 LOSS_Discriminator: 0.13056959211826324\n",
            "ITERATION_NO.: 470 LOSS_Generator: 6.898107528686523 LOSS_Discriminator: 0.13073575496673584\n",
            "ITERATION_NO.: 471 LOSS_Generator: 8.815596580505371 LOSS_Discriminator: 0.063838891685009\n",
            "ITERATION_NO.: 472 LOSS_Generator: 9.955066680908203 LOSS_Discriminator: 0.05131639540195465\n",
            "ITERATION_NO.: 473 LOSS_Generator: 10.609597206115723 LOSS_Discriminator: 0.12437202036380768\n",
            "ITERATION_NO.: 474 LOSS_Generator: 9.249520301818848 LOSS_Discriminator: 0.12112002074718475\n",
            "ITERATION_NO.: 475 LOSS_Generator: 8.286859512329102 LOSS_Discriminator: 0.18418800830841064\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.545495986938477 LOSS_Discriminator: 0.05112866684794426\n",
            "ITERATION_NO.: 477 LOSS_Generator: 4.531135082244873 LOSS_Discriminator: 0.12286978960037231\n",
            "ITERATION_NO.: 478 LOSS_Generator: 5.562320709228516 LOSS_Discriminator: 0.18251609802246094\n",
            "ITERATION_NO.: 479 LOSS_Generator: 7.457432270050049 LOSS_Discriminator: 0.07259920984506607\n",
            "ITERATION_NO.: 480 LOSS_Generator: 8.603058815002441 LOSS_Discriminator: 0.12362994998693466\n",
            "ITERATION_NO.: 481 LOSS_Generator: 9.16674518585205 LOSS_Discriminator: 0.11082552373409271\n",
            "ITERATION_NO.: 482 LOSS_Generator: 9.593476295471191 LOSS_Discriminator: 0.057769324630498886\n",
            "ITERATION_NO.: 483 LOSS_Generator: 8.583269119262695 LOSS_Discriminator: 0.09919741749763489\n",
            "ITERATION_NO.: 484 LOSS_Generator: 8.420434951782227 LOSS_Discriminator: 0.06400106847286224\n",
            "ITERATION_NO.: 485 LOSS_Generator: 6.535907745361328 LOSS_Discriminator: 0.030601266771554947\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.911478042602539 LOSS_Discriminator: 0.06915923207998276\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.001498222351074 LOSS_Discriminator: 0.11284369975328445\n",
            "ITERATION_NO.: 488 LOSS_Generator: 7.010796546936035 LOSS_Discriminator: 0.07278671115636826\n",
            "ITERATION_NO.: 489 LOSS_Generator: 7.481212615966797 LOSS_Discriminator: 0.08066363632678986\n",
            "ITERATION_NO.: 490 LOSS_Generator: 8.130475044250488 LOSS_Discriminator: 0.024630360305309296\n",
            "ITERATION_NO.: 491 LOSS_Generator: 9.1421480178833 LOSS_Discriminator: 0.06390929967164993\n",
            "ITERATION_NO.: 492 LOSS_Generator: 8.591184616088867 LOSS_Discriminator: 0.03473036736249924\n",
            "ITERATION_NO.: 493 LOSS_Generator: 8.374896049499512 LOSS_Discriminator: 0.07629106193780899\n",
            "ITERATION_NO.: 494 LOSS_Generator: 8.103076934814453 LOSS_Discriminator: 0.02714136242866516\n",
            "ITERATION_NO.: 495 LOSS_Generator: 7.380385875701904 LOSS_Discriminator: 0.014763973653316498\n",
            "ITERATION_NO.: 496 LOSS_Generator: 7.418694019317627 LOSS_Discriminator: 0.017463497817516327\n",
            "ITERATION_NO.: 497 LOSS_Generator: 7.013964653015137 LOSS_Discriminator: 0.06214016675949097\n",
            "ITERATION_NO.: 498 LOSS_Generator: 6.580992221832275 LOSS_Discriminator: 0.02675769105553627\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.183016300201416 LOSS_Discriminator: 0.05411604791879654\n",
            "ITERATION_NO.: 500 LOSS_Generator: 7.504385471343994 LOSS_Discriminator: 0.09934721887111664\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.003909111022949 LOSS_Discriminator: 0.033210862427949905\n",
            "ITERATION_NO.: 502 LOSS_Generator: 7.520637035369873 LOSS_Discriminator: 0.025578472763299942\n",
            "ITERATION_NO.: 503 LOSS_Generator: 7.090273857116699 LOSS_Discriminator: 0.08718464523553848\n",
            "ITERATION_NO.: 504 LOSS_Generator: 7.091442108154297 LOSS_Discriminator: 0.03407100960612297\n",
            "ITERATION_NO.: 505 LOSS_Generator: 6.652087211608887 LOSS_Discriminator: 0.13609980046749115\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.9561381340026855 LOSS_Discriminator: 0.06498527526855469\n",
            "ITERATION_NO.: 507 LOSS_Generator: 6.854632377624512 LOSS_Discriminator: 0.13644926249980927\n",
            "ITERATION_NO.: 508 LOSS_Generator: 7.744764804840088 LOSS_Discriminator: 0.04399226978421211\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.212529182434082 LOSS_Discriminator: 0.06372906267642975\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.4788408279418945 LOSS_Discriminator: 0.052791528403759\n",
            "ITERATION_NO.: 511 LOSS_Generator: 6.807520866394043 LOSS_Discriminator: 0.06037034094333649\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.043351650238037 LOSS_Discriminator: 0.036760248243808746\n",
            "ITERATION_NO.: 513 LOSS_Generator: 7.3582024574279785 LOSS_Discriminator: 0.04890965670347214\n",
            "ITERATION_NO.: 514 LOSS_Generator: 7.555901527404785 LOSS_Discriminator: 0.0429394468665123\n",
            "ITERATION_NO.: 515 LOSS_Generator: 7.459902763366699 LOSS_Discriminator: 0.06873553991317749\n",
            "ITERATION_NO.: 516 LOSS_Generator: 7.800311088562012 LOSS_Discriminator: 0.05808836966753006\n",
            "ITERATION_NO.: 517 LOSS_Generator: 7.05155086517334 LOSS_Discriminator: 0.056780122220516205\n",
            "ITERATION_NO.: 518 LOSS_Generator: 6.827152729034424 LOSS_Discriminator: 0.06651575118303299\n",
            "ITERATION_NO.: 519 LOSS_Generator: 6.352138519287109 LOSS_Discriminator: 0.06893792748451233\n",
            "ITERATION_NO.: 520 LOSS_Generator: 5.915747165679932 LOSS_Discriminator: 0.058629024773836136\n",
            "ITERATION_NO.: 521 LOSS_Generator: 6.516490459442139 LOSS_Discriminator: 0.06349723041057587\n",
            "ITERATION_NO.: 522 LOSS_Generator: 6.534064292907715 LOSS_Discriminator: 0.07162285596132278\n",
            "ITERATION_NO.: 523 LOSS_Generator: 7.314363956451416 LOSS_Discriminator: 0.04838036373257637\n",
            "ITERATION_NO.: 524 LOSS_Generator: 7.819858551025391 LOSS_Discriminator: 0.07370083034038544\n",
            "ITERATION_NO.: 525 LOSS_Generator: 7.188554763793945 LOSS_Discriminator: 0.07297684997320175\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.3194355964660645 LOSS_Discriminator: 0.04431798681616783\n",
            "ITERATION_NO.: 527 LOSS_Generator: 7.1801934242248535 LOSS_Discriminator: 0.056768450886011124\n",
            "ITERATION_NO.: 528 LOSS_Generator: 6.864019393920898 LOSS_Discriminator: 0.06364450603723526\n",
            "ITERATION_NO.: 529 LOSS_Generator: 6.1563639640808105 LOSS_Discriminator: 0.06662620604038239\n",
            "ITERATION_NO.: 530 LOSS_Generator: 6.257354736328125 LOSS_Discriminator: 0.09465817362070084\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.177392959594727 LOSS_Discriminator: 0.06098625808954239\n",
            "ITERATION_NO.: 532 LOSS_Generator: 7.346235275268555 LOSS_Discriminator: 0.02528216317296028\n",
            "ITERATION_NO.: 533 LOSS_Generator: 8.113022804260254 LOSS_Discriminator: 0.044843271374702454\n",
            "ITERATION_NO.: 534 LOSS_Generator: 8.03797435760498 LOSS_Discriminator: 0.123468779027462\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.040980339050293 LOSS_Discriminator: 0.06698447465896606\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.706808090209961 LOSS_Discriminator: 0.02003459818661213\n",
            "ITERATION_NO.: 537 LOSS_Generator: 7.25208854675293 LOSS_Discriminator: 0.1103615090250969\n",
            "ITERATION_NO.: 538 LOSS_Generator: 7.367743015289307 LOSS_Discriminator: 0.0635669007897377\n",
            "ITERATION_NO.: 539 LOSS_Generator: 8.288888931274414 LOSS_Discriminator: 0.04240487888455391\n",
            "ITERATION_NO.: 540 LOSS_Generator: 8.644969940185547 LOSS_Discriminator: 0.05283667892217636\n",
            "ITERATION_NO.: 541 LOSS_Generator: 9.023636817932129 LOSS_Discriminator: 0.11600521206855774\n",
            "ITERATION_NO.: 542 LOSS_Generator: 7.657739162445068 LOSS_Discriminator: 0.13125509023666382\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.304399013519287 LOSS_Discriminator: 0.05736212804913521\n",
            "ITERATION_NO.: 544 LOSS_Generator: 5.7766642570495605 LOSS_Discriminator: 0.06558547168970108\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.344008922576904 LOSS_Discriminator: 0.12312877923250198\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.928549766540527 LOSS_Discriminator: 0.043354105204343796\n",
            "ITERATION_NO.: 547 LOSS_Generator: 8.198025703430176 LOSS_Discriminator: 0.05625229328870773\n",
            "ITERATION_NO.: 548 LOSS_Generator: 7.651539325714111 LOSS_Discriminator: 0.0857628583908081\n",
            "ITERATION_NO.: 549 LOSS_Generator: 7.522597789764404 LOSS_Discriminator: 0.052104756236076355\n",
            "ITERATION_NO.: 550 LOSS_Generator: 7.317262172698975 LOSS_Discriminator: 0.05904746428132057\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.033497333526611 LOSS_Discriminator: 0.06746247410774231\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.165369033813477 LOSS_Discriminator: 0.023301169276237488\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.537115573883057 LOSS_Discriminator: 0.04823842644691467\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.445512771606445 LOSS_Discriminator: 0.0836692601442337\n",
            "ITERATION_NO.: 555 LOSS_Generator: 6.891411781311035 LOSS_Discriminator: 0.03731353580951691\n",
            "ITERATION_NO.: 556 LOSS_Generator: 7.427464008331299 LOSS_Discriminator: 0.032575078308582306\n",
            "ITERATION_NO.: 557 LOSS_Generator: 8.082723617553711 LOSS_Discriminator: 0.021379364654421806\n",
            "ITERATION_NO.: 558 LOSS_Generator: 8.302477836608887 LOSS_Discriminator: 0.043162181973457336\n",
            "ITERATION_NO.: 559 LOSS_Generator: 7.80709171295166 LOSS_Discriminator: 0.07962118089199066\n",
            "ITERATION_NO.: 560 LOSS_Generator: 7.562658309936523 LOSS_Discriminator: 0.03830980882048607\n",
            "ITERATION_NO.: 561 LOSS_Generator: 7.447638511657715 LOSS_Discriminator: 0.04814065247774124\n",
            "ITERATION_NO.: 562 LOSS_Generator: 7.113300323486328 LOSS_Discriminator: 0.03712243214249611\n",
            "ITERATION_NO.: 563 LOSS_Generator: 7.770811557769775 LOSS_Discriminator: 0.06607453525066376\n",
            "ITERATION_NO.: 564 LOSS_Generator: 8.171525001525879 LOSS_Discriminator: 0.07012498378753662\n",
            "ITERATION_NO.: 565 LOSS_Generator: 8.366276741027832 LOSS_Discriminator: 0.0924985408782959\n",
            "ITERATION_NO.: 566 LOSS_Generator: 7.682127475738525 LOSS_Discriminator: 0.09555983543395996\n",
            "ITERATION_NO.: 567 LOSS_Generator: 7.556105136871338 LOSS_Discriminator: 0.06808456778526306\n",
            "ITERATION_NO.: 568 LOSS_Generator: 7.539923191070557 LOSS_Discriminator: 0.03946399688720703\n",
            "ITERATION_NO.: 569 LOSS_Generator: 7.809061050415039 LOSS_Discriminator: 0.05519479885697365\n",
            "ITERATION_NO.: 570 LOSS_Generator: 8.07396411895752 LOSS_Discriminator: 0.06319067627191544\n",
            "ITERATION_NO.: 571 LOSS_Generator: 8.278794288635254 LOSS_Discriminator: 0.05261782556772232\n",
            "ITERATION_NO.: 572 LOSS_Generator: 8.044682502746582 LOSS_Discriminator: 0.027018805965781212\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.900897979736328 LOSS_Discriminator: 0.0949377566576004\n",
            "ITERATION_NO.: 574 LOSS_Generator: 6.712815761566162 LOSS_Discriminator: 0.041213616728782654\n",
            "ITERATION_NO.: 575 LOSS_Generator: 6.267255783081055 LOSS_Discriminator: 0.05128910765051842\n",
            "ITERATION_NO.: 576 LOSS_Generator: 6.626599311828613 LOSS_Discriminator: 0.04838180914521217\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.950800895690918 LOSS_Discriminator: 0.033203497529029846\n",
            "ITERATION_NO.: 578 LOSS_Generator: 8.171357154846191 LOSS_Discriminator: 0.033205434679985046\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.264923095703125 LOSS_Discriminator: 0.12313897162675858\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.926351547241211 LOSS_Discriminator: 0.015389171428978443\n",
            "ITERATION_NO.: 581 LOSS_Generator: 6.8294219970703125 LOSS_Discriminator: 0.049403540790081024\n",
            "ITERATION_NO.: 582 LOSS_Generator: 6.112019062042236 LOSS_Discriminator: 0.07613945007324219\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.547463417053223 LOSS_Discriminator: 0.07954540103673935\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.9988226890563965 LOSS_Discriminator: 0.07311058789491653\n",
            "ITERATION_NO.: 585 LOSS_Generator: 7.905221939086914 LOSS_Discriminator: 0.038452453911304474\n",
            "ITERATION_NO.: 586 LOSS_Generator: 8.67536449432373 LOSS_Discriminator: 0.07011222839355469\n",
            "ITERATION_NO.: 587 LOSS_Generator: 8.326412200927734 LOSS_Discriminator: 0.05716419219970703\n",
            "ITERATION_NO.: 588 LOSS_Generator: 7.807092666625977 LOSS_Discriminator: 0.06264923512935638\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.6984543800354 LOSS_Discriminator: 0.05419725924730301\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.9033403396606445 LOSS_Discriminator: 0.0298442579805851\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.91158390045166 LOSS_Discriminator: 0.08148245513439178\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.077157020568848 LOSS_Discriminator: 0.04331933334469795\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.8153767585754395 LOSS_Discriminator: 0.060583729296922684\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.680511474609375 LOSS_Discriminator: 0.01909577287733555\n",
            "ITERATION_NO.: 595 LOSS_Generator: 8.273436546325684 LOSS_Discriminator: 0.05469527468085289\n",
            "ITERATION_NO.: 596 LOSS_Generator: 7.052485942840576 LOSS_Discriminator: 0.07792720943689346\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.94374942779541 LOSS_Discriminator: 0.036554332822561264\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.893251419067383 LOSS_Discriminator: 0.043152499943971634\n",
            "ITERATION_NO.: 599 LOSS_Generator: 7.067391872406006 LOSS_Discriminator: 0.07711726427078247\n",
            "ITERATION_NO.: 600 LOSS_Generator: 7.661993980407715 LOSS_Discriminator: 0.118895024061203\n",
            "EPOCH OVER: 43\n",
            "ITERATION_NO.: 1 LOSS_Generator: 7.852155685424805 LOSS_Discriminator: 0.11436290293931961\n",
            "ITERATION_NO.: 2 LOSS_Generator: 7.605195999145508 LOSS_Discriminator: 0.03493565693497658\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.719262599945068 LOSS_Discriminator: 0.05113736540079117\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.422184944152832 LOSS_Discriminator: 0.017323438078165054\n",
            "ITERATION_NO.: 5 LOSS_Generator: 7.215528964996338 LOSS_Discriminator: 0.025076821446418762\n",
            "ITERATION_NO.: 6 LOSS_Generator: 7.675914764404297 LOSS_Discriminator: 0.08131571114063263\n",
            "ITERATION_NO.: 7 LOSS_Generator: 7.521442890167236 LOSS_Discriminator: 0.048019737005233765\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.940192699432373 LOSS_Discriminator: 0.08453923463821411\n",
            "ITERATION_NO.: 9 LOSS_Generator: 8.242457389831543 LOSS_Discriminator: 0.09598065167665482\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.8724284172058105 LOSS_Discriminator: 0.03866412118077278\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.43465518951416 LOSS_Discriminator: 0.06104674190282822\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.295897960662842 LOSS_Discriminator: 0.052572689950466156\n",
            "ITERATION_NO.: 13 LOSS_Generator: 7.19648551940918 LOSS_Discriminator: 0.1057204008102417\n",
            "ITERATION_NO.: 14 LOSS_Generator: 7.174912929534912 LOSS_Discriminator: 0.08803869783878326\n",
            "ITERATION_NO.: 15 LOSS_Generator: 6.640064239501953 LOSS_Discriminator: 0.05987558513879776\n",
            "ITERATION_NO.: 16 LOSS_Generator: 6.285028457641602 LOSS_Discriminator: 0.08433359861373901\n",
            "ITERATION_NO.: 17 LOSS_Generator: 6.287661075592041 LOSS_Discriminator: 0.05196978151798248\n",
            "ITERATION_NO.: 18 LOSS_Generator: 7.379148960113525 LOSS_Discriminator: 0.028102779760956764\n",
            "ITERATION_NO.: 19 LOSS_Generator: 8.076492309570312 LOSS_Discriminator: 0.09301728010177612\n",
            "ITERATION_NO.: 20 LOSS_Generator: 8.047797203063965 LOSS_Discriminator: 0.06472988426685333\n",
            "ITERATION_NO.: 21 LOSS_Generator: 7.49102783203125 LOSS_Discriminator: 0.045438818633556366\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.3648200035095215 LOSS_Discriminator: 0.026440856978297234\n",
            "ITERATION_NO.: 23 LOSS_Generator: 6.9689507484436035 LOSS_Discriminator: 0.06941384077072144\n",
            "ITERATION_NO.: 24 LOSS_Generator: 6.862351894378662 LOSS_Discriminator: 0.09173592925071716\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.170417308807373 LOSS_Discriminator: 0.064229317009449\n",
            "ITERATION_NO.: 26 LOSS_Generator: 7.916155815124512 LOSS_Discriminator: 0.06419393420219421\n",
            "ITERATION_NO.: 27 LOSS_Generator: 8.16459846496582 LOSS_Discriminator: 0.03920559957623482\n",
            "ITERATION_NO.: 28 LOSS_Generator: 7.832510948181152 LOSS_Discriminator: 0.07398896664381027\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.453374862670898 LOSS_Discriminator: 0.07420556247234344\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.12026834487915 LOSS_Discriminator: 0.1094454750418663\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.8124566078186035 LOSS_Discriminator: 0.07333169877529144\n",
            "ITERATION_NO.: 32 LOSS_Generator: 7.861487865447998 LOSS_Discriminator: 0.056836456060409546\n",
            "ITERATION_NO.: 33 LOSS_Generator: 8.378527641296387 LOSS_Discriminator: 0.03128910809755325\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.27958869934082 LOSS_Discriminator: 0.09963192045688629\n",
            "ITERATION_NO.: 35 LOSS_Generator: 7.997216701507568 LOSS_Discriminator: 0.12085245549678802\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.635984420776367 LOSS_Discriminator: 0.05616874247789383\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.6082329750061035 LOSS_Discriminator: 0.05623506009578705\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.625764846801758 LOSS_Discriminator: 0.09053157269954681\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.9887375831604 LOSS_Discriminator: 0.08872540295124054\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.5580925941467285 LOSS_Discriminator: 0.04235886037349701\n",
            "ITERATION_NO.: 41 LOSS_Generator: 8.408283233642578 LOSS_Discriminator: 0.029551224783062935\n",
            "ITERATION_NO.: 42 LOSS_Generator: 8.440397262573242 LOSS_Discriminator: 0.027689866721630096\n",
            "ITERATION_NO.: 43 LOSS_Generator: 8.511070251464844 LOSS_Discriminator: 0.14938613772392273\n",
            "ITERATION_NO.: 44 LOSS_Generator: 7.733574867248535 LOSS_Discriminator: 0.019698496907949448\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.667230129241943 LOSS_Discriminator: 0.019908059388399124\n",
            "ITERATION_NO.: 46 LOSS_Generator: 6.688372611999512 LOSS_Discriminator: 0.06250826269388199\n",
            "ITERATION_NO.: 47 LOSS_Generator: 6.463426113128662 LOSS_Discriminator: 0.04276838153600693\n",
            "ITERATION_NO.: 48 LOSS_Generator: 6.879030704498291 LOSS_Discriminator: 0.05954768881201744\n",
            "ITERATION_NO.: 49 LOSS_Generator: 6.947068691253662 LOSS_Discriminator: 0.07291199266910553\n",
            "ITERATION_NO.: 50 LOSS_Generator: 7.578265190124512 LOSS_Discriminator: 0.030427053570747375\n",
            "ITERATION_NO.: 51 LOSS_Generator: 7.213398456573486 LOSS_Discriminator: 0.04830038920044899\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.216041088104248 LOSS_Discriminator: 0.09437744319438934\n",
            "ITERATION_NO.: 53 LOSS_Generator: 6.871334075927734 LOSS_Discriminator: 0.04685543477535248\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.620237827301025 LOSS_Discriminator: 0.08476100862026215\n",
            "ITERATION_NO.: 55 LOSS_Generator: 6.558417797088623 LOSS_Discriminator: 0.056069061160087585\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.644938945770264 LOSS_Discriminator: 0.06384356319904327\n",
            "ITERATION_NO.: 57 LOSS_Generator: 7.947054862976074 LOSS_Discriminator: 0.06762444972991943\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.867396354675293 LOSS_Discriminator: 0.030246831476688385\n",
            "ITERATION_NO.: 59 LOSS_Generator: 7.977175712585449 LOSS_Discriminator: 0.05802594870328903\n",
            "ITERATION_NO.: 60 LOSS_Generator: 7.999481201171875 LOSS_Discriminator: 0.0865093246102333\n",
            "ITERATION_NO.: 61 LOSS_Generator: 8.318642616271973 LOSS_Discriminator: 0.0801803395152092\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.96926736831665 LOSS_Discriminator: 0.11632825434207916\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.594754695892334 LOSS_Discriminator: 0.04590313136577606\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.859756946563721 LOSS_Discriminator: 0.10569047927856445\n",
            "ITERATION_NO.: 65 LOSS_Generator: 7.599092483520508 LOSS_Discriminator: 0.04204753786325455\n",
            "ITERATION_NO.: 66 LOSS_Generator: 7.534458160400391 LOSS_Discriminator: 0.05021267384290695\n",
            "ITERATION_NO.: 67 LOSS_Generator: 8.164366722106934 LOSS_Discriminator: 0.07174094021320343\n",
            "ITERATION_NO.: 68 LOSS_Generator: 8.142788887023926 LOSS_Discriminator: 0.06030073016881943\n",
            "ITERATION_NO.: 69 LOSS_Generator: 7.852207183837891 LOSS_Discriminator: 0.0446314737200737\n",
            "ITERATION_NO.: 70 LOSS_Generator: 7.694201469421387 LOSS_Discriminator: 0.041981473565101624\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.677100658416748 LOSS_Discriminator: 0.15019959211349487\n",
            "ITERATION_NO.: 72 LOSS_Generator: 7.687562465667725 LOSS_Discriminator: 0.07356768846511841\n",
            "ITERATION_NO.: 73 LOSS_Generator: 7.440539360046387 LOSS_Discriminator: 0.07623063027858734\n",
            "ITERATION_NO.: 74 LOSS_Generator: 6.179177284240723 LOSS_Discriminator: 0.12776681780815125\n",
            "ITERATION_NO.: 75 LOSS_Generator: 7.002141952514648 LOSS_Discriminator: 0.06945003569126129\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.213191509246826 LOSS_Discriminator: 0.06723760813474655\n",
            "ITERATION_NO.: 77 LOSS_Generator: 8.388835906982422 LOSS_Discriminator: 0.08410312980413437\n",
            "ITERATION_NO.: 78 LOSS_Generator: 8.944878578186035 LOSS_Discriminator: 0.08033934980630875\n",
            "ITERATION_NO.: 79 LOSS_Generator: 9.489676475524902 LOSS_Discriminator: 0.06736491620540619\n",
            "ITERATION_NO.: 80 LOSS_Generator: 8.297933578491211 LOSS_Discriminator: 0.12091328203678131\n",
            "ITERATION_NO.: 81 LOSS_Generator: 7.551806449890137 LOSS_Discriminator: 0.08379169553518295\n",
            "ITERATION_NO.: 82 LOSS_Generator: 7.110414028167725 LOSS_Discriminator: 0.11300499737262726\n",
            "ITERATION_NO.: 83 LOSS_Generator: 6.804189682006836 LOSS_Discriminator: 0.12161844968795776\n",
            "ITERATION_NO.: 84 LOSS_Generator: 7.254266262054443 LOSS_Discriminator: 0.13537244498729706\n",
            "ITERATION_NO.: 85 LOSS_Generator: 8.585328102111816 LOSS_Discriminator: 0.058016855269670486\n",
            "ITERATION_NO.: 86 LOSS_Generator: 8.365631103515625 LOSS_Discriminator: 0.07081564515829086\n",
            "ITERATION_NO.: 87 LOSS_Generator: 8.352657318115234 LOSS_Discriminator: 0.0789126604795456\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.408680438995361 LOSS_Discriminator: 0.013321955688297749\n",
            "ITERATION_NO.: 89 LOSS_Generator: 6.982308387756348 LOSS_Discriminator: 0.05608068406581879\n",
            "ITERATION_NO.: 90 LOSS_Generator: 6.827610492706299 LOSS_Discriminator: 0.05683903396129608\n",
            "ITERATION_NO.: 91 LOSS_Generator: 6.16840934753418 LOSS_Discriminator: 0.05055442079901695\n",
            "ITERATION_NO.: 92 LOSS_Generator: 7.470308303833008 LOSS_Discriminator: 0.081708624958992\n",
            "ITERATION_NO.: 93 LOSS_Generator: 8.622018814086914 LOSS_Discriminator: 0.06985838711261749\n",
            "ITERATION_NO.: 94 LOSS_Generator: 8.372979164123535 LOSS_Discriminator: 0.0872432217001915\n",
            "ITERATION_NO.: 95 LOSS_Generator: 8.228370666503906 LOSS_Discriminator: 0.0932033583521843\n",
            "ITERATION_NO.: 96 LOSS_Generator: 7.277975559234619 LOSS_Discriminator: 0.07761986553668976\n",
            "ITERATION_NO.: 97 LOSS_Generator: 5.501680374145508 LOSS_Discriminator: 0.038828615099191666\n",
            "ITERATION_NO.: 98 LOSS_Generator: 6.198867321014404 LOSS_Discriminator: 0.13600099086761475\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.042168617248535 LOSS_Discriminator: 0.0628729984164238\n",
            "ITERATION_NO.: 100 LOSS_Generator: 8.2294282913208 LOSS_Discriminator: 0.03134675323963165\n",
            "ITERATION_NO.: 101 LOSS_Generator: 8.566707611083984 LOSS_Discriminator: 0.12861251831054688\n",
            "ITERATION_NO.: 102 LOSS_Generator: 8.101966857910156 LOSS_Discriminator: 0.08426909148693085\n",
            "ITERATION_NO.: 103 LOSS_Generator: 8.06981372833252 LOSS_Discriminator: 0.028976529836654663\n",
            "ITERATION_NO.: 104 LOSS_Generator: 7.535403251647949 LOSS_Discriminator: 0.04366917908191681\n",
            "ITERATION_NO.: 105 LOSS_Generator: 7.0739336013793945 LOSS_Discriminator: 0.06610305607318878\n",
            "ITERATION_NO.: 106 LOSS_Generator: 6.661288261413574 LOSS_Discriminator: 0.036407895386219025\n",
            "ITERATION_NO.: 107 LOSS_Generator: 7.423579692840576 LOSS_Discriminator: 0.03183882310986519\n",
            "ITERATION_NO.: 108 LOSS_Generator: 7.012930393218994 LOSS_Discriminator: 0.04224223643541336\n",
            "ITERATION_NO.: 109 LOSS_Generator: 7.578107833862305 LOSS_Discriminator: 0.0496685691177845\n",
            "ITERATION_NO.: 110 LOSS_Generator: 8.005227088928223 LOSS_Discriminator: 0.07468865811824799\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.752742767333984 LOSS_Discriminator: 0.05159979313611984\n",
            "ITERATION_NO.: 112 LOSS_Generator: 7.904453754425049 LOSS_Discriminator: 0.06204662472009659\n",
            "ITERATION_NO.: 113 LOSS_Generator: 6.968734264373779 LOSS_Discriminator: 0.13501648604869843\n",
            "ITERATION_NO.: 114 LOSS_Generator: 6.2473883628845215 LOSS_Discriminator: 0.05151302367448807\n",
            "ITERATION_NO.: 115 LOSS_Generator: 5.920820713043213 LOSS_Discriminator: 0.07624223828315735\n",
            "ITERATION_NO.: 116 LOSS_Generator: 6.923611640930176 LOSS_Discriminator: 0.053872428834438324\n",
            "ITERATION_NO.: 117 LOSS_Generator: 7.539638042449951 LOSS_Discriminator: 0.03069278597831726\n",
            "ITERATION_NO.: 118 LOSS_Generator: 8.131012916564941 LOSS_Discriminator: 0.0824851393699646\n",
            "ITERATION_NO.: 119 LOSS_Generator: 8.733481407165527 LOSS_Discriminator: 0.02438734844326973\n",
            "ITERATION_NO.: 120 LOSS_Generator: 8.677242279052734 LOSS_Discriminator: 0.06932847201824188\n",
            "ITERATION_NO.: 121 LOSS_Generator: 8.145605087280273 LOSS_Discriminator: 0.0697089210152626\n",
            "ITERATION_NO.: 122 LOSS_Generator: 6.944149971008301 LOSS_Discriminator: 0.13908123970031738\n",
            "ITERATION_NO.: 123 LOSS_Generator: 5.698256969451904 LOSS_Discriminator: 0.06741558015346527\n",
            "ITERATION_NO.: 124 LOSS_Generator: 6.0695576667785645 LOSS_Discriminator: 0.11323288828134537\n",
            "ITERATION_NO.: 125 LOSS_Generator: 7.229336738586426 LOSS_Discriminator: 0.09271982312202454\n",
            "ITERATION_NO.: 126 LOSS_Generator: 8.447948455810547 LOSS_Discriminator: 0.09224911779165268\n",
            "ITERATION_NO.: 127 LOSS_Generator: 8.58230972290039 LOSS_Discriminator: 0.08697661757469177\n",
            "ITERATION_NO.: 128 LOSS_Generator: 8.524511337280273 LOSS_Discriminator: 0.12195703387260437\n",
            "ITERATION_NO.: 129 LOSS_Generator: 8.08267593383789 LOSS_Discriminator: 0.07730173319578171\n",
            "ITERATION_NO.: 130 LOSS_Generator: 7.041749954223633 LOSS_Discriminator: 0.09471794962882996\n",
            "ITERATION_NO.: 131 LOSS_Generator: 6.106626033782959 LOSS_Discriminator: 0.040820326656103134\n",
            "ITERATION_NO.: 132 LOSS_Generator: 6.77663516998291 LOSS_Discriminator: 0.1287916898727417\n",
            "ITERATION_NO.: 133 LOSS_Generator: 7.025030136108398 LOSS_Discriminator: 0.06804080307483673\n",
            "ITERATION_NO.: 134 LOSS_Generator: 8.238097190856934 LOSS_Discriminator: 0.08512920141220093\n",
            "ITERATION_NO.: 135 LOSS_Generator: 8.026851654052734 LOSS_Discriminator: 0.04199981689453125\n",
            "ITERATION_NO.: 136 LOSS_Generator: 7.670094013214111 LOSS_Discriminator: 0.13224811851978302\n",
            "ITERATION_NO.: 137 LOSS_Generator: 6.73197078704834 LOSS_Discriminator: 0.06207120418548584\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.7548675537109375 LOSS_Discriminator: 0.1398153007030487\n",
            "ITERATION_NO.: 139 LOSS_Generator: 7.416931629180908 LOSS_Discriminator: 0.06844943016767502\n",
            "ITERATION_NO.: 140 LOSS_Generator: 8.151562690734863 LOSS_Discriminator: 0.04303833097219467\n",
            "ITERATION_NO.: 141 LOSS_Generator: 8.719156265258789 LOSS_Discriminator: 0.06775058805942535\n",
            "ITERATION_NO.: 142 LOSS_Generator: 8.429579734802246 LOSS_Discriminator: 0.09293518960475922\n",
            "ITERATION_NO.: 143 LOSS_Generator: 7.96226692199707 LOSS_Discriminator: 0.030455388128757477\n",
            "ITERATION_NO.: 144 LOSS_Generator: 6.867700576782227 LOSS_Discriminator: 0.05703195556998253\n",
            "ITERATION_NO.: 145 LOSS_Generator: 6.250729560852051 LOSS_Discriminator: 0.06773815304040909\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.771773815155029 LOSS_Discriminator: 0.0905950516462326\n",
            "ITERATION_NO.: 147 LOSS_Generator: 7.643512725830078 LOSS_Discriminator: 0.06622699648141861\n",
            "ITERATION_NO.: 148 LOSS_Generator: 8.165875434875488 LOSS_Discriminator: 0.026023782789707184\n",
            "ITERATION_NO.: 149 LOSS_Generator: 8.237345695495605 LOSS_Discriminator: 0.13859862089157104\n",
            "ITERATION_NO.: 150 LOSS_Generator: 7.4113616943359375 LOSS_Discriminator: 0.0886690616607666\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.486398696899414 LOSS_Discriminator: 0.029288066551089287\n",
            "ITERATION_NO.: 152 LOSS_Generator: 6.0040998458862305 LOSS_Discriminator: 0.08736874163150787\n",
            "ITERATION_NO.: 153 LOSS_Generator: 6.129368782043457 LOSS_Discriminator: 0.06986318528652191\n",
            "ITERATION_NO.: 154 LOSS_Generator: 6.741342067718506 LOSS_Discriminator: 0.033184148371219635\n",
            "ITERATION_NO.: 155 LOSS_Generator: 8.331351280212402 LOSS_Discriminator: 0.08356044441461563\n",
            "ITERATION_NO.: 156 LOSS_Generator: 8.656582832336426 LOSS_Discriminator: 0.054871030151844025\n",
            "ITERATION_NO.: 157 LOSS_Generator: 8.876112937927246 LOSS_Discriminator: 0.08176550269126892\n",
            "ITERATION_NO.: 158 LOSS_Generator: 7.790534019470215 LOSS_Discriminator: 0.07740595936775208\n",
            "ITERATION_NO.: 159 LOSS_Generator: 7.680351734161377 LOSS_Discriminator: 0.05221432447433472\n",
            "ITERATION_NO.: 160 LOSS_Generator: 6.702120304107666 LOSS_Discriminator: 0.13155002892017365\n",
            "ITERATION_NO.: 161 LOSS_Generator: 5.5902419090271 LOSS_Discriminator: 0.07992172986268997\n",
            "ITERATION_NO.: 162 LOSS_Generator: 6.869646072387695 LOSS_Discriminator: 0.1267235428094864\n",
            "ITERATION_NO.: 163 LOSS_Generator: 7.642542839050293 LOSS_Discriminator: 0.04835261404514313\n",
            "ITERATION_NO.: 164 LOSS_Generator: 8.857640266418457 LOSS_Discriminator: 0.04188945144414902\n",
            "ITERATION_NO.: 165 LOSS_Generator: 8.410799980163574 LOSS_Discriminator: 0.09458175301551819\n",
            "ITERATION_NO.: 166 LOSS_Generator: 7.620184421539307 LOSS_Discriminator: 0.0760011151432991\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.429312229156494 LOSS_Discriminator: 0.09917742013931274\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.7622575759887695 LOSS_Discriminator: 0.03930787742137909\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.948186874389648 LOSS_Discriminator: 0.10434004664421082\n",
            "ITERATION_NO.: 170 LOSS_Generator: 7.225531101226807 LOSS_Discriminator: 0.05086555331945419\n",
            "ITERATION_NO.: 171 LOSS_Generator: 7.637486457824707 LOSS_Discriminator: 0.036723777651786804\n",
            "ITERATION_NO.: 172 LOSS_Generator: 8.343413352966309 LOSS_Discriminator: 0.03149699792265892\n",
            "ITERATION_NO.: 173 LOSS_Generator: 8.452949523925781 LOSS_Discriminator: 0.043512050062417984\n",
            "ITERATION_NO.: 174 LOSS_Generator: 8.632984161376953 LOSS_Discriminator: 0.04829568788409233\n",
            "ITERATION_NO.: 175 LOSS_Generator: 7.6973395347595215 LOSS_Discriminator: 0.09262923151254654\n",
            "ITERATION_NO.: 176 LOSS_Generator: 7.2677459716796875 LOSS_Discriminator: 0.07188748568296432\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.489043712615967 LOSS_Discriminator: 0.04368412122130394\n",
            "ITERATION_NO.: 178 LOSS_Generator: 7.176154136657715 LOSS_Discriminator: 0.04022837057709694\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.1366987228393555 LOSS_Discriminator: 0.07929104566574097\n",
            "ITERATION_NO.: 180 LOSS_Generator: 7.942108631134033 LOSS_Discriminator: 0.06480010598897934\n",
            "ITERATION_NO.: 181 LOSS_Generator: 7.317744255065918 LOSS_Discriminator: 0.07228484749794006\n",
            "ITERATION_NO.: 182 LOSS_Generator: 7.224868297576904 LOSS_Discriminator: 0.11904454976320267\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.487964630126953 LOSS_Discriminator: 0.05275967717170715\n",
            "ITERATION_NO.: 184 LOSS_Generator: 7.5571208000183105 LOSS_Discriminator: 0.03506908193230629\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.264252185821533 LOSS_Discriminator: 0.08799746632575989\n",
            "ITERATION_NO.: 186 LOSS_Generator: 7.671628475189209 LOSS_Discriminator: 0.16299526393413544\n",
            "ITERATION_NO.: 187 LOSS_Generator: 7.324582576751709 LOSS_Discriminator: 0.03872157633304596\n",
            "ITERATION_NO.: 188 LOSS_Generator: 7.242844104766846 LOSS_Discriminator: 0.08011592924594879\n",
            "ITERATION_NO.: 189 LOSS_Generator: 7.407963752746582 LOSS_Discriminator: 0.04826778545975685\n",
            "ITERATION_NO.: 190 LOSS_Generator: 7.789182186126709 LOSS_Discriminator: 0.0634109228849411\n",
            "ITERATION_NO.: 191 LOSS_Generator: 7.224858283996582 LOSS_Discriminator: 0.07780367136001587\n",
            "ITERATION_NO.: 192 LOSS_Generator: 7.368326187133789 LOSS_Discriminator: 0.08279696106910706\n",
            "ITERATION_NO.: 193 LOSS_Generator: 7.993231296539307 LOSS_Discriminator: 0.07813440263271332\n",
            "ITERATION_NO.: 194 LOSS_Generator: 7.0463972091674805 LOSS_Discriminator: 0.09968474507331848\n",
            "ITERATION_NO.: 195 LOSS_Generator: 7.165024280548096 LOSS_Discriminator: 0.038116153329610825\n",
            "ITERATION_NO.: 196 LOSS_Generator: 7.849675178527832 LOSS_Discriminator: 0.037899915128946304\n",
            "ITERATION_NO.: 197 LOSS_Generator: 7.4689412117004395 LOSS_Discriminator: 0.06663808226585388\n",
            "ITERATION_NO.: 198 LOSS_Generator: 7.58978271484375 LOSS_Discriminator: 0.03676067292690277\n",
            "ITERATION_NO.: 199 LOSS_Generator: 7.304712295532227 LOSS_Discriminator: 0.10251319408416748\n",
            "ITERATION_NO.: 200 LOSS_Generator: 7.074885368347168 LOSS_Discriminator: 0.035657256841659546\n",
            "ITERATION_NO.: 201 LOSS_Generator: 6.6858320236206055 LOSS_Discriminator: 0.05488000810146332\n",
            "ITERATION_NO.: 202 LOSS_Generator: 5.972151279449463 LOSS_Discriminator: 0.08444507420063019\n",
            "ITERATION_NO.: 203 LOSS_Generator: 6.385678291320801 LOSS_Discriminator: 0.056312814354896545\n",
            "ITERATION_NO.: 204 LOSS_Generator: 6.6306986808776855 LOSS_Discriminator: 0.04915549233555794\n",
            "ITERATION_NO.: 205 LOSS_Generator: 6.876071929931641 LOSS_Discriminator: 0.05056124925613403\n",
            "ITERATION_NO.: 206 LOSS_Generator: 7.356815338134766 LOSS_Discriminator: 0.05483730882406235\n",
            "ITERATION_NO.: 207 LOSS_Generator: 7.220582962036133 LOSS_Discriminator: 0.042449578642845154\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.4113264083862305 LOSS_Discriminator: 0.050785087049007416\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.23676872253418 LOSS_Discriminator: 0.08317762613296509\n",
            "ITERATION_NO.: 210 LOSS_Generator: 7.0262532234191895 LOSS_Discriminator: 0.08469285815954208\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.222349166870117 LOSS_Discriminator: 0.06408295035362244\n",
            "ITERATION_NO.: 212 LOSS_Generator: 6.8313446044921875 LOSS_Discriminator: 0.027461620047688484\n",
            "ITERATION_NO.: 213 LOSS_Generator: 7.273245811462402 LOSS_Discriminator: 0.0538126677274704\n",
            "ITERATION_NO.: 214 LOSS_Generator: 7.362705707550049 LOSS_Discriminator: 0.04321518540382385\n",
            "ITERATION_NO.: 215 LOSS_Generator: 7.17057991027832 LOSS_Discriminator: 0.09639661014080048\n",
            "ITERATION_NO.: 216 LOSS_Generator: 6.754133224487305 LOSS_Discriminator: 0.0764194130897522\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.78095817565918 LOSS_Discriminator: 0.09550420939922333\n",
            "ITERATION_NO.: 218 LOSS_Generator: 7.22728157043457 LOSS_Discriminator: 0.12462417781352997\n",
            "ITERATION_NO.: 219 LOSS_Generator: 7.504662990570068 LOSS_Discriminator: 0.03396821767091751\n",
            "ITERATION_NO.: 220 LOSS_Generator: 7.148820400238037 LOSS_Discriminator: 0.11654745042324066\n",
            "ITERATION_NO.: 221 LOSS_Generator: 6.915864944458008 LOSS_Discriminator: 0.06926749646663666\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.424262523651123 LOSS_Discriminator: 0.06128814443945885\n",
            "ITERATION_NO.: 223 LOSS_Generator: 7.168926239013672 LOSS_Discriminator: 0.06155194342136383\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.6280670166015625 LOSS_Discriminator: 0.05445684492588043\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.644288539886475 LOSS_Discriminator: 0.1407943218946457\n",
            "ITERATION_NO.: 226 LOSS_Generator: 7.693907260894775 LOSS_Discriminator: 0.051625847816467285\n",
            "ITERATION_NO.: 227 LOSS_Generator: 7.731804847717285 LOSS_Discriminator: 0.05749567225575447\n",
            "ITERATION_NO.: 228 LOSS_Generator: 7.149595260620117 LOSS_Discriminator: 0.12442284822463989\n",
            "ITERATION_NO.: 229 LOSS_Generator: 7.166762828826904 LOSS_Discriminator: 0.08822176605463028\n",
            "ITERATION_NO.: 230 LOSS_Generator: 6.1259894371032715 LOSS_Discriminator: 0.14792785048484802\n",
            "ITERATION_NO.: 231 LOSS_Generator: 5.529686450958252 LOSS_Discriminator: 0.12919704616069794\n",
            "ITERATION_NO.: 232 LOSS_Generator: 6.163816452026367 LOSS_Discriminator: 0.10317640006542206\n",
            "ITERATION_NO.: 233 LOSS_Generator: 7.1209716796875 LOSS_Discriminator: 0.1115153431892395\n",
            "ITERATION_NO.: 234 LOSS_Generator: 7.680951118469238 LOSS_Discriminator: 0.06402496993541718\n",
            "ITERATION_NO.: 235 LOSS_Generator: 8.527799606323242 LOSS_Discriminator: 0.09100542962551117\n",
            "ITERATION_NO.: 236 LOSS_Generator: 7.115570545196533 LOSS_Discriminator: 0.0980035811662674\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.078685760498047 LOSS_Discriminator: 0.06448161602020264\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.967466354370117 LOSS_Discriminator: 0.06727008521556854\n",
            "ITERATION_NO.: 239 LOSS_Generator: 6.170405387878418 LOSS_Discriminator: 0.05698448419570923\n",
            "ITERATION_NO.: 240 LOSS_Generator: 6.967123508453369 LOSS_Discriminator: 0.06654015928506851\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.8088579177856445 LOSS_Discriminator: 0.05792776122689247\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.859576225280762 LOSS_Discriminator: 0.02293451502919197\n",
            "ITERATION_NO.: 243 LOSS_Generator: 7.658393383026123 LOSS_Discriminator: 0.043553583323955536\n",
            "ITERATION_NO.: 244 LOSS_Generator: 8.065201759338379 LOSS_Discriminator: 0.12133470922708511\n",
            "ITERATION_NO.: 245 LOSS_Generator: 8.537047386169434 LOSS_Discriminator: 0.025205671787261963\n",
            "ITERATION_NO.: 246 LOSS_Generator: 8.117040634155273 LOSS_Discriminator: 0.08048416674137115\n",
            "ITERATION_NO.: 247 LOSS_Generator: 7.251091480255127 LOSS_Discriminator: 0.05626417323946953\n",
            "ITERATION_NO.: 248 LOSS_Generator: 7.364509105682373 LOSS_Discriminator: 0.05083125829696655\n",
            "ITERATION_NO.: 249 LOSS_Generator: 7.6847758293151855 LOSS_Discriminator: 0.035882920026779175\n",
            "ITERATION_NO.: 250 LOSS_Generator: 8.000884056091309 LOSS_Discriminator: 0.023827720433473587\n",
            "ITERATION_NO.: 251 LOSS_Generator: 7.3794660568237305 LOSS_Discriminator: 0.0901995599269867\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.041894435882568 LOSS_Discriminator: 0.023482292890548706\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.815200328826904 LOSS_Discriminator: 0.02117745205760002\n",
            "ITERATION_NO.: 254 LOSS_Generator: 7.040640354156494 LOSS_Discriminator: 0.030629858374595642\n",
            "ITERATION_NO.: 255 LOSS_Generator: 7.283348560333252 LOSS_Discriminator: 0.08036570250988007\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.675397872924805 LOSS_Discriminator: 0.04462432861328125\n",
            "ITERATION_NO.: 257 LOSS_Generator: 6.935502529144287 LOSS_Discriminator: 0.10707317292690277\n",
            "ITERATION_NO.: 258 LOSS_Generator: 6.311892032623291 LOSS_Discriminator: 0.04834914579987526\n",
            "ITERATION_NO.: 259 LOSS_Generator: 6.433363437652588 LOSS_Discriminator: 0.07549606263637543\n",
            "ITERATION_NO.: 260 LOSS_Generator: 6.482043266296387 LOSS_Discriminator: 0.06043224036693573\n",
            "ITERATION_NO.: 261 LOSS_Generator: 8.130888938903809 LOSS_Discriminator: 0.07648323476314545\n",
            "ITERATION_NO.: 262 LOSS_Generator: 8.154621124267578 LOSS_Discriminator: 0.07453842461109161\n",
            "ITERATION_NO.: 263 LOSS_Generator: 7.810372829437256 LOSS_Discriminator: 0.05182190239429474\n",
            "ITERATION_NO.: 264 LOSS_Generator: 7.77664852142334 LOSS_Discriminator: 0.10791928321123123\n",
            "ITERATION_NO.: 265 LOSS_Generator: 6.973504066467285 LOSS_Discriminator: 0.050498634576797485\n",
            "ITERATION_NO.: 266 LOSS_Generator: 6.667306900024414 LOSS_Discriminator: 0.04863111674785614\n",
            "ITERATION_NO.: 267 LOSS_Generator: 6.913436412811279 LOSS_Discriminator: 0.06070684641599655\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.059924125671387 LOSS_Discriminator: 0.02936471439898014\n",
            "ITERATION_NO.: 269 LOSS_Generator: 7.018912315368652 LOSS_Discriminator: 0.05216776952147484\n",
            "ITERATION_NO.: 270 LOSS_Generator: 6.89175271987915 LOSS_Discriminator: 0.09471674263477325\n",
            "ITERATION_NO.: 271 LOSS_Generator: 6.698952674865723 LOSS_Discriminator: 0.06952088326215744\n",
            "ITERATION_NO.: 272 LOSS_Generator: 6.536909580230713 LOSS_Discriminator: 0.08606910705566406\n",
            "ITERATION_NO.: 273 LOSS_Generator: 6.9636640548706055 LOSS_Discriminator: 0.06285960972309113\n",
            "ITERATION_NO.: 274 LOSS_Generator: 7.123884201049805 LOSS_Discriminator: 0.046636056154966354\n",
            "ITERATION_NO.: 275 LOSS_Generator: 7.705044746398926 LOSS_Discriminator: 0.08289479464292526\n",
            "ITERATION_NO.: 276 LOSS_Generator: 6.759820461273193 LOSS_Discriminator: 0.12320663779973984\n",
            "ITERATION_NO.: 277 LOSS_Generator: 6.8307929039001465 LOSS_Discriminator: 0.054183147847652435\n",
            "ITERATION_NO.: 278 LOSS_Generator: 7.102113723754883 LOSS_Discriminator: 0.0768633782863617\n",
            "ITERATION_NO.: 279 LOSS_Generator: 7.141803741455078 LOSS_Discriminator: 0.04197481647133827\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.621664524078369 LOSS_Discriminator: 0.03441379964351654\n",
            "ITERATION_NO.: 281 LOSS_Generator: 7.865767955780029 LOSS_Discriminator: 0.040356263518333435\n",
            "ITERATION_NO.: 282 LOSS_Generator: 7.295137882232666 LOSS_Discriminator: 0.045867957174777985\n",
            "ITERATION_NO.: 283 LOSS_Generator: 7.475961685180664 LOSS_Discriminator: 0.0477493554353714\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.158839702606201 LOSS_Discriminator: 0.023948371410369873\n",
            "ITERATION_NO.: 285 LOSS_Generator: 6.986306190490723 LOSS_Discriminator: 0.08199810981750488\n",
            "ITERATION_NO.: 286 LOSS_Generator: 6.494166851043701 LOSS_Discriminator: 0.05255744606256485\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.999551296234131 LOSS_Discriminator: 0.032713938504457474\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.242309093475342 LOSS_Discriminator: 0.058451950550079346\n",
            "ITERATION_NO.: 289 LOSS_Generator: 7.713763236999512 LOSS_Discriminator: 0.06175577640533447\n",
            "ITERATION_NO.: 290 LOSS_Generator: 7.679889678955078 LOSS_Discriminator: 0.08411066234111786\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.302463531494141 LOSS_Discriminator: 0.06844330579042435\n",
            "ITERATION_NO.: 292 LOSS_Generator: 6.766744613647461 LOSS_Discriminator: 0.06945623457431793\n",
            "ITERATION_NO.: 293 LOSS_Generator: 6.255547523498535 LOSS_Discriminator: 0.1137273982167244\n",
            "ITERATION_NO.: 294 LOSS_Generator: 6.988250732421875 LOSS_Discriminator: 0.15818706154823303\n",
            "ITERATION_NO.: 295 LOSS_Generator: 7.814251899719238 LOSS_Discriminator: 0.028287608176469803\n",
            "ITERATION_NO.: 296 LOSS_Generator: 8.206811904907227 LOSS_Discriminator: 0.012601877562701702\n",
            "ITERATION_NO.: 297 LOSS_Generator: 8.993034362792969 LOSS_Discriminator: 0.061522476375103\n",
            "ITERATION_NO.: 298 LOSS_Generator: 8.034610748291016 LOSS_Discriminator: 0.09311982989311218\n",
            "ITERATION_NO.: 299 LOSS_Generator: 7.382685661315918 LOSS_Discriminator: 0.05234174430370331\n",
            "ITERATION_NO.: 300 LOSS_Generator: 6.459583282470703 LOSS_Discriminator: 0.10013079643249512\n",
            "ITERATION_NO.: 301 LOSS_Generator: 6.487306118011475 LOSS_Discriminator: 0.08352065086364746\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.5535888671875 LOSS_Discriminator: 0.06094709783792496\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.871946811676025 LOSS_Discriminator: 0.07456164062023163\n",
            "ITERATION_NO.: 304 LOSS_Generator: 7.549994945526123 LOSS_Discriminator: 0.036750853061676025\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.2240800857543945 LOSS_Discriminator: 0.04876777529716492\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.576712131500244 LOSS_Discriminator: 0.07630661129951477\n",
            "ITERATION_NO.: 307 LOSS_Generator: 6.750371932983398 LOSS_Discriminator: 0.12107503414154053\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.034392833709717 LOSS_Discriminator: 0.05442417412996292\n",
            "ITERATION_NO.: 309 LOSS_Generator: 5.871912956237793 LOSS_Discriminator: 0.08313929289579391\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.50070333480835 LOSS_Discriminator: 0.03210778534412384\n",
            "ITERATION_NO.: 311 LOSS_Generator: 7.955102920532227 LOSS_Discriminator: 0.05923756957054138\n",
            "ITERATION_NO.: 312 LOSS_Generator: 7.597599029541016 LOSS_Discriminator: 0.07386799156665802\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.6829094886779785 LOSS_Discriminator: 0.037270061671733856\n",
            "ITERATION_NO.: 314 LOSS_Generator: 7.230470657348633 LOSS_Discriminator: 0.044566810131073\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.604590892791748 LOSS_Discriminator: 0.033505961298942566\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.38808536529541 LOSS_Discriminator: 0.08190131187438965\n",
            "ITERATION_NO.: 317 LOSS_Generator: 6.527524471282959 LOSS_Discriminator: 0.05800269916653633\n",
            "ITERATION_NO.: 318 LOSS_Generator: 7.7671074867248535 LOSS_Discriminator: 0.09221861511468887\n",
            "ITERATION_NO.: 319 LOSS_Generator: 7.928743362426758 LOSS_Discriminator: 0.03618382662534714\n",
            "ITERATION_NO.: 320 LOSS_Generator: 9.48411750793457 LOSS_Discriminator: 0.040585391223430634\n",
            "ITERATION_NO.: 321 LOSS_Generator: 9.131928443908691 LOSS_Discriminator: 0.11063233762979507\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.199303150177002 LOSS_Discriminator: 0.04648899286985397\n",
            "ITERATION_NO.: 323 LOSS_Generator: 6.390007972717285 LOSS_Discriminator: 0.07999466359615326\n",
            "ITERATION_NO.: 324 LOSS_Generator: 6.146516799926758 LOSS_Discriminator: 0.13952170312404633\n",
            "ITERATION_NO.: 325 LOSS_Generator: 6.648294448852539 LOSS_Discriminator: 0.08209718763828278\n",
            "ITERATION_NO.: 326 LOSS_Generator: 7.677274703979492 LOSS_Discriminator: 0.05963548272848129\n",
            "ITERATION_NO.: 327 LOSS_Generator: 8.565592765808105 LOSS_Discriminator: 0.035798873752355576\n",
            "ITERATION_NO.: 328 LOSS_Generator: 8.195968627929688 LOSS_Discriminator: 0.09611733257770538\n",
            "ITERATION_NO.: 329 LOSS_Generator: 8.059993743896484 LOSS_Discriminator: 0.026723507791757584\n",
            "ITERATION_NO.: 330 LOSS_Generator: 7.557782173156738 LOSS_Discriminator: 0.07158362865447998\n",
            "ITERATION_NO.: 331 LOSS_Generator: 6.557714939117432 LOSS_Discriminator: 0.03883133828639984\n",
            "ITERATION_NO.: 332 LOSS_Generator: 6.587084770202637 LOSS_Discriminator: 0.049192942678928375\n",
            "ITERATION_NO.: 333 LOSS_Generator: 7.07301139831543 LOSS_Discriminator: 0.11833224445581436\n",
            "ITERATION_NO.: 334 LOSS_Generator: 8.219446182250977 LOSS_Discriminator: 0.07284632325172424\n",
            "ITERATION_NO.: 335 LOSS_Generator: 8.509159088134766 LOSS_Discriminator: 0.052698664367198944\n",
            "ITERATION_NO.: 336 LOSS_Generator: 7.907656669616699 LOSS_Discriminator: 0.03234076499938965\n",
            "ITERATION_NO.: 337 LOSS_Generator: 7.068365573883057 LOSS_Discriminator: 0.06941400468349457\n",
            "ITERATION_NO.: 338 LOSS_Generator: 7.351044178009033 LOSS_Discriminator: 0.11575746536254883\n",
            "ITERATION_NO.: 339 LOSS_Generator: 7.4781107902526855 LOSS_Discriminator: 0.10293103009462357\n",
            "ITERATION_NO.: 340 LOSS_Generator: 8.098474502563477 LOSS_Discriminator: 0.06553319096565247\n",
            "ITERATION_NO.: 341 LOSS_Generator: 7.663245677947998 LOSS_Discriminator: 0.0389690026640892\n",
            "ITERATION_NO.: 342 LOSS_Generator: 8.355420112609863 LOSS_Discriminator: 0.028029438108205795\n",
            "ITERATION_NO.: 343 LOSS_Generator: 7.261460781097412 LOSS_Discriminator: 0.062486860901117325\n",
            "ITERATION_NO.: 344 LOSS_Generator: 7.657505035400391 LOSS_Discriminator: 0.055497586727142334\n",
            "ITERATION_NO.: 345 LOSS_Generator: 7.269561290740967 LOSS_Discriminator: 0.0522773377597332\n",
            "ITERATION_NO.: 346 LOSS_Generator: 7.897364616394043 LOSS_Discriminator: 0.10546034574508667\n",
            "ITERATION_NO.: 347 LOSS_Generator: 7.481176376342773 LOSS_Discriminator: 0.03710152581334114\n",
            "ITERATION_NO.: 348 LOSS_Generator: 7.393037796020508 LOSS_Discriminator: 0.04748096317052841\n",
            "ITERATION_NO.: 349 LOSS_Generator: 7.399392604827881 LOSS_Discriminator: 0.1155150979757309\n",
            "ITERATION_NO.: 350 LOSS_Generator: 6.9441118240356445 LOSS_Discriminator: 0.06994030624628067\n",
            "ITERATION_NO.: 351 LOSS_Generator: 7.506143093109131 LOSS_Discriminator: 0.0881514698266983\n",
            "ITERATION_NO.: 352 LOSS_Generator: 6.666487216949463 LOSS_Discriminator: 0.1429496556520462\n",
            "ITERATION_NO.: 353 LOSS_Generator: 6.668838024139404 LOSS_Discriminator: 0.07660982012748718\n",
            "ITERATION_NO.: 354 LOSS_Generator: 6.759352207183838 LOSS_Discriminator: 0.07354921102523804\n",
            "ITERATION_NO.: 355 LOSS_Generator: 7.338256359100342 LOSS_Discriminator: 0.06570093333721161\n",
            "ITERATION_NO.: 356 LOSS_Generator: 8.267934799194336 LOSS_Discriminator: 0.04751289635896683\n",
            "ITERATION_NO.: 357 LOSS_Generator: 8.428744316101074 LOSS_Discriminator: 0.09467698633670807\n",
            "ITERATION_NO.: 358 LOSS_Generator: 7.867131233215332 LOSS_Discriminator: 0.120306596159935\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.226482391357422 LOSS_Discriminator: 0.0806734561920166\n",
            "ITERATION_NO.: 360 LOSS_Generator: 6.041527271270752 LOSS_Discriminator: 0.09632982313632965\n",
            "ITERATION_NO.: 361 LOSS_Generator: 6.127884387969971 LOSS_Discriminator: 0.0978732705116272\n",
            "ITERATION_NO.: 362 LOSS_Generator: 6.981231689453125 LOSS_Discriminator: 0.08908587694168091\n",
            "ITERATION_NO.: 363 LOSS_Generator: 7.259201526641846 LOSS_Discriminator: 0.09935963153839111\n",
            "ITERATION_NO.: 364 LOSS_Generator: 7.597637176513672 LOSS_Discriminator: 0.08882585912942886\n",
            "ITERATION_NO.: 365 LOSS_Generator: 6.9204936027526855 LOSS_Discriminator: 0.06965666264295578\n",
            "ITERATION_NO.: 366 LOSS_Generator: 7.159171104431152 LOSS_Discriminator: 0.07345110177993774\n",
            "ITERATION_NO.: 367 LOSS_Generator: 7.095208644866943 LOSS_Discriminator: 0.09906545281410217\n",
            "ITERATION_NO.: 368 LOSS_Generator: 7.404760837554932 LOSS_Discriminator: 0.08215034008026123\n",
            "ITERATION_NO.: 369 LOSS_Generator: 7.743807792663574 LOSS_Discriminator: 0.07727682590484619\n",
            "ITERATION_NO.: 370 LOSS_Generator: 7.247501850128174 LOSS_Discriminator: 0.07333622872829437\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.460266590118408 LOSS_Discriminator: 0.030460957437753677\n",
            "ITERATION_NO.: 372 LOSS_Generator: 8.199143409729004 LOSS_Discriminator: 0.05838516354560852\n",
            "ITERATION_NO.: 373 LOSS_Generator: 7.237534046173096 LOSS_Discriminator: 0.04013653099536896\n",
            "ITERATION_NO.: 374 LOSS_Generator: 7.484220504760742 LOSS_Discriminator: 0.06554332375526428\n",
            "ITERATION_NO.: 375 LOSS_Generator: 8.547161102294922 LOSS_Discriminator: 0.13047613203525543\n",
            "ITERATION_NO.: 376 LOSS_Generator: 9.105249404907227 LOSS_Discriminator: 0.07526935636997223\n",
            "ITERATION_NO.: 377 LOSS_Generator: 8.55337905883789 LOSS_Discriminator: 0.034963302314281464\n",
            "ITERATION_NO.: 378 LOSS_Generator: 7.8369951248168945 LOSS_Discriminator: 0.14574123919010162\n",
            "ITERATION_NO.: 379 LOSS_Generator: 6.919582366943359 LOSS_Discriminator: 0.06790706515312195\n",
            "ITERATION_NO.: 380 LOSS_Generator: 6.849384307861328 LOSS_Discriminator: 0.06974463909864426\n",
            "ITERATION_NO.: 381 LOSS_Generator: 7.905518054962158 LOSS_Discriminator: 0.08901321887969971\n",
            "ITERATION_NO.: 382 LOSS_Generator: 8.415645599365234 LOSS_Discriminator: 0.04517723619937897\n",
            "ITERATION_NO.: 383 LOSS_Generator: 8.862571716308594 LOSS_Discriminator: 0.05306176096200943\n",
            "ITERATION_NO.: 384 LOSS_Generator: 8.111474990844727 LOSS_Discriminator: 0.0697396993637085\n",
            "ITERATION_NO.: 385 LOSS_Generator: 7.6145758628845215 LOSS_Discriminator: 0.06248953193426132\n",
            "ITERATION_NO.: 386 LOSS_Generator: 6.405323505401611 LOSS_Discriminator: 0.06820452213287354\n",
            "ITERATION_NO.: 387 LOSS_Generator: 6.021005153656006 LOSS_Discriminator: 0.06404093652963638\n",
            "ITERATION_NO.: 388 LOSS_Generator: 6.457867622375488 LOSS_Discriminator: 0.0501064658164978\n",
            "ITERATION_NO.: 389 LOSS_Generator: 7.354893207550049 LOSS_Discriminator: 0.07273393869400024\n",
            "ITERATION_NO.: 390 LOSS_Generator: 8.55843734741211 LOSS_Discriminator: 0.08872507512569427\n",
            "ITERATION_NO.: 391 LOSS_Generator: 9.315281867980957 LOSS_Discriminator: 0.03853010758757591\n",
            "ITERATION_NO.: 392 LOSS_Generator: 9.338492393493652 LOSS_Discriminator: 0.12277799844741821\n",
            "ITERATION_NO.: 393 LOSS_Generator: 9.004450798034668 LOSS_Discriminator: 0.06286017596721649\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.213549613952637 LOSS_Discriminator: 0.07586206495761871\n",
            "ITERATION_NO.: 395 LOSS_Generator: 6.8749680519104 LOSS_Discriminator: 0.052415892481803894\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.9854350090026855 LOSS_Discriminator: 0.09288914501667023\n",
            "ITERATION_NO.: 397 LOSS_Generator: 7.255520820617676 LOSS_Discriminator: 0.03192208334803581\n",
            "ITERATION_NO.: 398 LOSS_Generator: 7.579117298126221 LOSS_Discriminator: 0.029069285839796066\n",
            "ITERATION_NO.: 399 LOSS_Generator: 7.785299777984619 LOSS_Discriminator: 0.038982175290584564\n",
            "ITERATION_NO.: 400 LOSS_Generator: 8.86325454711914 LOSS_Discriminator: 0.058455102145671844\n",
            "ITERATION_NO.: 401 LOSS_Generator: 9.624183654785156 LOSS_Discriminator: 0.05731566622853279\n",
            "ITERATION_NO.: 402 LOSS_Generator: 9.058358192443848 LOSS_Discriminator: 0.059128351509571075\n",
            "ITERATION_NO.: 403 LOSS_Generator: 8.172351837158203 LOSS_Discriminator: 0.13680079579353333\n",
            "ITERATION_NO.: 404 LOSS_Generator: 6.695220947265625 LOSS_Discriminator: 0.025088585913181305\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.449517250061035 LOSS_Discriminator: 0.20611858367919922\n",
            "ITERATION_NO.: 406 LOSS_Generator: 7.054050445556641 LOSS_Discriminator: 0.08069413155317307\n",
            "ITERATION_NO.: 407 LOSS_Generator: 7.292621612548828 LOSS_Discriminator: 0.08043719083070755\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.872167587280273 LOSS_Discriminator: 0.03771639242768288\n",
            "ITERATION_NO.: 409 LOSS_Generator: 8.191585540771484 LOSS_Discriminator: 0.08248592913150787\n",
            "ITERATION_NO.: 410 LOSS_Generator: 8.077528953552246 LOSS_Discriminator: 0.044474292546510696\n",
            "ITERATION_NO.: 411 LOSS_Generator: 8.738119125366211 LOSS_Discriminator: 0.07239711284637451\n",
            "ITERATION_NO.: 412 LOSS_Generator: 8.269670486450195 LOSS_Discriminator: 0.029142409563064575\n",
            "ITERATION_NO.: 413 LOSS_Generator: 8.340899467468262 LOSS_Discriminator: 0.09949687123298645\n",
            "ITERATION_NO.: 414 LOSS_Generator: 7.374078273773193 LOSS_Discriminator: 0.050522785633802414\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.518776893615723 LOSS_Discriminator: 0.03126868978142738\n",
            "ITERATION_NO.: 416 LOSS_Generator: 6.521090507507324 LOSS_Discriminator: 0.07745173573493958\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.256290435791016 LOSS_Discriminator: 0.06979452818632126\n",
            "ITERATION_NO.: 418 LOSS_Generator: 7.5152153968811035 LOSS_Discriminator: 0.028824305161833763\n",
            "ITERATION_NO.: 419 LOSS_Generator: 7.803738594055176 LOSS_Discriminator: 0.043212197721004486\n",
            "ITERATION_NO.: 420 LOSS_Generator: 8.842367172241211 LOSS_Discriminator: 0.058179739862680435\n",
            "ITERATION_NO.: 421 LOSS_Generator: 7.860325813293457 LOSS_Discriminator: 0.034947194159030914\n",
            "ITERATION_NO.: 422 LOSS_Generator: 7.0632758140563965 LOSS_Discriminator: 0.14406661689281464\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.251737117767334 LOSS_Discriminator: 0.1209312155842781\n",
            "ITERATION_NO.: 424 LOSS_Generator: 5.742233753204346 LOSS_Discriminator: 0.05830198526382446\n",
            "ITERATION_NO.: 425 LOSS_Generator: 7.479379177093506 LOSS_Discriminator: 0.09930452704429626\n",
            "ITERATION_NO.: 426 LOSS_Generator: 8.824076652526855 LOSS_Discriminator: 0.06258153170347214\n",
            "ITERATION_NO.: 427 LOSS_Generator: 9.062695503234863 LOSS_Discriminator: 0.038286566734313965\n",
            "ITERATION_NO.: 428 LOSS_Generator: 8.860828399658203 LOSS_Discriminator: 0.1586799919605255\n",
            "ITERATION_NO.: 429 LOSS_Generator: 8.092714309692383 LOSS_Discriminator: 0.04985685646533966\n",
            "ITERATION_NO.: 430 LOSS_Generator: 7.535094738006592 LOSS_Discriminator: 0.033306702971458435\n",
            "ITERATION_NO.: 431 LOSS_Generator: 7.419680118560791 LOSS_Discriminator: 0.039039433002471924\n",
            "ITERATION_NO.: 432 LOSS_Generator: 6.819710731506348 LOSS_Discriminator: 0.11795638501644135\n",
            "ITERATION_NO.: 433 LOSS_Generator: 6.906402587890625 LOSS_Discriminator: 0.09169909358024597\n",
            "ITERATION_NO.: 434 LOSS_Generator: 8.147690773010254 LOSS_Discriminator: 0.04168613627552986\n",
            "ITERATION_NO.: 435 LOSS_Generator: 8.565010070800781 LOSS_Discriminator: 0.05929732322692871\n",
            "ITERATION_NO.: 436 LOSS_Generator: 8.858360290527344 LOSS_Discriminator: 0.02048744633793831\n",
            "ITERATION_NO.: 437 LOSS_Generator: 8.506269454956055 LOSS_Discriminator: 0.06631138920783997\n",
            "ITERATION_NO.: 438 LOSS_Generator: 8.55196475982666 LOSS_Discriminator: 0.03584793210029602\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.9305315017700195 LOSS_Discriminator: 0.12085461616516113\n",
            "ITERATION_NO.: 440 LOSS_Generator: 6.285881996154785 LOSS_Discriminator: 0.06012624502182007\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.168845176696777 LOSS_Discriminator: 0.11105038225650787\n",
            "ITERATION_NO.: 442 LOSS_Generator: 7.1390838623046875 LOSS_Discriminator: 0.0945112407207489\n",
            "ITERATION_NO.: 443 LOSS_Generator: 8.566417694091797 LOSS_Discriminator: 0.06723105907440186\n",
            "ITERATION_NO.: 444 LOSS_Generator: 9.907245635986328 LOSS_Discriminator: 0.03266792371869087\n",
            "ITERATION_NO.: 445 LOSS_Generator: 10.000039100646973 LOSS_Discriminator: 0.12461210042238235\n",
            "ITERATION_NO.: 446 LOSS_Generator: 8.830190658569336 LOSS_Discriminator: 0.09339246153831482\n",
            "ITERATION_NO.: 447 LOSS_Generator: 8.313821792602539 LOSS_Discriminator: 0.08151552081108093\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.921621799468994 LOSS_Discriminator: 0.10248559713363647\n",
            "ITERATION_NO.: 449 LOSS_Generator: 6.446303844451904 LOSS_Discriminator: 0.029220562428236008\n",
            "ITERATION_NO.: 450 LOSS_Generator: 6.862866401672363 LOSS_Discriminator: 0.07781373709440231\n",
            "ITERATION_NO.: 451 LOSS_Generator: 8.026991844177246 LOSS_Discriminator: 0.10012924671173096\n",
            "ITERATION_NO.: 452 LOSS_Generator: 9.124855041503906 LOSS_Discriminator: 0.01903991959989071\n",
            "ITERATION_NO.: 453 LOSS_Generator: 8.962440490722656 LOSS_Discriminator: 0.07386944442987442\n",
            "ITERATION_NO.: 454 LOSS_Generator: 8.820595741271973 LOSS_Discriminator: 0.04375308007001877\n",
            "ITERATION_NO.: 455 LOSS_Generator: 7.398343563079834 LOSS_Discriminator: 0.10781290382146835\n",
            "ITERATION_NO.: 456 LOSS_Generator: 6.133952617645264 LOSS_Discriminator: 0.09765788912773132\n",
            "ITERATION_NO.: 457 LOSS_Generator: 5.840824127197266 LOSS_Discriminator: 0.05415915697813034\n",
            "ITERATION_NO.: 458 LOSS_Generator: 6.458402633666992 LOSS_Discriminator: 0.07148996740579605\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.861631870269775 LOSS_Discriminator: 0.02378115803003311\n",
            "ITERATION_NO.: 460 LOSS_Generator: 7.213559150695801 LOSS_Discriminator: 0.03878675773739815\n",
            "ITERATION_NO.: 461 LOSS_Generator: 7.856632709503174 LOSS_Discriminator: 0.08350134640932083\n",
            "ITERATION_NO.: 462 LOSS_Generator: 7.87824821472168 LOSS_Discriminator: 0.07005216926336288\n",
            "ITERATION_NO.: 463 LOSS_Generator: 7.532047271728516 LOSS_Discriminator: 0.03898477926850319\n",
            "ITERATION_NO.: 464 LOSS_Generator: 8.25523853302002 LOSS_Discriminator: 0.029666144400835037\n",
            "ITERATION_NO.: 465 LOSS_Generator: 7.999897480010986 LOSS_Discriminator: 0.05097505450248718\n",
            "ITERATION_NO.: 466 LOSS_Generator: 7.404506683349609 LOSS_Discriminator: 0.027415286749601364\n",
            "ITERATION_NO.: 467 LOSS_Generator: 7.78046989440918 LOSS_Discriminator: 0.09786780178546906\n",
            "ITERATION_NO.: 468 LOSS_Generator: 7.398535251617432 LOSS_Discriminator: 0.02023475617170334\n",
            "ITERATION_NO.: 469 LOSS_Generator: 6.817785263061523 LOSS_Discriminator: 0.1097349002957344\n",
            "ITERATION_NO.: 470 LOSS_Generator: 7.21878719329834 LOSS_Discriminator: 0.040464382618665695\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.652381420135498 LOSS_Discriminator: 0.06083761155605316\n",
            "ITERATION_NO.: 472 LOSS_Generator: 7.393548965454102 LOSS_Discriminator: 0.07072209566831589\n",
            "ITERATION_NO.: 473 LOSS_Generator: 7.480612754821777 LOSS_Discriminator: 0.023236263543367386\n",
            "ITERATION_NO.: 474 LOSS_Generator: 7.657448768615723 LOSS_Discriminator: 0.05041418597102165\n",
            "ITERATION_NO.: 475 LOSS_Generator: 8.058862686157227 LOSS_Discriminator: 0.06390047073364258\n",
            "ITERATION_NO.: 476 LOSS_Generator: 7.0186381340026855 LOSS_Discriminator: 0.0615348182618618\n",
            "ITERATION_NO.: 477 LOSS_Generator: 7.168712615966797 LOSS_Discriminator: 0.0650327280163765\n",
            "ITERATION_NO.: 478 LOSS_Generator: 7.24098014831543 LOSS_Discriminator: 0.08499187231063843\n",
            "ITERATION_NO.: 479 LOSS_Generator: 7.616086959838867 LOSS_Discriminator: 0.08258657902479172\n",
            "ITERATION_NO.: 480 LOSS_Generator: 6.815764904022217 LOSS_Discriminator: 0.048841819167137146\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.7405524253845215 LOSS_Discriminator: 0.08414210379123688\n",
            "ITERATION_NO.: 482 LOSS_Generator: 7.004209995269775 LOSS_Discriminator: 0.028296876698732376\n",
            "ITERATION_NO.: 483 LOSS_Generator: 7.614312171936035 LOSS_Discriminator: 0.04240410402417183\n",
            "ITERATION_NO.: 484 LOSS_Generator: 7.808047294616699 LOSS_Discriminator: 0.05832557752728462\n",
            "ITERATION_NO.: 485 LOSS_Generator: 7.708513259887695 LOSS_Discriminator: 0.04340142011642456\n",
            "ITERATION_NO.: 486 LOSS_Generator: 7.645818710327148 LOSS_Discriminator: 0.03633689135313034\n",
            "ITERATION_NO.: 487 LOSS_Generator: 7.229384899139404 LOSS_Discriminator: 0.03822658956050873\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.564200401306152 LOSS_Discriminator: 0.1229531466960907\n",
            "ITERATION_NO.: 489 LOSS_Generator: 6.980301380157471 LOSS_Discriminator: 0.07216809689998627\n",
            "ITERATION_NO.: 490 LOSS_Generator: 7.051453113555908 LOSS_Discriminator: 0.05317571386694908\n",
            "ITERATION_NO.: 491 LOSS_Generator: 7.476037502288818 LOSS_Discriminator: 0.05149151012301445\n",
            "ITERATION_NO.: 492 LOSS_Generator: 7.878826141357422 LOSS_Discriminator: 0.04828456789255142\n",
            "ITERATION_NO.: 493 LOSS_Generator: 8.483613014221191 LOSS_Discriminator: 0.03255918622016907\n",
            "ITERATION_NO.: 494 LOSS_Generator: 8.815260887145996 LOSS_Discriminator: 0.028751259669661522\n",
            "ITERATION_NO.: 495 LOSS_Generator: 8.50995922088623 LOSS_Discriminator: 0.0351007841527462\n",
            "ITERATION_NO.: 496 LOSS_Generator: 8.532986640930176 LOSS_Discriminator: 0.09864408522844315\n",
            "ITERATION_NO.: 497 LOSS_Generator: 7.517098426818848 LOSS_Discriminator: 0.10489828884601593\n",
            "ITERATION_NO.: 498 LOSS_Generator: 6.9420061111450195 LOSS_Discriminator: 0.06095525622367859\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.049871921539307 LOSS_Discriminator: 0.08327459543943405\n",
            "ITERATION_NO.: 500 LOSS_Generator: 6.322503566741943 LOSS_Discriminator: 0.09359658509492874\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.1055474281311035 LOSS_Discriminator: 0.027960674837231636\n",
            "ITERATION_NO.: 502 LOSS_Generator: 7.6400675773620605 LOSS_Discriminator: 0.03696297109127045\n",
            "ITERATION_NO.: 503 LOSS_Generator: 8.198803901672363 LOSS_Discriminator: 0.09342150390148163\n",
            "ITERATION_NO.: 504 LOSS_Generator: 8.245569229125977 LOSS_Discriminator: 0.02743058279156685\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.736444473266602 LOSS_Discriminator: 0.10309620201587677\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.599191665649414 LOSS_Discriminator: 0.0709582269191742\n",
            "ITERATION_NO.: 507 LOSS_Generator: 6.011743545532227 LOSS_Discriminator: 0.04694030433893204\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.219430923461914 LOSS_Discriminator: 0.04391894489526749\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.034755229949951 LOSS_Discriminator: 0.06740362197160721\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.876558303833008 LOSS_Discriminator: 0.08761999756097794\n",
            "ITERATION_NO.: 511 LOSS_Generator: 8.510598182678223 LOSS_Discriminator: 0.07903213053941727\n",
            "ITERATION_NO.: 512 LOSS_Generator: 9.193663597106934 LOSS_Discriminator: 0.08916913717985153\n",
            "ITERATION_NO.: 513 LOSS_Generator: 8.537206649780273 LOSS_Discriminator: 0.0719599723815918\n",
            "ITERATION_NO.: 514 LOSS_Generator: 8.877745628356934 LOSS_Discriminator: 0.022444674745202065\n",
            "ITERATION_NO.: 515 LOSS_Generator: 7.242452621459961 LOSS_Discriminator: 0.107272207736969\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.057794094085693 LOSS_Discriminator: 0.05883429944515228\n",
            "ITERATION_NO.: 517 LOSS_Generator: 5.833735942840576 LOSS_Discriminator: 0.12633830308914185\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.112446308135986 LOSS_Discriminator: 0.06820869445800781\n",
            "ITERATION_NO.: 519 LOSS_Generator: 7.919858455657959 LOSS_Discriminator: 0.03412232547998428\n",
            "ITERATION_NO.: 520 LOSS_Generator: 8.393507957458496 LOSS_Discriminator: 0.06258360296487808\n",
            "ITERATION_NO.: 521 LOSS_Generator: 8.541897773742676 LOSS_Discriminator: 0.044405438005924225\n",
            "ITERATION_NO.: 522 LOSS_Generator: 8.869119644165039 LOSS_Discriminator: 0.05481480062007904\n",
            "ITERATION_NO.: 523 LOSS_Generator: 7.8044586181640625 LOSS_Discriminator: 0.13413284718990326\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.1016974449157715 LOSS_Discriminator: 0.09338050335645676\n",
            "ITERATION_NO.: 525 LOSS_Generator: 5.2632951736450195 LOSS_Discriminator: 0.07003875076770782\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.787105560302734 LOSS_Discriminator: 0.11017711460590363\n",
            "ITERATION_NO.: 527 LOSS_Generator: 7.172854423522949 LOSS_Discriminator: 0.08283049613237381\n",
            "ITERATION_NO.: 528 LOSS_Generator: 8.548554420471191 LOSS_Discriminator: 0.04781770333647728\n",
            "ITERATION_NO.: 529 LOSS_Generator: 9.565524101257324 LOSS_Discriminator: 0.07266208529472351\n",
            "ITERATION_NO.: 530 LOSS_Generator: 8.862079620361328 LOSS_Discriminator: 0.20304036140441895\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.699164867401123 LOSS_Discriminator: 0.1755399852991104\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.429318428039551 LOSS_Discriminator: 0.0645899623632431\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.218806743621826 LOSS_Discriminator: 0.0942927896976471\n",
            "ITERATION_NO.: 534 LOSS_Generator: 5.618500232696533 LOSS_Discriminator: 0.14846792817115784\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.199275016784668 LOSS_Discriminator: 0.09601640701293945\n",
            "ITERATION_NO.: 536 LOSS_Generator: 8.59788990020752 LOSS_Discriminator: 0.12549254298210144\n",
            "ITERATION_NO.: 537 LOSS_Generator: 9.071677207946777 LOSS_Discriminator: 0.12100685387849808\n",
            "ITERATION_NO.: 538 LOSS_Generator: 8.172479629516602 LOSS_Discriminator: 0.051120974123477936\n",
            "ITERATION_NO.: 539 LOSS_Generator: 7.819364547729492 LOSS_Discriminator: 0.008028128184378147\n",
            "ITERATION_NO.: 540 LOSS_Generator: 7.426469802856445 LOSS_Discriminator: 0.1077301949262619\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.540046215057373 LOSS_Discriminator: 0.04254312068223953\n",
            "ITERATION_NO.: 542 LOSS_Generator: 5.869228363037109 LOSS_Discriminator: 0.10289666056632996\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.210911273956299 LOSS_Discriminator: 0.15411189198493958\n",
            "ITERATION_NO.: 544 LOSS_Generator: 7.467285633087158 LOSS_Discriminator: 0.06246785447001457\n",
            "ITERATION_NO.: 545 LOSS_Generator: 7.65977668762207 LOSS_Discriminator: 0.05273779481649399\n",
            "ITERATION_NO.: 546 LOSS_Generator: 8.167664527893066 LOSS_Discriminator: 0.05239430069923401\n",
            "ITERATION_NO.: 547 LOSS_Generator: 8.544397354125977 LOSS_Discriminator: 0.0368754006922245\n",
            "ITERATION_NO.: 548 LOSS_Generator: 7.768563747406006 LOSS_Discriminator: 0.050840333104133606\n",
            "ITERATION_NO.: 549 LOSS_Generator: 7.333866119384766 LOSS_Discriminator: 0.06398122757673264\n",
            "ITERATION_NO.: 550 LOSS_Generator: 7.477677822113037 LOSS_Discriminator: 0.05447711423039436\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.219410419464111 LOSS_Discriminator: 0.07106270641088486\n",
            "ITERATION_NO.: 552 LOSS_Generator: 7.4930267333984375 LOSS_Discriminator: 0.048669345676898956\n",
            "ITERATION_NO.: 553 LOSS_Generator: 8.219186782836914 LOSS_Discriminator: 0.04445099085569382\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.894818305969238 LOSS_Discriminator: 0.08415256440639496\n",
            "ITERATION_NO.: 555 LOSS_Generator: 7.451671600341797 LOSS_Discriminator: 0.02982097864151001\n",
            "ITERATION_NO.: 556 LOSS_Generator: 7.340306282043457 LOSS_Discriminator: 0.04247938096523285\n",
            "ITERATION_NO.: 557 LOSS_Generator: 7.920926094055176 LOSS_Discriminator: 0.04916185885667801\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.698293685913086 LOSS_Discriminator: 0.083518847823143\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.978943347930908 LOSS_Discriminator: 0.05694251507520676\n",
            "ITERATION_NO.: 560 LOSS_Generator: 6.697852611541748 LOSS_Discriminator: 0.09220287948846817\n",
            "ITERATION_NO.: 561 LOSS_Generator: 7.243414878845215 LOSS_Discriminator: 0.03283688798546791\n",
            "ITERATION_NO.: 562 LOSS_Generator: 8.307097434997559 LOSS_Discriminator: 0.056034475564956665\n",
            "ITERATION_NO.: 563 LOSS_Generator: 8.228135108947754 LOSS_Discriminator: 0.09379351884126663\n",
            "ITERATION_NO.: 564 LOSS_Generator: 7.327306747436523 LOSS_Discriminator: 0.0869629830121994\n",
            "ITERATION_NO.: 565 LOSS_Generator: 6.692629814147949 LOSS_Discriminator: 0.069747194647789\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.72627067565918 LOSS_Discriminator: 0.038960859179496765\n",
            "ITERATION_NO.: 567 LOSS_Generator: 5.906784534454346 LOSS_Discriminator: 0.06551185250282288\n",
            "ITERATION_NO.: 568 LOSS_Generator: 6.62502908706665 LOSS_Discriminator: 0.13885807991027832\n",
            "ITERATION_NO.: 569 LOSS_Generator: 7.455289363861084 LOSS_Discriminator: 0.08175309002399445\n",
            "ITERATION_NO.: 570 LOSS_Generator: 7.5754804611206055 LOSS_Discriminator: 0.045675210654735565\n",
            "ITERATION_NO.: 571 LOSS_Generator: 8.245567321777344 LOSS_Discriminator: 0.06247927248477936\n",
            "ITERATION_NO.: 572 LOSS_Generator: 7.224205493927002 LOSS_Discriminator: 0.10460082441568375\n",
            "ITERATION_NO.: 573 LOSS_Generator: 6.15949821472168 LOSS_Discriminator: 0.09700458496809006\n",
            "ITERATION_NO.: 574 LOSS_Generator: 5.757055759429932 LOSS_Discriminator: 0.09243644773960114\n",
            "ITERATION_NO.: 575 LOSS_Generator: 6.539278507232666 LOSS_Discriminator: 0.051198944449424744\n",
            "ITERATION_NO.: 576 LOSS_Generator: 6.9034104347229 LOSS_Discriminator: 0.05166399106383324\n",
            "ITERATION_NO.: 577 LOSS_Generator: 7.0598554611206055 LOSS_Discriminator: 0.0921323224902153\n",
            "ITERATION_NO.: 578 LOSS_Generator: 7.028440952301025 LOSS_Discriminator: 0.07990511506795883\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.728998184204102 LOSS_Discriminator: 0.02848375402390957\n",
            "ITERATION_NO.: 580 LOSS_Generator: 7.335351467132568 LOSS_Discriminator: 0.09573045372962952\n",
            "ITERATION_NO.: 581 LOSS_Generator: 6.976762771606445 LOSS_Discriminator: 0.06571033596992493\n",
            "ITERATION_NO.: 582 LOSS_Generator: 6.823002815246582 LOSS_Discriminator: 0.04742419719696045\n",
            "ITERATION_NO.: 583 LOSS_Generator: 7.617342948913574 LOSS_Discriminator: 0.05429569259285927\n",
            "ITERATION_NO.: 584 LOSS_Generator: 7.143543243408203 LOSS_Discriminator: 0.05987338721752167\n",
            "ITERATION_NO.: 585 LOSS_Generator: 7.741633415222168 LOSS_Discriminator: 0.026590030640363693\n",
            "ITERATION_NO.: 586 LOSS_Generator: 7.795212268829346 LOSS_Discriminator: 0.07253998517990112\n",
            "ITERATION_NO.: 587 LOSS_Generator: 7.081679821014404 LOSS_Discriminator: 0.04108754172921181\n",
            "ITERATION_NO.: 588 LOSS_Generator: 7.036880016326904 LOSS_Discriminator: 0.05359099060297012\n",
            "ITERATION_NO.: 589 LOSS_Generator: 7.3521833419799805 LOSS_Discriminator: 0.05662541836500168\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.501890182495117 LOSS_Discriminator: 0.0696159154176712\n",
            "ITERATION_NO.: 591 LOSS_Generator: 7.164491176605225 LOSS_Discriminator: 0.03833380341529846\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.454517364501953 LOSS_Discriminator: 0.030703432857990265\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.932069778442383 LOSS_Discriminator: 0.05523766577243805\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.798882484436035 LOSS_Discriminator: 0.05320195108652115\n",
            "ITERATION_NO.: 595 LOSS_Generator: 7.3656110763549805 LOSS_Discriminator: 0.09007129818201065\n",
            "ITERATION_NO.: 596 LOSS_Generator: 6.932368755340576 LOSS_Discriminator: 0.1643926203250885\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.251000881195068 LOSS_Discriminator: 0.041768062859773636\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.944669246673584 LOSS_Discriminator: 0.08628584444522858\n",
            "ITERATION_NO.: 599 LOSS_Generator: 7.540747165679932 LOSS_Discriminator: 0.10682249069213867\n",
            "ITERATION_NO.: 600 LOSS_Generator: 8.56546688079834 LOSS_Discriminator: 0.03312517702579498\n",
            "EPOCH OVER: 44\n",
            "ITERATION_NO.: 1 LOSS_Generator: 8.612403869628906 LOSS_Discriminator: 0.02441171556711197\n",
            "ITERATION_NO.: 2 LOSS_Generator: 8.82247543334961 LOSS_Discriminator: 0.11529212445020676\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.047358989715576 LOSS_Discriminator: 0.10625220835208893\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.062859058380127 LOSS_Discriminator: 0.05936184898018837\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.961575508117676 LOSS_Discriminator: 0.05338504910469055\n",
            "ITERATION_NO.: 6 LOSS_Generator: 6.582891941070557 LOSS_Discriminator: 0.06447997689247131\n",
            "ITERATION_NO.: 7 LOSS_Generator: 7.025515079498291 LOSS_Discriminator: 0.06992436945438385\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.950422286987305 LOSS_Discriminator: 0.10137523710727692\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.185595512390137 LOSS_Discriminator: 0.09243004769086838\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.485839366912842 LOSS_Discriminator: 0.052384406328201294\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.224043369293213 LOSS_Discriminator: 0.08851635456085205\n",
            "ITERATION_NO.: 12 LOSS_Generator: 5.8697309494018555 LOSS_Discriminator: 0.06048504635691643\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.804534435272217 LOSS_Discriminator: 0.0876961499452591\n",
            "ITERATION_NO.: 14 LOSS_Generator: 8.12027645111084 LOSS_Discriminator: 0.08657513558864594\n",
            "ITERATION_NO.: 15 LOSS_Generator: 8.430569648742676 LOSS_Discriminator: 0.05284198746085167\n",
            "ITERATION_NO.: 16 LOSS_Generator: 8.965392112731934 LOSS_Discriminator: 0.06357862055301666\n",
            "ITERATION_NO.: 17 LOSS_Generator: 9.174043655395508 LOSS_Discriminator: 0.17172463238239288\n",
            "ITERATION_NO.: 18 LOSS_Generator: 8.377870559692383 LOSS_Discriminator: 0.01671905256807804\n",
            "ITERATION_NO.: 19 LOSS_Generator: 7.5843377113342285 LOSS_Discriminator: 0.03875850513577461\n",
            "ITERATION_NO.: 20 LOSS_Generator: 7.195184707641602 LOSS_Discriminator: 0.06596918404102325\n",
            "ITERATION_NO.: 21 LOSS_Generator: 7.097970008850098 LOSS_Discriminator: 0.0603010356426239\n",
            "ITERATION_NO.: 22 LOSS_Generator: 6.648923397064209 LOSS_Discriminator: 0.06257252395153046\n",
            "ITERATION_NO.: 23 LOSS_Generator: 7.0747528076171875 LOSS_Discriminator: 0.09882243722677231\n",
            "ITERATION_NO.: 24 LOSS_Generator: 7.883038520812988 LOSS_Discriminator: 0.03716486692428589\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.921927452087402 LOSS_Discriminator: 0.04139505326747894\n",
            "ITERATION_NO.: 26 LOSS_Generator: 8.919878959655762 LOSS_Discriminator: 0.08092641830444336\n",
            "ITERATION_NO.: 27 LOSS_Generator: 8.03586196899414 LOSS_Discriminator: 0.05200310796499252\n",
            "ITERATION_NO.: 28 LOSS_Generator: 8.333590507507324 LOSS_Discriminator: 0.05499373748898506\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.940433502197266 LOSS_Discriminator: 0.04664912447333336\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.207205772399902 LOSS_Discriminator: 0.06705424189567566\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.5264410972595215 LOSS_Discriminator: 0.03292519971728325\n",
            "ITERATION_NO.: 32 LOSS_Generator: 7.522736072540283 LOSS_Discriminator: 0.09138913452625275\n",
            "ITERATION_NO.: 33 LOSS_Generator: 7.834737777709961 LOSS_Discriminator: 0.04697427153587341\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.158454895019531 LOSS_Discriminator: 0.07607096433639526\n",
            "ITERATION_NO.: 35 LOSS_Generator: 7.865858554840088 LOSS_Discriminator: 0.04756134748458862\n",
            "ITERATION_NO.: 36 LOSS_Generator: 8.034907341003418 LOSS_Discriminator: 0.14701960980892181\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.665887355804443 LOSS_Discriminator: 0.11273667216300964\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.677865505218506 LOSS_Discriminator: 0.04752374440431595\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.44110107421875 LOSS_Discriminator: 0.08534012734889984\n",
            "ITERATION_NO.: 40 LOSS_Generator: 6.992812633514404 LOSS_Discriminator: 0.04846280440688133\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.209527492523193 LOSS_Discriminator: 0.026701640337705612\n",
            "ITERATION_NO.: 42 LOSS_Generator: 7.876735687255859 LOSS_Discriminator: 0.0884389579296112\n",
            "ITERATION_NO.: 43 LOSS_Generator: 8.123230934143066 LOSS_Discriminator: 0.05966046452522278\n",
            "ITERATION_NO.: 44 LOSS_Generator: 8.45162582397461 LOSS_Discriminator: 0.01987861469388008\n",
            "ITERATION_NO.: 45 LOSS_Generator: 8.763480186462402 LOSS_Discriminator: 0.0447465255856514\n",
            "ITERATION_NO.: 46 LOSS_Generator: 8.659614562988281 LOSS_Discriminator: 0.0433775819838047\n",
            "ITERATION_NO.: 47 LOSS_Generator: 7.234675407409668 LOSS_Discriminator: 0.10925140976905823\n",
            "ITERATION_NO.: 48 LOSS_Generator: 6.773624897003174 LOSS_Discriminator: 0.06309311836957932\n",
            "ITERATION_NO.: 49 LOSS_Generator: 6.801279067993164 LOSS_Discriminator: 0.07326313108205795\n",
            "ITERATION_NO.: 50 LOSS_Generator: 6.675776481628418 LOSS_Discriminator: 0.0580962672829628\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.510049819946289 LOSS_Discriminator: 0.045709215104579926\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.606874465942383 LOSS_Discriminator: 0.04079589247703552\n",
            "ITERATION_NO.: 53 LOSS_Generator: 7.641650199890137 LOSS_Discriminator: 0.05379999801516533\n",
            "ITERATION_NO.: 54 LOSS_Generator: 7.344636917114258 LOSS_Discriminator: 0.08738104999065399\n",
            "ITERATION_NO.: 55 LOSS_Generator: 6.868813991546631 LOSS_Discriminator: 0.059327878057956696\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.622858047485352 LOSS_Discriminator: 0.028998088091611862\n",
            "ITERATION_NO.: 57 LOSS_Generator: 7.107934951782227 LOSS_Discriminator: 0.054268985986709595\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.2061991691589355 LOSS_Discriminator: 0.07737923413515091\n",
            "ITERATION_NO.: 59 LOSS_Generator: 7.952385902404785 LOSS_Discriminator: 0.031617067754268646\n",
            "ITERATION_NO.: 60 LOSS_Generator: 7.853682994842529 LOSS_Discriminator: 0.07680940628051758\n",
            "ITERATION_NO.: 61 LOSS_Generator: 7.848929405212402 LOSS_Discriminator: 0.09303136169910431\n",
            "ITERATION_NO.: 62 LOSS_Generator: 7.258031845092773 LOSS_Discriminator: 0.07656769454479218\n",
            "ITERATION_NO.: 63 LOSS_Generator: 6.5158185958862305 LOSS_Discriminator: 0.029976654797792435\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.591945171356201 LOSS_Discriminator: 0.0900220274925232\n",
            "ITERATION_NO.: 65 LOSS_Generator: 6.873466968536377 LOSS_Discriminator: 0.06443983316421509\n",
            "ITERATION_NO.: 66 LOSS_Generator: 8.128680229187012 LOSS_Discriminator: 0.02506496012210846\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.888050079345703 LOSS_Discriminator: 0.03447078540921211\n",
            "ITERATION_NO.: 68 LOSS_Generator: 8.411092758178711 LOSS_Discriminator: 0.071755051612854\n",
            "ITERATION_NO.: 69 LOSS_Generator: 7.9765777587890625 LOSS_Discriminator: 0.051360711455345154\n",
            "ITERATION_NO.: 70 LOSS_Generator: 6.864993095397949 LOSS_Discriminator: 0.05120585858821869\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.332210540771484 LOSS_Discriminator: 0.07999792695045471\n",
            "ITERATION_NO.: 72 LOSS_Generator: 7.048330783843994 LOSS_Discriminator: 0.0810038298368454\n",
            "ITERATION_NO.: 73 LOSS_Generator: 8.060164451599121 LOSS_Discriminator: 0.04207246005535126\n",
            "ITERATION_NO.: 74 LOSS_Generator: 8.302175521850586 LOSS_Discriminator: 0.07024957239627838\n",
            "ITERATION_NO.: 75 LOSS_Generator: 8.132026672363281 LOSS_Discriminator: 0.06529713422060013\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.633128643035889 LOSS_Discriminator: 0.1562725305557251\n",
            "ITERATION_NO.: 77 LOSS_Generator: 7.254765510559082 LOSS_Discriminator: 0.046829722821712494\n",
            "ITERATION_NO.: 78 LOSS_Generator: 5.8665242195129395 LOSS_Discriminator: 0.06018000841140747\n",
            "ITERATION_NO.: 79 LOSS_Generator: 6.473153114318848 LOSS_Discriminator: 0.0757637694478035\n",
            "ITERATION_NO.: 80 LOSS_Generator: 7.164084434509277 LOSS_Discriminator: 0.04471928998827934\n",
            "ITERATION_NO.: 81 LOSS_Generator: 8.39964771270752 LOSS_Discriminator: 0.07096391916275024\n",
            "ITERATION_NO.: 82 LOSS_Generator: 7.602990627288818 LOSS_Discriminator: 0.05292440950870514\n",
            "ITERATION_NO.: 83 LOSS_Generator: 7.733806610107422 LOSS_Discriminator: 0.0752369835972786\n",
            "ITERATION_NO.: 84 LOSS_Generator: 7.056492328643799 LOSS_Discriminator: 0.09827087819576263\n",
            "ITERATION_NO.: 85 LOSS_Generator: 6.632997989654541 LOSS_Discriminator: 0.11216180771589279\n",
            "ITERATION_NO.: 86 LOSS_Generator: 6.399797439575195 LOSS_Discriminator: 0.10276573151350021\n",
            "ITERATION_NO.: 87 LOSS_Generator: 6.653387069702148 LOSS_Discriminator: 0.050333425402641296\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.511396884918213 LOSS_Discriminator: 0.04441656544804573\n",
            "ITERATION_NO.: 89 LOSS_Generator: 7.321227550506592 LOSS_Discriminator: 0.08951091766357422\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.504207611083984 LOSS_Discriminator: 0.10151661932468414\n",
            "ITERATION_NO.: 91 LOSS_Generator: 7.241919994354248 LOSS_Discriminator: 0.10240934789180756\n",
            "ITERATION_NO.: 92 LOSS_Generator: 6.754317760467529 LOSS_Discriminator: 0.088573157787323\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.661682605743408 LOSS_Discriminator: 0.09716340899467468\n",
            "ITERATION_NO.: 94 LOSS_Generator: 7.368430137634277 LOSS_Discriminator: 0.06915094703435898\n",
            "ITERATION_NO.: 95 LOSS_Generator: 7.735235691070557 LOSS_Discriminator: 0.11561145633459091\n",
            "ITERATION_NO.: 96 LOSS_Generator: 8.74774169921875 LOSS_Discriminator: 0.04130000248551369\n",
            "ITERATION_NO.: 97 LOSS_Generator: 9.499805450439453 LOSS_Discriminator: 0.1132296621799469\n",
            "ITERATION_NO.: 98 LOSS_Generator: 8.044219970703125 LOSS_Discriminator: 0.07688581198453903\n",
            "ITERATION_NO.: 99 LOSS_Generator: 8.559091567993164 LOSS_Discriminator: 0.06595060974359512\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.7958149909973145 LOSS_Discriminator: 0.15871137380599976\n",
            "ITERATION_NO.: 101 LOSS_Generator: 5.789370536804199 LOSS_Discriminator: 0.18564818799495697\n",
            "ITERATION_NO.: 102 LOSS_Generator: 6.4986090660095215 LOSS_Discriminator: 0.16850069165229797\n",
            "ITERATION_NO.: 103 LOSS_Generator: 7.625137805938721 LOSS_Discriminator: 0.08755955100059509\n",
            "ITERATION_NO.: 104 LOSS_Generator: 9.350770950317383 LOSS_Discriminator: 0.029008137062191963\n",
            "ITERATION_NO.: 105 LOSS_Generator: 9.587766647338867 LOSS_Discriminator: 0.1041942834854126\n",
            "ITERATION_NO.: 106 LOSS_Generator: 9.574503898620605 LOSS_Discriminator: 0.06707575917243958\n",
            "ITERATION_NO.: 107 LOSS_Generator: 8.915535926818848 LOSS_Discriminator: 0.12306759506464005\n",
            "ITERATION_NO.: 108 LOSS_Generator: 8.092577934265137 LOSS_Discriminator: 0.08295788615942001\n",
            "ITERATION_NO.: 109 LOSS_Generator: 7.8777689933776855 LOSS_Discriminator: 0.04941333457827568\n",
            "ITERATION_NO.: 110 LOSS_Generator: 7.172118663787842 LOSS_Discriminator: 0.10685791075229645\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.758190155029297 LOSS_Discriminator: 0.08406474441289902\n",
            "ITERATION_NO.: 112 LOSS_Generator: 7.607215404510498 LOSS_Discriminator: 0.04131752997636795\n",
            "ITERATION_NO.: 113 LOSS_Generator: 7.603580474853516 LOSS_Discriminator: 0.01655462011694908\n",
            "ITERATION_NO.: 114 LOSS_Generator: 8.108000755310059 LOSS_Discriminator: 0.045882366597652435\n",
            "ITERATION_NO.: 115 LOSS_Generator: 7.9625468254089355 LOSS_Discriminator: 0.05098860710859299\n",
            "ITERATION_NO.: 116 LOSS_Generator: 7.78746223449707 LOSS_Discriminator: 0.07360132783651352\n",
            "ITERATION_NO.: 117 LOSS_Generator: 7.171650409698486 LOSS_Discriminator: 0.0873367041349411\n",
            "ITERATION_NO.: 118 LOSS_Generator: 6.761352062225342 LOSS_Discriminator: 0.07492489367723465\n",
            "ITERATION_NO.: 119 LOSS_Generator: 6.536492347717285 LOSS_Discriminator: 0.06417490541934967\n",
            "ITERATION_NO.: 120 LOSS_Generator: 6.664815902709961 LOSS_Discriminator: 0.04120658338069916\n",
            "ITERATION_NO.: 121 LOSS_Generator: 7.536557197570801 LOSS_Discriminator: 0.05680428445339203\n",
            "ITERATION_NO.: 122 LOSS_Generator: 7.935971736907959 LOSS_Discriminator: 0.08967016637325287\n",
            "ITERATION_NO.: 123 LOSS_Generator: 8.464336395263672 LOSS_Discriminator: 0.05102412402629852\n",
            "ITERATION_NO.: 124 LOSS_Generator: 7.910367012023926 LOSS_Discriminator: 0.05579519271850586\n",
            "ITERATION_NO.: 125 LOSS_Generator: 7.026497840881348 LOSS_Discriminator: 0.05312272533774376\n",
            "ITERATION_NO.: 126 LOSS_Generator: 7.024686336517334 LOSS_Discriminator: 0.0658782422542572\n",
            "ITERATION_NO.: 127 LOSS_Generator: 6.0958638191223145 LOSS_Discriminator: 0.05744216591119766\n",
            "ITERATION_NO.: 128 LOSS_Generator: 7.864223003387451 LOSS_Discriminator: 0.05663276091217995\n",
            "ITERATION_NO.: 129 LOSS_Generator: 7.497938632965088 LOSS_Discriminator: 0.032105013728141785\n",
            "ITERATION_NO.: 130 LOSS_Generator: 8.189157485961914 LOSS_Discriminator: 0.03920911252498627\n",
            "ITERATION_NO.: 131 LOSS_Generator: 8.260165214538574 LOSS_Discriminator: 0.08705374598503113\n",
            "ITERATION_NO.: 132 LOSS_Generator: 8.205838203430176 LOSS_Discriminator: 0.12304680794477463\n",
            "ITERATION_NO.: 133 LOSS_Generator: 7.283346652984619 LOSS_Discriminator: 0.07194937020540237\n",
            "ITERATION_NO.: 134 LOSS_Generator: 5.881367206573486 LOSS_Discriminator: 0.12789291143417358\n",
            "ITERATION_NO.: 135 LOSS_Generator: 5.733186721801758 LOSS_Discriminator: 0.06274835765361786\n",
            "ITERATION_NO.: 136 LOSS_Generator: 6.849972724914551 LOSS_Discriminator: 0.06525617837905884\n",
            "ITERATION_NO.: 137 LOSS_Generator: 8.274518013000488 LOSS_Discriminator: 0.03093692660331726\n",
            "ITERATION_NO.: 138 LOSS_Generator: 8.555623054504395 LOSS_Discriminator: 0.08655232191085815\n",
            "ITERATION_NO.: 139 LOSS_Generator: 7.971291542053223 LOSS_Discriminator: 0.07716234028339386\n",
            "ITERATION_NO.: 140 LOSS_Generator: 7.741807460784912 LOSS_Discriminator: 0.10042579472064972\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.178992748260498 LOSS_Discriminator: 0.06994004547595978\n",
            "ITERATION_NO.: 142 LOSS_Generator: 5.879072666168213 LOSS_Discriminator: 0.08918597549200058\n",
            "ITERATION_NO.: 143 LOSS_Generator: 6.580138683319092 LOSS_Discriminator: 0.09536205232143402\n",
            "ITERATION_NO.: 144 LOSS_Generator: 6.930148124694824 LOSS_Discriminator: 0.08060868084430695\n",
            "ITERATION_NO.: 145 LOSS_Generator: 8.111937522888184 LOSS_Discriminator: 0.03397185727953911\n",
            "ITERATION_NO.: 146 LOSS_Generator: 8.227046966552734 LOSS_Discriminator: 0.07858534157276154\n",
            "ITERATION_NO.: 147 LOSS_Generator: 8.15083122253418 LOSS_Discriminator: 0.07951906323432922\n",
            "ITERATION_NO.: 148 LOSS_Generator: 8.142867088317871 LOSS_Discriminator: 0.054730527102947235\n",
            "ITERATION_NO.: 149 LOSS_Generator: 7.431561470031738 LOSS_Discriminator: 0.11758868396282196\n",
            "ITERATION_NO.: 150 LOSS_Generator: 7.15931510925293 LOSS_Discriminator: 0.07972574234008789\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.4356369972229 LOSS_Discriminator: 0.0485847070813179\n",
            "ITERATION_NO.: 152 LOSS_Generator: 6.69111442565918 LOSS_Discriminator: 0.06688253581523895\n",
            "ITERATION_NO.: 153 LOSS_Generator: 7.047346591949463 LOSS_Discriminator: 0.051480963826179504\n",
            "ITERATION_NO.: 154 LOSS_Generator: 8.31531047821045 LOSS_Discriminator: 0.06732666492462158\n",
            "ITERATION_NO.: 155 LOSS_Generator: 8.056011199951172 LOSS_Discriminator: 0.049042947590351105\n",
            "ITERATION_NO.: 156 LOSS_Generator: 8.665014266967773 LOSS_Discriminator: 0.07804993540048599\n",
            "ITERATION_NO.: 157 LOSS_Generator: 7.70535135269165 LOSS_Discriminator: 0.06207326054573059\n",
            "ITERATION_NO.: 158 LOSS_Generator: 8.021993637084961 LOSS_Discriminator: 0.11833623051643372\n",
            "ITERATION_NO.: 159 LOSS_Generator: 7.246887683868408 LOSS_Discriminator: 0.11199843138456345\n",
            "ITERATION_NO.: 160 LOSS_Generator: 6.487774848937988 LOSS_Discriminator: 0.0733800157904625\n",
            "ITERATION_NO.: 161 LOSS_Generator: 7.184359073638916 LOSS_Discriminator: 0.06690721958875656\n",
            "ITERATION_NO.: 162 LOSS_Generator: 7.803874969482422 LOSS_Discriminator: 0.04107566177845001\n",
            "ITERATION_NO.: 163 LOSS_Generator: 8.414066314697266 LOSS_Discriminator: 0.01671108603477478\n",
            "ITERATION_NO.: 164 LOSS_Generator: 8.756537437438965 LOSS_Discriminator: 0.088202565908432\n",
            "ITERATION_NO.: 165 LOSS_Generator: 7.668221950531006 LOSS_Discriminator: 0.07163198292255402\n",
            "ITERATION_NO.: 166 LOSS_Generator: 7.548776626586914 LOSS_Discriminator: 0.07096096873283386\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.654150485992432 LOSS_Discriminator: 0.05148645490407944\n",
            "ITERATION_NO.: 168 LOSS_Generator: 7.062266826629639 LOSS_Discriminator: 0.06981511414051056\n",
            "ITERATION_NO.: 169 LOSS_Generator: 7.4062933921813965 LOSS_Discriminator: 0.05931810289621353\n",
            "ITERATION_NO.: 170 LOSS_Generator: 8.048855781555176 LOSS_Discriminator: 0.036279354244470596\n",
            "ITERATION_NO.: 171 LOSS_Generator: 8.114238739013672 LOSS_Discriminator: 0.07806135714054108\n",
            "ITERATION_NO.: 172 LOSS_Generator: 7.899087905883789 LOSS_Discriminator: 0.05779776722192764\n",
            "ITERATION_NO.: 173 LOSS_Generator: 7.57757568359375 LOSS_Discriminator: 0.05514756962656975\n",
            "ITERATION_NO.: 174 LOSS_Generator: 7.554121017456055 LOSS_Discriminator: 0.09751859307289124\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.321115016937256 LOSS_Discriminator: 0.08438417315483093\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.610142707824707 LOSS_Discriminator: 0.06135469675064087\n",
            "ITERATION_NO.: 177 LOSS_Generator: 7.413705348968506 LOSS_Discriminator: 0.06682813912630081\n",
            "ITERATION_NO.: 178 LOSS_Generator: 7.97805643081665 LOSS_Discriminator: 0.09136635065078735\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.983185291290283 LOSS_Discriminator: 0.06291691213846207\n",
            "ITERATION_NO.: 180 LOSS_Generator: 8.058752059936523 LOSS_Discriminator: 0.05297910422086716\n",
            "ITERATION_NO.: 181 LOSS_Generator: 7.8078718185424805 LOSS_Discriminator: 0.0518038272857666\n",
            "ITERATION_NO.: 182 LOSS_Generator: 7.084744453430176 LOSS_Discriminator: 0.022776778787374496\n",
            "ITERATION_NO.: 183 LOSS_Generator: 6.819261074066162 LOSS_Discriminator: 0.01870501972734928\n",
            "ITERATION_NO.: 184 LOSS_Generator: 6.838672637939453 LOSS_Discriminator: 0.06015877425670624\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.235107898712158 LOSS_Discriminator: 0.05602194741368294\n",
            "ITERATION_NO.: 186 LOSS_Generator: 7.390655040740967 LOSS_Discriminator: 0.09369412064552307\n",
            "ITERATION_NO.: 187 LOSS_Generator: 7.006626605987549 LOSS_Discriminator: 0.03232523053884506\n",
            "ITERATION_NO.: 188 LOSS_Generator: 7.698385238647461 LOSS_Discriminator: 0.07308496534824371\n",
            "ITERATION_NO.: 189 LOSS_Generator: 6.879834175109863 LOSS_Discriminator: 0.023718662559986115\n",
            "ITERATION_NO.: 190 LOSS_Generator: 6.794810771942139 LOSS_Discriminator: 0.07137350738048553\n",
            "ITERATION_NO.: 191 LOSS_Generator: 6.892435073852539 LOSS_Discriminator: 0.049979668110609055\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.7292799949646 LOSS_Discriminator: 0.05071404576301575\n",
            "ITERATION_NO.: 193 LOSS_Generator: 7.3148274421691895 LOSS_Discriminator: 0.067721888422966\n",
            "ITERATION_NO.: 194 LOSS_Generator: 7.10512638092041 LOSS_Discriminator: 0.06081023067235947\n",
            "ITERATION_NO.: 195 LOSS_Generator: 7.639723300933838 LOSS_Discriminator: 0.060032766312360764\n",
            "ITERATION_NO.: 196 LOSS_Generator: 7.7229180335998535 LOSS_Discriminator: 0.08298419415950775\n",
            "ITERATION_NO.: 197 LOSS_Generator: 7.679612636566162 LOSS_Discriminator: 0.09492368251085281\n",
            "ITERATION_NO.: 198 LOSS_Generator: 7.186028957366943 LOSS_Discriminator: 0.035122938454151154\n",
            "ITERATION_NO.: 199 LOSS_Generator: 6.2934112548828125 LOSS_Discriminator: 0.04517180100083351\n",
            "ITERATION_NO.: 200 LOSS_Generator: 7.130666732788086 LOSS_Discriminator: 0.03815971314907074\n",
            "ITERATION_NO.: 201 LOSS_Generator: 7.697603702545166 LOSS_Discriminator: 0.04291578754782677\n",
            "ITERATION_NO.: 202 LOSS_Generator: 8.280343055725098 LOSS_Discriminator: 0.0563279464840889\n",
            "ITERATION_NO.: 203 LOSS_Generator: 7.927700996398926 LOSS_Discriminator: 0.07145608961582184\n",
            "ITERATION_NO.: 204 LOSS_Generator: 8.119521141052246 LOSS_Discriminator: 0.11141052842140198\n",
            "ITERATION_NO.: 205 LOSS_Generator: 7.208256244659424 LOSS_Discriminator: 0.023195262998342514\n",
            "ITERATION_NO.: 206 LOSS_Generator: 7.457198619842529 LOSS_Discriminator: 0.04104088991880417\n",
            "ITERATION_NO.: 207 LOSS_Generator: 7.5275959968566895 LOSS_Discriminator: 0.13857398927211761\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.768367767333984 LOSS_Discriminator: 0.05870286375284195\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.08917236328125 LOSS_Discriminator: 0.018669405952095985\n",
            "ITERATION_NO.: 210 LOSS_Generator: 7.236333847045898 LOSS_Discriminator: 0.07247773557901382\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.587466239929199 LOSS_Discriminator: 0.06650611013174057\n",
            "ITERATION_NO.: 212 LOSS_Generator: 8.030460357666016 LOSS_Discriminator: 0.028663722798228264\n",
            "ITERATION_NO.: 213 LOSS_Generator: 7.58883810043335 LOSS_Discriminator: 0.1155104786157608\n",
            "ITERATION_NO.: 214 LOSS_Generator: 7.763227939605713 LOSS_Discriminator: 0.059826772660017014\n",
            "ITERATION_NO.: 215 LOSS_Generator: 8.156432151794434 LOSS_Discriminator: 0.05001620948314667\n",
            "ITERATION_NO.: 216 LOSS_Generator: 7.427885055541992 LOSS_Discriminator: 0.09095938503742218\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.721102237701416 LOSS_Discriminator: 0.11536408960819244\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.901463031768799 LOSS_Discriminator: 0.12142658233642578\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.321590423583984 LOSS_Discriminator: 0.07090840488672256\n",
            "ITERATION_NO.: 220 LOSS_Generator: 7.225222110748291 LOSS_Discriminator: 0.1556982398033142\n",
            "ITERATION_NO.: 221 LOSS_Generator: 7.373002529144287 LOSS_Discriminator: 0.05953940004110336\n",
            "ITERATION_NO.: 222 LOSS_Generator: 8.618637084960938 LOSS_Discriminator: 0.06933221220970154\n",
            "ITERATION_NO.: 223 LOSS_Generator: 8.063883781433105 LOSS_Discriminator: 0.14443227648735046\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.7951178550720215 LOSS_Discriminator: 0.046945925801992416\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.382648468017578 LOSS_Discriminator: 0.023256460204720497\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.83229923248291 LOSS_Discriminator: 0.06838967651128769\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.733102321624756 LOSS_Discriminator: 0.06603062152862549\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.883610725402832 LOSS_Discriminator: 0.12480657547712326\n",
            "ITERATION_NO.: 229 LOSS_Generator: 8.309937477111816 LOSS_Discriminator: 0.07294068485498428\n",
            "ITERATION_NO.: 230 LOSS_Generator: 8.610892295837402 LOSS_Discriminator: 0.036411382257938385\n",
            "ITERATION_NO.: 231 LOSS_Generator: 8.610452651977539 LOSS_Discriminator: 0.03486284241080284\n",
            "ITERATION_NO.: 232 LOSS_Generator: 8.387849807739258 LOSS_Discriminator: 0.058990683406591415\n",
            "ITERATION_NO.: 233 LOSS_Generator: 7.562555313110352 LOSS_Discriminator: 0.09068377315998077\n",
            "ITERATION_NO.: 234 LOSS_Generator: 6.173195838928223 LOSS_Discriminator: 0.06325595080852509\n",
            "ITERATION_NO.: 235 LOSS_Generator: 5.2432684898376465 LOSS_Discriminator: 0.055142682045698166\n",
            "ITERATION_NO.: 236 LOSS_Generator: 6.033018112182617 LOSS_Discriminator: 0.09961119294166565\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.3271260261535645 LOSS_Discriminator: 0.08697303384542465\n",
            "ITERATION_NO.: 238 LOSS_Generator: 8.932458877563477 LOSS_Discriminator: 0.01630530320107937\n",
            "ITERATION_NO.: 239 LOSS_Generator: 9.426155090332031 LOSS_Discriminator: 0.06894953548908234\n",
            "ITERATION_NO.: 240 LOSS_Generator: 9.258703231811523 LOSS_Discriminator: 0.123386912047863\n",
            "ITERATION_NO.: 241 LOSS_Generator: 8.275507926940918 LOSS_Discriminator: 0.12768614292144775\n",
            "ITERATION_NO.: 242 LOSS_Generator: 7.292413711547852 LOSS_Discriminator: 0.04883315786719322\n",
            "ITERATION_NO.: 243 LOSS_Generator: 7.190780162811279 LOSS_Discriminator: 0.03810400143265724\n",
            "ITERATION_NO.: 244 LOSS_Generator: 6.445462703704834 LOSS_Discriminator: 0.043207425624132156\n",
            "ITERATION_NO.: 245 LOSS_Generator: 7.241224765777588 LOSS_Discriminator: 0.04496046155691147\n",
            "ITERATION_NO.: 246 LOSS_Generator: 7.849008560180664 LOSS_Discriminator: 0.041470978409051895\n",
            "ITERATION_NO.: 247 LOSS_Generator: 8.410357475280762 LOSS_Discriminator: 0.06637996435165405\n",
            "ITERATION_NO.: 248 LOSS_Generator: 8.15402603149414 LOSS_Discriminator: 0.06493857502937317\n",
            "ITERATION_NO.: 249 LOSS_Generator: 8.357978820800781 LOSS_Discriminator: 0.027401652187108994\n",
            "ITERATION_NO.: 250 LOSS_Generator: 8.591078758239746 LOSS_Discriminator: 0.02732778713107109\n",
            "ITERATION_NO.: 251 LOSS_Generator: 8.489940643310547 LOSS_Discriminator: 0.047955118119716644\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.61435079574585 LOSS_Discriminator: 0.03789148107171059\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.8769755363464355 LOSS_Discriminator: 0.029241779819130898\n",
            "ITERATION_NO.: 254 LOSS_Generator: 7.3006391525268555 LOSS_Discriminator: 0.04766412451863289\n",
            "ITERATION_NO.: 255 LOSS_Generator: 7.393979072570801 LOSS_Discriminator: 0.03718927130103111\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.290441989898682 LOSS_Discriminator: 0.058314405381679535\n",
            "ITERATION_NO.: 257 LOSS_Generator: 7.371091842651367 LOSS_Discriminator: 0.07580873370170593\n",
            "ITERATION_NO.: 258 LOSS_Generator: 7.559426784515381 LOSS_Discriminator: 0.08409054577350616\n",
            "ITERATION_NO.: 259 LOSS_Generator: 7.6929521560668945 LOSS_Discriminator: 0.08395999670028687\n",
            "ITERATION_NO.: 260 LOSS_Generator: 7.146451473236084 LOSS_Discriminator: 0.09739387780427933\n",
            "ITERATION_NO.: 261 LOSS_Generator: 7.366344451904297 LOSS_Discriminator: 0.038613103330135345\n",
            "ITERATION_NO.: 262 LOSS_Generator: 7.223850250244141 LOSS_Discriminator: 0.017189549282193184\n",
            "ITERATION_NO.: 263 LOSS_Generator: 7.969710826873779 LOSS_Discriminator: 0.09016114473342896\n",
            "ITERATION_NO.: 264 LOSS_Generator: 8.059070587158203 LOSS_Discriminator: 0.04670589417219162\n",
            "ITERATION_NO.: 265 LOSS_Generator: 7.731746673583984 LOSS_Discriminator: 0.056442271918058395\n",
            "ITERATION_NO.: 266 LOSS_Generator: 8.035840034484863 LOSS_Discriminator: 0.08140583336353302\n",
            "ITERATION_NO.: 267 LOSS_Generator: 8.152626991271973 LOSS_Discriminator: 0.05157073587179184\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.151374340057373 LOSS_Discriminator: 0.08030068874359131\n",
            "ITERATION_NO.: 269 LOSS_Generator: 7.268896579742432 LOSS_Discriminator: 0.1485050469636917\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.144204139709473 LOSS_Discriminator: 0.05111122876405716\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.068885326385498 LOSS_Discriminator: 0.06613205373287201\n",
            "ITERATION_NO.: 272 LOSS_Generator: 6.605815410614014 LOSS_Discriminator: 0.06957592070102692\n",
            "ITERATION_NO.: 273 LOSS_Generator: 6.627482891082764 LOSS_Discriminator: 0.06119594722986221\n",
            "ITERATION_NO.: 274 LOSS_Generator: 6.564582347869873 LOSS_Discriminator: 0.08581807464361191\n",
            "ITERATION_NO.: 275 LOSS_Generator: 6.561722278594971 LOSS_Discriminator: 0.03851354122161865\n",
            "ITERATION_NO.: 276 LOSS_Generator: 7.921806812286377 LOSS_Discriminator: 0.08815252780914307\n",
            "ITERATION_NO.: 277 LOSS_Generator: 8.145299911499023 LOSS_Discriminator: 0.038787513971328735\n",
            "ITERATION_NO.: 278 LOSS_Generator: 8.352716445922852 LOSS_Discriminator: 0.12254231423139572\n",
            "ITERATION_NO.: 279 LOSS_Generator: 7.724969863891602 LOSS_Discriminator: 0.02099473774433136\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.622211456298828 LOSS_Discriminator: 0.08450646698474884\n",
            "ITERATION_NO.: 281 LOSS_Generator: 6.9198126792907715 LOSS_Discriminator: 0.06239146366715431\n",
            "ITERATION_NO.: 282 LOSS_Generator: 6.9296650886535645 LOSS_Discriminator: 0.025988619774580002\n",
            "ITERATION_NO.: 283 LOSS_Generator: 7.456712245941162 LOSS_Discriminator: 0.06745918095111847\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.7433881759643555 LOSS_Discriminator: 0.06396850943565369\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.5563859939575195 LOSS_Discriminator: 0.07297197729349136\n",
            "ITERATION_NO.: 286 LOSS_Generator: 7.227914810180664 LOSS_Discriminator: 0.10436534136533737\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.6369452476501465 LOSS_Discriminator: 0.02187369205057621\n",
            "ITERATION_NO.: 288 LOSS_Generator: 6.571514129638672 LOSS_Discriminator: 0.08408750593662262\n",
            "ITERATION_NO.: 289 LOSS_Generator: 6.822893142700195 LOSS_Discriminator: 0.05666394531726837\n",
            "ITERATION_NO.: 290 LOSS_Generator: 7.533837795257568 LOSS_Discriminator: 0.06095504015684128\n",
            "ITERATION_NO.: 291 LOSS_Generator: 8.192220687866211 LOSS_Discriminator: 0.04033524543046951\n",
            "ITERATION_NO.: 292 LOSS_Generator: 8.424577713012695 LOSS_Discriminator: 0.040323399007320404\n",
            "ITERATION_NO.: 293 LOSS_Generator: 7.846208572387695 LOSS_Discriminator: 0.11190944910049438\n",
            "ITERATION_NO.: 294 LOSS_Generator: 7.036630153656006 LOSS_Discriminator: 0.11904372274875641\n",
            "ITERATION_NO.: 295 LOSS_Generator: 5.966062545776367 LOSS_Discriminator: 0.05509775131940842\n",
            "ITERATION_NO.: 296 LOSS_Generator: 6.68374490737915 LOSS_Discriminator: 0.08469097316265106\n",
            "ITERATION_NO.: 297 LOSS_Generator: 7.4729437828063965 LOSS_Discriminator: 0.024832986295223236\n",
            "ITERATION_NO.: 298 LOSS_Generator: 7.911172389984131 LOSS_Discriminator: 0.07514956593513489\n",
            "ITERATION_NO.: 299 LOSS_Generator: 8.333009719848633 LOSS_Discriminator: 0.056222960352897644\n",
            "ITERATION_NO.: 300 LOSS_Generator: 8.392472267150879 LOSS_Discriminator: 0.02018401399254799\n",
            "ITERATION_NO.: 301 LOSS_Generator: 7.654557704925537 LOSS_Discriminator: 0.05992686748504639\n",
            "ITERATION_NO.: 302 LOSS_Generator: 7.014272689819336 LOSS_Discriminator: 0.08108589053153992\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.274062633514404 LOSS_Discriminator: 0.043591879308223724\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.175589561462402 LOSS_Discriminator: 0.04363038390874863\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.214732646942139 LOSS_Discriminator: 0.08784983307123184\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.606400966644287 LOSS_Discriminator: 0.04265781119465828\n",
            "ITERATION_NO.: 307 LOSS_Generator: 8.687394142150879 LOSS_Discriminator: 0.030385542660951614\n",
            "ITERATION_NO.: 308 LOSS_Generator: 8.727055549621582 LOSS_Discriminator: 0.0796414315700531\n",
            "ITERATION_NO.: 309 LOSS_Generator: 7.656416416168213 LOSS_Discriminator: 0.09489050507545471\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.9591474533081055 LOSS_Discriminator: 0.08745861053466797\n",
            "ITERATION_NO.: 311 LOSS_Generator: 5.732208251953125 LOSS_Discriminator: 0.08359900116920471\n",
            "ITERATION_NO.: 312 LOSS_Generator: 5.885500907897949 LOSS_Discriminator: 0.13369262218475342\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.343640327453613 LOSS_Discriminator: 0.07096804678440094\n",
            "ITERATION_NO.: 314 LOSS_Generator: 7.95594596862793 LOSS_Discriminator: 0.05311291292309761\n",
            "ITERATION_NO.: 315 LOSS_Generator: 8.26797103881836 LOSS_Discriminator: 0.08386999368667603\n",
            "ITERATION_NO.: 316 LOSS_Generator: 8.231701850891113 LOSS_Discriminator: 0.03072819858789444\n",
            "ITERATION_NO.: 317 LOSS_Generator: 8.241889953613281 LOSS_Discriminator: 0.08677633106708527\n",
            "ITERATION_NO.: 318 LOSS_Generator: 7.470685005187988 LOSS_Discriminator: 0.08768703043460846\n",
            "ITERATION_NO.: 319 LOSS_Generator: 6.747506618499756 LOSS_Discriminator: 0.041265103965997696\n",
            "ITERATION_NO.: 320 LOSS_Generator: 6.534536838531494 LOSS_Discriminator: 0.06897944957017899\n",
            "ITERATION_NO.: 321 LOSS_Generator: 6.978145122528076 LOSS_Discriminator: 0.07489588111639023\n",
            "ITERATION_NO.: 322 LOSS_Generator: 6.784145355224609 LOSS_Discriminator: 0.09287592768669128\n",
            "ITERATION_NO.: 323 LOSS_Generator: 7.641834735870361 LOSS_Discriminator: 0.0324675589799881\n",
            "ITERATION_NO.: 324 LOSS_Generator: 8.571673393249512 LOSS_Discriminator: 0.01565897837281227\n",
            "ITERATION_NO.: 325 LOSS_Generator: 9.242462158203125 LOSS_Discriminator: 0.07272806763648987\n",
            "ITERATION_NO.: 326 LOSS_Generator: 8.44607925415039 LOSS_Discriminator: 0.05949884280562401\n",
            "ITERATION_NO.: 327 LOSS_Generator: 7.956765174865723 LOSS_Discriminator: 0.12673789262771606\n",
            "ITERATION_NO.: 328 LOSS_Generator: 7.01045036315918 LOSS_Discriminator: 0.051810842007398605\n",
            "ITERATION_NO.: 329 LOSS_Generator: 6.867999076843262 LOSS_Discriminator: 0.03741556406021118\n",
            "ITERATION_NO.: 330 LOSS_Generator: 6.834794998168945 LOSS_Discriminator: 0.05708114802837372\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.128780364990234 LOSS_Discriminator: 0.0154838552698493\n",
            "ITERATION_NO.: 332 LOSS_Generator: 7.5142621994018555 LOSS_Discriminator: 0.058092594146728516\n",
            "ITERATION_NO.: 333 LOSS_Generator: 8.013705253601074 LOSS_Discriminator: 0.05518926680088043\n",
            "ITERATION_NO.: 334 LOSS_Generator: 8.212272644042969 LOSS_Discriminator: 0.045677658170461655\n",
            "ITERATION_NO.: 335 LOSS_Generator: 7.773693084716797 LOSS_Discriminator: 0.07411755621433258\n",
            "ITERATION_NO.: 336 LOSS_Generator: 7.702142715454102 LOSS_Discriminator: 0.01408052071928978\n",
            "ITERATION_NO.: 337 LOSS_Generator: 7.1000542640686035 LOSS_Discriminator: 0.06222293898463249\n",
            "ITERATION_NO.: 338 LOSS_Generator: 7.182220458984375 LOSS_Discriminator: 0.04694095626473427\n",
            "ITERATION_NO.: 339 LOSS_Generator: 7.682405471801758 LOSS_Discriminator: 0.041724130511283875\n",
            "ITERATION_NO.: 340 LOSS_Generator: 8.562880516052246 LOSS_Discriminator: 0.09030181914567947\n",
            "ITERATION_NO.: 341 LOSS_Generator: 7.130211353302002 LOSS_Discriminator: 0.04736781120300293\n",
            "ITERATION_NO.: 342 LOSS_Generator: 7.3923773765563965 LOSS_Discriminator: 0.04135531932115555\n",
            "ITERATION_NO.: 343 LOSS_Generator: 7.2891645431518555 LOSS_Discriminator: 0.02461802214384079\n",
            "ITERATION_NO.: 344 LOSS_Generator: 7.621028423309326 LOSS_Discriminator: 0.06550104171037674\n",
            "ITERATION_NO.: 345 LOSS_Generator: 8.326227188110352 LOSS_Discriminator: 0.02517799660563469\n",
            "ITERATION_NO.: 346 LOSS_Generator: 8.711061477661133 LOSS_Discriminator: 0.019496941938996315\n",
            "ITERATION_NO.: 347 LOSS_Generator: 8.412274360656738 LOSS_Discriminator: 0.09758369624614716\n",
            "ITERATION_NO.: 348 LOSS_Generator: 7.569259166717529 LOSS_Discriminator: 0.0715002566576004\n",
            "ITERATION_NO.: 349 LOSS_Generator: 6.953434944152832 LOSS_Discriminator: 0.042267948389053345\n",
            "ITERATION_NO.: 350 LOSS_Generator: 7.558286666870117 LOSS_Discriminator: 0.07457694411277771\n",
            "ITERATION_NO.: 351 LOSS_Generator: 7.293057918548584 LOSS_Discriminator: 0.03208620101213455\n",
            "ITERATION_NO.: 352 LOSS_Generator: 8.188458442687988 LOSS_Discriminator: 0.05916278809309006\n",
            "ITERATION_NO.: 353 LOSS_Generator: 7.86838436126709 LOSS_Discriminator: 0.06836296617984772\n",
            "ITERATION_NO.: 354 LOSS_Generator: 7.315782070159912 LOSS_Discriminator: 0.10343867540359497\n",
            "ITERATION_NO.: 355 LOSS_Generator: 6.242712497711182 LOSS_Discriminator: 0.06485156714916229\n",
            "ITERATION_NO.: 356 LOSS_Generator: 6.622491836547852 LOSS_Discriminator: 0.11301300674676895\n",
            "ITERATION_NO.: 357 LOSS_Generator: 6.667197227478027 LOSS_Discriminator: 0.05058278515934944\n",
            "ITERATION_NO.: 358 LOSS_Generator: 7.270561695098877 LOSS_Discriminator: 0.06826257705688477\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.650747299194336 LOSS_Discriminator: 0.07435618340969086\n",
            "ITERATION_NO.: 360 LOSS_Generator: 7.321325778961182 LOSS_Discriminator: 0.031985290348529816\n",
            "ITERATION_NO.: 361 LOSS_Generator: 8.328739166259766 LOSS_Discriminator: 0.04602883383631706\n",
            "ITERATION_NO.: 362 LOSS_Generator: 8.521798133850098 LOSS_Discriminator: 0.04154513031244278\n",
            "ITERATION_NO.: 363 LOSS_Generator: 8.26907730102539 LOSS_Discriminator: 0.08802918344736099\n",
            "ITERATION_NO.: 364 LOSS_Generator: 8.565185546875 LOSS_Discriminator: 0.04490310698747635\n",
            "ITERATION_NO.: 365 LOSS_Generator: 7.291696548461914 LOSS_Discriminator: 0.10623060166835785\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.22946834564209 LOSS_Discriminator: 0.11736489832401276\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.397587776184082 LOSS_Discriminator: 0.13017627596855164\n",
            "ITERATION_NO.: 368 LOSS_Generator: 8.298611640930176 LOSS_Discriminator: 0.10355886071920395\n",
            "ITERATION_NO.: 369 LOSS_Generator: 9.585737228393555 LOSS_Discriminator: 0.050642386078834534\n",
            "ITERATION_NO.: 370 LOSS_Generator: 10.095191955566406 LOSS_Discriminator: 0.10881078243255615\n",
            "ITERATION_NO.: 371 LOSS_Generator: 9.781929016113281 LOSS_Discriminator: 0.10962699353694916\n",
            "ITERATION_NO.: 372 LOSS_Generator: 8.2789888381958 LOSS_Discriminator: 0.17473702132701874\n",
            "ITERATION_NO.: 373 LOSS_Generator: 7.033178806304932 LOSS_Discriminator: 0.049613792449235916\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.3523268699646 LOSS_Discriminator: 0.12812215089797974\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.466652870178223 LOSS_Discriminator: 0.10495211184024811\n",
            "ITERATION_NO.: 376 LOSS_Generator: 7.355681896209717 LOSS_Discriminator: 0.0712948739528656\n",
            "ITERATION_NO.: 377 LOSS_Generator: 8.477356910705566 LOSS_Discriminator: 0.03159163147211075\n",
            "ITERATION_NO.: 378 LOSS_Generator: 8.167530059814453 LOSS_Discriminator: 0.029048960655927658\n",
            "ITERATION_NO.: 379 LOSS_Generator: 8.576066017150879 LOSS_Discriminator: 0.1270836889743805\n",
            "ITERATION_NO.: 380 LOSS_Generator: 8.258838653564453 LOSS_Discriminator: 0.07828505337238312\n",
            "ITERATION_NO.: 381 LOSS_Generator: 7.647924900054932 LOSS_Discriminator: 0.05401337519288063\n",
            "ITERATION_NO.: 382 LOSS_Generator: 7.6520609855651855 LOSS_Discriminator: 0.06334833800792694\n",
            "ITERATION_NO.: 383 LOSS_Generator: 7.366481781005859 LOSS_Discriminator: 0.062110524624586105\n",
            "ITERATION_NO.: 384 LOSS_Generator: 8.294837951660156 LOSS_Discriminator: 0.07024169713258743\n",
            "ITERATION_NO.: 385 LOSS_Generator: 8.531091690063477 LOSS_Discriminator: 0.029342513531446457\n",
            "ITERATION_NO.: 386 LOSS_Generator: 9.735610008239746 LOSS_Discriminator: 0.05715010315179825\n",
            "ITERATION_NO.: 387 LOSS_Generator: 9.051431655883789 LOSS_Discriminator: 0.12655103206634521\n",
            "ITERATION_NO.: 388 LOSS_Generator: 8.009334564208984 LOSS_Discriminator: 0.06840889900922775\n",
            "ITERATION_NO.: 389 LOSS_Generator: 7.695352554321289 LOSS_Discriminator: 0.035075001418590546\n",
            "ITERATION_NO.: 390 LOSS_Generator: 7.875000476837158 LOSS_Discriminator: 0.07580393552780151\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.9820122718811035 LOSS_Discriminator: 0.06877864897251129\n",
            "ITERATION_NO.: 392 LOSS_Generator: 6.532817363739014 LOSS_Discriminator: 0.07771112024784088\n",
            "ITERATION_NO.: 393 LOSS_Generator: 7.498145580291748 LOSS_Discriminator: 0.09197892993688583\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.328865051269531 LOSS_Discriminator: 0.05695090442895889\n",
            "ITERATION_NO.: 395 LOSS_Generator: 8.903550148010254 LOSS_Discriminator: 0.030687708407640457\n",
            "ITERATION_NO.: 396 LOSS_Generator: 9.173437118530273 LOSS_Discriminator: 0.1357448846101761\n",
            "ITERATION_NO.: 397 LOSS_Generator: 7.666046619415283 LOSS_Discriminator: 0.1432293951511383\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.789326190948486 LOSS_Discriminator: 0.07280044257640839\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.754861354827881 LOSS_Discriminator: 0.062372658401727676\n",
            "ITERATION_NO.: 400 LOSS_Generator: 6.3322906494140625 LOSS_Discriminator: 0.05398574471473694\n",
            "ITERATION_NO.: 401 LOSS_Generator: 7.791660308837891 LOSS_Discriminator: 0.056272998452186584\n",
            "ITERATION_NO.: 402 LOSS_Generator: 8.58190631866455 LOSS_Discriminator: 0.10752244293689728\n",
            "ITERATION_NO.: 403 LOSS_Generator: 8.843694686889648 LOSS_Discriminator: 0.09280896186828613\n",
            "ITERATION_NO.: 404 LOSS_Generator: 8.140149116516113 LOSS_Discriminator: 0.06560724973678589\n",
            "ITERATION_NO.: 405 LOSS_Generator: 7.495548725128174 LOSS_Discriminator: 0.06111106276512146\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.982112407684326 LOSS_Discriminator: 0.04735390469431877\n",
            "ITERATION_NO.: 407 LOSS_Generator: 6.663984298706055 LOSS_Discriminator: 0.11274413764476776\n",
            "ITERATION_NO.: 408 LOSS_Generator: 6.7029709815979 LOSS_Discriminator: 0.09009426832199097\n",
            "ITERATION_NO.: 409 LOSS_Generator: 7.500813007354736 LOSS_Discriminator: 0.09340182691812515\n",
            "ITERATION_NO.: 410 LOSS_Generator: 7.540677070617676 LOSS_Discriminator: 0.049935076385736465\n",
            "ITERATION_NO.: 411 LOSS_Generator: 7.449351787567139 LOSS_Discriminator: 0.09316916763782501\n",
            "ITERATION_NO.: 412 LOSS_Generator: 7.4193878173828125 LOSS_Discriminator: 0.06796393543481827\n",
            "ITERATION_NO.: 413 LOSS_Generator: 6.931991100311279 LOSS_Discriminator: 0.054065998643636703\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.536909103393555 LOSS_Discriminator: 0.04015533998608589\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.26882791519165 LOSS_Discriminator: 0.03693392872810364\n",
            "ITERATION_NO.: 416 LOSS_Generator: 6.6797871589660645 LOSS_Discriminator: 0.05677028000354767\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.173129081726074 LOSS_Discriminator: 0.06230679154396057\n",
            "ITERATION_NO.: 418 LOSS_Generator: 7.724395751953125 LOSS_Discriminator: 0.05373525619506836\n",
            "ITERATION_NO.: 419 LOSS_Generator: 8.419859886169434 LOSS_Discriminator: 0.05178884416818619\n",
            "ITERATION_NO.: 420 LOSS_Generator: 7.371135234832764 LOSS_Discriminator: 0.09594149887561798\n",
            "ITERATION_NO.: 421 LOSS_Generator: 6.607497692108154 LOSS_Discriminator: 0.05213245749473572\n",
            "ITERATION_NO.: 422 LOSS_Generator: 6.679412841796875 LOSS_Discriminator: 0.09522736072540283\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.810309886932373 LOSS_Discriminator: 0.07859741151332855\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.405930042266846 LOSS_Discriminator: 0.028194136917591095\n",
            "ITERATION_NO.: 425 LOSS_Generator: 7.889120578765869 LOSS_Discriminator: 0.026037055999040604\n",
            "ITERATION_NO.: 426 LOSS_Generator: 8.50271987915039 LOSS_Discriminator: 0.019307158887386322\n",
            "ITERATION_NO.: 427 LOSS_Generator: 8.417646408081055 LOSS_Discriminator: 0.06306329369544983\n",
            "ITERATION_NO.: 428 LOSS_Generator: 6.820065975189209 LOSS_Discriminator: 0.05998222157359123\n",
            "ITERATION_NO.: 429 LOSS_Generator: 7.003904819488525 LOSS_Discriminator: 0.04890080541372299\n",
            "ITERATION_NO.: 430 LOSS_Generator: 6.402883529663086 LOSS_Discriminator: 0.06337372958660126\n",
            "ITERATION_NO.: 431 LOSS_Generator: 6.724949359893799 LOSS_Discriminator: 0.04971043020486832\n",
            "ITERATION_NO.: 432 LOSS_Generator: 7.6219964027404785 LOSS_Discriminator: 0.04643455520272255\n",
            "ITERATION_NO.: 433 LOSS_Generator: 7.657891273498535 LOSS_Discriminator: 0.06300153583288193\n",
            "ITERATION_NO.: 434 LOSS_Generator: 7.4857563972473145 LOSS_Discriminator: 0.06911101192235947\n",
            "ITERATION_NO.: 435 LOSS_Generator: 7.220449924468994 LOSS_Discriminator: 0.07043140381574631\n",
            "ITERATION_NO.: 436 LOSS_Generator: 7.086122035980225 LOSS_Discriminator: 0.04606678709387779\n",
            "ITERATION_NO.: 437 LOSS_Generator: 7.037846088409424 LOSS_Discriminator: 0.06951983273029327\n",
            "ITERATION_NO.: 438 LOSS_Generator: 7.732388496398926 LOSS_Discriminator: 0.10907276719808578\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.862265586853027 LOSS_Discriminator: 0.12142869085073471\n",
            "ITERATION_NO.: 440 LOSS_Generator: 7.8508830070495605 LOSS_Discriminator: 0.1042347252368927\n",
            "ITERATION_NO.: 441 LOSS_Generator: 6.962021350860596 LOSS_Discriminator: 0.06870441138744354\n",
            "ITERATION_NO.: 442 LOSS_Generator: 6.703887462615967 LOSS_Discriminator: 0.06102468818426132\n",
            "ITERATION_NO.: 443 LOSS_Generator: 6.812249183654785 LOSS_Discriminator: 0.12223175913095474\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.606597423553467 LOSS_Discriminator: 0.049142420291900635\n",
            "ITERATION_NO.: 445 LOSS_Generator: 8.450647354125977 LOSS_Discriminator: 0.11778633296489716\n",
            "ITERATION_NO.: 446 LOSS_Generator: 7.774487495422363 LOSS_Discriminator: 0.11488185077905655\n",
            "ITERATION_NO.: 447 LOSS_Generator: 7.089178562164307 LOSS_Discriminator: 0.11733587831258774\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.507055282592773 LOSS_Discriminator: 0.08111464977264404\n",
            "ITERATION_NO.: 449 LOSS_Generator: 6.040902137756348 LOSS_Discriminator: 0.11420111358165741\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.4388427734375 LOSS_Discriminator: 0.10253049433231354\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.923937797546387 LOSS_Discriminator: 0.03807627409696579\n",
            "ITERATION_NO.: 452 LOSS_Generator: 9.004805564880371 LOSS_Discriminator: 0.06694330275058746\n",
            "ITERATION_NO.: 453 LOSS_Generator: 9.181720733642578 LOSS_Discriminator: 0.052432116121053696\n",
            "ITERATION_NO.: 454 LOSS_Generator: 8.527710914611816 LOSS_Discriminator: 0.06160639226436615\n",
            "ITERATION_NO.: 455 LOSS_Generator: 7.586162567138672 LOSS_Discriminator: 0.09090349078178406\n",
            "ITERATION_NO.: 456 LOSS_Generator: 7.245065689086914 LOSS_Discriminator: 0.08054620027542114\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.052380561828613 LOSS_Discriminator: 0.048858508467674255\n",
            "ITERATION_NO.: 458 LOSS_Generator: 6.51519775390625 LOSS_Discriminator: 0.0889660120010376\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.059333324432373 LOSS_Discriminator: 0.10647574067115784\n",
            "ITERATION_NO.: 460 LOSS_Generator: 7.442195892333984 LOSS_Discriminator: 0.09490201622247696\n",
            "ITERATION_NO.: 461 LOSS_Generator: 8.339804649353027 LOSS_Discriminator: 0.03849557787179947\n",
            "ITERATION_NO.: 462 LOSS_Generator: 8.513809204101562 LOSS_Discriminator: 0.029093168675899506\n",
            "ITERATION_NO.: 463 LOSS_Generator: 8.19934368133545 LOSS_Discriminator: 0.11811402440071106\n",
            "ITERATION_NO.: 464 LOSS_Generator: 6.928058624267578 LOSS_Discriminator: 0.029746687039732933\n",
            "ITERATION_NO.: 465 LOSS_Generator: 6.0990777015686035 LOSS_Discriminator: 0.05891376733779907\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.4624409675598145 LOSS_Discriminator: 0.0579850971698761\n",
            "ITERATION_NO.: 467 LOSS_Generator: 7.464900493621826 LOSS_Discriminator: 0.0704532340168953\n",
            "ITERATION_NO.: 468 LOSS_Generator: 8.366023063659668 LOSS_Discriminator: 0.06051367521286011\n",
            "ITERATION_NO.: 469 LOSS_Generator: 8.188983917236328 LOSS_Discriminator: 0.04892493039369583\n",
            "ITERATION_NO.: 470 LOSS_Generator: 8.200729370117188 LOSS_Discriminator: 0.06001468747854233\n",
            "ITERATION_NO.: 471 LOSS_Generator: 7.9735565185546875 LOSS_Discriminator: 0.12039875984191895\n",
            "ITERATION_NO.: 472 LOSS_Generator: 7.3039655685424805 LOSS_Discriminator: 0.041109636425971985\n",
            "ITERATION_NO.: 473 LOSS_Generator: 6.5532636642456055 LOSS_Discriminator: 0.07158660888671875\n",
            "ITERATION_NO.: 474 LOSS_Generator: 6.046898365020752 LOSS_Discriminator: 0.14344409108161926\n",
            "ITERATION_NO.: 475 LOSS_Generator: 6.8773016929626465 LOSS_Discriminator: 0.11985796689987183\n",
            "ITERATION_NO.: 476 LOSS_Generator: 7.3777756690979 LOSS_Discriminator: 0.09935620427131653\n",
            "ITERATION_NO.: 477 LOSS_Generator: 8.016160011291504 LOSS_Discriminator: 0.029957281425595284\n",
            "ITERATION_NO.: 478 LOSS_Generator: 8.593825340270996 LOSS_Discriminator: 0.08190588653087616\n",
            "ITERATION_NO.: 479 LOSS_Generator: 9.107752799987793 LOSS_Discriminator: 0.058618396520614624\n",
            "ITERATION_NO.: 480 LOSS_Generator: 7.863499641418457 LOSS_Discriminator: 0.10859446227550507\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.804455757141113 LOSS_Discriminator: 0.06797891110181808\n",
            "ITERATION_NO.: 482 LOSS_Generator: 5.5238938331604 LOSS_Discriminator: 0.10888002812862396\n",
            "ITERATION_NO.: 483 LOSS_Generator: 5.6667938232421875 LOSS_Discriminator: 0.09181027859449387\n",
            "ITERATION_NO.: 484 LOSS_Generator: 6.4472784996032715 LOSS_Discriminator: 0.11752244830131531\n",
            "ITERATION_NO.: 485 LOSS_Generator: 7.768146991729736 LOSS_Discriminator: 0.07078487426042557\n",
            "ITERATION_NO.: 486 LOSS_Generator: 9.48450756072998 LOSS_Discriminator: 0.08734875917434692\n",
            "ITERATION_NO.: 487 LOSS_Generator: 9.079517364501953 LOSS_Discriminator: 0.12735618650913239\n",
            "ITERATION_NO.: 488 LOSS_Generator: 8.208321571350098 LOSS_Discriminator: 0.061924390494823456\n",
            "ITERATION_NO.: 489 LOSS_Generator: 7.9946513175964355 LOSS_Discriminator: 0.031384412199258804\n",
            "ITERATION_NO.: 490 LOSS_Generator: 6.643502235412598 LOSS_Discriminator: 0.10998985171318054\n",
            "ITERATION_NO.: 491 LOSS_Generator: 7.045457363128662 LOSS_Discriminator: 0.05328608304262161\n",
            "ITERATION_NO.: 492 LOSS_Generator: 7.251562118530273 LOSS_Discriminator: 0.07456117123365402\n",
            "ITERATION_NO.: 493 LOSS_Generator: 7.980301380157471 LOSS_Discriminator: 0.03321271389722824\n",
            "ITERATION_NO.: 494 LOSS_Generator: 7.7408881187438965 LOSS_Discriminator: 0.06119073927402496\n",
            "ITERATION_NO.: 495 LOSS_Generator: 7.875723361968994 LOSS_Discriminator: 0.06668861955404282\n",
            "ITERATION_NO.: 496 LOSS_Generator: 7.549380302429199 LOSS_Discriminator: 0.08239594846963882\n",
            "ITERATION_NO.: 497 LOSS_Generator: 6.635501861572266 LOSS_Discriminator: 0.12825314700603485\n",
            "ITERATION_NO.: 498 LOSS_Generator: 6.235052108764648 LOSS_Discriminator: 0.04369470849633217\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.3416852951049805 LOSS_Discriminator: 0.0649549588561058\n",
            "ITERATION_NO.: 500 LOSS_Generator: 9.015817642211914 LOSS_Discriminator: 0.0387113094329834\n",
            "ITERATION_NO.: 501 LOSS_Generator: 9.428566932678223 LOSS_Discriminator: 0.06916394829750061\n",
            "ITERATION_NO.: 502 LOSS_Generator: 9.019789695739746 LOSS_Discriminator: 0.03209935873746872\n",
            "ITERATION_NO.: 503 LOSS_Generator: 8.620532035827637 LOSS_Discriminator: 0.10522778332233429\n",
            "ITERATION_NO.: 504 LOSS_Generator: 8.310026168823242 LOSS_Discriminator: 0.08325661718845367\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.235340118408203 LOSS_Discriminator: 0.056012071669101715\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.308620452880859 LOSS_Discriminator: 0.07006850838661194\n",
            "ITERATION_NO.: 507 LOSS_Generator: 6.1410722732543945 LOSS_Discriminator: 0.042689405381679535\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.867867946624756 LOSS_Discriminator: 0.04599933326244354\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.202639102935791 LOSS_Discriminator: 0.05872766673564911\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.75048828125 LOSS_Discriminator: 0.07274511456489563\n",
            "ITERATION_NO.: 511 LOSS_Generator: 7.807285308837891 LOSS_Discriminator: 0.07404216378927231\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.910193920135498 LOSS_Discriminator: 0.0473596565425396\n",
            "ITERATION_NO.: 513 LOSS_Generator: 8.028989791870117 LOSS_Discriminator: 0.015313753858208656\n",
            "ITERATION_NO.: 514 LOSS_Generator: 7.363828659057617 LOSS_Discriminator: 0.07475312054157257\n",
            "ITERATION_NO.: 515 LOSS_Generator: 6.348163604736328 LOSS_Discriminator: 0.04743461683392525\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.080016613006592 LOSS_Discriminator: 0.09065189957618713\n",
            "ITERATION_NO.: 517 LOSS_Generator: 6.289525032043457 LOSS_Discriminator: 0.08258387446403503\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.432365894317627 LOSS_Discriminator: 0.03419133275747299\n",
            "ITERATION_NO.: 519 LOSS_Generator: 8.305704116821289 LOSS_Discriminator: 0.08155371248722076\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.741133213043213 LOSS_Discriminator: 0.1391078382730484\n",
            "ITERATION_NO.: 521 LOSS_Generator: 6.6893229484558105 LOSS_Discriminator: 0.07120022922754288\n",
            "ITERATION_NO.: 522 LOSS_Generator: 5.977718353271484 LOSS_Discriminator: 0.14008945226669312\n",
            "ITERATION_NO.: 523 LOSS_Generator: 5.912649154663086 LOSS_Discriminator: 0.09606631100177765\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.32711124420166 LOSS_Discriminator: 0.08860383927822113\n",
            "ITERATION_NO.: 525 LOSS_Generator: 7.562737941741943 LOSS_Discriminator: 0.034631166607141495\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.569929122924805 LOSS_Discriminator: 0.06472744047641754\n",
            "ITERATION_NO.: 527 LOSS_Generator: 7.978053092956543 LOSS_Discriminator: 0.056969400495290756\n",
            "ITERATION_NO.: 528 LOSS_Generator: 8.652599334716797 LOSS_Discriminator: 0.03261512517929077\n",
            "ITERATION_NO.: 529 LOSS_Generator: 8.213850021362305 LOSS_Discriminator: 0.09926605224609375\n",
            "ITERATION_NO.: 530 LOSS_Generator: 8.23585319519043 LOSS_Discriminator: 0.011988029815256596\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.378815174102783 LOSS_Discriminator: 0.09958389401435852\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.408781051635742 LOSS_Discriminator: 0.0878189206123352\n",
            "ITERATION_NO.: 533 LOSS_Generator: 5.86728048324585 LOSS_Discriminator: 0.06484033912420273\n",
            "ITERATION_NO.: 534 LOSS_Generator: 6.406848907470703 LOSS_Discriminator: 0.11241273581981659\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.174527168273926 LOSS_Discriminator: 0.043928638100624084\n",
            "ITERATION_NO.: 536 LOSS_Generator: 7.480070114135742 LOSS_Discriminator: 0.04643159359693527\n",
            "ITERATION_NO.: 537 LOSS_Generator: 8.330562591552734 LOSS_Discriminator: 0.06260812282562256\n",
            "ITERATION_NO.: 538 LOSS_Generator: 8.069002151489258 LOSS_Discriminator: 0.04028421640396118\n",
            "ITERATION_NO.: 539 LOSS_Generator: 8.058724403381348 LOSS_Discriminator: 0.08501584082841873\n",
            "ITERATION_NO.: 540 LOSS_Generator: 8.024221420288086 LOSS_Discriminator: 0.0847654640674591\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.925812244415283 LOSS_Discriminator: 0.0461886040866375\n",
            "ITERATION_NO.: 542 LOSS_Generator: 6.739254951477051 LOSS_Discriminator: 0.08868424594402313\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.800534248352051 LOSS_Discriminator: 0.07222559303045273\n",
            "ITERATION_NO.: 544 LOSS_Generator: 7.625981330871582 LOSS_Discriminator: 0.053899459540843964\n",
            "ITERATION_NO.: 545 LOSS_Generator: 8.739363670349121 LOSS_Discriminator: 0.022004760801792145\n",
            "ITERATION_NO.: 546 LOSS_Generator: 8.640959739685059 LOSS_Discriminator: 0.060455113649368286\n",
            "ITERATION_NO.: 547 LOSS_Generator: 7.431910514831543 LOSS_Discriminator: 0.09041059017181396\n",
            "ITERATION_NO.: 548 LOSS_Generator: 7.650516986846924 LOSS_Discriminator: 0.07045987248420715\n",
            "ITERATION_NO.: 549 LOSS_Generator: 6.522343158721924 LOSS_Discriminator: 0.09392546862363815\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.912875175476074 LOSS_Discriminator: 0.05338812619447708\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.399594783782959 LOSS_Discriminator: 0.049847401678562164\n",
            "ITERATION_NO.: 552 LOSS_Generator: 7.512632369995117 LOSS_Discriminator: 0.04745333269238472\n",
            "ITERATION_NO.: 553 LOSS_Generator: 7.959447860717773 LOSS_Discriminator: 0.048914965242147446\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.998696804046631 LOSS_Discriminator: 0.051398467272520065\n",
            "ITERATION_NO.: 555 LOSS_Generator: 7.819632053375244 LOSS_Discriminator: 0.04963032901287079\n",
            "ITERATION_NO.: 556 LOSS_Generator: 8.171404838562012 LOSS_Discriminator: 0.043494030833244324\n",
            "ITERATION_NO.: 557 LOSS_Generator: 8.002676010131836 LOSS_Discriminator: 0.07291107624769211\n",
            "ITERATION_NO.: 558 LOSS_Generator: 7.327770233154297 LOSS_Discriminator: 0.06102752685546875\n",
            "ITERATION_NO.: 559 LOSS_Generator: 7.554058074951172 LOSS_Discriminator: 0.032430000603199005\n",
            "ITERATION_NO.: 560 LOSS_Generator: 7.099666595458984 LOSS_Discriminator: 0.06320016831159592\n",
            "ITERATION_NO.: 561 LOSS_Generator: 7.055164337158203 LOSS_Discriminator: 0.07624447345733643\n",
            "ITERATION_NO.: 562 LOSS_Generator: 7.748495101928711 LOSS_Discriminator: 0.058200202882289886\n",
            "ITERATION_NO.: 563 LOSS_Generator: 8.02644157409668 LOSS_Discriminator: 0.03687089681625366\n",
            "ITERATION_NO.: 564 LOSS_Generator: 8.138843536376953 LOSS_Discriminator: 0.03706701099872589\n",
            "ITERATION_NO.: 565 LOSS_Generator: 8.397793769836426 LOSS_Discriminator: 0.04628915712237358\n",
            "ITERATION_NO.: 566 LOSS_Generator: 8.144269943237305 LOSS_Discriminator: 0.0939815491437912\n",
            "ITERATION_NO.: 567 LOSS_Generator: 8.220098495483398 LOSS_Discriminator: 0.11844280362129211\n",
            "ITERATION_NO.: 568 LOSS_Generator: 7.426319599151611 LOSS_Discriminator: 0.0768684595823288\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.717710494995117 LOSS_Discriminator: 0.036907244473695755\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.70405387878418 LOSS_Discriminator: 0.05564706772565842\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.522477626800537 LOSS_Discriminator: 0.05323375016450882\n",
            "ITERATION_NO.: 572 LOSS_Generator: 7.4290032386779785 LOSS_Discriminator: 0.047915268689394\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.5436553955078125 LOSS_Discriminator: 0.034615658223629\n",
            "ITERATION_NO.: 574 LOSS_Generator: 8.324554443359375 LOSS_Discriminator: 0.059462908655405045\n",
            "ITERATION_NO.: 575 LOSS_Generator: 8.068824768066406 LOSS_Discriminator: 0.06011943519115448\n",
            "ITERATION_NO.: 576 LOSS_Generator: 7.118441581726074 LOSS_Discriminator: 0.0928683951497078\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.947391986846924 LOSS_Discriminator: 0.03592098504304886\n",
            "ITERATION_NO.: 578 LOSS_Generator: 7.034759998321533 LOSS_Discriminator: 0.043535102158784866\n",
            "ITERATION_NO.: 579 LOSS_Generator: 6.948390007019043 LOSS_Discriminator: 0.08652248233556747\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.990203380584717 LOSS_Discriminator: 0.06532590091228485\n",
            "ITERATION_NO.: 581 LOSS_Generator: 7.770424842834473 LOSS_Discriminator: 0.06486331671476364\n",
            "ITERATION_NO.: 582 LOSS_Generator: 7.2865753173828125 LOSS_Discriminator: 0.10243532061576843\n",
            "ITERATION_NO.: 583 LOSS_Generator: 7.6158061027526855 LOSS_Discriminator: 0.04466043412685394\n",
            "ITERATION_NO.: 584 LOSS_Generator: 7.6748528480529785 LOSS_Discriminator: 0.027109481394290924\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.55815315246582 LOSS_Discriminator: 0.08923372626304626\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.4150543212890625 LOSS_Discriminator: 0.091974638402462\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.601236343383789 LOSS_Discriminator: 0.13874222338199615\n",
            "ITERATION_NO.: 588 LOSS_Generator: 8.981128692626953 LOSS_Discriminator: 0.05569690465927124\n",
            "ITERATION_NO.: 589 LOSS_Generator: 8.491193771362305 LOSS_Discriminator: 0.1937178075313568\n",
            "ITERATION_NO.: 590 LOSS_Generator: 7.694631576538086 LOSS_Discriminator: 0.09551984816789627\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.3586106300354 LOSS_Discriminator: 0.07754155993461609\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.118466377258301 LOSS_Discriminator: 0.05721774697303772\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.710606098175049 LOSS_Discriminator: 0.041187435388565063\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.962829113006592 LOSS_Discriminator: 0.1128786951303482\n",
            "ITERATION_NO.: 595 LOSS_Generator: 7.460421562194824 LOSS_Discriminator: 0.0650872141122818\n",
            "ITERATION_NO.: 596 LOSS_Generator: 7.503220081329346 LOSS_Discriminator: 0.03663559630513191\n",
            "ITERATION_NO.: 597 LOSS_Generator: 7.269234657287598 LOSS_Discriminator: 0.05439935252070427\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.698953628540039 LOSS_Discriminator: 0.06104680895805359\n",
            "ITERATION_NO.: 599 LOSS_Generator: 7.117356777191162 LOSS_Discriminator: 0.08368729799985886\n",
            "ITERATION_NO.: 600 LOSS_Generator: 7.383364677429199 LOSS_Discriminator: 0.07384087145328522\n",
            "EPOCH OVER: 45\n",
            "ITERATION_NO.: 1 LOSS_Generator: 7.411815166473389 LOSS_Discriminator: 0.03447691351175308\n",
            "ITERATION_NO.: 2 LOSS_Generator: 8.192214012145996 LOSS_Discriminator: 0.04471103847026825\n",
            "ITERATION_NO.: 3 LOSS_Generator: 8.088176727294922 LOSS_Discriminator: 0.07479254901409149\n",
            "ITERATION_NO.: 4 LOSS_Generator: 8.011850357055664 LOSS_Discriminator: 0.10551009327173233\n",
            "ITERATION_NO.: 5 LOSS_Generator: 7.289782524108887 LOSS_Discriminator: 0.06230104714632034\n",
            "ITERATION_NO.: 6 LOSS_Generator: 7.4324116706848145 LOSS_Discriminator: 0.05943332612514496\n",
            "ITERATION_NO.: 7 LOSS_Generator: 6.6811442375183105 LOSS_Discriminator: 0.06365601718425751\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.2373223304748535 LOSS_Discriminator: 0.040333762764930725\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.928989410400391 LOSS_Discriminator: 0.04837220907211304\n",
            "ITERATION_NO.: 10 LOSS_Generator: 8.30360221862793 LOSS_Discriminator: 0.1022893488407135\n",
            "ITERATION_NO.: 11 LOSS_Generator: 8.434428215026855 LOSS_Discriminator: 0.062283169478178024\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.463461875915527 LOSS_Discriminator: 0.08946569263935089\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.230642795562744 LOSS_Discriminator: 0.12642455101013184\n",
            "ITERATION_NO.: 14 LOSS_Generator: 5.435610771179199 LOSS_Discriminator: 0.06622037291526794\n",
            "ITERATION_NO.: 15 LOSS_Generator: 5.712441444396973 LOSS_Discriminator: 0.14121484756469727\n",
            "ITERATION_NO.: 16 LOSS_Generator: 6.434873104095459 LOSS_Discriminator: 0.1357947587966919\n",
            "ITERATION_NO.: 17 LOSS_Generator: 7.762712001800537 LOSS_Discriminator: 0.06595181673765182\n",
            "ITERATION_NO.: 18 LOSS_Generator: 8.562679290771484 LOSS_Discriminator: 0.04716687649488449\n",
            "ITERATION_NO.: 19 LOSS_Generator: 8.581208229064941 LOSS_Discriminator: 0.1048397421836853\n",
            "ITERATION_NO.: 20 LOSS_Generator: 7.761770248413086 LOSS_Discriminator: 0.06807585060596466\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.423046112060547 LOSS_Discriminator: 0.05224201828241348\n",
            "ITERATION_NO.: 22 LOSS_Generator: 6.411074638366699 LOSS_Discriminator: 0.055204931646585464\n",
            "ITERATION_NO.: 23 LOSS_Generator: 6.68856954574585 LOSS_Discriminator: 0.0849919393658638\n",
            "ITERATION_NO.: 24 LOSS_Generator: 7.49478006362915 LOSS_Discriminator: 0.1143890917301178\n",
            "ITERATION_NO.: 25 LOSS_Generator: 8.06856918334961 LOSS_Discriminator: 0.08847320079803467\n",
            "ITERATION_NO.: 26 LOSS_Generator: 8.987980842590332 LOSS_Discriminator: 0.08186142146587372\n",
            "ITERATION_NO.: 27 LOSS_Generator: 9.19011402130127 LOSS_Discriminator: 0.05937594920396805\n",
            "ITERATION_NO.: 28 LOSS_Generator: 7.8233771324157715 LOSS_Discriminator: 0.07761959731578827\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.8170695304870605 LOSS_Discriminator: 0.056984271854162216\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.102440357208252 LOSS_Discriminator: 0.08756966888904572\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.418092727661133 LOSS_Discriminator: 0.11225555837154388\n",
            "ITERATION_NO.: 32 LOSS_Generator: 7.74558162689209 LOSS_Discriminator: 0.07132253795862198\n",
            "ITERATION_NO.: 33 LOSS_Generator: 8.25944995880127 LOSS_Discriminator: 0.062315504997968674\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.297369003295898 LOSS_Discriminator: 0.08979658037424088\n",
            "ITERATION_NO.: 35 LOSS_Generator: 8.067790985107422 LOSS_Discriminator: 0.10020077228546143\n",
            "ITERATION_NO.: 36 LOSS_Generator: 7.5561394691467285 LOSS_Discriminator: 0.03893914818763733\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.9295477867126465 LOSS_Discriminator: 0.07394200563430786\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.863913059234619 LOSS_Discriminator: 0.0765688568353653\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.583381175994873 LOSS_Discriminator: 0.04856707528233528\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.2241106033325195 LOSS_Discriminator: 0.06092889979481697\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.759639263153076 LOSS_Discriminator: 0.09161257743835449\n",
            "ITERATION_NO.: 42 LOSS_Generator: 7.637404918670654 LOSS_Discriminator: 0.0507420152425766\n",
            "ITERATION_NO.: 43 LOSS_Generator: 8.110426902770996 LOSS_Discriminator: 0.04928717762231827\n",
            "ITERATION_NO.: 44 LOSS_Generator: 7.2010345458984375 LOSS_Discriminator: 0.07618693262338638\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.819261074066162 LOSS_Discriminator: 0.023189043626189232\n",
            "ITERATION_NO.: 46 LOSS_Generator: 6.493458271026611 LOSS_Discriminator: 0.0714465007185936\n",
            "ITERATION_NO.: 47 LOSS_Generator: 6.922300815582275 LOSS_Discriminator: 0.044413112103939056\n",
            "ITERATION_NO.: 48 LOSS_Generator: 6.684974193572998 LOSS_Discriminator: 0.021429970860481262\n",
            "ITERATION_NO.: 49 LOSS_Generator: 7.176170825958252 LOSS_Discriminator: 0.055533334612846375\n",
            "ITERATION_NO.: 50 LOSS_Generator: 8.173200607299805 LOSS_Discriminator: 0.08574894815683365\n",
            "ITERATION_NO.: 51 LOSS_Generator: 7.916714668273926 LOSS_Discriminator: 0.06737000495195389\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.430960655212402 LOSS_Discriminator: 0.06040157750248909\n",
            "ITERATION_NO.: 53 LOSS_Generator: 7.146770477294922 LOSS_Discriminator: 0.06142737716436386\n",
            "ITERATION_NO.: 54 LOSS_Generator: 6.1956787109375 LOSS_Discriminator: 0.028018593788146973\n",
            "ITERATION_NO.: 55 LOSS_Generator: 7.018413066864014 LOSS_Discriminator: 0.02305830642580986\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.878027439117432 LOSS_Discriminator: 0.051057539880275726\n",
            "ITERATION_NO.: 57 LOSS_Generator: 7.4042887687683105 LOSS_Discriminator: 0.05476390942931175\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.551662445068359 LOSS_Discriminator: 0.015603258274495602\n",
            "ITERATION_NO.: 59 LOSS_Generator: 7.798525810241699 LOSS_Discriminator: 0.07297931611537933\n",
            "ITERATION_NO.: 60 LOSS_Generator: 6.939520835876465 LOSS_Discriminator: 0.05530755594372749\n",
            "ITERATION_NO.: 61 LOSS_Generator: 7.2863054275512695 LOSS_Discriminator: 0.0481383316218853\n",
            "ITERATION_NO.: 62 LOSS_Generator: 7.079635143280029 LOSS_Discriminator: 0.09836284816265106\n",
            "ITERATION_NO.: 63 LOSS_Generator: 7.499172210693359 LOSS_Discriminator: 0.06644211709499359\n",
            "ITERATION_NO.: 64 LOSS_Generator: 7.904248237609863 LOSS_Discriminator: 0.13440117239952087\n",
            "ITERATION_NO.: 65 LOSS_Generator: 7.741800308227539 LOSS_Discriminator: 0.06130164489150047\n",
            "ITERATION_NO.: 66 LOSS_Generator: 7.742619037628174 LOSS_Discriminator: 0.020326245576143265\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.627604961395264 LOSS_Discriminator: 0.052746981382369995\n",
            "ITERATION_NO.: 68 LOSS_Generator: 7.662534713745117 LOSS_Discriminator: 0.05504880100488663\n",
            "ITERATION_NO.: 69 LOSS_Generator: 7.623619556427002 LOSS_Discriminator: 0.03679933398962021\n",
            "ITERATION_NO.: 70 LOSS_Generator: 7.814605236053467 LOSS_Discriminator: 0.05581921339035034\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.538444995880127 LOSS_Discriminator: 0.03982572257518768\n",
            "ITERATION_NO.: 72 LOSS_Generator: 7.555152416229248 LOSS_Discriminator: 0.09857893735170364\n",
            "ITERATION_NO.: 73 LOSS_Generator: 7.033900737762451 LOSS_Discriminator: 0.045794159173965454\n",
            "ITERATION_NO.: 74 LOSS_Generator: 7.002272129058838 LOSS_Discriminator: 0.06986816227436066\n",
            "ITERATION_NO.: 75 LOSS_Generator: 6.951703071594238 LOSS_Discriminator: 0.056641586124897\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.573320388793945 LOSS_Discriminator: 0.058692850172519684\n",
            "ITERATION_NO.: 77 LOSS_Generator: 7.535421848297119 LOSS_Discriminator: 0.10850296914577484\n",
            "ITERATION_NO.: 78 LOSS_Generator: 8.006343841552734 LOSS_Discriminator: 0.07780339568853378\n",
            "ITERATION_NO.: 79 LOSS_Generator: 7.704470157623291 LOSS_Discriminator: 0.05249068886041641\n",
            "ITERATION_NO.: 80 LOSS_Generator: 6.86794376373291 LOSS_Discriminator: 0.11126536130905151\n",
            "ITERATION_NO.: 81 LOSS_Generator: 6.961733818054199 LOSS_Discriminator: 0.11851554363965988\n",
            "ITERATION_NO.: 82 LOSS_Generator: 7.038105487823486 LOSS_Discriminator: 0.04959021508693695\n",
            "ITERATION_NO.: 83 LOSS_Generator: 6.748591899871826 LOSS_Discriminator: 0.0661080852150917\n",
            "ITERATION_NO.: 84 LOSS_Generator: 7.019128322601318 LOSS_Discriminator: 0.05112732946872711\n",
            "ITERATION_NO.: 85 LOSS_Generator: 8.006537437438965 LOSS_Discriminator: 0.04845399782061577\n",
            "ITERATION_NO.: 86 LOSS_Generator: 8.607272148132324 LOSS_Discriminator: 0.04993931204080582\n",
            "ITERATION_NO.: 87 LOSS_Generator: 8.099994659423828 LOSS_Discriminator: 0.14151404798030853\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.612241268157959 LOSS_Discriminator: 0.06581464409828186\n",
            "ITERATION_NO.: 89 LOSS_Generator: 7.840743541717529 LOSS_Discriminator: 0.06150631606578827\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.402620315551758 LOSS_Discriminator: 0.10254854708909988\n",
            "ITERATION_NO.: 91 LOSS_Generator: 7.090031147003174 LOSS_Discriminator: 0.05148298665881157\n",
            "ITERATION_NO.: 92 LOSS_Generator: 6.982729911804199 LOSS_Discriminator: 0.05914711207151413\n",
            "ITERATION_NO.: 93 LOSS_Generator: 7.309479236602783 LOSS_Discriminator: 0.0637274757027626\n",
            "ITERATION_NO.: 94 LOSS_Generator: 6.999844551086426 LOSS_Discriminator: 0.10546105355024338\n",
            "ITERATION_NO.: 95 LOSS_Generator: 6.7673749923706055 LOSS_Discriminator: 0.09800738096237183\n",
            "ITERATION_NO.: 96 LOSS_Generator: 6.6083984375 LOSS_Discriminator: 0.1124807670712471\n",
            "ITERATION_NO.: 97 LOSS_Generator: 6.751567363739014 LOSS_Discriminator: 0.0730140209197998\n",
            "ITERATION_NO.: 98 LOSS_Generator: 7.926104545593262 LOSS_Discriminator: 0.08693726360797882\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.908355712890625 LOSS_Discriminator: 0.0682569146156311\n",
            "ITERATION_NO.: 100 LOSS_Generator: 8.3536958694458 LOSS_Discriminator: 0.09488143026828766\n",
            "ITERATION_NO.: 101 LOSS_Generator: 8.445866584777832 LOSS_Discriminator: 0.06266932934522629\n",
            "ITERATION_NO.: 102 LOSS_Generator: 7.625320911407471 LOSS_Discriminator: 0.04222526401281357\n",
            "ITERATION_NO.: 103 LOSS_Generator: 7.134654521942139 LOSS_Discriminator: 0.09732042253017426\n",
            "ITERATION_NO.: 104 LOSS_Generator: 6.334448337554932 LOSS_Discriminator: 0.1780518889427185\n",
            "ITERATION_NO.: 105 LOSS_Generator: 6.192047119140625 LOSS_Discriminator: 0.03778994828462601\n",
            "ITERATION_NO.: 106 LOSS_Generator: 6.865636825561523 LOSS_Discriminator: 0.09764118492603302\n",
            "ITERATION_NO.: 107 LOSS_Generator: 8.071895599365234 LOSS_Discriminator: 0.05394402891397476\n",
            "ITERATION_NO.: 108 LOSS_Generator: 8.900802612304688 LOSS_Discriminator: 0.06836342811584473\n",
            "ITERATION_NO.: 109 LOSS_Generator: 8.955183982849121 LOSS_Discriminator: 0.09937979280948639\n",
            "ITERATION_NO.: 110 LOSS_Generator: 8.295544624328613 LOSS_Discriminator: 0.056468427181243896\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.289694309234619 LOSS_Discriminator: 0.028907135128974915\n",
            "ITERATION_NO.: 112 LOSS_Generator: 6.828005313873291 LOSS_Discriminator: 0.06292320787906647\n",
            "ITERATION_NO.: 113 LOSS_Generator: 6.0981364250183105 LOSS_Discriminator: 0.07684486359357834\n",
            "ITERATION_NO.: 114 LOSS_Generator: 6.186516284942627 LOSS_Discriminator: 0.10019007325172424\n",
            "ITERATION_NO.: 115 LOSS_Generator: 7.695552349090576 LOSS_Discriminator: 0.0685839056968689\n",
            "ITERATION_NO.: 116 LOSS_Generator: 8.61835765838623 LOSS_Discriminator: 0.04996194690465927\n",
            "ITERATION_NO.: 117 LOSS_Generator: 9.669968605041504 LOSS_Discriminator: 0.08803185820579529\n",
            "ITERATION_NO.: 118 LOSS_Generator: 9.115806579589844 LOSS_Discriminator: 0.14009836316108704\n",
            "ITERATION_NO.: 119 LOSS_Generator: 8.251112937927246 LOSS_Discriminator: 0.08534136414527893\n",
            "ITERATION_NO.: 120 LOSS_Generator: 6.394503116607666 LOSS_Discriminator: 0.047997310757637024\n",
            "ITERATION_NO.: 121 LOSS_Generator: 5.800909042358398 LOSS_Discriminator: 0.05620517209172249\n",
            "ITERATION_NO.: 122 LOSS_Generator: 5.90595817565918 LOSS_Discriminator: 0.09958654642105103\n",
            "ITERATION_NO.: 123 LOSS_Generator: 6.792227268218994 LOSS_Discriminator: 0.0689658671617508\n",
            "ITERATION_NO.: 124 LOSS_Generator: 8.527631759643555 LOSS_Discriminator: 0.05405952036380768\n",
            "ITERATION_NO.: 125 LOSS_Generator: 8.998191833496094 LOSS_Discriminator: 0.11183740943670273\n",
            "ITERATION_NO.: 126 LOSS_Generator: 8.773955345153809 LOSS_Discriminator: 0.040843747556209564\n",
            "ITERATION_NO.: 127 LOSS_Generator: 7.973512172698975 LOSS_Discriminator: 0.09177719801664352\n",
            "ITERATION_NO.: 128 LOSS_Generator: 7.338839054107666 LOSS_Discriminator: 0.046854082494974136\n",
            "ITERATION_NO.: 129 LOSS_Generator: 6.373812675476074 LOSS_Discriminator: 0.1194329559803009\n",
            "ITERATION_NO.: 130 LOSS_Generator: 6.760234832763672 LOSS_Discriminator: 0.07032579183578491\n",
            "ITERATION_NO.: 131 LOSS_Generator: 6.837924957275391 LOSS_Discriminator: 0.06508372724056244\n",
            "ITERATION_NO.: 132 LOSS_Generator: 7.4518609046936035 LOSS_Discriminator: 0.0701092854142189\n",
            "ITERATION_NO.: 133 LOSS_Generator: 7.929498195648193 LOSS_Discriminator: 0.04225747287273407\n",
            "ITERATION_NO.: 134 LOSS_Generator: 7.664419651031494 LOSS_Discriminator: 0.07267111539840698\n",
            "ITERATION_NO.: 135 LOSS_Generator: 7.792162895202637 LOSS_Discriminator: 0.016908710822463036\n",
            "ITERATION_NO.: 136 LOSS_Generator: 7.949131011962891 LOSS_Discriminator: 0.08572791516780853\n",
            "ITERATION_NO.: 137 LOSS_Generator: 7.529066562652588 LOSS_Discriminator: 0.05107306316494942\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.911316394805908 LOSS_Discriminator: 0.052734505385160446\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.343130588531494 LOSS_Discriminator: 0.060634054243564606\n",
            "ITERATION_NO.: 140 LOSS_Generator: 6.762406826019287 LOSS_Discriminator: 0.11916711926460266\n",
            "ITERATION_NO.: 141 LOSS_Generator: 7.567223072052002 LOSS_Discriminator: 0.08658666908740997\n",
            "ITERATION_NO.: 142 LOSS_Generator: 7.760329723358154 LOSS_Discriminator: 0.06278683245182037\n",
            "ITERATION_NO.: 143 LOSS_Generator: 7.772799491882324 LOSS_Discriminator: 0.12578746676445007\n",
            "ITERATION_NO.: 144 LOSS_Generator: 8.188626289367676 LOSS_Discriminator: 0.03600195422768593\n",
            "ITERATION_NO.: 145 LOSS_Generator: 7.952549934387207 LOSS_Discriminator: 0.061084408313035965\n",
            "ITERATION_NO.: 146 LOSS_Generator: 6.784801483154297 LOSS_Discriminator: 0.059123001992702484\n",
            "ITERATION_NO.: 147 LOSS_Generator: 7.030415058135986 LOSS_Discriminator: 0.0656522735953331\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.695113658905029 LOSS_Discriminator: 0.057365238666534424\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.7329816818237305 LOSS_Discriminator: 0.07416804134845734\n",
            "ITERATION_NO.: 150 LOSS_Generator: 6.990016937255859 LOSS_Discriminator: 0.022773902863264084\n",
            "ITERATION_NO.: 151 LOSS_Generator: 7.397755146026611 LOSS_Discriminator: 0.05351702496409416\n",
            "ITERATION_NO.: 152 LOSS_Generator: 6.306061267852783 LOSS_Discriminator: 0.2477397620677948\n",
            "ITERATION_NO.: 153 LOSS_Generator: 5.696352481842041 LOSS_Discriminator: 0.08903734385967255\n",
            "ITERATION_NO.: 154 LOSS_Generator: 5.935349941253662 LOSS_Discriminator: 0.11371414363384247\n",
            "ITERATION_NO.: 155 LOSS_Generator: 7.418675422668457 LOSS_Discriminator: 0.08115480840206146\n",
            "ITERATION_NO.: 156 LOSS_Generator: 8.421103477478027 LOSS_Discriminator: 0.09561630338430405\n",
            "ITERATION_NO.: 157 LOSS_Generator: 8.00810432434082 LOSS_Discriminator: 0.08553691953420639\n",
            "ITERATION_NO.: 158 LOSS_Generator: 7.756744861602783 LOSS_Discriminator: 0.06337369978427887\n",
            "ITERATION_NO.: 159 LOSS_Generator: 6.9999098777771 LOSS_Discriminator: 0.20564420521259308\n",
            "ITERATION_NO.: 160 LOSS_Generator: 6.333044528961182 LOSS_Discriminator: 0.04698702320456505\n",
            "ITERATION_NO.: 161 LOSS_Generator: 6.20050048828125 LOSS_Discriminator: 0.06361570954322815\n",
            "ITERATION_NO.: 162 LOSS_Generator: 7.037872314453125 LOSS_Discriminator: 0.05006716400384903\n",
            "ITERATION_NO.: 163 LOSS_Generator: 7.964066028594971 LOSS_Discriminator: 0.02374231442809105\n",
            "ITERATION_NO.: 164 LOSS_Generator: 8.586864471435547 LOSS_Discriminator: 0.04160095751285553\n",
            "ITERATION_NO.: 165 LOSS_Generator: 8.91100788116455 LOSS_Discriminator: 0.10913501679897308\n",
            "ITERATION_NO.: 166 LOSS_Generator: 7.84698486328125 LOSS_Discriminator: 0.09430760145187378\n",
            "ITERATION_NO.: 167 LOSS_Generator: 7.207043647766113 LOSS_Discriminator: 0.027824614197015762\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.466181755065918 LOSS_Discriminator: 0.057925768196582794\n",
            "ITERATION_NO.: 169 LOSS_Generator: 5.433350086212158 LOSS_Discriminator: 0.09057331085205078\n",
            "ITERATION_NO.: 170 LOSS_Generator: 6.325625419616699 LOSS_Discriminator: 0.09197551757097244\n",
            "ITERATION_NO.: 171 LOSS_Generator: 6.77646017074585 LOSS_Discriminator: 0.03289607912302017\n",
            "ITERATION_NO.: 172 LOSS_Generator: 7.789669036865234 LOSS_Discriminator: 0.05141086503863335\n",
            "ITERATION_NO.: 173 LOSS_Generator: 9.060138702392578 LOSS_Discriminator: 0.055362116545438766\n",
            "ITERATION_NO.: 174 LOSS_Generator: 8.401514053344727 LOSS_Discriminator: 0.0979347974061966\n",
            "ITERATION_NO.: 175 LOSS_Generator: 7.192140102386475 LOSS_Discriminator: 0.11879121512174606\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.789234638214111 LOSS_Discriminator: 0.021022161468863487\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.2484846115112305 LOSS_Discriminator: 0.08398119360208511\n",
            "ITERATION_NO.: 178 LOSS_Generator: 6.793892860412598 LOSS_Discriminator: 0.032654546201229095\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.696519374847412 LOSS_Discriminator: 0.06071346998214722\n",
            "ITERATION_NO.: 180 LOSS_Generator: 8.25581169128418 LOSS_Discriminator: 0.062480855733156204\n",
            "ITERATION_NO.: 181 LOSS_Generator: 8.452652931213379 LOSS_Discriminator: 0.08893468976020813\n",
            "ITERATION_NO.: 182 LOSS_Generator: 7.851656436920166 LOSS_Discriminator: 0.06868449598550797\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.4719557762146 LOSS_Discriminator: 0.03310706466436386\n",
            "ITERATION_NO.: 184 LOSS_Generator: 6.322098255157471 LOSS_Discriminator: 0.046132661402225494\n",
            "ITERATION_NO.: 185 LOSS_Generator: 6.105701923370361 LOSS_Discriminator: 0.07045802474021912\n",
            "ITERATION_NO.: 186 LOSS_Generator: 6.11440372467041 LOSS_Discriminator: 0.07256687432527542\n",
            "ITERATION_NO.: 187 LOSS_Generator: 7.305327892303467 LOSS_Discriminator: 0.09403322637081146\n",
            "ITERATION_NO.: 188 LOSS_Generator: 8.06568717956543 LOSS_Discriminator: 0.11802954971790314\n",
            "ITERATION_NO.: 189 LOSS_Generator: 7.922219276428223 LOSS_Discriminator: 0.06609367579221725\n",
            "ITERATION_NO.: 190 LOSS_Generator: 8.116535186767578 LOSS_Discriminator: 0.11924237012863159\n",
            "ITERATION_NO.: 191 LOSS_Generator: 7.551624298095703 LOSS_Discriminator: 0.04637574404478073\n",
            "ITERATION_NO.: 192 LOSS_Generator: 6.431834697723389 LOSS_Discriminator: 0.06987466663122177\n",
            "ITERATION_NO.: 193 LOSS_Generator: 6.295532703399658 LOSS_Discriminator: 0.10114467144012451\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.363214492797852 LOSS_Discriminator: 0.13892604410648346\n",
            "ITERATION_NO.: 195 LOSS_Generator: 7.087449550628662 LOSS_Discriminator: 0.1253224015235901\n",
            "ITERATION_NO.: 196 LOSS_Generator: 8.850923538208008 LOSS_Discriminator: 0.05531337112188339\n",
            "ITERATION_NO.: 197 LOSS_Generator: 8.995580673217773 LOSS_Discriminator: 0.026359962299466133\n",
            "ITERATION_NO.: 198 LOSS_Generator: 8.755581855773926 LOSS_Discriminator: 0.10896198451519012\n",
            "ITERATION_NO.: 199 LOSS_Generator: 8.019437789916992 LOSS_Discriminator: 0.051279742270708084\n",
            "ITERATION_NO.: 200 LOSS_Generator: 6.666254043579102 LOSS_Discriminator: 0.05916289612650871\n",
            "ITERATION_NO.: 201 LOSS_Generator: 5.96066951751709 LOSS_Discriminator: 0.10277330130338669\n",
            "ITERATION_NO.: 202 LOSS_Generator: 6.243100643157959 LOSS_Discriminator: 0.08552395552396774\n",
            "ITERATION_NO.: 203 LOSS_Generator: 7.404697895050049 LOSS_Discriminator: 0.08694066107273102\n",
            "ITERATION_NO.: 204 LOSS_Generator: 8.457549095153809 LOSS_Discriminator: 0.02242310717701912\n",
            "ITERATION_NO.: 205 LOSS_Generator: 8.239887237548828 LOSS_Discriminator: 0.05949530750513077\n",
            "ITERATION_NO.: 206 LOSS_Generator: 8.9759521484375 LOSS_Discriminator: 0.05881451070308685\n",
            "ITERATION_NO.: 207 LOSS_Generator: 8.12381362915039 LOSS_Discriminator: 0.058010946959257126\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.862264633178711 LOSS_Discriminator: 0.025508472695946693\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.840380668640137 LOSS_Discriminator: 0.03811711072921753\n",
            "ITERATION_NO.: 210 LOSS_Generator: 7.584583282470703 LOSS_Discriminator: 0.13771693408489227\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.2924017906188965 LOSS_Discriminator: 0.031900353729724884\n",
            "ITERATION_NO.: 212 LOSS_Generator: 7.349094390869141 LOSS_Discriminator: 0.032686442136764526\n",
            "ITERATION_NO.: 213 LOSS_Generator: 6.831830978393555 LOSS_Discriminator: 0.05904038995504379\n",
            "ITERATION_NO.: 214 LOSS_Generator: 7.221923828125 LOSS_Discriminator: 0.08251938223838806\n",
            "ITERATION_NO.: 215 LOSS_Generator: 7.020094394683838 LOSS_Discriminator: 0.08868443965911865\n",
            "ITERATION_NO.: 216 LOSS_Generator: 7.598639488220215 LOSS_Discriminator: 0.03778763487935066\n",
            "ITERATION_NO.: 217 LOSS_Generator: 7.806706428527832 LOSS_Discriminator: 0.08666922152042389\n",
            "ITERATION_NO.: 218 LOSS_Generator: 7.753585338592529 LOSS_Discriminator: 0.04242537543177605\n",
            "ITERATION_NO.: 219 LOSS_Generator: 6.970611095428467 LOSS_Discriminator: 0.05373021960258484\n",
            "ITERATION_NO.: 220 LOSS_Generator: 7.379146099090576 LOSS_Discriminator: 0.07177005708217621\n",
            "ITERATION_NO.: 221 LOSS_Generator: 7.235949516296387 LOSS_Discriminator: 0.026449572294950485\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.901602745056152 LOSS_Discriminator: 0.09115664660930634\n",
            "ITERATION_NO.: 223 LOSS_Generator: 6.577571392059326 LOSS_Discriminator: 0.0632079467177391\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.422358512878418 LOSS_Discriminator: 0.07884493470191956\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.564594745635986 LOSS_Discriminator: 0.02461169846355915\n",
            "ITERATION_NO.: 226 LOSS_Generator: 6.9508056640625 LOSS_Discriminator: 0.038703449070453644\n",
            "ITERATION_NO.: 227 LOSS_Generator: 7.552097320556641 LOSS_Discriminator: 0.05594906583428383\n",
            "ITERATION_NO.: 228 LOSS_Generator: 7.504671096801758 LOSS_Discriminator: 0.1002325564622879\n",
            "ITERATION_NO.: 229 LOSS_Generator: 6.554694175720215 LOSS_Discriminator: 0.0845755785703659\n",
            "ITERATION_NO.: 230 LOSS_Generator: 6.243635177612305 LOSS_Discriminator: 0.07801778614521027\n",
            "ITERATION_NO.: 231 LOSS_Generator: 6.480062484741211 LOSS_Discriminator: 0.08537560701370239\n",
            "ITERATION_NO.: 232 LOSS_Generator: 7.75950813293457 LOSS_Discriminator: 0.049498558044433594\n",
            "ITERATION_NO.: 233 LOSS_Generator: 7.907999038696289 LOSS_Discriminator: 0.09402629733085632\n",
            "ITERATION_NO.: 234 LOSS_Generator: 8.829744338989258 LOSS_Discriminator: 0.04255545139312744\n",
            "ITERATION_NO.: 235 LOSS_Generator: 7.910497665405273 LOSS_Discriminator: 0.08431768417358398\n",
            "ITERATION_NO.: 236 LOSS_Generator: 6.354071617126465 LOSS_Discriminator: 0.08269457519054413\n",
            "ITERATION_NO.: 237 LOSS_Generator: 6.003741264343262 LOSS_Discriminator: 0.09861713647842407\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.4352593421936035 LOSS_Discriminator: 0.0716836005449295\n",
            "ITERATION_NO.: 239 LOSS_Generator: 7.2751946449279785 LOSS_Discriminator: 0.06923501938581467\n",
            "ITERATION_NO.: 240 LOSS_Generator: 8.140615463256836 LOSS_Discriminator: 0.03481069207191467\n",
            "ITERATION_NO.: 241 LOSS_Generator: 8.963191986083984 LOSS_Discriminator: 0.06830977648496628\n",
            "ITERATION_NO.: 242 LOSS_Generator: 9.052700996398926 LOSS_Discriminator: 0.03133665397763252\n",
            "ITERATION_NO.: 243 LOSS_Generator: 8.350258827209473 LOSS_Discriminator: 0.09629188477993011\n",
            "ITERATION_NO.: 244 LOSS_Generator: 7.688389778137207 LOSS_Discriminator: 0.09666115790605545\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.72884464263916 LOSS_Discriminator: 0.06769829988479614\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.295307636260986 LOSS_Discriminator: 0.05836339294910431\n",
            "ITERATION_NO.: 247 LOSS_Generator: 6.830552577972412 LOSS_Discriminator: 0.055236171931028366\n",
            "ITERATION_NO.: 248 LOSS_Generator: 8.085436820983887 LOSS_Discriminator: 0.05337812006473541\n",
            "ITERATION_NO.: 249 LOSS_Generator: 8.322673797607422 LOSS_Discriminator: 0.043899476528167725\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.945281505584717 LOSS_Discriminator: 0.09111664444208145\n",
            "ITERATION_NO.: 251 LOSS_Generator: 7.916030406951904 LOSS_Discriminator: 0.02670067921280861\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.32081413269043 LOSS_Discriminator: 0.04056090861558914\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.968261241912842 LOSS_Discriminator: 0.031016238033771515\n",
            "ITERATION_NO.: 254 LOSS_Generator: 6.875413417816162 LOSS_Discriminator: 0.07020477205514908\n",
            "ITERATION_NO.: 255 LOSS_Generator: 6.433537483215332 LOSS_Discriminator: 0.045789871364831924\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.188841342926025 LOSS_Discriminator: 0.05269021913409233\n",
            "ITERATION_NO.: 257 LOSS_Generator: 7.799778938293457 LOSS_Discriminator: 0.034461576491594315\n",
            "ITERATION_NO.: 258 LOSS_Generator: 8.748274803161621 LOSS_Discriminator: 0.03815912455320358\n",
            "ITERATION_NO.: 259 LOSS_Generator: 8.725828170776367 LOSS_Discriminator: 0.08415679633617401\n",
            "ITERATION_NO.: 260 LOSS_Generator: 7.8897199630737305 LOSS_Discriminator: 0.0925101637840271\n",
            "ITERATION_NO.: 261 LOSS_Generator: 7.762828350067139 LOSS_Discriminator: 0.04721762239933014\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.596163272857666 LOSS_Discriminator: 0.023909227922558784\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.756726264953613 LOSS_Discriminator: 0.06595905870199203\n",
            "ITERATION_NO.: 264 LOSS_Generator: 7.151188850402832 LOSS_Discriminator: 0.04592123627662659\n",
            "ITERATION_NO.: 265 LOSS_Generator: 7.424661159515381 LOSS_Discriminator: 0.08525754511356354\n",
            "ITERATION_NO.: 266 LOSS_Generator: 7.531816482543945 LOSS_Discriminator: 0.04326175898313522\n",
            "ITERATION_NO.: 267 LOSS_Generator: 6.965118408203125 LOSS_Discriminator: 0.09768101572990417\n",
            "ITERATION_NO.: 268 LOSS_Generator: 6.937129497528076 LOSS_Discriminator: 0.09712296724319458\n",
            "ITERATION_NO.: 269 LOSS_Generator: 7.029231071472168 LOSS_Discriminator: 0.0839812159538269\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.4311089515686035 LOSS_Discriminator: 0.03205544501543045\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.647180080413818 LOSS_Discriminator: 0.07095031440258026\n",
            "ITERATION_NO.: 272 LOSS_Generator: 7.772462368011475 LOSS_Discriminator: 0.04971463233232498\n",
            "ITERATION_NO.: 273 LOSS_Generator: 7.836343288421631 LOSS_Discriminator: 0.06587901711463928\n",
            "ITERATION_NO.: 274 LOSS_Generator: 7.929299831390381 LOSS_Discriminator: 0.11250628530979156\n",
            "ITERATION_NO.: 275 LOSS_Generator: 7.870814323425293 LOSS_Discriminator: 0.04086373746395111\n",
            "ITERATION_NO.: 276 LOSS_Generator: 7.6284050941467285 LOSS_Discriminator: 0.06946126371622086\n",
            "ITERATION_NO.: 277 LOSS_Generator: 7.0094099044799805 LOSS_Discriminator: 0.10052786767482758\n",
            "ITERATION_NO.: 278 LOSS_Generator: 6.444271087646484 LOSS_Discriminator: 0.0641498789191246\n",
            "ITERATION_NO.: 279 LOSS_Generator: 6.87837028503418 LOSS_Discriminator: 0.06512163579463959\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.859745025634766 LOSS_Discriminator: 0.06645634770393372\n",
            "ITERATION_NO.: 281 LOSS_Generator: 8.579728126525879 LOSS_Discriminator: 0.07012709975242615\n",
            "ITERATION_NO.: 282 LOSS_Generator: 8.7287015914917 LOSS_Discriminator: 0.08572879433631897\n",
            "ITERATION_NO.: 283 LOSS_Generator: 8.204875946044922 LOSS_Discriminator: 0.06139782816171646\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.718088150024414 LOSS_Discriminator: 0.09687413275241852\n",
            "ITERATION_NO.: 285 LOSS_Generator: 6.717462539672852 LOSS_Discriminator: 0.021765440702438354\n",
            "ITERATION_NO.: 286 LOSS_Generator: 6.678619861602783 LOSS_Discriminator: 0.08717800676822662\n",
            "ITERATION_NO.: 287 LOSS_Generator: 7.366134166717529 LOSS_Discriminator: 0.07164986431598663\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.8389201164245605 LOSS_Discriminator: 0.03647777438163757\n",
            "ITERATION_NO.: 289 LOSS_Generator: 8.536797523498535 LOSS_Discriminator: 0.06580431759357452\n",
            "ITERATION_NO.: 290 LOSS_Generator: 8.172891616821289 LOSS_Discriminator: 0.07292569428682327\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.992929458618164 LOSS_Discriminator: 0.05858752503991127\n",
            "ITERATION_NO.: 292 LOSS_Generator: 7.800798416137695 LOSS_Discriminator: 0.03618638962507248\n",
            "ITERATION_NO.: 293 LOSS_Generator: 7.600551128387451 LOSS_Discriminator: 0.07924312353134155\n",
            "ITERATION_NO.: 294 LOSS_Generator: 6.439495086669922 LOSS_Discriminator: 0.07360225915908813\n",
            "ITERATION_NO.: 295 LOSS_Generator: 6.475925445556641 LOSS_Discriminator: 0.056681230664253235\n",
            "ITERATION_NO.: 296 LOSS_Generator: 7.212340831756592 LOSS_Discriminator: 0.05386235564947128\n",
            "ITERATION_NO.: 297 LOSS_Generator: 8.259265899658203 LOSS_Discriminator: 0.09876912832260132\n",
            "ITERATION_NO.: 298 LOSS_Generator: 8.513973236083984 LOSS_Discriminator: 0.039754871279001236\n",
            "ITERATION_NO.: 299 LOSS_Generator: 8.28625774383545 LOSS_Discriminator: 0.05277901887893677\n",
            "ITERATION_NO.: 300 LOSS_Generator: 7.92318868637085 LOSS_Discriminator: 0.0273080263286829\n",
            "ITERATION_NO.: 301 LOSS_Generator: 7.905192852020264 LOSS_Discriminator: 0.021879173815250397\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.743311882019043 LOSS_Discriminator: 0.04408682882785797\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.947688579559326 LOSS_Discriminator: 0.0866188257932663\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.550409317016602 LOSS_Discriminator: 0.07135899364948273\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.0806884765625 LOSS_Discriminator: 0.07481272518634796\n",
            "ITERATION_NO.: 306 LOSS_Generator: 8.431103706359863 LOSS_Discriminator: 0.040791161358356476\n",
            "ITERATION_NO.: 307 LOSS_Generator: 9.212891578674316 LOSS_Discriminator: 0.04917902499437332\n",
            "ITERATION_NO.: 308 LOSS_Generator: 8.50173282623291 LOSS_Discriminator: 0.0757756382226944\n",
            "ITERATION_NO.: 309 LOSS_Generator: 8.376838684082031 LOSS_Discriminator: 0.14753639698028564\n",
            "ITERATION_NO.: 310 LOSS_Generator: 7.0093770027160645 LOSS_Discriminator: 0.05710399150848389\n",
            "ITERATION_NO.: 311 LOSS_Generator: 6.715083122253418 LOSS_Discriminator: 0.03305979445576668\n",
            "ITERATION_NO.: 312 LOSS_Generator: 6.781179904937744 LOSS_Discriminator: 0.06981179118156433\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.251569747924805 LOSS_Discriminator: 0.09519334137439728\n",
            "ITERATION_NO.: 314 LOSS_Generator: 7.5676703453063965 LOSS_Discriminator: 0.08626651763916016\n",
            "ITERATION_NO.: 315 LOSS_Generator: 8.682194709777832 LOSS_Discriminator: 0.04822858050465584\n",
            "ITERATION_NO.: 316 LOSS_Generator: 8.617439270019531 LOSS_Discriminator: 0.054661888629198074\n",
            "ITERATION_NO.: 317 LOSS_Generator: 8.146716117858887 LOSS_Discriminator: 0.06128210946917534\n",
            "ITERATION_NO.: 318 LOSS_Generator: 7.2214860916137695 LOSS_Discriminator: 0.03151838481426239\n",
            "ITERATION_NO.: 319 LOSS_Generator: 6.313767910003662 LOSS_Discriminator: 0.056848302483558655\n",
            "ITERATION_NO.: 320 LOSS_Generator: 7.129932880401611 LOSS_Discriminator: 0.09237219393253326\n",
            "ITERATION_NO.: 321 LOSS_Generator: 6.754147529602051 LOSS_Discriminator: 0.0778927132487297\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.4862961769104 LOSS_Discriminator: 0.14100679755210876\n",
            "ITERATION_NO.: 323 LOSS_Generator: 8.188997268676758 LOSS_Discriminator: 0.03970271721482277\n",
            "ITERATION_NO.: 324 LOSS_Generator: 7.991617202758789 LOSS_Discriminator: 0.06280504167079926\n",
            "ITERATION_NO.: 325 LOSS_Generator: 8.002006530761719 LOSS_Discriminator: 0.037992075085639954\n",
            "ITERATION_NO.: 326 LOSS_Generator: 7.2415900230407715 LOSS_Discriminator: 0.09024596214294434\n",
            "ITERATION_NO.: 327 LOSS_Generator: 6.699258327484131 LOSS_Discriminator: 0.03810573369264603\n",
            "ITERATION_NO.: 328 LOSS_Generator: 6.967912673950195 LOSS_Discriminator: 0.06545360386371613\n",
            "ITERATION_NO.: 329 LOSS_Generator: 7.209664344787598 LOSS_Discriminator: 0.04445212334394455\n",
            "ITERATION_NO.: 330 LOSS_Generator: 7.2401123046875 LOSS_Discriminator: 0.07522068172693253\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.429500579833984 LOSS_Discriminator: 0.06822916865348816\n",
            "ITERATION_NO.: 332 LOSS_Generator: 8.481738090515137 LOSS_Discriminator: 0.017216306179761887\n",
            "ITERATION_NO.: 333 LOSS_Generator: 8.589905738830566 LOSS_Discriminator: 0.05745397508144379\n",
            "ITERATION_NO.: 334 LOSS_Generator: 8.009106636047363 LOSS_Discriminator: 0.07396386563777924\n",
            "ITERATION_NO.: 335 LOSS_Generator: 6.746047496795654 LOSS_Discriminator: 0.13988156616687775\n",
            "ITERATION_NO.: 336 LOSS_Generator: 6.326822280883789 LOSS_Discriminator: 0.09689274430274963\n",
            "ITERATION_NO.: 337 LOSS_Generator: 6.6520209312438965 LOSS_Discriminator: 0.07751047611236572\n",
            "ITERATION_NO.: 338 LOSS_Generator: 7.117886543273926 LOSS_Discriminator: 0.052555542439222336\n",
            "ITERATION_NO.: 339 LOSS_Generator: 7.627617835998535 LOSS_Discriminator: 0.06454914808273315\n",
            "ITERATION_NO.: 340 LOSS_Generator: 7.832070350646973 LOSS_Discriminator: 0.06760536879301071\n",
            "ITERATION_NO.: 341 LOSS_Generator: 7.783383846282959 LOSS_Discriminator: 0.12757346034049988\n",
            "ITERATION_NO.: 342 LOSS_Generator: 7.778052806854248 LOSS_Discriminator: 0.09186379611492157\n",
            "ITERATION_NO.: 343 LOSS_Generator: 6.586811542510986 LOSS_Discriminator: 0.06230815500020981\n",
            "ITERATION_NO.: 344 LOSS_Generator: 6.183406352996826 LOSS_Discriminator: 0.07433771342039108\n",
            "ITERATION_NO.: 345 LOSS_Generator: 6.2785868644714355 LOSS_Discriminator: 0.09727416932582855\n",
            "ITERATION_NO.: 346 LOSS_Generator: 7.118802070617676 LOSS_Discriminator: 0.03326088935136795\n",
            "ITERATION_NO.: 347 LOSS_Generator: 8.080370903015137 LOSS_Discriminator: 0.06300897896289825\n",
            "ITERATION_NO.: 348 LOSS_Generator: 8.340429306030273 LOSS_Discriminator: 0.09807917475700378\n",
            "ITERATION_NO.: 349 LOSS_Generator: 8.282737731933594 LOSS_Discriminator: 0.05044962465763092\n",
            "ITERATION_NO.: 350 LOSS_Generator: 8.938024520874023 LOSS_Discriminator: 0.01906287670135498\n",
            "ITERATION_NO.: 351 LOSS_Generator: 8.29980182647705 LOSS_Discriminator: 0.09529094398021698\n",
            "ITERATION_NO.: 352 LOSS_Generator: 7.9045000076293945 LOSS_Discriminator: 0.017411869019269943\n",
            "ITERATION_NO.: 353 LOSS_Generator: 6.939367294311523 LOSS_Discriminator: 0.09813374280929565\n",
            "ITERATION_NO.: 354 LOSS_Generator: 6.159067153930664 LOSS_Discriminator: 0.07476812601089478\n",
            "ITERATION_NO.: 355 LOSS_Generator: 6.38817024230957 LOSS_Discriminator: 0.038130927830934525\n",
            "ITERATION_NO.: 356 LOSS_Generator: 7.067451000213623 LOSS_Discriminator: 0.040671899914741516\n",
            "ITERATION_NO.: 357 LOSS_Generator: 7.331506252288818 LOSS_Discriminator: 0.06429623812437057\n",
            "ITERATION_NO.: 358 LOSS_Generator: 7.823554515838623 LOSS_Discriminator: 0.0350886806845665\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.64010763168335 LOSS_Discriminator: 0.041063033044338226\n",
            "ITERATION_NO.: 360 LOSS_Generator: 7.712759971618652 LOSS_Discriminator: 0.06861920654773712\n",
            "ITERATION_NO.: 361 LOSS_Generator: 7.538778305053711 LOSS_Discriminator: 0.0322209894657135\n",
            "ITERATION_NO.: 362 LOSS_Generator: 6.5829949378967285 LOSS_Discriminator: 0.035795778036117554\n",
            "ITERATION_NO.: 363 LOSS_Generator: 6.506369113922119 LOSS_Discriminator: 0.06671728193759918\n",
            "ITERATION_NO.: 364 LOSS_Generator: 6.86874532699585 LOSS_Discriminator: 0.0456242635846138\n",
            "ITERATION_NO.: 365 LOSS_Generator: 6.993401527404785 LOSS_Discriminator: 0.10636621713638306\n",
            "ITERATION_NO.: 366 LOSS_Generator: 7.042096138000488 LOSS_Discriminator: 0.14249570667743683\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.831558704376221 LOSS_Discriminator: 0.05380678176879883\n",
            "ITERATION_NO.: 368 LOSS_Generator: 7.521580219268799 LOSS_Discriminator: 0.04317302629351616\n",
            "ITERATION_NO.: 369 LOSS_Generator: 7.686093807220459 LOSS_Discriminator: 0.11493515968322754\n",
            "ITERATION_NO.: 370 LOSS_Generator: 7.2985100746154785 LOSS_Discriminator: 0.032317813485860825\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.227636814117432 LOSS_Discriminator: 0.03756866976618767\n",
            "ITERATION_NO.: 372 LOSS_Generator: 7.071657657623291 LOSS_Discriminator: 0.05043955519795418\n",
            "ITERATION_NO.: 373 LOSS_Generator: 7.362825393676758 LOSS_Discriminator: 0.05960401892662048\n",
            "ITERATION_NO.: 374 LOSS_Generator: 8.033724784851074 LOSS_Discriminator: 0.04345240816473961\n",
            "ITERATION_NO.: 375 LOSS_Generator: 8.150650978088379 LOSS_Discriminator: 0.05368372052907944\n",
            "ITERATION_NO.: 376 LOSS_Generator: 8.011900901794434 LOSS_Discriminator: 0.13312126696109772\n",
            "ITERATION_NO.: 377 LOSS_Generator: 6.618459701538086 LOSS_Discriminator: 0.04303846135735512\n",
            "ITERATION_NO.: 378 LOSS_Generator: 6.716501235961914 LOSS_Discriminator: 0.08318035304546356\n",
            "ITERATION_NO.: 379 LOSS_Generator: 6.4645209312438965 LOSS_Discriminator: 0.07926224172115326\n",
            "ITERATION_NO.: 380 LOSS_Generator: 7.035354137420654 LOSS_Discriminator: 0.217537522315979\n",
            "ITERATION_NO.: 381 LOSS_Generator: 7.4378662109375 LOSS_Discriminator: 0.08751779794692993\n",
            "ITERATION_NO.: 382 LOSS_Generator: 8.181841850280762 LOSS_Discriminator: 0.022352948784828186\n",
            "ITERATION_NO.: 383 LOSS_Generator: 9.55958366394043 LOSS_Discriminator: 0.1356963962316513\n",
            "ITERATION_NO.: 384 LOSS_Generator: 8.370960235595703 LOSS_Discriminator: 0.05383022502064705\n",
            "ITERATION_NO.: 385 LOSS_Generator: 7.594881534576416 LOSS_Discriminator: 0.057713836431503296\n",
            "ITERATION_NO.: 386 LOSS_Generator: 6.8912458419799805 LOSS_Discriminator: 0.08828052133321762\n",
            "ITERATION_NO.: 387 LOSS_Generator: 6.572845935821533 LOSS_Discriminator: 0.07953956723213196\n",
            "ITERATION_NO.: 388 LOSS_Generator: 7.363000869750977 LOSS_Discriminator: 0.06393850594758987\n",
            "ITERATION_NO.: 389 LOSS_Generator: 8.280324935913086 LOSS_Discriminator: 0.08695271611213684\n",
            "ITERATION_NO.: 390 LOSS_Generator: 7.901068210601807 LOSS_Discriminator: 0.04301424324512482\n",
            "ITERATION_NO.: 391 LOSS_Generator: 7.7256083488464355 LOSS_Discriminator: 0.09224876761436462\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.256451606750488 LOSS_Discriminator: 0.022482819855213165\n",
            "ITERATION_NO.: 393 LOSS_Generator: 7.346147537231445 LOSS_Discriminator: 0.03892149776220322\n",
            "ITERATION_NO.: 394 LOSS_Generator: 7.177257537841797 LOSS_Discriminator: 0.11004413664340973\n",
            "ITERATION_NO.: 395 LOSS_Generator: 6.907154560089111 LOSS_Discriminator: 0.0706990659236908\n",
            "ITERATION_NO.: 396 LOSS_Generator: 6.910933971405029 LOSS_Discriminator: 0.03790111094713211\n",
            "ITERATION_NO.: 397 LOSS_Generator: 7.586004257202148 LOSS_Discriminator: 0.08238810300827026\n",
            "ITERATION_NO.: 398 LOSS_Generator: 7.4491047859191895 LOSS_Discriminator: 0.056777335703372955\n",
            "ITERATION_NO.: 399 LOSS_Generator: 7.681972026824951 LOSS_Discriminator: 0.054014239460229874\n",
            "ITERATION_NO.: 400 LOSS_Generator: 8.127108573913574 LOSS_Discriminator: 0.04750196635723114\n",
            "ITERATION_NO.: 401 LOSS_Generator: 7.770962715148926 LOSS_Discriminator: 0.07349991053342819\n",
            "ITERATION_NO.: 402 LOSS_Generator: 6.831610679626465 LOSS_Discriminator: 0.05895641818642616\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.63697624206543 LOSS_Discriminator: 0.0667547658085823\n",
            "ITERATION_NO.: 404 LOSS_Generator: 6.968595504760742 LOSS_Discriminator: 0.040798529982566833\n",
            "ITERATION_NO.: 405 LOSS_Generator: 7.045790195465088 LOSS_Discriminator: 0.08532381802797318\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.924849033355713 LOSS_Discriminator: 0.07708965986967087\n",
            "ITERATION_NO.: 407 LOSS_Generator: 7.306219577789307 LOSS_Discriminator: 0.06246674433350563\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.648831367492676 LOSS_Discriminator: 0.026246342808008194\n",
            "ITERATION_NO.: 409 LOSS_Generator: 7.516017436981201 LOSS_Discriminator: 0.04137733206152916\n",
            "ITERATION_NO.: 410 LOSS_Generator: 8.176337242126465 LOSS_Discriminator: 0.063282310962677\n",
            "ITERATION_NO.: 411 LOSS_Generator: 7.287891864776611 LOSS_Discriminator: 0.05231032893061638\n",
            "ITERATION_NO.: 412 LOSS_Generator: 7.4596476554870605 LOSS_Discriminator: 0.06185485050082207\n",
            "ITERATION_NO.: 413 LOSS_Generator: 6.038300037384033 LOSS_Discriminator: 0.10371262580156326\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.568341255187988 LOSS_Discriminator: 0.0588085874915123\n",
            "ITERATION_NO.: 415 LOSS_Generator: 6.597103118896484 LOSS_Discriminator: 0.027066081762313843\n",
            "ITERATION_NO.: 416 LOSS_Generator: 7.4420647621154785 LOSS_Discriminator: 0.05127117037773132\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.378053665161133 LOSS_Discriminator: 0.11199064552783966\n",
            "ITERATION_NO.: 418 LOSS_Generator: 7.14394998550415 LOSS_Discriminator: 0.04950389266014099\n",
            "ITERATION_NO.: 419 LOSS_Generator: 6.327962875366211 LOSS_Discriminator: 0.0734238252043724\n",
            "ITERATION_NO.: 420 LOSS_Generator: 5.734376430511475 LOSS_Discriminator: 0.06541021168231964\n",
            "ITERATION_NO.: 421 LOSS_Generator: 6.572047233581543 LOSS_Discriminator: 0.0767512395977974\n",
            "ITERATION_NO.: 422 LOSS_Generator: 6.868862152099609 LOSS_Discriminator: 0.07158898562192917\n",
            "ITERATION_NO.: 423 LOSS_Generator: 7.495215892791748 LOSS_Discriminator: 0.06711433827877045\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.503409385681152 LOSS_Discriminator: 0.09697455912828445\n",
            "ITERATION_NO.: 425 LOSS_Generator: 7.7962212562561035 LOSS_Discriminator: 0.0773911327123642\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.892477035522461 LOSS_Discriminator: 0.11625513434410095\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.280679702758789 LOSS_Discriminator: 0.11710603535175323\n",
            "ITERATION_NO.: 428 LOSS_Generator: 5.446994781494141 LOSS_Discriminator: 0.18990465998649597\n",
            "ITERATION_NO.: 429 LOSS_Generator: 7.669200897216797 LOSS_Discriminator: 0.14995735883712769\n",
            "ITERATION_NO.: 430 LOSS_Generator: 8.772773742675781 LOSS_Discriminator: 0.062230318784713745\n",
            "ITERATION_NO.: 431 LOSS_Generator: 9.820430755615234 LOSS_Discriminator: 0.0330345556139946\n",
            "ITERATION_NO.: 432 LOSS_Generator: 9.752513885498047 LOSS_Discriminator: 0.13052566349506378\n",
            "ITERATION_NO.: 433 LOSS_Generator: 9.263795852661133 LOSS_Discriminator: 0.09967699646949768\n",
            "ITERATION_NO.: 434 LOSS_Generator: 8.096894264221191 LOSS_Discriminator: 0.05194805562496185\n",
            "ITERATION_NO.: 435 LOSS_Generator: 7.012460231781006 LOSS_Discriminator: 0.025431357324123383\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.774406909942627 LOSS_Discriminator: 0.05113084241747856\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.471571445465088 LOSS_Discriminator: 0.07868237793445587\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.943024158477783 LOSS_Discriminator: 0.039581298828125\n",
            "ITERATION_NO.: 439 LOSS_Generator: 8.037138938903809 LOSS_Discriminator: 0.07217785716056824\n",
            "ITERATION_NO.: 440 LOSS_Generator: 9.091388702392578 LOSS_Discriminator: 0.04755775257945061\n",
            "ITERATION_NO.: 441 LOSS_Generator: 9.904082298278809 LOSS_Discriminator: 0.08889269083738327\n",
            "ITERATION_NO.: 442 LOSS_Generator: 9.387264251708984 LOSS_Discriminator: 0.04783375561237335\n",
            "ITERATION_NO.: 443 LOSS_Generator: 9.121135711669922 LOSS_Discriminator: 0.10871337354183197\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.394329071044922 LOSS_Discriminator: 0.07652845978736877\n",
            "ITERATION_NO.: 445 LOSS_Generator: 5.311262607574463 LOSS_Discriminator: 0.1323307305574417\n",
            "ITERATION_NO.: 446 LOSS_Generator: 5.686978340148926 LOSS_Discriminator: 0.1956024169921875\n",
            "ITERATION_NO.: 447 LOSS_Generator: 6.731243133544922 LOSS_Discriminator: 0.07823017984628677\n",
            "ITERATION_NO.: 448 LOSS_Generator: 7.618722915649414 LOSS_Discriminator: 0.0517018660902977\n",
            "ITERATION_NO.: 449 LOSS_Generator: 9.360025405883789 LOSS_Discriminator: 0.061015449464321136\n",
            "ITERATION_NO.: 450 LOSS_Generator: 8.831864356994629 LOSS_Discriminator: 0.06373737752437592\n",
            "ITERATION_NO.: 451 LOSS_Generator: 8.886573791503906 LOSS_Discriminator: 0.11160168051719666\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.516647815704346 LOSS_Discriminator: 0.014439055696129799\n",
            "ITERATION_NO.: 453 LOSS_Generator: 7.450905799865723 LOSS_Discriminator: 0.06534972786903381\n",
            "ITERATION_NO.: 454 LOSS_Generator: 6.965205669403076 LOSS_Discriminator: 0.10305564105510712\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.647397041320801 LOSS_Discriminator: 0.032905541360378265\n",
            "ITERATION_NO.: 456 LOSS_Generator: 7.706247329711914 LOSS_Discriminator: 0.08846340328454971\n",
            "ITERATION_NO.: 457 LOSS_Generator: 7.964561939239502 LOSS_Discriminator: 0.036893412470817566\n",
            "ITERATION_NO.: 458 LOSS_Generator: 8.081769943237305 LOSS_Discriminator: 0.07913504540920258\n",
            "ITERATION_NO.: 459 LOSS_Generator: 7.832446098327637 LOSS_Discriminator: 0.030280159786343575\n",
            "ITERATION_NO.: 460 LOSS_Generator: 8.106145858764648 LOSS_Discriminator: 0.03673878684639931\n",
            "ITERATION_NO.: 461 LOSS_Generator: 8.35737419128418 LOSS_Discriminator: 0.08964906632900238\n",
            "ITERATION_NO.: 462 LOSS_Generator: 7.256475925445557 LOSS_Discriminator: 0.0765407532453537\n",
            "ITERATION_NO.: 463 LOSS_Generator: 7.399593353271484 LOSS_Discriminator: 0.06750496476888657\n",
            "ITERATION_NO.: 464 LOSS_Generator: 7.693240165710449 LOSS_Discriminator: 0.049393072724342346\n",
            "ITERATION_NO.: 465 LOSS_Generator: 8.566563606262207 LOSS_Discriminator: 0.04749961197376251\n",
            "ITERATION_NO.: 466 LOSS_Generator: 8.683404922485352 LOSS_Discriminator: 0.06412435322999954\n",
            "ITERATION_NO.: 467 LOSS_Generator: 8.51430892944336 LOSS_Discriminator: 0.060906682163476944\n",
            "ITERATION_NO.: 468 LOSS_Generator: 8.530939102172852 LOSS_Discriminator: 0.09532810747623444\n",
            "ITERATION_NO.: 469 LOSS_Generator: 7.662352085113525 LOSS_Discriminator: 0.043750666081905365\n",
            "ITERATION_NO.: 470 LOSS_Generator: 7.29141092300415 LOSS_Discriminator: 0.10364563763141632\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.419548988342285 LOSS_Discriminator: 0.11352091282606125\n",
            "ITERATION_NO.: 472 LOSS_Generator: 6.433957576751709 LOSS_Discriminator: 0.09449747204780579\n",
            "ITERATION_NO.: 473 LOSS_Generator: 7.92817497253418 LOSS_Discriminator: 0.06403782963752747\n",
            "ITERATION_NO.: 474 LOSS_Generator: 8.791828155517578 LOSS_Discriminator: 0.0686023086309433\n",
            "ITERATION_NO.: 475 LOSS_Generator: 8.869996070861816 LOSS_Discriminator: 0.061925649642944336\n",
            "ITERATION_NO.: 476 LOSS_Generator: 8.849045753479004 LOSS_Discriminator: 0.07518427073955536\n",
            "ITERATION_NO.: 477 LOSS_Generator: 7.840824604034424 LOSS_Discriminator: 0.08055340498685837\n",
            "ITERATION_NO.: 478 LOSS_Generator: 6.904801845550537 LOSS_Discriminator: 0.033722952008247375\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.706665515899658 LOSS_Discriminator: 0.04778231680393219\n",
            "ITERATION_NO.: 480 LOSS_Generator: 6.489650249481201 LOSS_Discriminator: 0.05366351082921028\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.475612163543701 LOSS_Discriminator: 0.05854672193527222\n",
            "ITERATION_NO.: 482 LOSS_Generator: 7.7913408279418945 LOSS_Discriminator: 0.027656447142362595\n",
            "ITERATION_NO.: 483 LOSS_Generator: 7.6583356857299805 LOSS_Discriminator: 0.10533256828784943\n",
            "ITERATION_NO.: 484 LOSS_Generator: 8.235113143920898 LOSS_Discriminator: 0.0266652200371027\n",
            "ITERATION_NO.: 485 LOSS_Generator: 8.580401420593262 LOSS_Discriminator: 0.07040974497795105\n",
            "ITERATION_NO.: 486 LOSS_Generator: 7.698196887969971 LOSS_Discriminator: 0.08193087577819824\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.709538459777832 LOSS_Discriminator: 0.06009696424007416\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.4455485343933105 LOSS_Discriminator: 0.09390448033809662\n",
            "ITERATION_NO.: 489 LOSS_Generator: 6.549531936645508 LOSS_Discriminator: 0.06982450187206268\n",
            "ITERATION_NO.: 490 LOSS_Generator: 6.821129322052002 LOSS_Discriminator: 0.040056221187114716\n",
            "ITERATION_NO.: 491 LOSS_Generator: 7.418043613433838 LOSS_Discriminator: 0.07759249955415726\n",
            "ITERATION_NO.: 492 LOSS_Generator: 8.112363815307617 LOSS_Discriminator: 0.04348848760128021\n",
            "ITERATION_NO.: 493 LOSS_Generator: 7.849960803985596 LOSS_Discriminator: 0.1014462411403656\n",
            "ITERATION_NO.: 494 LOSS_Generator: 7.099526882171631 LOSS_Discriminator: 0.06209154054522514\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.918753147125244 LOSS_Discriminator: 0.0536581315100193\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.42440938949585 LOSS_Discriminator: 0.04047912731766701\n",
            "ITERATION_NO.: 497 LOSS_Generator: 6.2093729972839355 LOSS_Discriminator: 0.058566853404045105\n",
            "ITERATION_NO.: 498 LOSS_Generator: 7.329524993896484 LOSS_Discriminator: 0.04180757701396942\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.447556018829346 LOSS_Discriminator: 0.06041234731674194\n",
            "ITERATION_NO.: 500 LOSS_Generator: 8.013426780700684 LOSS_Discriminator: 0.021675841882824898\n",
            "ITERATION_NO.: 501 LOSS_Generator: 8.231298446655273 LOSS_Discriminator: 0.06790560483932495\n",
            "ITERATION_NO.: 502 LOSS_Generator: 8.138961791992188 LOSS_Discriminator: 0.08132866770029068\n",
            "ITERATION_NO.: 503 LOSS_Generator: 7.5477447509765625 LOSS_Discriminator: 0.08673421293497086\n",
            "ITERATION_NO.: 504 LOSS_Generator: 6.716291904449463 LOSS_Discriminator: 0.08831781148910522\n",
            "ITERATION_NO.: 505 LOSS_Generator: 6.667202949523926 LOSS_Discriminator: 0.07623986899852753\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.194659233093262 LOSS_Discriminator: 0.05979057401418686\n",
            "ITERATION_NO.: 507 LOSS_Generator: 6.595175266265869 LOSS_Discriminator: 0.07711981236934662\n",
            "ITERATION_NO.: 508 LOSS_Generator: 7.235663890838623 LOSS_Discriminator: 0.06130623817443848\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.517012119293213 LOSS_Discriminator: 0.02651226706802845\n",
            "ITERATION_NO.: 510 LOSS_Generator: 8.087102890014648 LOSS_Discriminator: 0.026439104229211807\n",
            "ITERATION_NO.: 511 LOSS_Generator: 8.122654914855957 LOSS_Discriminator: 0.0836111456155777\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.597592830657959 LOSS_Discriminator: 0.08749113976955414\n",
            "ITERATION_NO.: 513 LOSS_Generator: 7.1171674728393555 LOSS_Discriminator: 0.05323399230837822\n",
            "ITERATION_NO.: 514 LOSS_Generator: 6.47639274597168 LOSS_Discriminator: 0.09833711385726929\n",
            "ITERATION_NO.: 515 LOSS_Generator: 6.236029624938965 LOSS_Discriminator: 0.07429401576519012\n",
            "ITERATION_NO.: 516 LOSS_Generator: 7.013298988342285 LOSS_Discriminator: 0.042510904371738434\n",
            "ITERATION_NO.: 517 LOSS_Generator: 7.438216686248779 LOSS_Discriminator: 0.07563719153404236\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.928427219390869 LOSS_Discriminator: 0.07696714997291565\n",
            "ITERATION_NO.: 519 LOSS_Generator: 7.545413970947266 LOSS_Discriminator: 0.04895658791065216\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.931633472442627 LOSS_Discriminator: 0.04578737914562225\n",
            "ITERATION_NO.: 521 LOSS_Generator: 7.485142707824707 LOSS_Discriminator: 0.08610109984874725\n",
            "ITERATION_NO.: 522 LOSS_Generator: 7.185697555541992 LOSS_Discriminator: 0.060954250395298004\n",
            "ITERATION_NO.: 523 LOSS_Generator: 6.818009853363037 LOSS_Discriminator: 0.05561525374650955\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.997343063354492 LOSS_Discriminator: 0.05305801331996918\n",
            "ITERATION_NO.: 525 LOSS_Generator: 7.148584365844727 LOSS_Discriminator: 0.06833000481128693\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.967289447784424 LOSS_Discriminator: 0.06434754282236099\n",
            "ITERATION_NO.: 527 LOSS_Generator: 8.27470874786377 LOSS_Discriminator: 0.07847250998020172\n",
            "ITERATION_NO.: 528 LOSS_Generator: 8.181780815124512 LOSS_Discriminator: 0.04371411353349686\n",
            "ITERATION_NO.: 529 LOSS_Generator: 7.730655670166016 LOSS_Discriminator: 0.05103159323334694\n",
            "ITERATION_NO.: 530 LOSS_Generator: 6.803949356079102 LOSS_Discriminator: 0.056108225136995316\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.381326675415039 LOSS_Discriminator: 0.04848684370517731\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.969870090484619 LOSS_Discriminator: 0.062326639890670776\n",
            "ITERATION_NO.: 533 LOSS_Generator: 6.756866455078125 LOSS_Discriminator: 0.1116640642285347\n",
            "ITERATION_NO.: 534 LOSS_Generator: 6.6733078956604 LOSS_Discriminator: 0.033163249492645264\n",
            "ITERATION_NO.: 535 LOSS_Generator: 6.588209629058838 LOSS_Discriminator: 0.09722192585468292\n",
            "ITERATION_NO.: 536 LOSS_Generator: 7.696142196655273 LOSS_Discriminator: 0.103512704372406\n",
            "ITERATION_NO.: 537 LOSS_Generator: 7.960183143615723 LOSS_Discriminator: 0.055879250168800354\n",
            "ITERATION_NO.: 538 LOSS_Generator: 7.498765468597412 LOSS_Discriminator: 0.11143015325069427\n",
            "ITERATION_NO.: 539 LOSS_Generator: 8.674549102783203 LOSS_Discriminator: 0.07409527897834778\n",
            "ITERATION_NO.: 540 LOSS_Generator: 8.090723037719727 LOSS_Discriminator: 0.13122625648975372\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.959718704223633 LOSS_Discriminator: 0.10296668112277985\n",
            "ITERATION_NO.: 542 LOSS_Generator: 6.216001510620117 LOSS_Discriminator: 0.06413781642913818\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.8206787109375 LOSS_Discriminator: 0.11030271649360657\n",
            "ITERATION_NO.: 544 LOSS_Generator: 7.618435859680176 LOSS_Discriminator: 0.07108081877231598\n",
            "ITERATION_NO.: 545 LOSS_Generator: 8.53977108001709 LOSS_Discriminator: 0.07880444079637527\n",
            "ITERATION_NO.: 546 LOSS_Generator: 8.795352935791016 LOSS_Discriminator: 0.03841356933116913\n",
            "ITERATION_NO.: 547 LOSS_Generator: 8.257794380187988 LOSS_Discriminator: 0.13137710094451904\n",
            "ITERATION_NO.: 548 LOSS_Generator: 8.012070655822754 LOSS_Discriminator: 0.10970590263605118\n",
            "ITERATION_NO.: 549 LOSS_Generator: 6.61942195892334 LOSS_Discriminator: 0.0404139906167984\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.2084197998046875 LOSS_Discriminator: 0.06199297681450844\n",
            "ITERATION_NO.: 551 LOSS_Generator: 6.777479648590088 LOSS_Discriminator: 0.08036413043737411\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.460971832275391 LOSS_Discriminator: 0.12748348712921143\n",
            "ITERATION_NO.: 553 LOSS_Generator: 7.016841411590576 LOSS_Discriminator: 0.05334261804819107\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.230997085571289 LOSS_Discriminator: 0.07228899747133255\n",
            "ITERATION_NO.: 555 LOSS_Generator: 7.28337287902832 LOSS_Discriminator: 0.05363859981298447\n",
            "ITERATION_NO.: 556 LOSS_Generator: 7.340982437133789 LOSS_Discriminator: 0.01854279264807701\n",
            "ITERATION_NO.: 557 LOSS_Generator: 7.626495361328125 LOSS_Discriminator: 0.04859812557697296\n",
            "ITERATION_NO.: 558 LOSS_Generator: 7.2279534339904785 LOSS_Discriminator: 0.07822482287883759\n",
            "ITERATION_NO.: 559 LOSS_Generator: 7.047792434692383 LOSS_Discriminator: 0.03656509891152382\n",
            "ITERATION_NO.: 560 LOSS_Generator: 7.891596794128418 LOSS_Discriminator: 0.04904550313949585\n",
            "ITERATION_NO.: 561 LOSS_Generator: 6.905860424041748 LOSS_Discriminator: 0.1104133278131485\n",
            "ITERATION_NO.: 562 LOSS_Generator: 6.43658447265625 LOSS_Discriminator: 0.05585978925228119\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.346541881561279 LOSS_Discriminator: 0.12888935208320618\n",
            "ITERATION_NO.: 564 LOSS_Generator: 6.817241191864014 LOSS_Discriminator: 0.09803656488656998\n",
            "ITERATION_NO.: 565 LOSS_Generator: 7.710721492767334 LOSS_Discriminator: 0.10156643390655518\n",
            "ITERATION_NO.: 566 LOSS_Generator: 7.942230224609375 LOSS_Discriminator: 0.03978528454899788\n",
            "ITERATION_NO.: 567 LOSS_Generator: 8.317099571228027 LOSS_Discriminator: 0.054698970168828964\n",
            "ITERATION_NO.: 568 LOSS_Generator: 7.4605512619018555 LOSS_Discriminator: 0.06922229379415512\n",
            "ITERATION_NO.: 569 LOSS_Generator: 7.5294623374938965 LOSS_Discriminator: 0.1367104947566986\n",
            "ITERATION_NO.: 570 LOSS_Generator: 6.908944606781006 LOSS_Discriminator: 0.06046171113848686\n",
            "ITERATION_NO.: 571 LOSS_Generator: 7.027149677276611 LOSS_Discriminator: 0.09908182919025421\n",
            "ITERATION_NO.: 572 LOSS_Generator: 6.982149124145508 LOSS_Discriminator: 0.010176042094826698\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.419912815093994 LOSS_Discriminator: 0.07288503646850586\n",
            "ITERATION_NO.: 574 LOSS_Generator: 7.128385543823242 LOSS_Discriminator: 0.11887694150209427\n",
            "ITERATION_NO.: 575 LOSS_Generator: 7.051229476928711 LOSS_Discriminator: 0.07910645008087158\n",
            "ITERATION_NO.: 576 LOSS_Generator: 7.2386980056762695 LOSS_Discriminator: 0.15237879753112793\n",
            "ITERATION_NO.: 577 LOSS_Generator: 7.987940788269043 LOSS_Discriminator: 0.08747853338718414\n",
            "ITERATION_NO.: 578 LOSS_Generator: 7.8442158699035645 LOSS_Discriminator: 0.06469157338142395\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.52858829498291 LOSS_Discriminator: 0.07488325983285904\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.302219867706299 LOSS_Discriminator: 0.11996620893478394\n",
            "ITERATION_NO.: 581 LOSS_Generator: 7.374490261077881 LOSS_Discriminator: 0.06027283892035484\n",
            "ITERATION_NO.: 582 LOSS_Generator: 7.6391282081604 LOSS_Discriminator: 0.06744424998760223\n",
            "ITERATION_NO.: 583 LOSS_Generator: 8.354233741760254 LOSS_Discriminator: 0.0658649355173111\n",
            "ITERATION_NO.: 584 LOSS_Generator: 8.305402755737305 LOSS_Discriminator: 0.07311592996120453\n",
            "ITERATION_NO.: 585 LOSS_Generator: 8.706971168518066 LOSS_Discriminator: 0.07827339321374893\n",
            "ITERATION_NO.: 586 LOSS_Generator: 8.506630897521973 LOSS_Discriminator: 0.07838324457406998\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.650112152099609 LOSS_Discriminator: 0.06954767554998398\n",
            "ITERATION_NO.: 588 LOSS_Generator: 6.684935092926025 LOSS_Discriminator: 0.09754224866628647\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.681589126586914 LOSS_Discriminator: 0.05135234445333481\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.8461384773254395 LOSS_Discriminator: 0.09376370906829834\n",
            "ITERATION_NO.: 591 LOSS_Generator: 7.2555670738220215 LOSS_Discriminator: 0.11330653727054596\n",
            "ITERATION_NO.: 592 LOSS_Generator: 8.791507720947266 LOSS_Discriminator: 0.05209742859005928\n",
            "ITERATION_NO.: 593 LOSS_Generator: 8.41545581817627 LOSS_Discriminator: 0.05692851543426514\n",
            "ITERATION_NO.: 594 LOSS_Generator: 8.21677017211914 LOSS_Discriminator: 0.05163097381591797\n",
            "ITERATION_NO.: 595 LOSS_Generator: 7.965868949890137 LOSS_Discriminator: 0.039611272513866425\n",
            "ITERATION_NO.: 596 LOSS_Generator: 7.0757155418396 LOSS_Discriminator: 0.08255325257778168\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.863311767578125 LOSS_Discriminator: 0.0380287729203701\n",
            "ITERATION_NO.: 598 LOSS_Generator: 6.537661075592041 LOSS_Discriminator: 0.06538507342338562\n",
            "ITERATION_NO.: 599 LOSS_Generator: 7.944771766662598 LOSS_Discriminator: 0.07019839435815811\n",
            "ITERATION_NO.: 600 LOSS_Generator: 7.8083906173706055 LOSS_Discriminator: 0.07252398133277893\n",
            "EPOCH OVER: 46\n",
            "ITERATION_NO.: 1 LOSS_Generator: 8.391329765319824 LOSS_Discriminator: 0.031875599175691605\n",
            "ITERATION_NO.: 2 LOSS_Generator: 8.144667625427246 LOSS_Discriminator: 0.12965989112854004\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.135446548461914 LOSS_Discriminator: 0.059339091181755066\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.240091323852539 LOSS_Discriminator: 0.05197250843048096\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.947160720825195 LOSS_Discriminator: 0.057723306119441986\n",
            "ITERATION_NO.: 6 LOSS_Generator: 7.589052200317383 LOSS_Discriminator: 0.035483554005622864\n",
            "ITERATION_NO.: 7 LOSS_Generator: 7.240233898162842 LOSS_Discriminator: 0.03835621103644371\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.169705867767334 LOSS_Discriminator: 0.03502214699983597\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.658906936645508 LOSS_Discriminator: 0.03004707396030426\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.632813930511475 LOSS_Discriminator: 0.0604381300508976\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.200428009033203 LOSS_Discriminator: 0.053841449320316315\n",
            "ITERATION_NO.: 12 LOSS_Generator: 6.963736534118652 LOSS_Discriminator: 0.06033158302307129\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.194758892059326 LOSS_Discriminator: 0.04664154350757599\n",
            "ITERATION_NO.: 14 LOSS_Generator: 6.740044593811035 LOSS_Discriminator: 0.04026367515325546\n",
            "ITERATION_NO.: 15 LOSS_Generator: 7.122237682342529 LOSS_Discriminator: 0.0836828351020813\n",
            "ITERATION_NO.: 16 LOSS_Generator: 7.281018257141113 LOSS_Discriminator: 0.09592932462692261\n",
            "ITERATION_NO.: 17 LOSS_Generator: 7.6928863525390625 LOSS_Discriminator: 0.04288382828235626\n",
            "ITERATION_NO.: 18 LOSS_Generator: 7.722047805786133 LOSS_Discriminator: 0.05985262244939804\n",
            "ITERATION_NO.: 19 LOSS_Generator: 7.770451068878174 LOSS_Discriminator: 0.10251237452030182\n",
            "ITERATION_NO.: 20 LOSS_Generator: 6.610699653625488 LOSS_Discriminator: 0.04113912582397461\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.530889511108398 LOSS_Discriminator: 0.09841860830783844\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.36312198638916 LOSS_Discriminator: 0.09267028421163559\n",
            "ITERATION_NO.: 23 LOSS_Generator: 8.034161567687988 LOSS_Discriminator: 0.0670062005519867\n",
            "ITERATION_NO.: 24 LOSS_Generator: 8.187244415283203 LOSS_Discriminator: 0.02749168872833252\n",
            "ITERATION_NO.: 25 LOSS_Generator: 8.434672355651855 LOSS_Discriminator: 0.1550230085849762\n",
            "ITERATION_NO.: 26 LOSS_Generator: 7.910284042358398 LOSS_Discriminator: 0.07442902028560638\n",
            "ITERATION_NO.: 27 LOSS_Generator: 7.354498386383057 LOSS_Discriminator: 0.05013609677553177\n",
            "ITERATION_NO.: 28 LOSS_Generator: 6.218538761138916 LOSS_Discriminator: 0.04637831076979637\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.4683685302734375 LOSS_Discriminator: 0.05365997552871704\n",
            "ITERATION_NO.: 30 LOSS_Generator: 7.074951648712158 LOSS_Discriminator: 0.0629330649971962\n",
            "ITERATION_NO.: 31 LOSS_Generator: 7.617204666137695 LOSS_Discriminator: 0.05168493464589119\n",
            "ITERATION_NO.: 32 LOSS_Generator: 8.01120376586914 LOSS_Discriminator: 0.02234911359846592\n",
            "ITERATION_NO.: 33 LOSS_Generator: 8.197600364685059 LOSS_Discriminator: 0.041087254881858826\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.410675048828125 LOSS_Discriminator: 0.05032041668891907\n",
            "ITERATION_NO.: 35 LOSS_Generator: 7.878670692443848 LOSS_Discriminator: 0.0898534506559372\n",
            "ITERATION_NO.: 36 LOSS_Generator: 6.946171283721924 LOSS_Discriminator: 0.06573302298784256\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.357834339141846 LOSS_Discriminator: 0.06471274048089981\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.299530506134033 LOSS_Discriminator: 0.08751258254051208\n",
            "ITERATION_NO.: 39 LOSS_Generator: 6.859725475311279 LOSS_Discriminator: 0.05340234190225601\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.532795429229736 LOSS_Discriminator: 0.10711286962032318\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.318780899047852 LOSS_Discriminator: 0.046713367104530334\n",
            "ITERATION_NO.: 42 LOSS_Generator: 6.9372477531433105 LOSS_Discriminator: 0.1870289444923401\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.3476386070251465 LOSS_Discriminator: 0.1629646122455597\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.047543525695801 LOSS_Discriminator: 0.06634508073329926\n",
            "ITERATION_NO.: 45 LOSS_Generator: 7.656230449676514 LOSS_Discriminator: 0.0383882075548172\n",
            "ITERATION_NO.: 46 LOSS_Generator: 7.99127197265625 LOSS_Discriminator: 0.09413175284862518\n",
            "ITERATION_NO.: 47 LOSS_Generator: 7.896113395690918 LOSS_Discriminator: 0.09521612524986267\n",
            "ITERATION_NO.: 48 LOSS_Generator: 6.836694717407227 LOSS_Discriminator: 0.07579405605792999\n",
            "ITERATION_NO.: 49 LOSS_Generator: 6.6202616691589355 LOSS_Discriminator: 0.07364735007286072\n",
            "ITERATION_NO.: 50 LOSS_Generator: 6.127552032470703 LOSS_Discriminator: 0.10215523838996887\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.7148942947387695 LOSS_Discriminator: 0.06881014257669449\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.192709922790527 LOSS_Discriminator: 0.0727614313364029\n",
            "ITERATION_NO.: 53 LOSS_Generator: 8.221548080444336 LOSS_Discriminator: 0.11768229305744171\n",
            "ITERATION_NO.: 54 LOSS_Generator: 8.088159561157227 LOSS_Discriminator: 0.08458548784255981\n",
            "ITERATION_NO.: 55 LOSS_Generator: 6.738146781921387 LOSS_Discriminator: 0.09403879195451736\n",
            "ITERATION_NO.: 56 LOSS_Generator: 6.229928970336914 LOSS_Discriminator: 0.05677472800016403\n",
            "ITERATION_NO.: 57 LOSS_Generator: 6.655152797698975 LOSS_Discriminator: 0.08789877593517303\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.856042385101318 LOSS_Discriminator: 0.06699573248624802\n",
            "ITERATION_NO.: 59 LOSS_Generator: 8.573657035827637 LOSS_Discriminator: 0.043456509709358215\n",
            "ITERATION_NO.: 60 LOSS_Generator: 8.620924949645996 LOSS_Discriminator: 0.033908069133758545\n",
            "ITERATION_NO.: 61 LOSS_Generator: 8.739623069763184 LOSS_Discriminator: 0.01366920955479145\n",
            "ITERATION_NO.: 62 LOSS_Generator: 7.681704044342041 LOSS_Discriminator: 0.09458693116903305\n",
            "ITERATION_NO.: 63 LOSS_Generator: 7.140946865081787 LOSS_Discriminator: 0.1207289770245552\n",
            "ITERATION_NO.: 64 LOSS_Generator: 6.269543647766113 LOSS_Discriminator: 0.09810368716716766\n",
            "ITERATION_NO.: 65 LOSS_Generator: 5.591716766357422 LOSS_Discriminator: 0.06956096738576889\n",
            "ITERATION_NO.: 66 LOSS_Generator: 6.135199546813965 LOSS_Discriminator: 0.14621232450008392\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.900691986083984 LOSS_Discriminator: 0.03644487261772156\n",
            "ITERATION_NO.: 68 LOSS_Generator: 8.848539352416992 LOSS_Discriminator: 0.07463175058364868\n",
            "ITERATION_NO.: 69 LOSS_Generator: 8.689257621765137 LOSS_Discriminator: 0.09578309208154678\n",
            "ITERATION_NO.: 70 LOSS_Generator: 8.228874206542969 LOSS_Discriminator: 0.07438776642084122\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.235622406005859 LOSS_Discriminator: 0.046080805361270905\n",
            "ITERATION_NO.: 72 LOSS_Generator: 6.402826309204102 LOSS_Discriminator: 0.09425205737352371\n",
            "ITERATION_NO.: 73 LOSS_Generator: 6.910769462585449 LOSS_Discriminator: 0.07479267567396164\n",
            "ITERATION_NO.: 74 LOSS_Generator: 7.525601863861084 LOSS_Discriminator: 0.07744638621807098\n",
            "ITERATION_NO.: 75 LOSS_Generator: 7.928876876831055 LOSS_Discriminator: 0.05085034668445587\n",
            "ITERATION_NO.: 76 LOSS_Generator: 8.172602653503418 LOSS_Discriminator: 0.08697565644979477\n",
            "ITERATION_NO.: 77 LOSS_Generator: 8.2277193069458 LOSS_Discriminator: 0.04739341139793396\n",
            "ITERATION_NO.: 78 LOSS_Generator: 8.704510688781738 LOSS_Discriminator: 0.13915136456489563\n",
            "ITERATION_NO.: 79 LOSS_Generator: 7.820217132568359 LOSS_Discriminator: 0.11882570385932922\n",
            "ITERATION_NO.: 80 LOSS_Generator: 7.323381423950195 LOSS_Discriminator: 0.027747394517064095\n",
            "ITERATION_NO.: 81 LOSS_Generator: 6.182023525238037 LOSS_Discriminator: 0.03984864428639412\n",
            "ITERATION_NO.: 82 LOSS_Generator: 6.752595901489258 LOSS_Discriminator: 0.06729159504175186\n",
            "ITERATION_NO.: 83 LOSS_Generator: 7.591479301452637 LOSS_Discriminator: 0.028301365673542023\n",
            "ITERATION_NO.: 84 LOSS_Generator: 8.007317543029785 LOSS_Discriminator: 0.04837862774729729\n",
            "ITERATION_NO.: 85 LOSS_Generator: 8.307528495788574 LOSS_Discriminator: 0.040265053510665894\n",
            "ITERATION_NO.: 86 LOSS_Generator: 8.402761459350586 LOSS_Discriminator: 0.07542241364717484\n",
            "ITERATION_NO.: 87 LOSS_Generator: 7.606234550476074 LOSS_Discriminator: 0.045366376638412476\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.26132869720459 LOSS_Discriminator: 0.04498216509819031\n",
            "ITERATION_NO.: 89 LOSS_Generator: 7.208151817321777 LOSS_Discriminator: 0.05474722757935524\n",
            "ITERATION_NO.: 90 LOSS_Generator: 6.912869453430176 LOSS_Discriminator: 0.08227591216564178\n",
            "ITERATION_NO.: 91 LOSS_Generator: 7.009127616882324 LOSS_Discriminator: 0.047865815460681915\n",
            "ITERATION_NO.: 92 LOSS_Generator: 7.394585609436035 LOSS_Discriminator: 0.06976818293333054\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.673780918121338 LOSS_Discriminator: 0.044723570346832275\n",
            "ITERATION_NO.: 94 LOSS_Generator: 7.188528537750244 LOSS_Discriminator: 0.06819584965705872\n",
            "ITERATION_NO.: 95 LOSS_Generator: 6.927537441253662 LOSS_Discriminator: 0.09425878524780273\n",
            "ITERATION_NO.: 96 LOSS_Generator: 6.662359714508057 LOSS_Discriminator: 0.07204452157020569\n",
            "ITERATION_NO.: 97 LOSS_Generator: 7.271944999694824 LOSS_Discriminator: 0.05789025127887726\n",
            "ITERATION_NO.: 98 LOSS_Generator: 7.470579147338867 LOSS_Discriminator: 0.08806149661540985\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.439680099487305 LOSS_Discriminator: 0.01746244542300701\n",
            "ITERATION_NO.: 100 LOSS_Generator: 7.505615711212158 LOSS_Discriminator: 0.04205116629600525\n",
            "ITERATION_NO.: 101 LOSS_Generator: 7.228766441345215 LOSS_Discriminator: 0.053859513252973557\n",
            "ITERATION_NO.: 102 LOSS_Generator: 7.489349365234375 LOSS_Discriminator: 0.07901706546545029\n",
            "ITERATION_NO.: 103 LOSS_Generator: 7.14246940612793 LOSS_Discriminator: 0.06980375200510025\n",
            "ITERATION_NO.: 104 LOSS_Generator: 7.260931015014648 LOSS_Discriminator: 0.07292501628398895\n",
            "ITERATION_NO.: 105 LOSS_Generator: 7.378222465515137 LOSS_Discriminator: 0.0820726603269577\n",
            "ITERATION_NO.: 106 LOSS_Generator: 7.6269659996032715 LOSS_Discriminator: 0.02513333223760128\n",
            "ITERATION_NO.: 107 LOSS_Generator: 7.284543514251709 LOSS_Discriminator: 0.06356362998485565\n",
            "ITERATION_NO.: 108 LOSS_Generator: 7.233068466186523 LOSS_Discriminator: 0.028471648693084717\n",
            "ITERATION_NO.: 109 LOSS_Generator: 7.632507801055908 LOSS_Discriminator: 0.10420531034469604\n",
            "ITERATION_NO.: 110 LOSS_Generator: 8.077265739440918 LOSS_Discriminator: 0.08122925460338593\n",
            "ITERATION_NO.: 111 LOSS_Generator: 7.744401931762695 LOSS_Discriminator: 0.08911753445863724\n",
            "ITERATION_NO.: 112 LOSS_Generator: 6.050868511199951 LOSS_Discriminator: 0.06641563773155212\n",
            "ITERATION_NO.: 113 LOSS_Generator: 5.996751308441162 LOSS_Discriminator: 0.07089062035083771\n",
            "ITERATION_NO.: 114 LOSS_Generator: 7.1373772621154785 LOSS_Discriminator: 0.11646362394094467\n",
            "ITERATION_NO.: 115 LOSS_Generator: 8.265938758850098 LOSS_Discriminator: 0.08720213174819946\n",
            "ITERATION_NO.: 116 LOSS_Generator: 8.210290908813477 LOSS_Discriminator: 0.06961894780397415\n",
            "ITERATION_NO.: 117 LOSS_Generator: 8.207677841186523 LOSS_Discriminator: 0.05423104017972946\n",
            "ITERATION_NO.: 118 LOSS_Generator: 8.675890922546387 LOSS_Discriminator: 0.08034883439540863\n",
            "ITERATION_NO.: 119 LOSS_Generator: 6.8788604736328125 LOSS_Discriminator: 0.15939033031463623\n",
            "ITERATION_NO.: 120 LOSS_Generator: 5.868539810180664 LOSS_Discriminator: 0.059799257665872574\n",
            "ITERATION_NO.: 121 LOSS_Generator: 6.306865692138672 LOSS_Discriminator: 0.1678600013256073\n",
            "ITERATION_NO.: 122 LOSS_Generator: 7.031080722808838 LOSS_Discriminator: 0.05339762568473816\n",
            "ITERATION_NO.: 123 LOSS_Generator: 8.183550834655762 LOSS_Discriminator: 0.040220245718955994\n",
            "ITERATION_NO.: 124 LOSS_Generator: 8.369054794311523 LOSS_Discriminator: 0.12968799471855164\n",
            "ITERATION_NO.: 125 LOSS_Generator: 8.067276954650879 LOSS_Discriminator: 0.10302910208702087\n",
            "ITERATION_NO.: 126 LOSS_Generator: 7.091312408447266 LOSS_Discriminator: 0.010102413594722748\n",
            "ITERATION_NO.: 127 LOSS_Generator: 7.07015323638916 LOSS_Discriminator: 0.018825547769665718\n",
            "ITERATION_NO.: 128 LOSS_Generator: 6.605587005615234 LOSS_Discriminator: 0.07973435521125793\n",
            "ITERATION_NO.: 129 LOSS_Generator: 6.797414779663086 LOSS_Discriminator: 0.08267712593078613\n",
            "ITERATION_NO.: 130 LOSS_Generator: 7.089125156402588 LOSS_Discriminator: 0.07870322465896606\n",
            "ITERATION_NO.: 131 LOSS_Generator: 8.059581756591797 LOSS_Discriminator: 0.08761325478553772\n",
            "ITERATION_NO.: 132 LOSS_Generator: 7.375356674194336 LOSS_Discriminator: 0.05788993835449219\n",
            "ITERATION_NO.: 133 LOSS_Generator: 8.108428001403809 LOSS_Discriminator: 0.08086387813091278\n",
            "ITERATION_NO.: 134 LOSS_Generator: 7.73502254486084 LOSS_Discriminator: 0.06990844011306763\n",
            "ITERATION_NO.: 135 LOSS_Generator: 6.854707717895508 LOSS_Discriminator: 0.019283175468444824\n",
            "ITERATION_NO.: 136 LOSS_Generator: 7.000181198120117 LOSS_Discriminator: 0.06412188708782196\n",
            "ITERATION_NO.: 137 LOSS_Generator: 6.779116153717041 LOSS_Discriminator: 0.10972099006175995\n",
            "ITERATION_NO.: 138 LOSS_Generator: 7.988228797912598 LOSS_Discriminator: 0.052926480770111084\n",
            "ITERATION_NO.: 139 LOSS_Generator: 8.045907020568848 LOSS_Discriminator: 0.06591540575027466\n",
            "ITERATION_NO.: 140 LOSS_Generator: 7.120233058929443 LOSS_Discriminator: 0.15047629177570343\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.672267436981201 LOSS_Discriminator: 0.1351836770772934\n",
            "ITERATION_NO.: 142 LOSS_Generator: 6.145803928375244 LOSS_Discriminator: 0.09095779061317444\n",
            "ITERATION_NO.: 143 LOSS_Generator: 6.661785125732422 LOSS_Discriminator: 0.07622484117746353\n",
            "ITERATION_NO.: 144 LOSS_Generator: 7.677131175994873 LOSS_Discriminator: 0.08176387846469879\n",
            "ITERATION_NO.: 145 LOSS_Generator: 9.128000259399414 LOSS_Discriminator: 0.043590836226940155\n",
            "ITERATION_NO.: 146 LOSS_Generator: 9.185498237609863 LOSS_Discriminator: 0.11093946546316147\n",
            "ITERATION_NO.: 147 LOSS_Generator: 8.334081649780273 LOSS_Discriminator: 0.08442094922065735\n",
            "ITERATION_NO.: 148 LOSS_Generator: 7.429774284362793 LOSS_Discriminator: 0.04500268027186394\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.262453079223633 LOSS_Discriminator: 0.05963650345802307\n",
            "ITERATION_NO.: 150 LOSS_Generator: 4.872076511383057 LOSS_Discriminator: 0.13973821699619293\n",
            "ITERATION_NO.: 151 LOSS_Generator: 6.736269950866699 LOSS_Discriminator: 0.12900608777999878\n",
            "ITERATION_NO.: 152 LOSS_Generator: 8.065321922302246 LOSS_Discriminator: 0.04768797382712364\n",
            "ITERATION_NO.: 153 LOSS_Generator: 8.829329490661621 LOSS_Discriminator: 0.019918832927942276\n",
            "ITERATION_NO.: 154 LOSS_Generator: 9.05005168914795 LOSS_Discriminator: 0.09854818880558014\n",
            "ITERATION_NO.: 155 LOSS_Generator: 8.51176643371582 LOSS_Discriminator: 0.036378636956214905\n",
            "ITERATION_NO.: 156 LOSS_Generator: 8.266801834106445 LOSS_Discriminator: 0.05750015377998352\n",
            "ITERATION_NO.: 157 LOSS_Generator: 6.8750081062316895 LOSS_Discriminator: 0.049529269337654114\n",
            "ITERATION_NO.: 158 LOSS_Generator: 6.242877960205078 LOSS_Discriminator: 0.05827917158603668\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.7737250328063965 LOSS_Discriminator: 0.0419437438249588\n",
            "ITERATION_NO.: 160 LOSS_Generator: 5.911870002746582 LOSS_Discriminator: 0.09003061056137085\n",
            "ITERATION_NO.: 161 LOSS_Generator: 6.6869683265686035 LOSS_Discriminator: 0.06559416651725769\n",
            "ITERATION_NO.: 162 LOSS_Generator: 8.06430435180664 LOSS_Discriminator: 0.10567235946655273\n",
            "ITERATION_NO.: 163 LOSS_Generator: 9.085967063903809 LOSS_Discriminator: 0.06691908091306686\n",
            "ITERATION_NO.: 164 LOSS_Generator: 8.56269645690918 LOSS_Discriminator: 0.09077335149049759\n",
            "ITERATION_NO.: 165 LOSS_Generator: 7.200603485107422 LOSS_Discriminator: 0.1614513397216797\n",
            "ITERATION_NO.: 166 LOSS_Generator: 6.035975456237793 LOSS_Discriminator: 0.1417522430419922\n",
            "ITERATION_NO.: 167 LOSS_Generator: 5.054618835449219 LOSS_Discriminator: 0.07057520747184753\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.8962178230285645 LOSS_Discriminator: 0.13598136603832245\n",
            "ITERATION_NO.: 169 LOSS_Generator: 7.631328582763672 LOSS_Discriminator: 0.1276126354932785\n",
            "ITERATION_NO.: 170 LOSS_Generator: 9.052767753601074 LOSS_Discriminator: 0.024176089093089104\n",
            "ITERATION_NO.: 171 LOSS_Generator: 9.374202728271484 LOSS_Discriminator: 0.14357468485832214\n",
            "ITERATION_NO.: 172 LOSS_Generator: 8.840279579162598 LOSS_Discriminator: 0.1588594913482666\n",
            "ITERATION_NO.: 173 LOSS_Generator: 7.952509880065918 LOSS_Discriminator: 0.08942876756191254\n",
            "ITERATION_NO.: 174 LOSS_Generator: 6.5919189453125 LOSS_Discriminator: 0.05209098756313324\n",
            "ITERATION_NO.: 175 LOSS_Generator: 5.41845703125 LOSS_Discriminator: 0.06328171491622925\n",
            "ITERATION_NO.: 176 LOSS_Generator: 5.893942832946777 LOSS_Discriminator: 0.08459927141666412\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.5647292137146 LOSS_Discriminator: 0.11996624618768692\n",
            "ITERATION_NO.: 178 LOSS_Generator: 7.261624336242676 LOSS_Discriminator: 0.08191439509391785\n",
            "ITERATION_NO.: 179 LOSS_Generator: 8.91797161102295 LOSS_Discriminator: 0.04811490327119827\n",
            "ITERATION_NO.: 180 LOSS_Generator: 9.444727897644043 LOSS_Discriminator: 0.02956562489271164\n",
            "ITERATION_NO.: 181 LOSS_Generator: 8.662014961242676 LOSS_Discriminator: 0.060678355395793915\n",
            "ITERATION_NO.: 182 LOSS_Generator: 9.16832447052002 LOSS_Discriminator: 0.072858065366745\n",
            "ITERATION_NO.: 183 LOSS_Generator: 8.116509437561035 LOSS_Discriminator: 0.04380907118320465\n",
            "ITERATION_NO.: 184 LOSS_Generator: 7.285346508026123 LOSS_Discriminator: 0.028513260185718536\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.459121227264404 LOSS_Discriminator: 0.11805294454097748\n",
            "ITERATION_NO.: 186 LOSS_Generator: 7.2024006843566895 LOSS_Discriminator: 0.10239513963460922\n",
            "ITERATION_NO.: 187 LOSS_Generator: 6.850208759307861 LOSS_Discriminator: 0.07510679960250854\n",
            "ITERATION_NO.: 188 LOSS_Generator: 7.499040603637695 LOSS_Discriminator: 0.028488336130976677\n",
            "ITERATION_NO.: 189 LOSS_Generator: 7.679863452911377 LOSS_Discriminator: 0.04410368576645851\n",
            "ITERATION_NO.: 190 LOSS_Generator: 7.707223415374756 LOSS_Discriminator: 0.07184423506259918\n",
            "ITERATION_NO.: 191 LOSS_Generator: 8.122332572937012 LOSS_Discriminator: 0.04190870001912117\n",
            "ITERATION_NO.: 192 LOSS_Generator: 8.099225044250488 LOSS_Discriminator: 0.052553948014974594\n",
            "ITERATION_NO.: 193 LOSS_Generator: 7.8213396072387695 LOSS_Discriminator: 0.07502750307321548\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.418981552124023 LOSS_Discriminator: 0.07472603768110275\n",
            "ITERATION_NO.: 195 LOSS_Generator: 6.361729621887207 LOSS_Discriminator: 0.05061429738998413\n",
            "ITERATION_NO.: 196 LOSS_Generator: 6.606979846954346 LOSS_Discriminator: 0.0755581483244896\n",
            "ITERATION_NO.: 197 LOSS_Generator: 7.815475940704346 LOSS_Discriminator: 0.07589061558246613\n",
            "ITERATION_NO.: 198 LOSS_Generator: 8.443875312805176 LOSS_Discriminator: 0.03924880176782608\n",
            "ITERATION_NO.: 199 LOSS_Generator: 8.298474311828613 LOSS_Discriminator: 0.06757835298776627\n",
            "ITERATION_NO.: 200 LOSS_Generator: 7.68496561050415 LOSS_Discriminator: 0.09718005359172821\n",
            "ITERATION_NO.: 201 LOSS_Generator: 7.291886806488037 LOSS_Discriminator: 0.035016514360904694\n",
            "ITERATION_NO.: 202 LOSS_Generator: 6.420214653015137 LOSS_Discriminator: 0.06213747337460518\n",
            "ITERATION_NO.: 203 LOSS_Generator: 6.203207015991211 LOSS_Discriminator: 0.06229468435049057\n",
            "ITERATION_NO.: 204 LOSS_Generator: 5.887913703918457 LOSS_Discriminator: 0.0680820494890213\n",
            "ITERATION_NO.: 205 LOSS_Generator: 7.428940296173096 LOSS_Discriminator: 0.042880795896053314\n",
            "ITERATION_NO.: 206 LOSS_Generator: 8.055310249328613 LOSS_Discriminator: 0.03366783633828163\n",
            "ITERATION_NO.: 207 LOSS_Generator: 8.406269073486328 LOSS_Discriminator: 0.08095528185367584\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.737265110015869 LOSS_Discriminator: 0.1464754343032837\n",
            "ITERATION_NO.: 209 LOSS_Generator: 6.823763847351074 LOSS_Discriminator: 0.13474619388580322\n",
            "ITERATION_NO.: 210 LOSS_Generator: 6.284672260284424 LOSS_Discriminator: 0.10130614042282104\n",
            "ITERATION_NO.: 211 LOSS_Generator: 5.613272190093994 LOSS_Discriminator: 0.07999220490455627\n",
            "ITERATION_NO.: 212 LOSS_Generator: 6.100485801696777 LOSS_Discriminator: 0.08361929655075073\n",
            "ITERATION_NO.: 213 LOSS_Generator: 6.738497257232666 LOSS_Discriminator: 0.14600637555122375\n",
            "ITERATION_NO.: 214 LOSS_Generator: 7.673154830932617 LOSS_Discriminator: 0.10037816315889359\n",
            "ITERATION_NO.: 215 LOSS_Generator: 7.955883979797363 LOSS_Discriminator: 0.0379650853574276\n",
            "ITERATION_NO.: 216 LOSS_Generator: 7.435762405395508 LOSS_Discriminator: 0.10956471413373947\n",
            "ITERATION_NO.: 217 LOSS_Generator: 6.795895576477051 LOSS_Discriminator: 0.15366458892822266\n",
            "ITERATION_NO.: 218 LOSS_Generator: 5.481539726257324 LOSS_Discriminator: 0.10283505916595459\n",
            "ITERATION_NO.: 219 LOSS_Generator: 5.486494541168213 LOSS_Discriminator: 0.12791293859481812\n",
            "ITERATION_NO.: 220 LOSS_Generator: 6.944866180419922 LOSS_Discriminator: 0.10284273326396942\n",
            "ITERATION_NO.: 221 LOSS_Generator: 7.950349807739258 LOSS_Discriminator: 0.044048309326171875\n",
            "ITERATION_NO.: 222 LOSS_Generator: 8.1712064743042 LOSS_Discriminator: 0.03956935182213783\n",
            "ITERATION_NO.: 223 LOSS_Generator: 7.983335971832275 LOSS_Discriminator: 0.09278823435306549\n",
            "ITERATION_NO.: 224 LOSS_Generator: 8.062246322631836 LOSS_Discriminator: 0.05970718711614609\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.613597869873047 LOSS_Discriminator: 0.04292392358183861\n",
            "ITERATION_NO.: 226 LOSS_Generator: 7.357223987579346 LOSS_Discriminator: 0.09354788810014725\n",
            "ITERATION_NO.: 227 LOSS_Generator: 7.0093889236450195 LOSS_Discriminator: 0.05021411180496216\n",
            "ITERATION_NO.: 228 LOSS_Generator: 6.8601531982421875 LOSS_Discriminator: 0.07990917563438416\n",
            "ITERATION_NO.: 229 LOSS_Generator: 7.311233997344971 LOSS_Discriminator: 0.09674181789159775\n",
            "ITERATION_NO.: 230 LOSS_Generator: 7.363754749298096 LOSS_Discriminator: 0.05313793942332268\n",
            "ITERATION_NO.: 231 LOSS_Generator: 7.731919765472412 LOSS_Discriminator: 0.0640847235918045\n",
            "ITERATION_NO.: 232 LOSS_Generator: 7.806385517120361 LOSS_Discriminator: 0.03867195546627045\n",
            "ITERATION_NO.: 233 LOSS_Generator: 7.7721147537231445 LOSS_Discriminator: 0.042000871151685715\n",
            "ITERATION_NO.: 234 LOSS_Generator: 7.684699535369873 LOSS_Discriminator: 0.02710861898958683\n",
            "ITERATION_NO.: 235 LOSS_Generator: 7.74716329574585 LOSS_Discriminator: 0.07079658657312393\n",
            "ITERATION_NO.: 236 LOSS_Generator: 7.328826904296875 LOSS_Discriminator: 0.06474913656711578\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.428795337677002 LOSS_Discriminator: 0.08559440076351166\n",
            "ITERATION_NO.: 238 LOSS_Generator: 7.527225494384766 LOSS_Discriminator: 0.06717744469642639\n",
            "ITERATION_NO.: 239 LOSS_Generator: 6.490694999694824 LOSS_Discriminator: 0.06559741497039795\n",
            "ITERATION_NO.: 240 LOSS_Generator: 6.488593101501465 LOSS_Discriminator: 0.14035233855247498\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.597033500671387 LOSS_Discriminator: 0.08618360012769699\n",
            "ITERATION_NO.: 242 LOSS_Generator: 7.3445563316345215 LOSS_Discriminator: 0.07106810808181763\n",
            "ITERATION_NO.: 243 LOSS_Generator: 7.246316432952881 LOSS_Discriminator: 0.10087606310844421\n",
            "ITERATION_NO.: 244 LOSS_Generator: 7.59361457824707 LOSS_Discriminator: 0.15075550973415375\n",
            "ITERATION_NO.: 245 LOSS_Generator: 6.583867073059082 LOSS_Discriminator: 0.05947038531303406\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.388299465179443 LOSS_Discriminator: 0.08239477872848511\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.602855682373047 LOSS_Discriminator: 0.07194426655769348\n",
            "ITERATION_NO.: 248 LOSS_Generator: 5.857702255249023 LOSS_Discriminator: 0.0933823436498642\n",
            "ITERATION_NO.: 249 LOSS_Generator: 6.572017669677734 LOSS_Discriminator: 0.05430537462234497\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.896732807159424 LOSS_Discriminator: 0.055991873145103455\n",
            "ITERATION_NO.: 251 LOSS_Generator: 7.544449329376221 LOSS_Discriminator: 0.17195427417755127\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.586732387542725 LOSS_Discriminator: 0.12215599417686462\n",
            "ITERATION_NO.: 253 LOSS_Generator: 6.2851433753967285 LOSS_Discriminator: 0.0349673368036747\n",
            "ITERATION_NO.: 254 LOSS_Generator: 6.023283004760742 LOSS_Discriminator: 0.07717757672071457\n",
            "ITERATION_NO.: 255 LOSS_Generator: 6.81976318359375 LOSS_Discriminator: 0.0817764475941658\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.818119049072266 LOSS_Discriminator: 0.08495429158210754\n",
            "ITERATION_NO.: 257 LOSS_Generator: 8.735936164855957 LOSS_Discriminator: 0.092445969581604\n",
            "ITERATION_NO.: 258 LOSS_Generator: 8.386531829833984 LOSS_Discriminator: 0.10066527873277664\n",
            "ITERATION_NO.: 259 LOSS_Generator: 7.195051670074463 LOSS_Discriminator: 0.05933242291212082\n",
            "ITERATION_NO.: 260 LOSS_Generator: 6.724420070648193 LOSS_Discriminator: 0.05842551961541176\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.386918544769287 LOSS_Discriminator: 0.03892342373728752\n",
            "ITERATION_NO.: 262 LOSS_Generator: 5.9864020347595215 LOSS_Discriminator: 0.08664689213037491\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.246984481811523 LOSS_Discriminator: 0.08646731078624725\n",
            "ITERATION_NO.: 264 LOSS_Generator: 6.636847496032715 LOSS_Discriminator: 0.10783140361309052\n",
            "ITERATION_NO.: 265 LOSS_Generator: 7.301020622253418 LOSS_Discriminator: 0.04862271994352341\n",
            "ITERATION_NO.: 266 LOSS_Generator: 8.091907501220703 LOSS_Discriminator: 0.07053664326667786\n",
            "ITERATION_NO.: 267 LOSS_Generator: 7.630728721618652 LOSS_Discriminator: 0.045361585915088654\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.752458095550537 LOSS_Discriminator: 0.04758886620402336\n",
            "ITERATION_NO.: 269 LOSS_Generator: 7.5394110679626465 LOSS_Discriminator: 0.07894908636808395\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.045427322387695 LOSS_Discriminator: 0.07741202414035797\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.585239887237549 LOSS_Discriminator: 0.062257252633571625\n",
            "ITERATION_NO.: 272 LOSS_Generator: 7.7118096351623535 LOSS_Discriminator: 0.06890587508678436\n",
            "ITERATION_NO.: 273 LOSS_Generator: 8.370434761047363 LOSS_Discriminator: 0.05286065861582756\n",
            "ITERATION_NO.: 274 LOSS_Generator: 8.156794548034668 LOSS_Discriminator: 0.08977711945772171\n",
            "ITERATION_NO.: 275 LOSS_Generator: 7.9059739112854 LOSS_Discriminator: 0.05761360004544258\n",
            "ITERATION_NO.: 276 LOSS_Generator: 7.082757472991943 LOSS_Discriminator: 0.0599810853600502\n",
            "ITERATION_NO.: 277 LOSS_Generator: 6.666237831115723 LOSS_Discriminator: 0.08847121894359589\n",
            "ITERATION_NO.: 278 LOSS_Generator: 5.903463363647461 LOSS_Discriminator: 0.062385495752096176\n",
            "ITERATION_NO.: 279 LOSS_Generator: 6.039555072784424 LOSS_Discriminator: 0.15191948413848877\n",
            "ITERATION_NO.: 280 LOSS_Generator: 6.7930192947387695 LOSS_Discriminator: 0.0634765774011612\n",
            "ITERATION_NO.: 281 LOSS_Generator: 7.416591167449951 LOSS_Discriminator: 0.09701184928417206\n",
            "ITERATION_NO.: 282 LOSS_Generator: 7.558910369873047 LOSS_Discriminator: 0.15856251120567322\n",
            "ITERATION_NO.: 283 LOSS_Generator: 8.128622055053711 LOSS_Discriminator: 0.022403545677661896\n",
            "ITERATION_NO.: 284 LOSS_Generator: 8.403491020202637 LOSS_Discriminator: 0.08666804432868958\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.245436191558838 LOSS_Discriminator: 0.03378120809793472\n",
            "ITERATION_NO.: 286 LOSS_Generator: 6.445563316345215 LOSS_Discriminator: 0.08566245436668396\n",
            "ITERATION_NO.: 287 LOSS_Generator: 6.392054557800293 LOSS_Discriminator: 0.06592024117708206\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.078456878662109 LOSS_Discriminator: 0.11245104670524597\n",
            "ITERATION_NO.: 289 LOSS_Generator: 7.426486015319824 LOSS_Discriminator: 0.05222642421722412\n",
            "ITERATION_NO.: 290 LOSS_Generator: 7.375751972198486 LOSS_Discriminator: 0.047310762107372284\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.48642110824585 LOSS_Discriminator: 0.0802132785320282\n",
            "ITERATION_NO.: 292 LOSS_Generator: 7.375367641448975 LOSS_Discriminator: 0.015240179374814034\n",
            "ITERATION_NO.: 293 LOSS_Generator: 7.394114971160889 LOSS_Discriminator: 0.10551419854164124\n",
            "ITERATION_NO.: 294 LOSS_Generator: 6.701533317565918 LOSS_Discriminator: 0.06834004819393158\n",
            "ITERATION_NO.: 295 LOSS_Generator: 7.1280951499938965 LOSS_Discriminator: 0.11218678206205368\n",
            "ITERATION_NO.: 296 LOSS_Generator: 5.851859092712402 LOSS_Discriminator: 0.16436398029327393\n",
            "ITERATION_NO.: 297 LOSS_Generator: 5.897289276123047 LOSS_Discriminator: 0.12240061163902283\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.784462928771973 LOSS_Discriminator: 0.0909484401345253\n",
            "ITERATION_NO.: 299 LOSS_Generator: 8.032999038696289 LOSS_Discriminator: 0.06384167820215225\n",
            "ITERATION_NO.: 300 LOSS_Generator: 8.025185585021973 LOSS_Discriminator: 0.0748421922326088\n",
            "ITERATION_NO.: 301 LOSS_Generator: 7.534657001495361 LOSS_Discriminator: 0.07696311175823212\n",
            "ITERATION_NO.: 302 LOSS_Generator: 7.530874729156494 LOSS_Discriminator: 0.07971014082431793\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.796609878540039 LOSS_Discriminator: 0.035935282707214355\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.424236297607422 LOSS_Discriminator: 0.03571302071213722\n",
            "ITERATION_NO.: 305 LOSS_Generator: 7.023731708526611 LOSS_Discriminator: 0.0780777633190155\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.469411849975586 LOSS_Discriminator: 0.04543048143386841\n",
            "ITERATION_NO.: 307 LOSS_Generator: 7.104827880859375 LOSS_Discriminator: 0.057165004312992096\n",
            "ITERATION_NO.: 308 LOSS_Generator: 7.013028621673584 LOSS_Discriminator: 0.09582822024822235\n",
            "ITERATION_NO.: 309 LOSS_Generator: 6.869403839111328 LOSS_Discriminator: 0.03058214858174324\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.909275531768799 LOSS_Discriminator: 0.09108481556177139\n",
            "ITERATION_NO.: 311 LOSS_Generator: 7.430437088012695 LOSS_Discriminator: 0.06018775328993797\n",
            "ITERATION_NO.: 312 LOSS_Generator: 7.974236965179443 LOSS_Discriminator: 0.0708874762058258\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.809980392456055 LOSS_Discriminator: 0.08551286160945892\n",
            "ITERATION_NO.: 314 LOSS_Generator: 6.915782928466797 LOSS_Discriminator: 0.06385347992181778\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.485008716583252 LOSS_Discriminator: 0.03294244781136513\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.445172309875488 LOSS_Discriminator: 0.06698617339134216\n",
            "ITERATION_NO.: 317 LOSS_Generator: 7.528959274291992 LOSS_Discriminator: 0.09491422772407532\n",
            "ITERATION_NO.: 318 LOSS_Generator: 8.511752128601074 LOSS_Discriminator: 0.028691038489341736\n",
            "ITERATION_NO.: 319 LOSS_Generator: 8.889331817626953 LOSS_Discriminator: 0.0669623613357544\n",
            "ITERATION_NO.: 320 LOSS_Generator: 8.25471305847168 LOSS_Discriminator: 0.022496558725833893\n",
            "ITERATION_NO.: 321 LOSS_Generator: 7.882758617401123 LOSS_Discriminator: 0.07774612307548523\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.045860767364502 LOSS_Discriminator: 0.08079773187637329\n",
            "ITERATION_NO.: 323 LOSS_Generator: 5.9532694816589355 LOSS_Discriminator: 0.0807652473449707\n",
            "ITERATION_NO.: 324 LOSS_Generator: 6.149679183959961 LOSS_Discriminator: 0.16852286458015442\n",
            "ITERATION_NO.: 325 LOSS_Generator: 6.149165153503418 LOSS_Discriminator: 0.08745353668928146\n",
            "ITERATION_NO.: 326 LOSS_Generator: 6.840981960296631 LOSS_Discriminator: 0.08792566508054733\n",
            "ITERATION_NO.: 327 LOSS_Generator: 7.7053046226501465 LOSS_Discriminator: 0.036701928824186325\n",
            "ITERATION_NO.: 328 LOSS_Generator: 8.431246757507324 LOSS_Discriminator: 0.052342843264341354\n",
            "ITERATION_NO.: 329 LOSS_Generator: 8.667518615722656 LOSS_Discriminator: 0.04070707783102989\n",
            "ITERATION_NO.: 330 LOSS_Generator: 8.203343391418457 LOSS_Discriminator: 0.05383888632059097\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.8232622146606445 LOSS_Discriminator: 0.061469320207834244\n",
            "ITERATION_NO.: 332 LOSS_Generator: 7.615759372711182 LOSS_Discriminator: 0.0788295567035675\n",
            "ITERATION_NO.: 333 LOSS_Generator: 6.414466381072998 LOSS_Discriminator: 0.03300844877958298\n",
            "ITERATION_NO.: 334 LOSS_Generator: 5.9994282722473145 LOSS_Discriminator: 0.049690425395965576\n",
            "ITERATION_NO.: 335 LOSS_Generator: 6.6238484382629395 LOSS_Discriminator: 0.10172552615404129\n",
            "ITERATION_NO.: 336 LOSS_Generator: 7.441591262817383 LOSS_Discriminator: 0.07461205869913101\n",
            "ITERATION_NO.: 337 LOSS_Generator: 8.630517959594727 LOSS_Discriminator: 0.0688781887292862\n",
            "ITERATION_NO.: 338 LOSS_Generator: 8.581552505493164 LOSS_Discriminator: 0.12440815567970276\n",
            "ITERATION_NO.: 339 LOSS_Generator: 7.857003688812256 LOSS_Discriminator: 0.0494239442050457\n",
            "ITERATION_NO.: 340 LOSS_Generator: 7.02427864074707 LOSS_Discriminator: 0.07658012211322784\n",
            "ITERATION_NO.: 341 LOSS_Generator: 6.0511627197265625 LOSS_Discriminator: 0.07438486069440842\n",
            "ITERATION_NO.: 342 LOSS_Generator: 5.696253776550293 LOSS_Discriminator: 0.12482629716396332\n",
            "ITERATION_NO.: 343 LOSS_Generator: 6.409064292907715 LOSS_Discriminator: 0.057750970125198364\n",
            "ITERATION_NO.: 344 LOSS_Generator: 7.446193218231201 LOSS_Discriminator: 0.05009385943412781\n",
            "ITERATION_NO.: 345 LOSS_Generator: 8.281166076660156 LOSS_Discriminator: 0.02266671136021614\n",
            "ITERATION_NO.: 346 LOSS_Generator: 8.677184104919434 LOSS_Discriminator: 0.08792534470558167\n",
            "ITERATION_NO.: 347 LOSS_Generator: 7.9024834632873535 LOSS_Discriminator: 0.15329644083976746\n",
            "ITERATION_NO.: 348 LOSS_Generator: 6.532120227813721 LOSS_Discriminator: 0.06784990429878235\n",
            "ITERATION_NO.: 349 LOSS_Generator: 5.955070972442627 LOSS_Discriminator: 0.07061411440372467\n",
            "ITERATION_NO.: 350 LOSS_Generator: 6.528193473815918 LOSS_Discriminator: 0.09229154884815216\n",
            "ITERATION_NO.: 351 LOSS_Generator: 7.703550338745117 LOSS_Discriminator: 0.07690788805484772\n",
            "ITERATION_NO.: 352 LOSS_Generator: 8.003705024719238 LOSS_Discriminator: 0.049154289066791534\n",
            "ITERATION_NO.: 353 LOSS_Generator: 9.028693199157715 LOSS_Discriminator: 0.06380913406610489\n",
            "ITERATION_NO.: 354 LOSS_Generator: 8.544068336486816 LOSS_Discriminator: 0.08040590584278107\n",
            "ITERATION_NO.: 355 LOSS_Generator: 7.7224626541137695 LOSS_Discriminator: 0.05899976193904877\n",
            "ITERATION_NO.: 356 LOSS_Generator: 6.9081315994262695 LOSS_Discriminator: 0.05978567898273468\n",
            "ITERATION_NO.: 357 LOSS_Generator: 6.0298614501953125 LOSS_Discriminator: 0.07225863635540009\n",
            "ITERATION_NO.: 358 LOSS_Generator: 6.743889331817627 LOSS_Discriminator: 0.10731218010187149\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.570783615112305 LOSS_Discriminator: 0.06011778116226196\n",
            "ITERATION_NO.: 360 LOSS_Generator: 8.296483039855957 LOSS_Discriminator: 0.13860073685646057\n",
            "ITERATION_NO.: 361 LOSS_Generator: 8.295345306396484 LOSS_Discriminator: 0.04012021794915199\n",
            "ITERATION_NO.: 362 LOSS_Generator: 7.5585551261901855 LOSS_Discriminator: 0.07028421759605408\n",
            "ITERATION_NO.: 363 LOSS_Generator: 6.94085693359375 LOSS_Discriminator: 0.02942541614174843\n",
            "ITERATION_NO.: 364 LOSS_Generator: 5.4223713874816895 LOSS_Discriminator: 0.08986243605613708\n",
            "ITERATION_NO.: 365 LOSS_Generator: 5.7007880210876465 LOSS_Discriminator: 0.0890987291932106\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.8742570877075195 LOSS_Discriminator: 0.062335528433322906\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.455058574676514 LOSS_Discriminator: 0.05493449792265892\n",
            "ITERATION_NO.: 368 LOSS_Generator: 7.854769229888916 LOSS_Discriminator: 0.023492883890867233\n",
            "ITERATION_NO.: 369 LOSS_Generator: 8.868460655212402 LOSS_Discriminator: 0.04617810621857643\n",
            "ITERATION_NO.: 370 LOSS_Generator: 8.31623649597168 LOSS_Discriminator: 0.04103439673781395\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.995304107666016 LOSS_Discriminator: 0.05822689086198807\n",
            "ITERATION_NO.: 372 LOSS_Generator: 7.488659858703613 LOSS_Discriminator: 0.10719826817512512\n",
            "ITERATION_NO.: 373 LOSS_Generator: 6.7912516593933105 LOSS_Discriminator: 0.0778658390045166\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.22304630279541 LOSS_Discriminator: 0.07156834006309509\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.614157676696777 LOSS_Discriminator: 0.050898779183626175\n",
            "ITERATION_NO.: 376 LOSS_Generator: 6.9932661056518555 LOSS_Discriminator: 0.04205381125211716\n",
            "ITERATION_NO.: 377 LOSS_Generator: 7.474417209625244 LOSS_Discriminator: 0.03378935530781746\n",
            "ITERATION_NO.: 378 LOSS_Generator: 7.628473281860352 LOSS_Discriminator: 0.043253980576992035\n",
            "ITERATION_NO.: 379 LOSS_Generator: 7.839064121246338 LOSS_Discriminator: 0.053179480135440826\n",
            "ITERATION_NO.: 380 LOSS_Generator: 7.178686618804932 LOSS_Discriminator: 0.03136630728840828\n",
            "ITERATION_NO.: 381 LOSS_Generator: 7.384240627288818 LOSS_Discriminator: 0.027108706533908844\n",
            "ITERATION_NO.: 382 LOSS_Generator: 7.536172866821289 LOSS_Discriminator: 0.046401459723711014\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.558598518371582 LOSS_Discriminator: 0.05071933567523956\n",
            "ITERATION_NO.: 384 LOSS_Generator: 6.514291763305664 LOSS_Discriminator: 0.06790094822645187\n",
            "ITERATION_NO.: 385 LOSS_Generator: 6.450964450836182 LOSS_Discriminator: 0.04327761381864548\n",
            "ITERATION_NO.: 386 LOSS_Generator: 6.819836616516113 LOSS_Discriminator: 0.057942770421504974\n",
            "ITERATION_NO.: 387 LOSS_Generator: 6.929686069488525 LOSS_Discriminator: 0.06495174765586853\n",
            "ITERATION_NO.: 388 LOSS_Generator: 7.298659801483154 LOSS_Discriminator: 0.05360279232263565\n",
            "ITERATION_NO.: 389 LOSS_Generator: 7.9310302734375 LOSS_Discriminator: 0.02689652144908905\n",
            "ITERATION_NO.: 390 LOSS_Generator: 8.472114562988281 LOSS_Discriminator: 0.03323287516832352\n",
            "ITERATION_NO.: 391 LOSS_Generator: 8.450279235839844 LOSS_Discriminator: 0.12037641555070877\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.754556179046631 LOSS_Discriminator: 0.03842581808567047\n",
            "ITERATION_NO.: 393 LOSS_Generator: 7.385173797607422 LOSS_Discriminator: 0.03350502625107765\n",
            "ITERATION_NO.: 394 LOSS_Generator: 6.322223663330078 LOSS_Discriminator: 0.05663931742310524\n",
            "ITERATION_NO.: 395 LOSS_Generator: 5.931718826293945 LOSS_Discriminator: 0.12108118832111359\n",
            "ITERATION_NO.: 396 LOSS_Generator: 5.854245185852051 LOSS_Discriminator: 0.09511540085077286\n",
            "ITERATION_NO.: 397 LOSS_Generator: 7.148635387420654 LOSS_Discriminator: 0.150891974568367\n",
            "ITERATION_NO.: 398 LOSS_Generator: 8.151253700256348 LOSS_Discriminator: 0.04805309697985649\n",
            "ITERATION_NO.: 399 LOSS_Generator: 8.684085845947266 LOSS_Discriminator: 0.04920979589223862\n",
            "ITERATION_NO.: 400 LOSS_Generator: 9.660928726196289 LOSS_Discriminator: 0.20888900756835938\n",
            "ITERATION_NO.: 401 LOSS_Generator: 8.032272338867188 LOSS_Discriminator: 0.15794727206230164\n",
            "ITERATION_NO.: 402 LOSS_Generator: 7.466423988342285 LOSS_Discriminator: 0.074854776263237\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.723345756530762 LOSS_Discriminator: 0.08981314301490784\n",
            "ITERATION_NO.: 404 LOSS_Generator: 5.738234996795654 LOSS_Discriminator: 0.07831724733114243\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.521111011505127 LOSS_Discriminator: 0.09152796864509583\n",
            "ITERATION_NO.: 406 LOSS_Generator: 7.358147144317627 LOSS_Discriminator: 0.12318560481071472\n",
            "ITERATION_NO.: 407 LOSS_Generator: 9.168827056884766 LOSS_Discriminator: 0.057155922055244446\n",
            "ITERATION_NO.: 408 LOSS_Generator: 8.712989807128906 LOSS_Discriminator: 0.08136621117591858\n",
            "ITERATION_NO.: 409 LOSS_Generator: 8.147193908691406 LOSS_Discriminator: 0.12045155465602875\n",
            "ITERATION_NO.: 410 LOSS_Generator: 6.323923110961914 LOSS_Discriminator: 0.13925163447856903\n",
            "ITERATION_NO.: 411 LOSS_Generator: 4.456037998199463 LOSS_Discriminator: 0.10213462263345718\n",
            "ITERATION_NO.: 412 LOSS_Generator: 5.579113006591797 LOSS_Discriminator: 0.12471859157085419\n",
            "ITERATION_NO.: 413 LOSS_Generator: 7.430665493011475 LOSS_Discriminator: 0.12050837278366089\n",
            "ITERATION_NO.: 414 LOSS_Generator: 8.816808700561523 LOSS_Discriminator: 0.03436136990785599\n",
            "ITERATION_NO.: 415 LOSS_Generator: 9.440326690673828 LOSS_Discriminator: 0.22350850701332092\n",
            "ITERATION_NO.: 416 LOSS_Generator: 8.379849433898926 LOSS_Discriminator: 0.07778631150722504\n",
            "ITERATION_NO.: 417 LOSS_Generator: 7.263668060302734 LOSS_Discriminator: 0.04350975900888443\n",
            "ITERATION_NO.: 418 LOSS_Generator: 6.7610297203063965 LOSS_Discriminator: 0.02737431414425373\n",
            "ITERATION_NO.: 419 LOSS_Generator: 7.433394908905029 LOSS_Discriminator: 0.07647915184497833\n",
            "ITERATION_NO.: 420 LOSS_Generator: 6.872929573059082 LOSS_Discriminator: 0.08254235982894897\n",
            "ITERATION_NO.: 421 LOSS_Generator: 7.705784320831299 LOSS_Discriminator: 0.06979572027921677\n",
            "ITERATION_NO.: 422 LOSS_Generator: 8.012106895446777 LOSS_Discriminator: 0.061921317130327225\n",
            "ITERATION_NO.: 423 LOSS_Generator: 7.822065353393555 LOSS_Discriminator: 0.12755195796489716\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.1567535400390625 LOSS_Discriminator: 0.12881499528884888\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.418379306793213 LOSS_Discriminator: 0.10704571008682251\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.323025703430176 LOSS_Discriminator: 0.10455886274576187\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.930741786956787 LOSS_Discriminator: 0.07178401947021484\n",
            "ITERATION_NO.: 428 LOSS_Generator: 7.396530628204346 LOSS_Discriminator: 0.10049046576023102\n",
            "ITERATION_NO.: 429 LOSS_Generator: 8.006608009338379 LOSS_Discriminator: 0.12365840375423431\n",
            "ITERATION_NO.: 430 LOSS_Generator: 7.978680610656738 LOSS_Discriminator: 0.05996176600456238\n",
            "ITERATION_NO.: 431 LOSS_Generator: 7.149372100830078 LOSS_Discriminator: 0.10393379628658295\n",
            "ITERATION_NO.: 432 LOSS_Generator: 7.101935863494873 LOSS_Discriminator: 0.10002410411834717\n",
            "ITERATION_NO.: 433 LOSS_Generator: 6.196692943572998 LOSS_Discriminator: 0.03189079463481903\n",
            "ITERATION_NO.: 434 LOSS_Generator: 5.733391284942627 LOSS_Discriminator: 0.05424647033214569\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.881415843963623 LOSS_Discriminator: 0.08167406916618347\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.452121734619141 LOSS_Discriminator: 0.08987193554639816\n",
            "ITERATION_NO.: 437 LOSS_Generator: 7.194154262542725 LOSS_Discriminator: 0.07196295261383057\n",
            "ITERATION_NO.: 438 LOSS_Generator: 7.4247918128967285 LOSS_Discriminator: 0.0996551662683487\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.563422203063965 LOSS_Discriminator: 0.042579296976327896\n",
            "ITERATION_NO.: 440 LOSS_Generator: 8.113383293151855 LOSS_Discriminator: 0.07184186577796936\n",
            "ITERATION_NO.: 441 LOSS_Generator: 7.206264019012451 LOSS_Discriminator: 0.10599979758262634\n",
            "ITERATION_NO.: 442 LOSS_Generator: 7.7741899490356445 LOSS_Discriminator: 0.05387556552886963\n",
            "ITERATION_NO.: 443 LOSS_Generator: 7.50607442855835 LOSS_Discriminator: 0.03420189395546913\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.487936019897461 LOSS_Discriminator: 0.05744181573390961\n",
            "ITERATION_NO.: 445 LOSS_Generator: 7.380936145782471 LOSS_Discriminator: 0.07041563093662262\n",
            "ITERATION_NO.: 446 LOSS_Generator: 7.187763214111328 LOSS_Discriminator: 0.061915360391139984\n",
            "ITERATION_NO.: 447 LOSS_Generator: 7.205124378204346 LOSS_Discriminator: 0.0281380582600832\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.833216667175293 LOSS_Discriminator: 0.04517547786235809\n",
            "ITERATION_NO.: 449 LOSS_Generator: 7.3221964836120605 LOSS_Discriminator: 0.060284458100795746\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.474970817565918 LOSS_Discriminator: 0.037858154624700546\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.624089241027832 LOSS_Discriminator: 0.08838190138339996\n",
            "ITERATION_NO.: 452 LOSS_Generator: 6.902973175048828 LOSS_Discriminator: 0.15475432574748993\n",
            "ITERATION_NO.: 453 LOSS_Generator: 5.5525617599487305 LOSS_Discriminator: 0.12256356328725815\n",
            "ITERATION_NO.: 454 LOSS_Generator: 5.625847339630127 LOSS_Discriminator: 0.10386018455028534\n",
            "ITERATION_NO.: 455 LOSS_Generator: 7.063939094543457 LOSS_Discriminator: 0.12276869267225266\n",
            "ITERATION_NO.: 456 LOSS_Generator: 8.433420181274414 LOSS_Discriminator: 0.0538768395781517\n",
            "ITERATION_NO.: 457 LOSS_Generator: 8.421276092529297 LOSS_Discriminator: 0.09622374922037125\n",
            "ITERATION_NO.: 458 LOSS_Generator: 7.825417995452881 LOSS_Discriminator: 0.1451861560344696\n",
            "ITERATION_NO.: 459 LOSS_Generator: 7.751493453979492 LOSS_Discriminator: 0.019515149295330048\n",
            "ITERATION_NO.: 460 LOSS_Generator: 6.749363422393799 LOSS_Discriminator: 0.07447846978902817\n",
            "ITERATION_NO.: 461 LOSS_Generator: 5.720329761505127 LOSS_Discriminator: 0.069390207529068\n",
            "ITERATION_NO.: 462 LOSS_Generator: 5.110480785369873 LOSS_Discriminator: 0.09355062246322632\n",
            "ITERATION_NO.: 463 LOSS_Generator: 5.799615859985352 LOSS_Discriminator: 0.1426941156387329\n",
            "ITERATION_NO.: 464 LOSS_Generator: 7.117836952209473 LOSS_Discriminator: 0.11712773144245148\n",
            "ITERATION_NO.: 465 LOSS_Generator: 8.858668327331543 LOSS_Discriminator: 0.05238509178161621\n",
            "ITERATION_NO.: 466 LOSS_Generator: 9.126238822937012 LOSS_Discriminator: 0.07156044244766235\n",
            "ITERATION_NO.: 467 LOSS_Generator: 9.016687393188477 LOSS_Discriminator: 0.03478200361132622\n",
            "ITERATION_NO.: 468 LOSS_Generator: 8.855484962463379 LOSS_Discriminator: 0.056562766432762146\n",
            "ITERATION_NO.: 469 LOSS_Generator: 8.388986587524414 LOSS_Discriminator: 0.124680295586586\n",
            "ITERATION_NO.: 470 LOSS_Generator: 7.436132907867432 LOSS_Discriminator: 0.08129166811704636\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.083096504211426 LOSS_Discriminator: 0.10368901491165161\n",
            "ITERATION_NO.: 472 LOSS_Generator: 5.762191295623779 LOSS_Discriminator: 0.07039973139762878\n",
            "ITERATION_NO.: 473 LOSS_Generator: 5.874795913696289 LOSS_Discriminator: 0.15721598267555237\n",
            "ITERATION_NO.: 474 LOSS_Generator: 7.285681247711182 LOSS_Discriminator: 0.09307049214839935\n",
            "ITERATION_NO.: 475 LOSS_Generator: 7.937854766845703 LOSS_Discriminator: 0.06392785906791687\n",
            "ITERATION_NO.: 476 LOSS_Generator: 8.524292945861816 LOSS_Discriminator: 0.07753828167915344\n",
            "ITERATION_NO.: 477 LOSS_Generator: 7.77154541015625 LOSS_Discriminator: 0.14467453956604004\n",
            "ITERATION_NO.: 478 LOSS_Generator: 6.648785591125488 LOSS_Discriminator: 0.17302295565605164\n",
            "ITERATION_NO.: 479 LOSS_Generator: 5.973647594451904 LOSS_Discriminator: 0.08014431595802307\n",
            "ITERATION_NO.: 480 LOSS_Generator: 5.497159004211426 LOSS_Discriminator: 0.08110984414815903\n",
            "ITERATION_NO.: 481 LOSS_Generator: 7.011857986450195 LOSS_Discriminator: 0.1296926885843277\n",
            "ITERATION_NO.: 482 LOSS_Generator: 7.426231861114502 LOSS_Discriminator: 0.08354629576206207\n",
            "ITERATION_NO.: 483 LOSS_Generator: 8.172883987426758 LOSS_Discriminator: 0.051861491054296494\n",
            "ITERATION_NO.: 484 LOSS_Generator: 7.8895158767700195 LOSS_Discriminator: 0.09186725318431854\n",
            "ITERATION_NO.: 485 LOSS_Generator: 7.529037952423096 LOSS_Discriminator: 0.060651689767837524\n",
            "ITERATION_NO.: 486 LOSS_Generator: 7.555502414703369 LOSS_Discriminator: 0.041408441960811615\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.785353183746338 LOSS_Discriminator: 0.07089334726333618\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.0547990798950195 LOSS_Discriminator: 0.08676952123641968\n",
            "ITERATION_NO.: 489 LOSS_Generator: 6.597296714782715 LOSS_Discriminator: 0.06413640081882477\n",
            "ITERATION_NO.: 490 LOSS_Generator: 5.4546332359313965 LOSS_Discriminator: 0.055351778864860535\n",
            "ITERATION_NO.: 491 LOSS_Generator: 6.291851997375488 LOSS_Discriminator: 0.05777370557188988\n",
            "ITERATION_NO.: 492 LOSS_Generator: 6.987057685852051 LOSS_Discriminator: 0.03539019450545311\n",
            "ITERATION_NO.: 493 LOSS_Generator: 7.744787216186523 LOSS_Discriminator: 0.0629841536283493\n",
            "ITERATION_NO.: 494 LOSS_Generator: 8.230474472045898 LOSS_Discriminator: 0.051688894629478455\n",
            "ITERATION_NO.: 495 LOSS_Generator: 8.702868461608887 LOSS_Discriminator: 0.09146757423877716\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.945090293884277 LOSS_Discriminator: 0.12477383017539978\n",
            "ITERATION_NO.: 497 LOSS_Generator: 6.577052593231201 LOSS_Discriminator: 0.051962751895189285\n",
            "ITERATION_NO.: 498 LOSS_Generator: 5.687334060668945 LOSS_Discriminator: 0.042968761175870895\n",
            "ITERATION_NO.: 499 LOSS_Generator: 5.669830799102783 LOSS_Discriminator: 0.09185037016868591\n",
            "ITERATION_NO.: 500 LOSS_Generator: 6.806673049926758 LOSS_Discriminator: 0.0963461622595787\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.439085483551025 LOSS_Discriminator: 0.040266457945108414\n",
            "ITERATION_NO.: 502 LOSS_Generator: 8.363560676574707 LOSS_Discriminator: 0.045938171446323395\n",
            "ITERATION_NO.: 503 LOSS_Generator: 8.618701934814453 LOSS_Discriminator: 0.09217328578233719\n",
            "ITERATION_NO.: 504 LOSS_Generator: 7.450749397277832 LOSS_Discriminator: 0.08774804323911667\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.579405307769775 LOSS_Discriminator: 0.11569052934646606\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.129882335662842 LOSS_Discriminator: 0.09255868196487427\n",
            "ITERATION_NO.: 507 LOSS_Generator: 5.264045238494873 LOSS_Discriminator: 0.03904587775468826\n",
            "ITERATION_NO.: 508 LOSS_Generator: 6.401670932769775 LOSS_Discriminator: 0.09745726734399796\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.099026679992676 LOSS_Discriminator: 0.1326403021812439\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.350894927978516 LOSS_Discriminator: 0.05975267291069031\n",
            "ITERATION_NO.: 511 LOSS_Generator: 8.04105281829834 LOSS_Discriminator: 0.1549353003501892\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.020312309265137 LOSS_Discriminator: 0.1455172896385193\n",
            "ITERATION_NO.: 513 LOSS_Generator: 6.493945121765137 LOSS_Discriminator: 0.04460518807172775\n",
            "ITERATION_NO.: 514 LOSS_Generator: 6.346346378326416 LOSS_Discriminator: 0.1078358143568039\n",
            "ITERATION_NO.: 515 LOSS_Generator: 6.264138698577881 LOSS_Discriminator: 0.10531105101108551\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.838759899139404 LOSS_Discriminator: 0.09765671193599701\n",
            "ITERATION_NO.: 517 LOSS_Generator: 7.245440483093262 LOSS_Discriminator: 0.08489902317523956\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.716151237487793 LOSS_Discriminator: 0.10246676951646805\n",
            "ITERATION_NO.: 519 LOSS_Generator: 7.5768537521362305 LOSS_Discriminator: 0.07862059772014618\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.082868576049805 LOSS_Discriminator: 0.08434580266475677\n",
            "ITERATION_NO.: 521 LOSS_Generator: 6.924184799194336 LOSS_Discriminator: 0.1331188976764679\n",
            "ITERATION_NO.: 522 LOSS_Generator: 6.853512763977051 LOSS_Discriminator: 0.054995160549879074\n",
            "ITERATION_NO.: 523 LOSS_Generator: 7.407790660858154 LOSS_Discriminator: 0.018124781548976898\n",
            "ITERATION_NO.: 524 LOSS_Generator: 7.968921661376953 LOSS_Discriminator: 0.07507212460041046\n",
            "ITERATION_NO.: 525 LOSS_Generator: 7.2022294998168945 LOSS_Discriminator: 0.07426539808511734\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.211308002471924 LOSS_Discriminator: 0.0670875608921051\n",
            "ITERATION_NO.: 527 LOSS_Generator: 7.273589611053467 LOSS_Discriminator: 0.11390241980552673\n",
            "ITERATION_NO.: 528 LOSS_Generator: 7.3595404624938965 LOSS_Discriminator: 0.05437617003917694\n",
            "ITERATION_NO.: 529 LOSS_Generator: 6.709774017333984 LOSS_Discriminator: 0.061570677906274796\n",
            "ITERATION_NO.: 530 LOSS_Generator: 7.183164119720459 LOSS_Discriminator: 0.08005142956972122\n",
            "ITERATION_NO.: 531 LOSS_Generator: 7.516686916351318 LOSS_Discriminator: 0.056533463299274445\n",
            "ITERATION_NO.: 532 LOSS_Generator: 6.8187785148620605 LOSS_Discriminator: 0.11139151453971863\n",
            "ITERATION_NO.: 533 LOSS_Generator: 6.890928268432617 LOSS_Discriminator: 0.12212401628494263\n",
            "ITERATION_NO.: 534 LOSS_Generator: 6.924183368682861 LOSS_Discriminator: 0.07210943102836609\n",
            "ITERATION_NO.: 535 LOSS_Generator: 5.8787431716918945 LOSS_Discriminator: 0.08077793568372726\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.860574722290039 LOSS_Discriminator: 0.07819855213165283\n",
            "ITERATION_NO.: 537 LOSS_Generator: 7.163295745849609 LOSS_Discriminator: 0.02571493573486805\n",
            "ITERATION_NO.: 538 LOSS_Generator: 7.297083854675293 LOSS_Discriminator: 0.09532561153173447\n",
            "ITERATION_NO.: 539 LOSS_Generator: 8.092519760131836 LOSS_Discriminator: 0.052207864820957184\n",
            "ITERATION_NO.: 540 LOSS_Generator: 8.161246299743652 LOSS_Discriminator: 0.05489037558436394\n",
            "ITERATION_NO.: 541 LOSS_Generator: 7.767945766448975 LOSS_Discriminator: 0.06788203120231628\n",
            "ITERATION_NO.: 542 LOSS_Generator: 7.253460884094238 LOSS_Discriminator: 0.12480537593364716\n",
            "ITERATION_NO.: 543 LOSS_Generator: 7.199371337890625 LOSS_Discriminator: 0.05309414118528366\n",
            "ITERATION_NO.: 544 LOSS_Generator: 6.789003372192383 LOSS_Discriminator: 0.07015924900770187\n",
            "ITERATION_NO.: 545 LOSS_Generator: 7.055898666381836 LOSS_Discriminator: 0.008629383519291878\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.758502960205078 LOSS_Discriminator: 0.04883871227502823\n",
            "ITERATION_NO.: 547 LOSS_Generator: 7.082905292510986 LOSS_Discriminator: 0.058311283588409424\n",
            "ITERATION_NO.: 548 LOSS_Generator: 6.457849025726318 LOSS_Discriminator: 0.044608037918806076\n",
            "ITERATION_NO.: 549 LOSS_Generator: 6.652561187744141 LOSS_Discriminator: 0.060506463050842285\n",
            "ITERATION_NO.: 550 LOSS_Generator: 6.800700664520264 LOSS_Discriminator: 0.0871025025844574\n",
            "ITERATION_NO.: 551 LOSS_Generator: 6.478519916534424 LOSS_Discriminator: 0.062495797872543335\n",
            "ITERATION_NO.: 552 LOSS_Generator: 6.299687385559082 LOSS_Discriminator: 0.0519159771502018\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.770246505737305 LOSS_Discriminator: 0.06878150999546051\n",
            "ITERATION_NO.: 554 LOSS_Generator: 6.680934429168701 LOSS_Discriminator: 0.07627944648265839\n",
            "ITERATION_NO.: 555 LOSS_Generator: 6.865118503570557 LOSS_Discriminator: 0.07126299291849136\n",
            "ITERATION_NO.: 556 LOSS_Generator: 7.200726509094238 LOSS_Discriminator: 0.10690417885780334\n",
            "ITERATION_NO.: 557 LOSS_Generator: 7.329283237457275 LOSS_Discriminator: 0.11406457424163818\n",
            "ITERATION_NO.: 558 LOSS_Generator: 6.6481428146362305 LOSS_Discriminator: 0.07000012695789337\n",
            "ITERATION_NO.: 559 LOSS_Generator: 6.723106861114502 LOSS_Discriminator: 0.0799027681350708\n",
            "ITERATION_NO.: 560 LOSS_Generator: 6.19410514831543 LOSS_Discriminator: 0.09235963225364685\n",
            "ITERATION_NO.: 561 LOSS_Generator: 6.362899303436279 LOSS_Discriminator: 0.06281083822250366\n",
            "ITERATION_NO.: 562 LOSS_Generator: 6.722514629364014 LOSS_Discriminator: 0.07678528130054474\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.590150833129883 LOSS_Discriminator: 0.05907537043094635\n",
            "ITERATION_NO.: 564 LOSS_Generator: 6.766200065612793 LOSS_Discriminator: 0.061145633459091187\n",
            "ITERATION_NO.: 565 LOSS_Generator: 6.957578182220459 LOSS_Discriminator: 0.1415562480688095\n",
            "ITERATION_NO.: 566 LOSS_Generator: 5.8126654624938965 LOSS_Discriminator: 0.10140757262706757\n",
            "ITERATION_NO.: 567 LOSS_Generator: 6.344491481781006 LOSS_Discriminator: 0.05809459090232849\n",
            "ITERATION_NO.: 568 LOSS_Generator: 5.885281562805176 LOSS_Discriminator: 0.046002887189388275\n",
            "ITERATION_NO.: 569 LOSS_Generator: 7.500655651092529 LOSS_Discriminator: 0.022091176360845566\n",
            "ITERATION_NO.: 570 LOSS_Generator: 7.83697509765625 LOSS_Discriminator: 0.06564737856388092\n",
            "ITERATION_NO.: 571 LOSS_Generator: 7.192483425140381 LOSS_Discriminator: 0.06971466541290283\n",
            "ITERATION_NO.: 572 LOSS_Generator: 7.300147533416748 LOSS_Discriminator: 0.07458740472793579\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.207906723022461 LOSS_Discriminator: 0.03121262602508068\n",
            "ITERATION_NO.: 574 LOSS_Generator: 6.655026912689209 LOSS_Discriminator: 0.0598132386803627\n",
            "ITERATION_NO.: 575 LOSS_Generator: 6.673176288604736 LOSS_Discriminator: 0.03835305571556091\n",
            "ITERATION_NO.: 576 LOSS_Generator: 6.816730976104736 LOSS_Discriminator: 0.08820592612028122\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.772598743438721 LOSS_Discriminator: 0.06434670090675354\n",
            "ITERATION_NO.: 578 LOSS_Generator: 6.9715423583984375 LOSS_Discriminator: 0.08412031829357147\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.441015243530273 LOSS_Discriminator: 0.02569381520152092\n",
            "ITERATION_NO.: 580 LOSS_Generator: 7.720974922180176 LOSS_Discriminator: 0.02716672793030739\n",
            "ITERATION_NO.: 581 LOSS_Generator: 7.579627513885498 LOSS_Discriminator: 0.07722138613462448\n",
            "ITERATION_NO.: 582 LOSS_Generator: 7.0558857917785645 LOSS_Discriminator: 0.062498681247234344\n",
            "ITERATION_NO.: 583 LOSS_Generator: 6.3940534591674805 LOSS_Discriminator: 0.04227601736783981\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.343369007110596 LOSS_Discriminator: 0.07731848955154419\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.735698223114014 LOSS_Discriminator: 0.1031823456287384\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.449824810028076 LOSS_Discriminator: 0.10278356820344925\n",
            "ITERATION_NO.: 587 LOSS_Generator: 6.9359331130981445 LOSS_Discriminator: 0.07375484704971313\n",
            "ITERATION_NO.: 588 LOSS_Generator: 6.8187642097473145 LOSS_Discriminator: 0.07139457762241364\n",
            "ITERATION_NO.: 589 LOSS_Generator: 6.533968448638916 LOSS_Discriminator: 0.1239442303776741\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.488950252532959 LOSS_Discriminator: 0.07529780268669128\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.486316680908203 LOSS_Discriminator: 0.10409010946750641\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.318670749664307 LOSS_Discriminator: 0.05035240575671196\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.891871929168701 LOSS_Discriminator: 0.07828249782323837\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.334710597991943 LOSS_Discriminator: 0.08440963178873062\n",
            "ITERATION_NO.: 595 LOSS_Generator: 8.093282699584961 LOSS_Discriminator: 0.05359780043363571\n",
            "ITERATION_NO.: 596 LOSS_Generator: 7.224754810333252 LOSS_Discriminator: 0.05926670879125595\n",
            "ITERATION_NO.: 597 LOSS_Generator: 6.3304266929626465 LOSS_Discriminator: 0.09856875240802765\n",
            "ITERATION_NO.: 598 LOSS_Generator: 5.723668098449707 LOSS_Discriminator: 0.06923474371433258\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.309994697570801 LOSS_Discriminator: 0.03194677084684372\n",
            "ITERATION_NO.: 600 LOSS_Generator: 7.178452014923096 LOSS_Discriminator: 0.047362565994262695\n",
            "EPOCH OVER: 47\n",
            "ITERATION_NO.: 1 LOSS_Generator: 7.910064697265625 LOSS_Discriminator: 0.04520765691995621\n",
            "ITERATION_NO.: 2 LOSS_Generator: 7.7375617027282715 LOSS_Discriminator: 0.08666320145130157\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.715948581695557 LOSS_Discriminator: 0.06605423986911774\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.029913425445557 LOSS_Discriminator: 0.04875941947102547\n",
            "ITERATION_NO.: 5 LOSS_Generator: 6.394208908081055 LOSS_Discriminator: 0.060629673302173615\n",
            "ITERATION_NO.: 6 LOSS_Generator: 6.017431735992432 LOSS_Discriminator: 0.03695695474743843\n",
            "ITERATION_NO.: 7 LOSS_Generator: 5.937260150909424 LOSS_Discriminator: 0.11112616956233978\n",
            "ITERATION_NO.: 8 LOSS_Generator: 6.829168796539307 LOSS_Discriminator: 0.06510318070650101\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.616854190826416 LOSS_Discriminator: 0.05487176775932312\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.7581329345703125 LOSS_Discriminator: 0.05725240334868431\n",
            "ITERATION_NO.: 11 LOSS_Generator: 6.937238693237305 LOSS_Discriminator: 0.058310069143772125\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.012662410736084 LOSS_Discriminator: 0.06848388910293579\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.402534008026123 LOSS_Discriminator: 0.04658064618706703\n",
            "ITERATION_NO.: 14 LOSS_Generator: 6.020839214324951 LOSS_Discriminator: 0.07725927233695984\n",
            "ITERATION_NO.: 15 LOSS_Generator: 6.833032608032227 LOSS_Discriminator: 0.0570702850818634\n",
            "ITERATION_NO.: 16 LOSS_Generator: 6.944363594055176 LOSS_Discriminator: 0.0797361359000206\n",
            "ITERATION_NO.: 17 LOSS_Generator: 7.757630825042725 LOSS_Discriminator: 0.030477218329906464\n",
            "ITERATION_NO.: 18 LOSS_Generator: 7.75172233581543 LOSS_Discriminator: 0.0890464037656784\n",
            "ITERATION_NO.: 19 LOSS_Generator: 7.9326558113098145 LOSS_Discriminator: 0.07599058002233505\n",
            "ITERATION_NO.: 20 LOSS_Generator: 7.946490287780762 LOSS_Discriminator: 0.03393479436635971\n",
            "ITERATION_NO.: 21 LOSS_Generator: 7.979304790496826 LOSS_Discriminator: 0.09307818114757538\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.044045925140381 LOSS_Discriminator: 0.04368945211172104\n",
            "ITERATION_NO.: 23 LOSS_Generator: 6.173654556274414 LOSS_Discriminator: 0.06931886076927185\n",
            "ITERATION_NO.: 24 LOSS_Generator: 6.117241859436035 LOSS_Discriminator: 0.11985987424850464\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.187844276428223 LOSS_Discriminator: 0.061126649379730225\n",
            "ITERATION_NO.: 26 LOSS_Generator: 7.673396110534668 LOSS_Discriminator: 0.05439231917262077\n",
            "ITERATION_NO.: 27 LOSS_Generator: 8.042440414428711 LOSS_Discriminator: 0.061133500188589096\n",
            "ITERATION_NO.: 28 LOSS_Generator: 7.185418128967285 LOSS_Discriminator: 0.13114923238754272\n",
            "ITERATION_NO.: 29 LOSS_Generator: 7.068594932556152 LOSS_Discriminator: 0.07574958354234695\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.640873432159424 LOSS_Discriminator: 0.06424319744110107\n",
            "ITERATION_NO.: 31 LOSS_Generator: 7.383436679840088 LOSS_Discriminator: 0.0518944077193737\n",
            "ITERATION_NO.: 32 LOSS_Generator: 7.58944845199585 LOSS_Discriminator: 0.055140018463134766\n",
            "ITERATION_NO.: 33 LOSS_Generator: 6.310380935668945 LOSS_Discriminator: 0.03706338629126549\n",
            "ITERATION_NO.: 34 LOSS_Generator: 6.780311107635498 LOSS_Discriminator: 0.09498907625675201\n",
            "ITERATION_NO.: 35 LOSS_Generator: 6.263121128082275 LOSS_Discriminator: 0.05762428045272827\n",
            "ITERATION_NO.: 36 LOSS_Generator: 7.330235004425049 LOSS_Discriminator: 0.05366802215576172\n",
            "ITERATION_NO.: 37 LOSS_Generator: 7.6174798011779785 LOSS_Discriminator: 0.04400920867919922\n",
            "ITERATION_NO.: 38 LOSS_Generator: 8.264695167541504 LOSS_Discriminator: 0.05894099548459053\n",
            "ITERATION_NO.: 39 LOSS_Generator: 8.123638153076172 LOSS_Discriminator: 0.07529202103614807\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.581738471984863 LOSS_Discriminator: 0.08740203827619553\n",
            "ITERATION_NO.: 41 LOSS_Generator: 6.15639591217041 LOSS_Discriminator: 0.11458694934844971\n",
            "ITERATION_NO.: 42 LOSS_Generator: 5.9833197593688965 LOSS_Discriminator: 0.05080040916800499\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.116573333740234 LOSS_Discriminator: 0.07067744433879852\n",
            "ITERATION_NO.: 44 LOSS_Generator: 7.033149242401123 LOSS_Discriminator: 0.08817967772483826\n",
            "ITERATION_NO.: 45 LOSS_Generator: 8.247574806213379 LOSS_Discriminator: 0.028553646057844162\n",
            "ITERATION_NO.: 46 LOSS_Generator: 9.374911308288574 LOSS_Discriminator: 0.07025846093893051\n",
            "ITERATION_NO.: 47 LOSS_Generator: 8.912792205810547 LOSS_Discriminator: 0.134769469499588\n",
            "ITERATION_NO.: 48 LOSS_Generator: 7.198627948760986 LOSS_Discriminator: 0.1897311806678772\n",
            "ITERATION_NO.: 49 LOSS_Generator: 5.480585098266602 LOSS_Discriminator: 0.038927849382162094\n",
            "ITERATION_NO.: 50 LOSS_Generator: 5.393023490905762 LOSS_Discriminator: 0.18689098954200745\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.396190166473389 LOSS_Discriminator: 0.08588983863592148\n",
            "ITERATION_NO.: 52 LOSS_Generator: 7.871190071105957 LOSS_Discriminator: 0.024020783603191376\n",
            "ITERATION_NO.: 53 LOSS_Generator: 8.605440139770508 LOSS_Discriminator: 0.05720102787017822\n",
            "ITERATION_NO.: 54 LOSS_Generator: 9.485603332519531 LOSS_Discriminator: 0.04270303249359131\n",
            "ITERATION_NO.: 55 LOSS_Generator: 9.690162658691406 LOSS_Discriminator: 0.04089296609163284\n",
            "ITERATION_NO.: 56 LOSS_Generator: 9.041991233825684 LOSS_Discriminator: 0.08545012027025223\n",
            "ITERATION_NO.: 57 LOSS_Generator: 8.626461029052734 LOSS_Discriminator: 0.039484813809394836\n",
            "ITERATION_NO.: 58 LOSS_Generator: 7.197078704833984 LOSS_Discriminator: 0.08337147533893585\n",
            "ITERATION_NO.: 59 LOSS_Generator: 6.352927207946777 LOSS_Discriminator: 0.07143918424844742\n",
            "ITERATION_NO.: 60 LOSS_Generator: 5.650796413421631 LOSS_Discriminator: 0.0645817294716835\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.0084099769592285 LOSS_Discriminator: 0.08991111814975739\n",
            "ITERATION_NO.: 62 LOSS_Generator: 6.194521903991699 LOSS_Discriminator: 0.05498345568776131\n",
            "ITERATION_NO.: 63 LOSS_Generator: 7.321465969085693 LOSS_Discriminator: 0.06616419553756714\n",
            "ITERATION_NO.: 64 LOSS_Generator: 7.833603382110596 LOSS_Discriminator: 0.0930645763874054\n",
            "ITERATION_NO.: 65 LOSS_Generator: 8.149101257324219 LOSS_Discriminator: 0.012535463087260723\n",
            "ITERATION_NO.: 66 LOSS_Generator: 7.40778923034668 LOSS_Discriminator: 0.040944889187812805\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.4995832443237305 LOSS_Discriminator: 0.058320362120866776\n",
            "ITERATION_NO.: 68 LOSS_Generator: 7.008984565734863 LOSS_Discriminator: 0.05150903761386871\n",
            "ITERATION_NO.: 69 LOSS_Generator: 7.264325141906738 LOSS_Discriminator: 0.09313219785690308\n",
            "ITERATION_NO.: 70 LOSS_Generator: 7.364065170288086 LOSS_Discriminator: 0.046029672026634216\n",
            "ITERATION_NO.: 71 LOSS_Generator: 7.498785495758057 LOSS_Discriminator: 0.050295330584049225\n",
            "ITERATION_NO.: 72 LOSS_Generator: 7.872541427612305 LOSS_Discriminator: 0.03186919540166855\n",
            "ITERATION_NO.: 73 LOSS_Generator: 8.05289077758789 LOSS_Discriminator: 0.02673306316137314\n",
            "ITERATION_NO.: 74 LOSS_Generator: 7.556649684906006 LOSS_Discriminator: 0.08434830605983734\n",
            "ITERATION_NO.: 75 LOSS_Generator: 7.155699253082275 LOSS_Discriminator: 0.0656927078962326\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.2451701164245605 LOSS_Discriminator: 0.06452126801013947\n",
            "ITERATION_NO.: 77 LOSS_Generator: 7.2217583656311035 LOSS_Discriminator: 0.04395543783903122\n",
            "ITERATION_NO.: 78 LOSS_Generator: 6.991693496704102 LOSS_Discriminator: 0.039896465837955475\n",
            "ITERATION_NO.: 79 LOSS_Generator: 7.531613349914551 LOSS_Discriminator: 0.04181536287069321\n",
            "ITERATION_NO.: 80 LOSS_Generator: 8.188867568969727 LOSS_Discriminator: 0.0316334143280983\n",
            "ITERATION_NO.: 81 LOSS_Generator: 8.214917182922363 LOSS_Discriminator: 0.02100278064608574\n",
            "ITERATION_NO.: 82 LOSS_Generator: 8.254024505615234 LOSS_Discriminator: 0.034971050918102264\n",
            "ITERATION_NO.: 83 LOSS_Generator: 8.125877380371094 LOSS_Discriminator: 0.06507592648267746\n",
            "ITERATION_NO.: 84 LOSS_Generator: 7.310185432434082 LOSS_Discriminator: 0.0728849545121193\n",
            "ITERATION_NO.: 85 LOSS_Generator: 6.6092987060546875 LOSS_Discriminator: 0.026780303567647934\n",
            "ITERATION_NO.: 86 LOSS_Generator: 6.550241470336914 LOSS_Discriminator: 0.05801780894398689\n",
            "ITERATION_NO.: 87 LOSS_Generator: 6.789494037628174 LOSS_Discriminator: 0.0636637806892395\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.802911758422852 LOSS_Discriminator: 0.03223538026213646\n",
            "ITERATION_NO.: 89 LOSS_Generator: 8.365168571472168 LOSS_Discriminator: 0.08416816592216492\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.985702037811279 LOSS_Discriminator: 0.09062342345714569\n",
            "ITERATION_NO.: 91 LOSS_Generator: 7.343693256378174 LOSS_Discriminator: 0.0232709813863039\n",
            "ITERATION_NO.: 92 LOSS_Generator: 6.689966201782227 LOSS_Discriminator: 0.05478661507368088\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.4091572761535645 LOSS_Discriminator: 0.04094743728637695\n",
            "ITERATION_NO.: 94 LOSS_Generator: 6.078129291534424 LOSS_Discriminator: 0.04850953072309494\n",
            "ITERATION_NO.: 95 LOSS_Generator: 5.972109794616699 LOSS_Discriminator: 0.02078416384756565\n",
            "ITERATION_NO.: 96 LOSS_Generator: 6.465548515319824 LOSS_Discriminator: 0.011045628227293491\n",
            "ITERATION_NO.: 97 LOSS_Generator: 6.870158672332764 LOSS_Discriminator: 0.06458161771297455\n",
            "ITERATION_NO.: 98 LOSS_Generator: 6.439435958862305 LOSS_Discriminator: 0.12135852873325348\n",
            "ITERATION_NO.: 99 LOSS_Generator: 6.3749518394470215 LOSS_Discriminator: 0.08463454246520996\n",
            "ITERATION_NO.: 100 LOSS_Generator: 6.6002888679504395 LOSS_Discriminator: 0.08426575362682343\n",
            "ITERATION_NO.: 101 LOSS_Generator: 6.9329447746276855 LOSS_Discriminator: 0.05922277644276619\n",
            "ITERATION_NO.: 102 LOSS_Generator: 7.49693489074707 LOSS_Discriminator: 0.04314698278903961\n",
            "ITERATION_NO.: 103 LOSS_Generator: 7.716447353363037 LOSS_Discriminator: 0.047564852982759476\n",
            "ITERATION_NO.: 104 LOSS_Generator: 7.767031669616699 LOSS_Discriminator: 0.03736472874879837\n",
            "ITERATION_NO.: 105 LOSS_Generator: 8.40245246887207 LOSS_Discriminator: 0.1058729887008667\n",
            "ITERATION_NO.: 106 LOSS_Generator: 7.621972560882568 LOSS_Discriminator: 0.07062797248363495\n",
            "ITERATION_NO.: 107 LOSS_Generator: 6.816738128662109 LOSS_Discriminator: 0.10805700719356537\n",
            "ITERATION_NO.: 108 LOSS_Generator: 6.366660118103027 LOSS_Discriminator: 0.07427817583084106\n",
            "ITERATION_NO.: 109 LOSS_Generator: 6.319647789001465 LOSS_Discriminator: 0.06992246210575104\n",
            "ITERATION_NO.: 110 LOSS_Generator: 6.782440185546875 LOSS_Discriminator: 0.04754097759723663\n",
            "ITERATION_NO.: 111 LOSS_Generator: 8.585160255432129 LOSS_Discriminator: 0.029034603387117386\n",
            "ITERATION_NO.: 112 LOSS_Generator: 9.05156135559082 LOSS_Discriminator: 0.0649055689573288\n",
            "ITERATION_NO.: 113 LOSS_Generator: 8.864374160766602 LOSS_Discriminator: 0.19723927974700928\n",
            "ITERATION_NO.: 114 LOSS_Generator: 7.651328086853027 LOSS_Discriminator: 0.08891846984624863\n",
            "ITERATION_NO.: 115 LOSS_Generator: 6.5848822593688965 LOSS_Discriminator: 0.058449629694223404\n",
            "ITERATION_NO.: 116 LOSS_Generator: 6.301339149475098 LOSS_Discriminator: 0.09858036786317825\n",
            "ITERATION_NO.: 117 LOSS_Generator: 6.3672709465026855 LOSS_Discriminator: 0.11231875419616699\n",
            "ITERATION_NO.: 118 LOSS_Generator: 7.861392974853516 LOSS_Discriminator: 0.07525857537984848\n",
            "ITERATION_NO.: 119 LOSS_Generator: 9.189165115356445 LOSS_Discriminator: 0.046851303428411484\n",
            "ITERATION_NO.: 120 LOSS_Generator: 9.273250579833984 LOSS_Discriminator: 0.07993736863136292\n",
            "ITERATION_NO.: 121 LOSS_Generator: 8.3754243850708 LOSS_Discriminator: 0.07442419975996017\n",
            "ITERATION_NO.: 122 LOSS_Generator: 8.323659896850586 LOSS_Discriminator: 0.025036657229065895\n",
            "ITERATION_NO.: 123 LOSS_Generator: 7.227099418640137 LOSS_Discriminator: 0.06982237100601196\n",
            "ITERATION_NO.: 124 LOSS_Generator: 6.816575527191162 LOSS_Discriminator: 0.06995434314012527\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.451122283935547 LOSS_Discriminator: 0.05181527137756348\n",
            "ITERATION_NO.: 126 LOSS_Generator: 7.003934860229492 LOSS_Discriminator: 0.06644019484519958\n",
            "ITERATION_NO.: 127 LOSS_Generator: 7.905292987823486 LOSS_Discriminator: 0.07678521424531937\n",
            "ITERATION_NO.: 128 LOSS_Generator: 9.14841365814209 LOSS_Discriminator: 0.06651796400547028\n",
            "ITERATION_NO.: 129 LOSS_Generator: 9.452066421508789 LOSS_Discriminator: 0.0983901172876358\n",
            "ITERATION_NO.: 130 LOSS_Generator: 9.382414817810059 LOSS_Discriminator: 0.037392690777778625\n",
            "ITERATION_NO.: 131 LOSS_Generator: 8.093957901000977 LOSS_Discriminator: 0.08073900640010834\n",
            "ITERATION_NO.: 132 LOSS_Generator: 6.837069511413574 LOSS_Discriminator: 0.12389786541461945\n",
            "ITERATION_NO.: 133 LOSS_Generator: 6.663471221923828 LOSS_Discriminator: 0.054931968450546265\n",
            "ITERATION_NO.: 134 LOSS_Generator: 6.952800750732422 LOSS_Discriminator: 0.06730218231678009\n",
            "ITERATION_NO.: 135 LOSS_Generator: 7.2397356033325195 LOSS_Discriminator: 0.10068902373313904\n",
            "ITERATION_NO.: 136 LOSS_Generator: 8.23624038696289 LOSS_Discriminator: 0.044981539249420166\n",
            "ITERATION_NO.: 137 LOSS_Generator: 8.802728652954102 LOSS_Discriminator: 0.09375026077032089\n",
            "ITERATION_NO.: 138 LOSS_Generator: 8.015893936157227 LOSS_Discriminator: 0.11476673185825348\n",
            "ITERATION_NO.: 139 LOSS_Generator: 7.1351318359375 LOSS_Discriminator: 0.10142557322978973\n",
            "ITERATION_NO.: 140 LOSS_Generator: 5.810708522796631 LOSS_Discriminator: 0.10431957989931107\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.024217128753662 LOSS_Discriminator: 0.1328129917383194\n",
            "ITERATION_NO.: 142 LOSS_Generator: 7.583346366882324 LOSS_Discriminator: 0.11880521476268768\n",
            "ITERATION_NO.: 143 LOSS_Generator: 8.2039213180542 LOSS_Discriminator: 0.0660688579082489\n",
            "ITERATION_NO.: 144 LOSS_Generator: 8.541171073913574 LOSS_Discriminator: 0.13264530897140503\n",
            "ITERATION_NO.: 145 LOSS_Generator: 8.194696426391602 LOSS_Discriminator: 0.018184281885623932\n",
            "ITERATION_NO.: 146 LOSS_Generator: 7.494624137878418 LOSS_Discriminator: 0.0877508819103241\n",
            "ITERATION_NO.: 147 LOSS_Generator: 7.294559955596924 LOSS_Discriminator: 0.05292186886072159\n",
            "ITERATION_NO.: 148 LOSS_Generator: 6.253870964050293 LOSS_Discriminator: 0.06856653094291687\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.635120868682861 LOSS_Discriminator: 0.0474986657500267\n",
            "ITERATION_NO.: 150 LOSS_Generator: 7.110874652862549 LOSS_Discriminator: 0.09569059312343597\n",
            "ITERATION_NO.: 151 LOSS_Generator: 7.559947490692139 LOSS_Discriminator: 0.06395567208528519\n",
            "ITERATION_NO.: 152 LOSS_Generator: 8.097295761108398 LOSS_Discriminator: 0.04981251433491707\n",
            "ITERATION_NO.: 153 LOSS_Generator: 8.301669120788574 LOSS_Discriminator: 0.047852493822574615\n",
            "ITERATION_NO.: 154 LOSS_Generator: 8.220529556274414 LOSS_Discriminator: 0.08675343543291092\n",
            "ITERATION_NO.: 155 LOSS_Generator: 7.82725715637207 LOSS_Discriminator: 0.054044537246227264\n",
            "ITERATION_NO.: 156 LOSS_Generator: 9.006091117858887 LOSS_Discriminator: 0.04278206080198288\n",
            "ITERATION_NO.: 157 LOSS_Generator: 8.716303825378418 LOSS_Discriminator: 0.06785093992948532\n",
            "ITERATION_NO.: 158 LOSS_Generator: 8.255138397216797 LOSS_Discriminator: 0.0672575831413269\n",
            "ITERATION_NO.: 159 LOSS_Generator: 8.379867553710938 LOSS_Discriminator: 0.02643124759197235\n",
            "ITERATION_NO.: 160 LOSS_Generator: 7.971221923828125 LOSS_Discriminator: 0.07150870561599731\n",
            "ITERATION_NO.: 161 LOSS_Generator: 7.069199085235596 LOSS_Discriminator: 0.015421425923705101\n",
            "ITERATION_NO.: 162 LOSS_Generator: 7.727197647094727 LOSS_Discriminator: 0.0733640119433403\n",
            "ITERATION_NO.: 163 LOSS_Generator: 7.404896259307861 LOSS_Discriminator: 0.07583893835544586\n",
            "ITERATION_NO.: 164 LOSS_Generator: 7.1692094802856445 LOSS_Discriminator: 0.05681200325489044\n",
            "ITERATION_NO.: 165 LOSS_Generator: 7.67934513092041 LOSS_Discriminator: 0.08747902512550354\n",
            "ITERATION_NO.: 166 LOSS_Generator: 7.573904037475586 LOSS_Discriminator: 0.07258898764848709\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.589925765991211 LOSS_Discriminator: 0.09902997314929962\n",
            "ITERATION_NO.: 168 LOSS_Generator: 6.052276134490967 LOSS_Discriminator: 0.1450122892856598\n",
            "ITERATION_NO.: 169 LOSS_Generator: 6.007887363433838 LOSS_Discriminator: 0.06478506326675415\n",
            "ITERATION_NO.: 170 LOSS_Generator: 6.847440719604492 LOSS_Discriminator: 0.11667940020561218\n",
            "ITERATION_NO.: 171 LOSS_Generator: 7.577341079711914 LOSS_Discriminator: 0.04807813838124275\n",
            "ITERATION_NO.: 172 LOSS_Generator: 7.972141265869141 LOSS_Discriminator: 0.1442457139492035\n",
            "ITERATION_NO.: 173 LOSS_Generator: 7.495396614074707 LOSS_Discriminator: 0.0643286406993866\n",
            "ITERATION_NO.: 174 LOSS_Generator: 7.436781406402588 LOSS_Discriminator: 0.026937147602438927\n",
            "ITERATION_NO.: 175 LOSS_Generator: 6.785164833068848 LOSS_Discriminator: 0.06622831523418427\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.840749740600586 LOSS_Discriminator: 0.02678729221224785\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.11598014831543 LOSS_Discriminator: 0.11170278489589691\n",
            "ITERATION_NO.: 178 LOSS_Generator: 6.305910587310791 LOSS_Discriminator: 0.05602950230240822\n",
            "ITERATION_NO.: 179 LOSS_Generator: 6.778480052947998 LOSS_Discriminator: 0.03901252895593643\n",
            "ITERATION_NO.: 180 LOSS_Generator: 7.414633274078369 LOSS_Discriminator: 0.07489932328462601\n",
            "ITERATION_NO.: 181 LOSS_Generator: 8.10313606262207 LOSS_Discriminator: 0.05417938530445099\n",
            "ITERATION_NO.: 182 LOSS_Generator: 7.6102399826049805 LOSS_Discriminator: 0.07592709362506866\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.6201171875 LOSS_Discriminator: 0.10848788917064667\n",
            "ITERATION_NO.: 184 LOSS_Generator: 7.477721691131592 LOSS_Discriminator: 0.13272058963775635\n",
            "ITERATION_NO.: 185 LOSS_Generator: 5.78109884262085 LOSS_Discriminator: 0.11998382210731506\n",
            "ITERATION_NO.: 186 LOSS_Generator: 5.652364730834961 LOSS_Discriminator: 0.10118754953145981\n",
            "ITERATION_NO.: 187 LOSS_Generator: 6.260169506072998 LOSS_Discriminator: 0.1296115517616272\n",
            "ITERATION_NO.: 188 LOSS_Generator: 8.131914138793945 LOSS_Discriminator: 0.10202932357788086\n",
            "ITERATION_NO.: 189 LOSS_Generator: 9.2789888381958 LOSS_Discriminator: 0.046502966433763504\n",
            "ITERATION_NO.: 190 LOSS_Generator: 9.794055938720703 LOSS_Discriminator: 0.091004379093647\n",
            "ITERATION_NO.: 191 LOSS_Generator: 9.056539535522461 LOSS_Discriminator: 0.09208554029464722\n",
            "ITERATION_NO.: 192 LOSS_Generator: 8.541232109069824 LOSS_Discriminator: 0.0425434410572052\n",
            "ITERATION_NO.: 193 LOSS_Generator: 7.052862644195557 LOSS_Discriminator: 0.072921022772789\n",
            "ITERATION_NO.: 194 LOSS_Generator: 6.545352935791016 LOSS_Discriminator: 0.10493320226669312\n",
            "ITERATION_NO.: 195 LOSS_Generator: 6.781862735748291 LOSS_Discriminator: 0.05938895046710968\n",
            "ITERATION_NO.: 196 LOSS_Generator: 7.819002151489258 LOSS_Discriminator: 0.0722256451845169\n",
            "ITERATION_NO.: 197 LOSS_Generator: 8.040840148925781 LOSS_Discriminator: 0.03637436032295227\n",
            "ITERATION_NO.: 198 LOSS_Generator: 8.458913803100586 LOSS_Discriminator: 0.11203458905220032\n",
            "ITERATION_NO.: 199 LOSS_Generator: 8.673603057861328 LOSS_Discriminator: 0.017429564148187637\n",
            "ITERATION_NO.: 200 LOSS_Generator: 8.377603530883789 LOSS_Discriminator: 0.12389761209487915\n",
            "ITERATION_NO.: 201 LOSS_Generator: 7.806856632232666 LOSS_Discriminator: 0.06747332960367203\n",
            "ITERATION_NO.: 202 LOSS_Generator: 7.26339054107666 LOSS_Discriminator: 0.0781993567943573\n",
            "ITERATION_NO.: 203 LOSS_Generator: 7.239471435546875 LOSS_Discriminator: 0.09956599771976471\n",
            "ITERATION_NO.: 204 LOSS_Generator: 6.581431865692139 LOSS_Discriminator: 0.09379532933235168\n",
            "ITERATION_NO.: 205 LOSS_Generator: 6.8224029541015625 LOSS_Discriminator: 0.06986068934202194\n",
            "ITERATION_NO.: 206 LOSS_Generator: 7.5213212966918945 LOSS_Discriminator: 0.06357716768980026\n",
            "ITERATION_NO.: 207 LOSS_Generator: 7.821417808532715 LOSS_Discriminator: 0.02618328668177128\n",
            "ITERATION_NO.: 208 LOSS_Generator: 8.347407341003418 LOSS_Discriminator: 0.08232956379652023\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.849687576293945 LOSS_Discriminator: 0.031445808708667755\n",
            "ITERATION_NO.: 210 LOSS_Generator: 7.594539642333984 LOSS_Discriminator: 0.036924902349710464\n",
            "ITERATION_NO.: 211 LOSS_Generator: 7.378805637359619 LOSS_Discriminator: 0.09571399539709091\n",
            "ITERATION_NO.: 212 LOSS_Generator: 7.239065170288086 LOSS_Discriminator: 0.10181441903114319\n",
            "ITERATION_NO.: 213 LOSS_Generator: 6.530190467834473 LOSS_Discriminator: 0.06856048107147217\n",
            "ITERATION_NO.: 214 LOSS_Generator: 6.400837421417236 LOSS_Discriminator: 0.06789329648017883\n",
            "ITERATION_NO.: 215 LOSS_Generator: 7.281041145324707 LOSS_Discriminator: 0.0621313601732254\n",
            "ITERATION_NO.: 216 LOSS_Generator: 7.301745414733887 LOSS_Discriminator: 0.06811138987541199\n",
            "ITERATION_NO.: 217 LOSS_Generator: 8.412517547607422 LOSS_Discriminator: 0.050344035029411316\n",
            "ITERATION_NO.: 218 LOSS_Generator: 8.843751907348633 LOSS_Discriminator: 0.08198821544647217\n",
            "ITERATION_NO.: 219 LOSS_Generator: 8.695837020874023 LOSS_Discriminator: 0.06381691992282867\n",
            "ITERATION_NO.: 220 LOSS_Generator: 6.680995464324951 LOSS_Discriminator: 0.06292824447154999\n",
            "ITERATION_NO.: 221 LOSS_Generator: 5.5933837890625 LOSS_Discriminator: 0.14844492077827454\n",
            "ITERATION_NO.: 222 LOSS_Generator: 5.614480972290039 LOSS_Discriminator: 0.12169164419174194\n",
            "ITERATION_NO.: 223 LOSS_Generator: 6.547160625457764 LOSS_Discriminator: 0.09443002939224243\n",
            "ITERATION_NO.: 224 LOSS_Generator: 7.743517875671387 LOSS_Discriminator: 0.07120084762573242\n",
            "ITERATION_NO.: 225 LOSS_Generator: 7.882206439971924 LOSS_Discriminator: 0.12944895029067993\n",
            "ITERATION_NO.: 226 LOSS_Generator: 8.07550048828125 LOSS_Discriminator: 0.13447511196136475\n",
            "ITERATION_NO.: 227 LOSS_Generator: 6.6545729637146 LOSS_Discriminator: 0.06128942221403122\n",
            "ITERATION_NO.: 228 LOSS_Generator: 5.956264019012451 LOSS_Discriminator: 0.0856424942612648\n",
            "ITERATION_NO.: 229 LOSS_Generator: 5.8215651512146 LOSS_Discriminator: 0.12488800287246704\n",
            "ITERATION_NO.: 230 LOSS_Generator: 7.154573440551758 LOSS_Discriminator: 0.06683160364627838\n",
            "ITERATION_NO.: 231 LOSS_Generator: 7.173097610473633 LOSS_Discriminator: 0.05664816498756409\n",
            "ITERATION_NO.: 232 LOSS_Generator: 8.30781078338623 LOSS_Discriminator: 0.07207268476486206\n",
            "ITERATION_NO.: 233 LOSS_Generator: 8.382011413574219 LOSS_Discriminator: 0.07155356556177139\n",
            "ITERATION_NO.: 234 LOSS_Generator: 7.8047590255737305 LOSS_Discriminator: 0.052962303161621094\n",
            "ITERATION_NO.: 235 LOSS_Generator: 7.4048662185668945 LOSS_Discriminator: 0.034898027777671814\n",
            "ITERATION_NO.: 236 LOSS_Generator: 6.126217842102051 LOSS_Discriminator: 0.03924436494708061\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.165793418884277 LOSS_Discriminator: 0.04342316463589668\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.5777764320373535 LOSS_Discriminator: 0.06541794538497925\n",
            "ITERATION_NO.: 239 LOSS_Generator: 6.957362174987793 LOSS_Discriminator: 0.06689481437206268\n",
            "ITERATION_NO.: 240 LOSS_Generator: 7.643120765686035 LOSS_Discriminator: 0.07789680361747742\n",
            "ITERATION_NO.: 241 LOSS_Generator: 7.179792404174805 LOSS_Discriminator: 0.06764215230941772\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.883902549743652 LOSS_Discriminator: 0.09947787970304489\n",
            "ITERATION_NO.: 243 LOSS_Generator: 7.120048999786377 LOSS_Discriminator: 0.039574019610881805\n",
            "ITERATION_NO.: 244 LOSS_Generator: 7.077491283416748 LOSS_Discriminator: 0.0662243515253067\n",
            "ITERATION_NO.: 245 LOSS_Generator: 7.978808403015137 LOSS_Discriminator: 0.10043077915906906\n",
            "ITERATION_NO.: 246 LOSS_Generator: 7.613626003265381 LOSS_Discriminator: 0.10325361788272858\n",
            "ITERATION_NO.: 247 LOSS_Generator: 6.661777496337891 LOSS_Discriminator: 0.08299758285284042\n",
            "ITERATION_NO.: 248 LOSS_Generator: 6.722165107727051 LOSS_Discriminator: 0.0693434551358223\n",
            "ITERATION_NO.: 249 LOSS_Generator: 6.536135673522949 LOSS_Discriminator: 0.049831654876470566\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.083712100982666 LOSS_Discriminator: 0.09530334174633026\n",
            "ITERATION_NO.: 251 LOSS_Generator: 7.325016975402832 LOSS_Discriminator: 0.04075973480939865\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.299361705780029 LOSS_Discriminator: 0.037214137613773346\n",
            "ITERATION_NO.: 253 LOSS_Generator: 7.118899345397949 LOSS_Discriminator: 0.07470989972352982\n",
            "ITERATION_NO.: 254 LOSS_Generator: 7.069782257080078 LOSS_Discriminator: 0.07002967596054077\n",
            "ITERATION_NO.: 255 LOSS_Generator: 6.562731742858887 LOSS_Discriminator: 0.05948951095342636\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.3524169921875 LOSS_Discriminator: 0.08282989263534546\n",
            "ITERATION_NO.: 257 LOSS_Generator: 7.0065598487854 LOSS_Discriminator: 0.08380989730358124\n",
            "ITERATION_NO.: 258 LOSS_Generator: 7.149023056030273 LOSS_Discriminator: 0.08341668546199799\n",
            "ITERATION_NO.: 259 LOSS_Generator: 6.969564914703369 LOSS_Discriminator: 0.03411336615681648\n",
            "ITERATION_NO.: 260 LOSS_Generator: 7.3901686668396 LOSS_Discriminator: 0.09409622848033905\n",
            "ITERATION_NO.: 261 LOSS_Generator: 6.773164749145508 LOSS_Discriminator: 0.09223726391792297\n",
            "ITERATION_NO.: 262 LOSS_Generator: 6.860909461975098 LOSS_Discriminator: 0.04577546566724777\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.866240978240967 LOSS_Discriminator: 0.05913899093866348\n",
            "ITERATION_NO.: 264 LOSS_Generator: 7.224401950836182 LOSS_Discriminator: 0.06831622123718262\n",
            "ITERATION_NO.: 265 LOSS_Generator: 7.342181205749512 LOSS_Discriminator: 0.02623862214386463\n",
            "ITERATION_NO.: 266 LOSS_Generator: 6.797705173492432 LOSS_Discriminator: 0.13096006214618683\n",
            "ITERATION_NO.: 267 LOSS_Generator: 7.11741828918457 LOSS_Discriminator: 0.033345215022563934\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.647957801818848 LOSS_Discriminator: 0.04468884319067001\n",
            "ITERATION_NO.: 269 LOSS_Generator: 7.303843975067139 LOSS_Discriminator: 0.047672074288129807\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.1727094650268555 LOSS_Discriminator: 0.06620743125677109\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.564062118530273 LOSS_Discriminator: 0.05922425538301468\n",
            "ITERATION_NO.: 272 LOSS_Generator: 7.379448890686035 LOSS_Discriminator: 0.08307836949825287\n",
            "ITERATION_NO.: 273 LOSS_Generator: 7.705995559692383 LOSS_Discriminator: 0.04578441008925438\n",
            "ITERATION_NO.: 274 LOSS_Generator: 7.654844760894775 LOSS_Discriminator: 0.049995265901088715\n",
            "ITERATION_NO.: 275 LOSS_Generator: 7.5514655113220215 LOSS_Discriminator: 0.08347585797309875\n",
            "ITERATION_NO.: 276 LOSS_Generator: 6.865113735198975 LOSS_Discriminator: 0.016416210681200027\n",
            "ITERATION_NO.: 277 LOSS_Generator: 6.892090320587158 LOSS_Discriminator: 0.10068266093730927\n",
            "ITERATION_NO.: 278 LOSS_Generator: 7.470184326171875 LOSS_Discriminator: 0.08489527553319931\n",
            "ITERATION_NO.: 279 LOSS_Generator: 7.388946533203125 LOSS_Discriminator: 0.0264604315161705\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.422031879425049 LOSS_Discriminator: 0.10098093748092651\n",
            "ITERATION_NO.: 281 LOSS_Generator: 6.997024059295654 LOSS_Discriminator: 0.10462687909603119\n",
            "ITERATION_NO.: 282 LOSS_Generator: 5.894768714904785 LOSS_Discriminator: 0.05017673224210739\n",
            "ITERATION_NO.: 283 LOSS_Generator: 6.0543532371521 LOSS_Discriminator: 0.08510436117649078\n",
            "ITERATION_NO.: 284 LOSS_Generator: 6.802633285522461 LOSS_Discriminator: 0.04626981168985367\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.724145412445068 LOSS_Discriminator: 0.07340165972709656\n",
            "ITERATION_NO.: 286 LOSS_Generator: 8.07726764678955 LOSS_Discriminator: 0.046238187700510025\n",
            "ITERATION_NO.: 287 LOSS_Generator: 9.171273231506348 LOSS_Discriminator: 0.05631038919091225\n",
            "ITERATION_NO.: 288 LOSS_Generator: 9.198600769042969 LOSS_Discriminator: 0.078040212392807\n",
            "ITERATION_NO.: 289 LOSS_Generator: 8.395947456359863 LOSS_Discriminator: 0.03261270374059677\n",
            "ITERATION_NO.: 290 LOSS_Generator: 7.309308052062988 LOSS_Discriminator: 0.04776245355606079\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.095694541931152 LOSS_Discriminator: 0.07618497312068939\n",
            "ITERATION_NO.: 292 LOSS_Generator: 5.630614757537842 LOSS_Discriminator: 0.0739232525229454\n",
            "ITERATION_NO.: 293 LOSS_Generator: 5.723634243011475 LOSS_Discriminator: 0.07757823169231415\n",
            "ITERATION_NO.: 294 LOSS_Generator: 6.782291889190674 LOSS_Discriminator: 0.10367666184902191\n",
            "ITERATION_NO.: 295 LOSS_Generator: 8.875478744506836 LOSS_Discriminator: 0.062176868319511414\n",
            "ITERATION_NO.: 296 LOSS_Generator: 9.310470581054688 LOSS_Discriminator: 0.13423901796340942\n",
            "ITERATION_NO.: 297 LOSS_Generator: 8.902503967285156 LOSS_Discriminator: 0.14654362201690674\n",
            "ITERATION_NO.: 298 LOSS_Generator: 8.280549049377441 LOSS_Discriminator: 0.039981551468372345\n",
            "ITERATION_NO.: 299 LOSS_Generator: 7.592967510223389 LOSS_Discriminator: 0.02415085956454277\n",
            "ITERATION_NO.: 300 LOSS_Generator: 6.447495937347412 LOSS_Discriminator: 0.0911635011434555\n",
            "ITERATION_NO.: 301 LOSS_Generator: 6.5876994132995605 LOSS_Discriminator: 0.05554305016994476\n",
            "ITERATION_NO.: 302 LOSS_Generator: 6.779066562652588 LOSS_Discriminator: 0.041299961507320404\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.190919876098633 LOSS_Discriminator: 0.058941587805747986\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.850703716278076 LOSS_Discriminator: 0.06230204924941063\n",
            "ITERATION_NO.: 305 LOSS_Generator: 6.946676731109619 LOSS_Discriminator: 0.0652855783700943\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.2835373878479 LOSS_Discriminator: 0.017578456550836563\n",
            "ITERATION_NO.: 307 LOSS_Generator: 6.773146152496338 LOSS_Discriminator: 0.05080283060669899\n",
            "ITERATION_NO.: 308 LOSS_Generator: 6.676584243774414 LOSS_Discriminator: 0.09557446837425232\n",
            "ITERATION_NO.: 309 LOSS_Generator: 6.399569511413574 LOSS_Discriminator: 0.045799724757671356\n",
            "ITERATION_NO.: 310 LOSS_Generator: 7.0365447998046875 LOSS_Discriminator: 0.04466943442821503\n",
            "ITERATION_NO.: 311 LOSS_Generator: 7.368343353271484 LOSS_Discriminator: 0.039663005620241165\n",
            "ITERATION_NO.: 312 LOSS_Generator: 8.040343284606934 LOSS_Discriminator: 0.05110825598239899\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.872207164764404 LOSS_Discriminator: 0.027973812073469162\n",
            "ITERATION_NO.: 314 LOSS_Generator: 7.463803768157959 LOSS_Discriminator: 0.049844443798065186\n",
            "ITERATION_NO.: 315 LOSS_Generator: 6.692837715148926 LOSS_Discriminator: 0.085690438747406\n",
            "ITERATION_NO.: 316 LOSS_Generator: 6.5393853187561035 LOSS_Discriminator: 0.030330989509820938\n",
            "ITERATION_NO.: 317 LOSS_Generator: 6.092155933380127 LOSS_Discriminator: 0.09834496676921844\n",
            "ITERATION_NO.: 318 LOSS_Generator: 7.047598361968994 LOSS_Discriminator: 0.09107443690299988\n",
            "ITERATION_NO.: 319 LOSS_Generator: 7.801666259765625 LOSS_Discriminator: 0.09666075557470322\n",
            "ITERATION_NO.: 320 LOSS_Generator: 8.551918983459473 LOSS_Discriminator: 0.05516604706645012\n",
            "ITERATION_NO.: 321 LOSS_Generator: 8.453936576843262 LOSS_Discriminator: 0.09734582901000977\n",
            "ITERATION_NO.: 322 LOSS_Generator: 6.937978744506836 LOSS_Discriminator: 0.04236447066068649\n",
            "ITERATION_NO.: 323 LOSS_Generator: 6.73933219909668 LOSS_Discriminator: 0.041114673018455505\n",
            "ITERATION_NO.: 324 LOSS_Generator: 6.122095108032227 LOSS_Discriminator: 0.11858227103948593\n",
            "ITERATION_NO.: 325 LOSS_Generator: 6.421967029571533 LOSS_Discriminator: 0.08787290751934052\n",
            "ITERATION_NO.: 326 LOSS_Generator: 7.411118030548096 LOSS_Discriminator: 0.057614609599113464\n",
            "ITERATION_NO.: 327 LOSS_Generator: 8.433579444885254 LOSS_Discriminator: 0.02035205252468586\n",
            "ITERATION_NO.: 328 LOSS_Generator: 9.029760360717773 LOSS_Discriminator: 0.12167970836162567\n",
            "ITERATION_NO.: 329 LOSS_Generator: 8.823044776916504 LOSS_Discriminator: 0.11951055377721786\n",
            "ITERATION_NO.: 330 LOSS_Generator: 7.255821704864502 LOSS_Discriminator: 0.11124610900878906\n",
            "ITERATION_NO.: 331 LOSS_Generator: 5.8690619468688965 LOSS_Discriminator: 0.020499860867857933\n",
            "ITERATION_NO.: 332 LOSS_Generator: 5.674257755279541 LOSS_Discriminator: 0.1682228147983551\n",
            "ITERATION_NO.: 333 LOSS_Generator: 6.925359725952148 LOSS_Discriminator: 0.08094478398561478\n",
            "ITERATION_NO.: 334 LOSS_Generator: 7.360280990600586 LOSS_Discriminator: 0.07055778056383133\n",
            "ITERATION_NO.: 335 LOSS_Generator: 8.352428436279297 LOSS_Discriminator: 0.0241472776979208\n",
            "ITERATION_NO.: 336 LOSS_Generator: 8.678716659545898 LOSS_Discriminator: 0.049921952188014984\n",
            "ITERATION_NO.: 337 LOSS_Generator: 7.994992733001709 LOSS_Discriminator: 0.032909512519836426\n",
            "ITERATION_NO.: 338 LOSS_Generator: 8.085159301757812 LOSS_Discriminator: 0.03051389940083027\n",
            "ITERATION_NO.: 339 LOSS_Generator: 7.044152736663818 LOSS_Discriminator: 0.0825345516204834\n",
            "ITERATION_NO.: 340 LOSS_Generator: 7.356976509094238 LOSS_Discriminator: 0.06945772469043732\n",
            "ITERATION_NO.: 341 LOSS_Generator: 7.024376392364502 LOSS_Discriminator: 0.13383394479751587\n",
            "ITERATION_NO.: 342 LOSS_Generator: 7.144896030426025 LOSS_Discriminator: 0.09187380224466324\n",
            "ITERATION_NO.: 343 LOSS_Generator: 7.560082912445068 LOSS_Discriminator: 0.061252057552337646\n",
            "ITERATION_NO.: 344 LOSS_Generator: 7.385824680328369 LOSS_Discriminator: 0.034546561539173126\n",
            "ITERATION_NO.: 345 LOSS_Generator: 7.8539557456970215 LOSS_Discriminator: 0.037121012806892395\n",
            "ITERATION_NO.: 346 LOSS_Generator: 7.251763820648193 LOSS_Discriminator: 0.0490269660949707\n",
            "ITERATION_NO.: 347 LOSS_Generator: 6.894554615020752 LOSS_Discriminator: 0.04429461061954498\n",
            "ITERATION_NO.: 348 LOSS_Generator: 7.303220748901367 LOSS_Discriminator: 0.07466438412666321\n",
            "ITERATION_NO.: 349 LOSS_Generator: 6.962820529937744 LOSS_Discriminator: 0.07499980181455612\n",
            "ITERATION_NO.: 350 LOSS_Generator: 6.96259880065918 LOSS_Discriminator: 0.08140046149492264\n",
            "ITERATION_NO.: 351 LOSS_Generator: 6.728777885437012 LOSS_Discriminator: 0.048597291111946106\n",
            "ITERATION_NO.: 352 LOSS_Generator: 6.678913116455078 LOSS_Discriminator: 0.07909335196018219\n",
            "ITERATION_NO.: 353 LOSS_Generator: 7.011883735656738 LOSS_Discriminator: 0.08360541611909866\n",
            "ITERATION_NO.: 354 LOSS_Generator: 7.351089000701904 LOSS_Discriminator: 0.02314090169966221\n",
            "ITERATION_NO.: 355 LOSS_Generator: 8.060965538024902 LOSS_Discriminator: 0.06813716888427734\n",
            "ITERATION_NO.: 356 LOSS_Generator: 7.683032035827637 LOSS_Discriminator: 0.060919687151908875\n",
            "ITERATION_NO.: 357 LOSS_Generator: 7.910241603851318 LOSS_Discriminator: 0.07110700756311417\n",
            "ITERATION_NO.: 358 LOSS_Generator: 6.823133945465088 LOSS_Discriminator: 0.06560052931308746\n",
            "ITERATION_NO.: 359 LOSS_Generator: 6.630430221557617 LOSS_Discriminator: 0.06563851237297058\n",
            "ITERATION_NO.: 360 LOSS_Generator: 6.300648212432861 LOSS_Discriminator: 0.06709302961826324\n",
            "ITERATION_NO.: 361 LOSS_Generator: 6.454738140106201 LOSS_Discriminator: 0.09634869545698166\n",
            "ITERATION_NO.: 362 LOSS_Generator: 6.744697093963623 LOSS_Discriminator: 0.04363638162612915\n",
            "ITERATION_NO.: 363 LOSS_Generator: 7.585811138153076 LOSS_Discriminator: 0.06963958591222763\n",
            "ITERATION_NO.: 364 LOSS_Generator: 7.421604633331299 LOSS_Discriminator: 0.06859081238508224\n",
            "ITERATION_NO.: 365 LOSS_Generator: 7.1723856925964355 LOSS_Discriminator: 0.08587945997714996\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.661747455596924 LOSS_Discriminator: 0.06049029156565666\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.499967098236084 LOSS_Discriminator: 0.0669824406504631\n",
            "ITERATION_NO.: 368 LOSS_Generator: 6.888057231903076 LOSS_Discriminator: 0.04573068767786026\n",
            "ITERATION_NO.: 369 LOSS_Generator: 6.406633377075195 LOSS_Discriminator: 0.07850337028503418\n",
            "ITERATION_NO.: 370 LOSS_Generator: 6.996889591217041 LOSS_Discriminator: 0.051020216196775436\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.031184196472168 LOSS_Discriminator: 0.03782200813293457\n",
            "ITERATION_NO.: 372 LOSS_Generator: 7.523654937744141 LOSS_Discriminator: 0.05416188761591911\n",
            "ITERATION_NO.: 373 LOSS_Generator: 7.304314613342285 LOSS_Discriminator: 0.07909819483757019\n",
            "ITERATION_NO.: 374 LOSS_Generator: 7.3038201332092285 LOSS_Discriminator: 0.08121176064014435\n",
            "ITERATION_NO.: 375 LOSS_Generator: 7.719781398773193 LOSS_Discriminator: 0.10285453498363495\n",
            "ITERATION_NO.: 376 LOSS_Generator: 6.762332916259766 LOSS_Discriminator: 0.07931938022375107\n",
            "ITERATION_NO.: 377 LOSS_Generator: 7.430506706237793 LOSS_Discriminator: 0.05057675391435623\n",
            "ITERATION_NO.: 378 LOSS_Generator: 7.003139495849609 LOSS_Discriminator: 0.14047139883041382\n",
            "ITERATION_NO.: 379 LOSS_Generator: 7.651463031768799 LOSS_Discriminator: 0.05261771380901337\n",
            "ITERATION_NO.: 380 LOSS_Generator: 7.90488338470459 LOSS_Discriminator: 0.12042457610368729\n",
            "ITERATION_NO.: 381 LOSS_Generator: 6.176504611968994 LOSS_Discriminator: 0.03262599557638168\n",
            "ITERATION_NO.: 382 LOSS_Generator: 7.1512346267700195 LOSS_Discriminator: 0.051228806376457214\n",
            "ITERATION_NO.: 383 LOSS_Generator: 6.538557052612305 LOSS_Discriminator: 0.09835696965456009\n",
            "ITERATION_NO.: 384 LOSS_Generator: 6.750918388366699 LOSS_Discriminator: 0.09805487841367722\n",
            "ITERATION_NO.: 385 LOSS_Generator: 6.878983020782471 LOSS_Discriminator: 0.058768514543771744\n",
            "ITERATION_NO.: 386 LOSS_Generator: 8.0374755859375 LOSS_Discriminator: 0.03362283855676651\n",
            "ITERATION_NO.: 387 LOSS_Generator: 8.199602127075195 LOSS_Discriminator: 0.10790832340717316\n",
            "ITERATION_NO.: 388 LOSS_Generator: 7.9563727378845215 LOSS_Discriminator: 0.0480949692428112\n",
            "ITERATION_NO.: 389 LOSS_Generator: 7.050835132598877 LOSS_Discriminator: 0.1564522683620453\n",
            "ITERATION_NO.: 390 LOSS_Generator: 6.404550075531006 LOSS_Discriminator: 0.05905299633741379\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.290023326873779 LOSS_Discriminator: 0.10785046964883804\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.512169361114502 LOSS_Discriminator: 0.040147338062524796\n",
            "ITERATION_NO.: 393 LOSS_Generator: 8.354412078857422 LOSS_Discriminator: 0.08547107875347137\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.581021308898926 LOSS_Discriminator: 0.07174394279718399\n",
            "ITERATION_NO.: 395 LOSS_Generator: 8.012133598327637 LOSS_Discriminator: 0.11758264899253845\n",
            "ITERATION_NO.: 396 LOSS_Generator: 7.633695125579834 LOSS_Discriminator: 0.02672208845615387\n",
            "ITERATION_NO.: 397 LOSS_Generator: 6.540017127990723 LOSS_Discriminator: 0.041849035769701004\n",
            "ITERATION_NO.: 398 LOSS_Generator: 6.410279750823975 LOSS_Discriminator: 0.09471766650676727\n",
            "ITERATION_NO.: 399 LOSS_Generator: 6.5773396492004395 LOSS_Discriminator: 0.05065286159515381\n",
            "ITERATION_NO.: 400 LOSS_Generator: 7.121881484985352 LOSS_Discriminator: 0.059245236217975616\n",
            "ITERATION_NO.: 401 LOSS_Generator: 7.632202625274658 LOSS_Discriminator: 0.06674680858850479\n",
            "ITERATION_NO.: 402 LOSS_Generator: 7.101032733917236 LOSS_Discriminator: 0.15403984487056732\n",
            "ITERATION_NO.: 403 LOSS_Generator: 6.72475528717041 LOSS_Discriminator: 0.10032232105731964\n",
            "ITERATION_NO.: 404 LOSS_Generator: 6.491848945617676 LOSS_Discriminator: 0.049140334129333496\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.1529645919799805 LOSS_Discriminator: 0.05290515720844269\n",
            "ITERATION_NO.: 406 LOSS_Generator: 7.240579605102539 LOSS_Discriminator: 0.07374308258295059\n",
            "ITERATION_NO.: 407 LOSS_Generator: 7.897138595581055 LOSS_Discriminator: 0.07400219887495041\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.481980800628662 LOSS_Discriminator: 0.16374003887176514\n",
            "ITERATION_NO.: 409 LOSS_Generator: 6.289722919464111 LOSS_Discriminator: 0.10959385335445404\n",
            "ITERATION_NO.: 410 LOSS_Generator: 6.3552165031433105 LOSS_Discriminator: 0.08029326796531677\n",
            "ITERATION_NO.: 411 LOSS_Generator: 6.910376071929932 LOSS_Discriminator: 0.09587013721466064\n",
            "ITERATION_NO.: 412 LOSS_Generator: 7.2332353591918945 LOSS_Discriminator: 0.07172996550798416\n",
            "ITERATION_NO.: 413 LOSS_Generator: 7.619419574737549 LOSS_Discriminator: 0.05160369351506233\n",
            "ITERATION_NO.: 414 LOSS_Generator: 8.175278663635254 LOSS_Discriminator: 0.06953639537096024\n",
            "ITERATION_NO.: 415 LOSS_Generator: 8.04906940460205 LOSS_Discriminator: 0.07680882513523102\n",
            "ITERATION_NO.: 416 LOSS_Generator: 7.8136305809021 LOSS_Discriminator: 0.0987609401345253\n",
            "ITERATION_NO.: 417 LOSS_Generator: 6.823453903198242 LOSS_Discriminator: 0.04472094774246216\n",
            "ITERATION_NO.: 418 LOSS_Generator: 6.733688831329346 LOSS_Discriminator: 0.04413735866546631\n",
            "ITERATION_NO.: 419 LOSS_Generator: 6.503436088562012 LOSS_Discriminator: 0.022756049409508705\n",
            "ITERATION_NO.: 420 LOSS_Generator: 7.783658504486084 LOSS_Discriminator: 0.07780112326145172\n",
            "ITERATION_NO.: 421 LOSS_Generator: 8.801346778869629 LOSS_Discriminator: 0.07221969217061996\n",
            "ITERATION_NO.: 422 LOSS_Generator: 9.60452651977539 LOSS_Discriminator: 0.07232999801635742\n",
            "ITERATION_NO.: 423 LOSS_Generator: 9.37386703491211 LOSS_Discriminator: 0.11793191730976105\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.850703239440918 LOSS_Discriminator: 0.04341622069478035\n",
            "ITERATION_NO.: 425 LOSS_Generator: 6.842128753662109 LOSS_Discriminator: 0.05423540249466896\n",
            "ITERATION_NO.: 426 LOSS_Generator: 6.465574741363525 LOSS_Discriminator: 0.056398455053567886\n",
            "ITERATION_NO.: 427 LOSS_Generator: 6.503422260284424 LOSS_Discriminator: 0.07440800964832306\n",
            "ITERATION_NO.: 428 LOSS_Generator: 7.2351975440979 LOSS_Discriminator: 0.08805759251117706\n",
            "ITERATION_NO.: 429 LOSS_Generator: 7.953327655792236 LOSS_Discriminator: 0.12071596831083298\n",
            "ITERATION_NO.: 430 LOSS_Generator: 8.31745433807373 LOSS_Discriminator: 0.021216336637735367\n",
            "ITERATION_NO.: 431 LOSS_Generator: 8.674151420593262 LOSS_Discriminator: 0.02758914604783058\n",
            "ITERATION_NO.: 432 LOSS_Generator: 8.407066345214844 LOSS_Discriminator: 0.08710385859012604\n",
            "ITERATION_NO.: 433 LOSS_Generator: 7.674569606781006 LOSS_Discriminator: 0.04605025798082352\n",
            "ITERATION_NO.: 434 LOSS_Generator: 6.640486240386963 LOSS_Discriminator: 0.12112978100776672\n",
            "ITERATION_NO.: 435 LOSS_Generator: 5.849557876586914 LOSS_Discriminator: 0.059309862554073334\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.041257381439209 LOSS_Discriminator: 0.07471530884504318\n",
            "ITERATION_NO.: 437 LOSS_Generator: 6.636219501495361 LOSS_Discriminator: 0.04934757202863693\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.929610729217529 LOSS_Discriminator: 0.08212900906801224\n",
            "ITERATION_NO.: 439 LOSS_Generator: 7.044772148132324 LOSS_Discriminator: 0.04836604744195938\n",
            "ITERATION_NO.: 440 LOSS_Generator: 7.243688583374023 LOSS_Discriminator: 0.0385998971760273\n",
            "ITERATION_NO.: 441 LOSS_Generator: 7.591318130493164 LOSS_Discriminator: 0.055007461458444595\n",
            "ITERATION_NO.: 442 LOSS_Generator: 8.236272811889648 LOSS_Discriminator: 0.10688330978155136\n",
            "ITERATION_NO.: 443 LOSS_Generator: 7.765132904052734 LOSS_Discriminator: 0.07642073184251785\n",
            "ITERATION_NO.: 444 LOSS_Generator: 8.069133758544922 LOSS_Discriminator: 0.0893000066280365\n",
            "ITERATION_NO.: 445 LOSS_Generator: 8.319619178771973 LOSS_Discriminator: 0.028124528005719185\n",
            "ITERATION_NO.: 446 LOSS_Generator: 8.080436706542969 LOSS_Discriminator: 0.13027817010879517\n",
            "ITERATION_NO.: 447 LOSS_Generator: 7.580984115600586 LOSS_Discriminator: 0.09004035592079163\n",
            "ITERATION_NO.: 448 LOSS_Generator: 7.0614519119262695 LOSS_Discriminator: 0.07140028476715088\n",
            "ITERATION_NO.: 449 LOSS_Generator: 6.427575588226318 LOSS_Discriminator: 0.10124346613883972\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.2337751388549805 LOSS_Discriminator: 0.08525954931974411\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.742054462432861 LOSS_Discriminator: 0.05683611333370209\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.894381523132324 LOSS_Discriminator: 0.04664750397205353\n",
            "ITERATION_NO.: 453 LOSS_Generator: 7.867932319641113 LOSS_Discriminator: 0.05994802713394165\n",
            "ITERATION_NO.: 454 LOSS_Generator: 7.835050106048584 LOSS_Discriminator: 0.09219146519899368\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.956021308898926 LOSS_Discriminator: 0.06728801131248474\n",
            "ITERATION_NO.: 456 LOSS_Generator: 7.680150985717773 LOSS_Discriminator: 0.09257622063159943\n",
            "ITERATION_NO.: 457 LOSS_Generator: 7.557764053344727 LOSS_Discriminator: 0.09132363647222519\n",
            "ITERATION_NO.: 458 LOSS_Generator: 7.925064563751221 LOSS_Discriminator: 0.09095654636621475\n",
            "ITERATION_NO.: 459 LOSS_Generator: 8.094921112060547 LOSS_Discriminator: 0.04528326541185379\n",
            "ITERATION_NO.: 460 LOSS_Generator: 7.880013465881348 LOSS_Discriminator: 0.04009637236595154\n",
            "ITERATION_NO.: 461 LOSS_Generator: 7.228650093078613 LOSS_Discriminator: 0.06703326106071472\n",
            "ITERATION_NO.: 462 LOSS_Generator: 7.584841251373291 LOSS_Discriminator: 0.047034889459609985\n",
            "ITERATION_NO.: 463 LOSS_Generator: 7.640072822570801 LOSS_Discriminator: 0.10954004526138306\n",
            "ITERATION_NO.: 464 LOSS_Generator: 7.8155927658081055 LOSS_Discriminator: 0.10741899907588959\n",
            "ITERATION_NO.: 465 LOSS_Generator: 7.881250381469727 LOSS_Discriminator: 0.04108579456806183\n",
            "ITERATION_NO.: 466 LOSS_Generator: 7.645954132080078 LOSS_Discriminator: 0.03408515080809593\n",
            "ITERATION_NO.: 467 LOSS_Generator: 7.2276458740234375 LOSS_Discriminator: 0.07349520921707153\n",
            "ITERATION_NO.: 468 LOSS_Generator: 6.983546733856201 LOSS_Discriminator: 0.05117841809988022\n",
            "ITERATION_NO.: 469 LOSS_Generator: 7.164648532867432 LOSS_Discriminator: 0.0722360610961914\n",
            "ITERATION_NO.: 470 LOSS_Generator: 7.4500322341918945 LOSS_Discriminator: 0.04522000998258591\n",
            "ITERATION_NO.: 471 LOSS_Generator: 7.5897393226623535 LOSS_Discriminator: 0.0863497406244278\n",
            "ITERATION_NO.: 472 LOSS_Generator: 8.13720417022705 LOSS_Discriminator: 0.0337810218334198\n",
            "ITERATION_NO.: 473 LOSS_Generator: 8.206345558166504 LOSS_Discriminator: 0.07937946915626526\n",
            "ITERATION_NO.: 474 LOSS_Generator: 7.797306060791016 LOSS_Discriminator: 0.06978641450405121\n",
            "ITERATION_NO.: 475 LOSS_Generator: 6.493120193481445 LOSS_Discriminator: 0.11546158790588379\n",
            "ITERATION_NO.: 476 LOSS_Generator: 5.914997577667236 LOSS_Discriminator: 0.10176937282085419\n",
            "ITERATION_NO.: 477 LOSS_Generator: 6.462161064147949 LOSS_Discriminator: 0.1121235191822052\n",
            "ITERATION_NO.: 478 LOSS_Generator: 7.348140239715576 LOSS_Discriminator: 0.02116631343960762\n",
            "ITERATION_NO.: 479 LOSS_Generator: 8.270267486572266 LOSS_Discriminator: 0.038980573415756226\n",
            "ITERATION_NO.: 480 LOSS_Generator: 7.622026443481445 LOSS_Discriminator: 0.1093507781624794\n",
            "ITERATION_NO.: 481 LOSS_Generator: 7.404397010803223 LOSS_Discriminator: 0.02173050120472908\n",
            "ITERATION_NO.: 482 LOSS_Generator: 7.458424091339111 LOSS_Discriminator: 0.047958143055438995\n",
            "ITERATION_NO.: 483 LOSS_Generator: 7.053485870361328 LOSS_Discriminator: 0.05777481198310852\n",
            "ITERATION_NO.: 484 LOSS_Generator: 7.935042858123779 LOSS_Discriminator: 0.05177231505513191\n",
            "ITERATION_NO.: 485 LOSS_Generator: 8.014806747436523 LOSS_Discriminator: 0.021343600004911423\n",
            "ITERATION_NO.: 486 LOSS_Generator: 8.588122367858887 LOSS_Discriminator: 0.09244932234287262\n",
            "ITERATION_NO.: 487 LOSS_Generator: 8.143941879272461 LOSS_Discriminator: 0.08161585032939911\n",
            "ITERATION_NO.: 488 LOSS_Generator: 7.2166900634765625 LOSS_Discriminator: 0.044592007994651794\n",
            "ITERATION_NO.: 489 LOSS_Generator: 7.285327911376953 LOSS_Discriminator: 0.07030966877937317\n",
            "ITERATION_NO.: 490 LOSS_Generator: 6.534844398498535 LOSS_Discriminator: 0.09089791774749756\n",
            "ITERATION_NO.: 491 LOSS_Generator: 7.434113025665283 LOSS_Discriminator: 0.07026802748441696\n",
            "ITERATION_NO.: 492 LOSS_Generator: 7.526230335235596 LOSS_Discriminator: 0.06051667034626007\n",
            "ITERATION_NO.: 493 LOSS_Generator: 7.890130519866943 LOSS_Discriminator: 0.15132629871368408\n",
            "ITERATION_NO.: 494 LOSS_Generator: 7.61488151550293 LOSS_Discriminator: 0.06008903682231903\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.360713481903076 LOSS_Discriminator: 0.07678195834159851\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.126234531402588 LOSS_Discriminator: 0.0739922747015953\n",
            "ITERATION_NO.: 497 LOSS_Generator: 7.014789581298828 LOSS_Discriminator: 0.08062716573476791\n",
            "ITERATION_NO.: 498 LOSS_Generator: 9.080406188964844 LOSS_Discriminator: 0.045610588043928146\n",
            "ITERATION_NO.: 499 LOSS_Generator: 9.195862770080566 LOSS_Discriminator: 0.1314348727464676\n",
            "ITERATION_NO.: 500 LOSS_Generator: 8.901961326599121 LOSS_Discriminator: 0.07590421289205551\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.957444667816162 LOSS_Discriminator: 0.18186339735984802\n",
            "ITERATION_NO.: 502 LOSS_Generator: 6.381404876708984 LOSS_Discriminator: 0.11391356587409973\n",
            "ITERATION_NO.: 503 LOSS_Generator: 4.994946002960205 LOSS_Discriminator: 0.10504505038261414\n",
            "ITERATION_NO.: 504 LOSS_Generator: 6.039478302001953 LOSS_Discriminator: 0.11369660496711731\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.777194976806641 LOSS_Discriminator: 0.09826524555683136\n",
            "ITERATION_NO.: 506 LOSS_Generator: 8.527361869812012 LOSS_Discriminator: 0.05712401494383812\n",
            "ITERATION_NO.: 507 LOSS_Generator: 9.009775161743164 LOSS_Discriminator: 0.07038170099258423\n",
            "ITERATION_NO.: 508 LOSS_Generator: 9.164789199829102 LOSS_Discriminator: 0.046118415892124176\n",
            "ITERATION_NO.: 509 LOSS_Generator: 8.557315826416016 LOSS_Discriminator: 0.08233596384525299\n",
            "ITERATION_NO.: 510 LOSS_Generator: 8.119573593139648 LOSS_Discriminator: 0.08096092939376831\n",
            "ITERATION_NO.: 511 LOSS_Generator: 6.964019298553467 LOSS_Discriminator: 0.08908545970916748\n",
            "ITERATION_NO.: 512 LOSS_Generator: 6.1919331550598145 LOSS_Discriminator: 0.08269527554512024\n",
            "ITERATION_NO.: 513 LOSS_Generator: 7.0736165046691895 LOSS_Discriminator: 0.05406349152326584\n",
            "ITERATION_NO.: 514 LOSS_Generator: 7.9832634925842285 LOSS_Discriminator: 0.031123947352170944\n",
            "ITERATION_NO.: 515 LOSS_Generator: 7.998995304107666 LOSS_Discriminator: 0.10288751125335693\n",
            "ITERATION_NO.: 516 LOSS_Generator: 8.312698364257812 LOSS_Discriminator: 0.036798350512981415\n",
            "ITERATION_NO.: 517 LOSS_Generator: 8.764113426208496 LOSS_Discriminator: 0.06684815883636475\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.940584659576416 LOSS_Discriminator: 0.06785143911838531\n",
            "ITERATION_NO.: 519 LOSS_Generator: 7.121736526489258 LOSS_Discriminator: 0.04714589938521385\n",
            "ITERATION_NO.: 520 LOSS_Generator: 7.455608367919922 LOSS_Discriminator: 0.08909163624048233\n",
            "ITERATION_NO.: 521 LOSS_Generator: 7.729771137237549 LOSS_Discriminator: 0.057130105793476105\n",
            "ITERATION_NO.: 522 LOSS_Generator: 8.352456092834473 LOSS_Discriminator: 0.04224070906639099\n",
            "ITERATION_NO.: 523 LOSS_Generator: 8.605155944824219 LOSS_Discriminator: 0.04413936287164688\n",
            "ITERATION_NO.: 524 LOSS_Generator: 8.076937675476074 LOSS_Discriminator: 0.09750615060329437\n",
            "ITERATION_NO.: 525 LOSS_Generator: 6.843142509460449 LOSS_Discriminator: 0.10608907043933868\n",
            "ITERATION_NO.: 526 LOSS_Generator: 5.98924446105957 LOSS_Discriminator: 0.03595287352800369\n",
            "ITERATION_NO.: 527 LOSS_Generator: 6.026442050933838 LOSS_Discriminator: 0.11169074475765228\n",
            "ITERATION_NO.: 528 LOSS_Generator: 6.957478046417236 LOSS_Discriminator: 0.026221834123134613\n",
            "ITERATION_NO.: 529 LOSS_Generator: 7.304605484008789 LOSS_Discriminator: 0.02982495352625847\n",
            "ITERATION_NO.: 530 LOSS_Generator: 8.091839790344238 LOSS_Discriminator: 0.07318738847970963\n",
            "ITERATION_NO.: 531 LOSS_Generator: 8.698629379272461 LOSS_Discriminator: 0.044235315173864365\n",
            "ITERATION_NO.: 532 LOSS_Generator: 8.516672134399414 LOSS_Discriminator: 0.03770897537469864\n",
            "ITERATION_NO.: 533 LOSS_Generator: 8.188436508178711 LOSS_Discriminator: 0.03944418206810951\n",
            "ITERATION_NO.: 534 LOSS_Generator: 7.373621940612793 LOSS_Discriminator: 0.10493789613246918\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.14335823059082 LOSS_Discriminator: 0.05561763420701027\n",
            "ITERATION_NO.: 536 LOSS_Generator: 6.1640496253967285 LOSS_Discriminator: 0.06847461313009262\n",
            "ITERATION_NO.: 537 LOSS_Generator: 6.045472621917725 LOSS_Discriminator: 0.05410832166671753\n",
            "ITERATION_NO.: 538 LOSS_Generator: 6.28825569152832 LOSS_Discriminator: 0.09300480782985687\n",
            "ITERATION_NO.: 539 LOSS_Generator: 6.983241558074951 LOSS_Discriminator: 0.09262751787900925\n",
            "ITERATION_NO.: 540 LOSS_Generator: 7.392366409301758 LOSS_Discriminator: 0.0751098170876503\n",
            "ITERATION_NO.: 541 LOSS_Generator: 7.876615047454834 LOSS_Discriminator: 0.056056827306747437\n",
            "ITERATION_NO.: 542 LOSS_Generator: 7.199786186218262 LOSS_Discriminator: 0.1320507824420929\n",
            "ITERATION_NO.: 543 LOSS_Generator: 5.89893913269043 LOSS_Discriminator: 0.06799286603927612\n",
            "ITERATION_NO.: 544 LOSS_Generator: 6.83090877532959 LOSS_Discriminator: 0.05494050309062004\n",
            "ITERATION_NO.: 545 LOSS_Generator: 6.743229389190674 LOSS_Discriminator: 0.055892594158649445\n",
            "ITERATION_NO.: 546 LOSS_Generator: 6.84083366394043 LOSS_Discriminator: 0.1230953186750412\n",
            "ITERATION_NO.: 547 LOSS_Generator: 6.939586639404297 LOSS_Discriminator: 0.07789824903011322\n",
            "ITERATION_NO.: 548 LOSS_Generator: 7.270998477935791 LOSS_Discriminator: 0.09224257618188858\n",
            "ITERATION_NO.: 549 LOSS_Generator: 8.752768516540527 LOSS_Discriminator: 0.03952464088797569\n",
            "ITERATION_NO.: 550 LOSS_Generator: 8.38129997253418 LOSS_Discriminator: 0.06073752045631409\n",
            "ITERATION_NO.: 551 LOSS_Generator: 7.846895694732666 LOSS_Discriminator: 0.10823361575603485\n",
            "ITERATION_NO.: 552 LOSS_Generator: 7.100089073181152 LOSS_Discriminator: 0.1201334223151207\n",
            "ITERATION_NO.: 553 LOSS_Generator: 6.5638298988342285 LOSS_Discriminator: 0.04445764422416687\n",
            "ITERATION_NO.: 554 LOSS_Generator: 5.6807942390441895 LOSS_Discriminator: 0.08279964327812195\n",
            "ITERATION_NO.: 555 LOSS_Generator: 6.4475202560424805 LOSS_Discriminator: 0.07665523141622543\n",
            "ITERATION_NO.: 556 LOSS_Generator: 8.00990104675293 LOSS_Discriminator: 0.03749648481607437\n",
            "ITERATION_NO.: 557 LOSS_Generator: 8.402022361755371 LOSS_Discriminator: 0.04632261395454407\n",
            "ITERATION_NO.: 558 LOSS_Generator: 8.1831693649292 LOSS_Discriminator: 0.09902217984199524\n",
            "ITERATION_NO.: 559 LOSS_Generator: 7.418057918548584 LOSS_Discriminator: 0.06627778708934784\n",
            "ITERATION_NO.: 560 LOSS_Generator: 7.560123443603516 LOSS_Discriminator: 0.07383061945438385\n",
            "ITERATION_NO.: 561 LOSS_Generator: 7.063084602355957 LOSS_Discriminator: 0.08081367611885071\n",
            "ITERATION_NO.: 562 LOSS_Generator: 6.168936252593994 LOSS_Discriminator: 0.07335096597671509\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.529766082763672 LOSS_Discriminator: 0.06472931802272797\n",
            "ITERATION_NO.: 564 LOSS_Generator: 7.075300693511963 LOSS_Discriminator: 0.0626530572772026\n",
            "ITERATION_NO.: 565 LOSS_Generator: 7.450319766998291 LOSS_Discriminator: 0.06857451051473618\n",
            "ITERATION_NO.: 566 LOSS_Generator: 7.861842632293701 LOSS_Discriminator: 0.17048177123069763\n",
            "ITERATION_NO.: 567 LOSS_Generator: 8.122901916503906 LOSS_Discriminator: 0.08779069781303406\n",
            "ITERATION_NO.: 568 LOSS_Generator: 7.616912364959717 LOSS_Discriminator: 0.06420595198869705\n",
            "ITERATION_NO.: 569 LOSS_Generator: 6.216692924499512 LOSS_Discriminator: 0.048095207661390305\n",
            "ITERATION_NO.: 570 LOSS_Generator: 5.714749336242676 LOSS_Discriminator: 0.08550436794757843\n",
            "ITERATION_NO.: 571 LOSS_Generator: 6.436933517456055 LOSS_Discriminator: 0.10594071447849274\n",
            "ITERATION_NO.: 572 LOSS_Generator: 7.674349308013916 LOSS_Discriminator: 0.06012856215238571\n",
            "ITERATION_NO.: 573 LOSS_Generator: 7.782515048980713 LOSS_Discriminator: 0.08587856590747833\n",
            "ITERATION_NO.: 574 LOSS_Generator: 7.399763107299805 LOSS_Discriminator: 0.10038585960865021\n",
            "ITERATION_NO.: 575 LOSS_Generator: 7.1277594566345215 LOSS_Discriminator: 0.03818489611148834\n",
            "ITERATION_NO.: 576 LOSS_Generator: 7.006953239440918 LOSS_Discriminator: 0.06462567299604416\n",
            "ITERATION_NO.: 577 LOSS_Generator: 6.559183120727539 LOSS_Discriminator: 0.06652902066707611\n",
            "ITERATION_NO.: 578 LOSS_Generator: 6.849664211273193 LOSS_Discriminator: 0.09955648332834244\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.4715423583984375 LOSS_Discriminator: 0.046529509127140045\n",
            "ITERATION_NO.: 580 LOSS_Generator: 8.476234436035156 LOSS_Discriminator: 0.06821279972791672\n",
            "ITERATION_NO.: 581 LOSS_Generator: 8.499983787536621 LOSS_Discriminator: 0.06329865753650665\n",
            "ITERATION_NO.: 582 LOSS_Generator: 8.091796875 LOSS_Discriminator: 0.06749306619167328\n",
            "ITERATION_NO.: 583 LOSS_Generator: 7.791331768035889 LOSS_Discriminator: 0.07189077138900757\n",
            "ITERATION_NO.: 584 LOSS_Generator: 6.836045742034912 LOSS_Discriminator: 0.07661785185337067\n",
            "ITERATION_NO.: 585 LOSS_Generator: 6.270053863525391 LOSS_Discriminator: 0.04180515930056572\n",
            "ITERATION_NO.: 586 LOSS_Generator: 6.587339401245117 LOSS_Discriminator: 0.13410790264606476\n",
            "ITERATION_NO.: 587 LOSS_Generator: 7.591912746429443 LOSS_Discriminator: 0.039423685520887375\n",
            "ITERATION_NO.: 588 LOSS_Generator: 8.515734672546387 LOSS_Discriminator: 0.07800929993391037\n",
            "ITERATION_NO.: 589 LOSS_Generator: 8.581365585327148 LOSS_Discriminator: 0.12529504299163818\n",
            "ITERATION_NO.: 590 LOSS_Generator: 7.734011173248291 LOSS_Discriminator: 0.12361738085746765\n",
            "ITERATION_NO.: 591 LOSS_Generator: 6.68775749206543 LOSS_Discriminator: 0.04801085218787193\n",
            "ITERATION_NO.: 592 LOSS_Generator: 6.7327728271484375 LOSS_Discriminator: 0.07642283290624619\n",
            "ITERATION_NO.: 593 LOSS_Generator: 6.538400173187256 LOSS_Discriminator: 0.051696471869945526\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.083609580993652 LOSS_Discriminator: 0.06181563064455986\n",
            "ITERATION_NO.: 595 LOSS_Generator: 8.002411842346191 LOSS_Discriminator: 0.035052340477705\n",
            "ITERATION_NO.: 596 LOSS_Generator: 8.054800033569336 LOSS_Discriminator: 0.05656632035970688\n",
            "ITERATION_NO.: 597 LOSS_Generator: 7.963526725769043 LOSS_Discriminator: 0.11439073830842972\n",
            "ITERATION_NO.: 598 LOSS_Generator: 7.222862720489502 LOSS_Discriminator: 0.05621166527271271\n",
            "ITERATION_NO.: 599 LOSS_Generator: 6.782867908477783 LOSS_Discriminator: 0.053208015859127045\n",
            "ITERATION_NO.: 600 LOSS_Generator: 6.750383377075195 LOSS_Discriminator: 0.05070328712463379\n",
            "EPOCH OVER: 48\n",
            "ITERATION_NO.: 1 LOSS_Generator: 6.9611334800720215 LOSS_Discriminator: 0.06981609761714935\n",
            "ITERATION_NO.: 2 LOSS_Generator: 6.863163471221924 LOSS_Discriminator: 0.08310067653656006\n",
            "ITERATION_NO.: 3 LOSS_Generator: 7.254824161529541 LOSS_Discriminator: 0.10113078355789185\n",
            "ITERATION_NO.: 4 LOSS_Generator: 7.446768283843994 LOSS_Discriminator: 0.054598644375801086\n",
            "ITERATION_NO.: 5 LOSS_Generator: 7.548861503601074 LOSS_Discriminator: 0.08302000164985657\n",
            "ITERATION_NO.: 6 LOSS_Generator: 7.496399402618408 LOSS_Discriminator: 0.06206860393285751\n",
            "ITERATION_NO.: 7 LOSS_Generator: 7.381148815155029 LOSS_Discriminator: 0.06284542381763458\n",
            "ITERATION_NO.: 8 LOSS_Generator: 7.490717887878418 LOSS_Discriminator: 0.03953215479850769\n",
            "ITERATION_NO.: 9 LOSS_Generator: 7.120401382446289 LOSS_Discriminator: 0.017687618732452393\n",
            "ITERATION_NO.: 10 LOSS_Generator: 7.035956382751465 LOSS_Discriminator: 0.06345164775848389\n",
            "ITERATION_NO.: 11 LOSS_Generator: 7.505802631378174 LOSS_Discriminator: 0.047747962176799774\n",
            "ITERATION_NO.: 12 LOSS_Generator: 7.0944108963012695 LOSS_Discriminator: 0.08693350851535797\n",
            "ITERATION_NO.: 13 LOSS_Generator: 6.919170379638672 LOSS_Discriminator: 0.07661819458007812\n",
            "ITERATION_NO.: 14 LOSS_Generator: 6.973637104034424 LOSS_Discriminator: 0.0379064716398716\n",
            "ITERATION_NO.: 15 LOSS_Generator: 6.818406581878662 LOSS_Discriminator: 0.04384031891822815\n",
            "ITERATION_NO.: 16 LOSS_Generator: 8.114721298217773 LOSS_Discriminator: 0.06746806204319\n",
            "ITERATION_NO.: 17 LOSS_Generator: 7.705604076385498 LOSS_Discriminator: 0.04463274031877518\n",
            "ITERATION_NO.: 18 LOSS_Generator: 7.346956729888916 LOSS_Discriminator: 0.09387080371379852\n",
            "ITERATION_NO.: 19 LOSS_Generator: 6.928472518920898 LOSS_Discriminator: 0.02970978431403637\n",
            "ITERATION_NO.: 20 LOSS_Generator: 6.953441143035889 LOSS_Discriminator: 0.01816645637154579\n",
            "ITERATION_NO.: 21 LOSS_Generator: 6.918336868286133 LOSS_Discriminator: 0.03287689387798309\n",
            "ITERATION_NO.: 22 LOSS_Generator: 7.330941200256348 LOSS_Discriminator: 0.05348415672779083\n",
            "ITERATION_NO.: 23 LOSS_Generator: 7.00068473815918 LOSS_Discriminator: 0.13067534565925598\n",
            "ITERATION_NO.: 24 LOSS_Generator: 6.967416763305664 LOSS_Discriminator: 0.03681746870279312\n",
            "ITERATION_NO.: 25 LOSS_Generator: 7.045820713043213 LOSS_Discriminator: 0.04351818934082985\n",
            "ITERATION_NO.: 26 LOSS_Generator: 7.214828968048096 LOSS_Discriminator: 0.09769866615533829\n",
            "ITERATION_NO.: 27 LOSS_Generator: 7.5352020263671875 LOSS_Discriminator: 0.05217756703495979\n",
            "ITERATION_NO.: 28 LOSS_Generator: 7.015171527862549 LOSS_Discriminator: 0.0893186628818512\n",
            "ITERATION_NO.: 29 LOSS_Generator: 6.971163749694824 LOSS_Discriminator: 0.09640645980834961\n",
            "ITERATION_NO.: 30 LOSS_Generator: 6.274677753448486 LOSS_Discriminator: 0.06466131657361984\n",
            "ITERATION_NO.: 31 LOSS_Generator: 6.395932197570801 LOSS_Discriminator: 0.07891571521759033\n",
            "ITERATION_NO.: 32 LOSS_Generator: 6.798409461975098 LOSS_Discriminator: 0.0517684631049633\n",
            "ITERATION_NO.: 33 LOSS_Generator: 7.730412006378174 LOSS_Discriminator: 0.09159643948078156\n",
            "ITERATION_NO.: 34 LOSS_Generator: 8.004312515258789 LOSS_Discriminator: 0.0473773255944252\n",
            "ITERATION_NO.: 35 LOSS_Generator: 7.626628398895264 LOSS_Discriminator: 0.0689496099948883\n",
            "ITERATION_NO.: 36 LOSS_Generator: 7.615694522857666 LOSS_Discriminator: 0.03343543782830238\n",
            "ITERATION_NO.: 37 LOSS_Generator: 6.576956748962402 LOSS_Discriminator: 0.02563527598977089\n",
            "ITERATION_NO.: 38 LOSS_Generator: 6.895500659942627 LOSS_Discriminator: 0.03172492980957031\n",
            "ITERATION_NO.: 39 LOSS_Generator: 7.001850128173828 LOSS_Discriminator: 0.05296248570084572\n",
            "ITERATION_NO.: 40 LOSS_Generator: 7.852354526519775 LOSS_Discriminator: 0.03475106507539749\n",
            "ITERATION_NO.: 41 LOSS_Generator: 7.824805736541748 LOSS_Discriminator: 0.044354479759931564\n",
            "ITERATION_NO.: 42 LOSS_Generator: 7.3902764320373535 LOSS_Discriminator: 0.08776231110095978\n",
            "ITERATION_NO.: 43 LOSS_Generator: 6.902583122253418 LOSS_Discriminator: 0.04269283264875412\n",
            "ITERATION_NO.: 44 LOSS_Generator: 6.6658477783203125 LOSS_Discriminator: 0.08440014719963074\n",
            "ITERATION_NO.: 45 LOSS_Generator: 6.220731258392334 LOSS_Discriminator: 0.06300555169582367\n",
            "ITERATION_NO.: 46 LOSS_Generator: 7.343807220458984 LOSS_Discriminator: 0.0871807336807251\n",
            "ITERATION_NO.: 47 LOSS_Generator: 7.742570877075195 LOSS_Discriminator: 0.054076820611953735\n",
            "ITERATION_NO.: 48 LOSS_Generator: 7.677446365356445 LOSS_Discriminator: 0.0735630914568901\n",
            "ITERATION_NO.: 49 LOSS_Generator: 7.6978888511657715 LOSS_Discriminator: 0.09852340072393417\n",
            "ITERATION_NO.: 50 LOSS_Generator: 7.0759358406066895 LOSS_Discriminator: 0.1356346607208252\n",
            "ITERATION_NO.: 51 LOSS_Generator: 6.533243179321289 LOSS_Discriminator: 0.1542556881904602\n",
            "ITERATION_NO.: 52 LOSS_Generator: 6.5383620262146 LOSS_Discriminator: 0.08355532586574554\n",
            "ITERATION_NO.: 53 LOSS_Generator: 7.420207977294922 LOSS_Discriminator: 0.06174424663186073\n",
            "ITERATION_NO.: 54 LOSS_Generator: 7.92600154876709 LOSS_Discriminator: 0.08991079032421112\n",
            "ITERATION_NO.: 55 LOSS_Generator: 7.168097972869873 LOSS_Discriminator: 0.0561506450176239\n",
            "ITERATION_NO.: 56 LOSS_Generator: 7.813549995422363 LOSS_Discriminator: 0.03310511261224747\n",
            "ITERATION_NO.: 57 LOSS_Generator: 7.073904991149902 LOSS_Discriminator: 0.0449858158826828\n",
            "ITERATION_NO.: 58 LOSS_Generator: 6.868715763092041 LOSS_Discriminator: 0.16435225307941437\n",
            "ITERATION_NO.: 59 LOSS_Generator: 6.706981658935547 LOSS_Discriminator: 0.029593225568532944\n",
            "ITERATION_NO.: 60 LOSS_Generator: 6.6327738761901855 LOSS_Discriminator: 0.038951124995946884\n",
            "ITERATION_NO.: 61 LOSS_Generator: 6.375514984130859 LOSS_Discriminator: 0.0911039337515831\n",
            "ITERATION_NO.: 62 LOSS_Generator: 7.5815205574035645 LOSS_Discriminator: 0.11206799745559692\n",
            "ITERATION_NO.: 63 LOSS_Generator: 8.110113143920898 LOSS_Discriminator: 0.06580017507076263\n",
            "ITERATION_NO.: 64 LOSS_Generator: 8.47127628326416 LOSS_Discriminator: 0.01288744155317545\n",
            "ITERATION_NO.: 65 LOSS_Generator: 9.0911865234375 LOSS_Discriminator: 0.054062895476818085\n",
            "ITERATION_NO.: 66 LOSS_Generator: 8.537582397460938 LOSS_Discriminator: 0.12096166610717773\n",
            "ITERATION_NO.: 67 LOSS_Generator: 7.693118095397949 LOSS_Discriminator: 0.08443272113800049\n",
            "ITERATION_NO.: 68 LOSS_Generator: 6.773938179016113 LOSS_Discriminator: 0.061043865978717804\n",
            "ITERATION_NO.: 69 LOSS_Generator: 6.522822856903076 LOSS_Discriminator: 0.09005744010210037\n",
            "ITERATION_NO.: 70 LOSS_Generator: 6.235234260559082 LOSS_Discriminator: 0.08016365021467209\n",
            "ITERATION_NO.: 71 LOSS_Generator: 5.724843978881836 LOSS_Discriminator: 0.07564815878868103\n",
            "ITERATION_NO.: 72 LOSS_Generator: 6.659860134124756 LOSS_Discriminator: 0.1330900490283966\n",
            "ITERATION_NO.: 73 LOSS_Generator: 7.1298370361328125 LOSS_Discriminator: 0.09521670639514923\n",
            "ITERATION_NO.: 74 LOSS_Generator: 6.818549156188965 LOSS_Discriminator: 0.059325188398361206\n",
            "ITERATION_NO.: 75 LOSS_Generator: 7.52081298828125 LOSS_Discriminator: 0.07401655614376068\n",
            "ITERATION_NO.: 76 LOSS_Generator: 7.715625762939453 LOSS_Discriminator: 0.05668947100639343\n",
            "ITERATION_NO.: 77 LOSS_Generator: 7.446219444274902 LOSS_Discriminator: 0.09852741658687592\n",
            "ITERATION_NO.: 78 LOSS_Generator: 6.864850044250488 LOSS_Discriminator: 0.0444159209728241\n",
            "ITERATION_NO.: 79 LOSS_Generator: 7.320754528045654 LOSS_Discriminator: 0.06985434889793396\n",
            "ITERATION_NO.: 80 LOSS_Generator: 7.2604851722717285 LOSS_Discriminator: 0.031703099608421326\n",
            "ITERATION_NO.: 81 LOSS_Generator: 7.0505781173706055 LOSS_Discriminator: 0.10915111005306244\n",
            "ITERATION_NO.: 82 LOSS_Generator: 6.959041595458984 LOSS_Discriminator: 0.03632551059126854\n",
            "ITERATION_NO.: 83 LOSS_Generator: 7.133608341217041 LOSS_Discriminator: 0.0659322440624237\n",
            "ITERATION_NO.: 84 LOSS_Generator: 6.455086708068848 LOSS_Discriminator: 0.07459452748298645\n",
            "ITERATION_NO.: 85 LOSS_Generator: 6.704677581787109 LOSS_Discriminator: 0.06995839625597\n",
            "ITERATION_NO.: 86 LOSS_Generator: 7.169078350067139 LOSS_Discriminator: 0.08295869827270508\n",
            "ITERATION_NO.: 87 LOSS_Generator: 7.557366847991943 LOSS_Discriminator: 0.02878827601671219\n",
            "ITERATION_NO.: 88 LOSS_Generator: 7.2020344734191895 LOSS_Discriminator: 0.05600493773818016\n",
            "ITERATION_NO.: 89 LOSS_Generator: 7.395915985107422 LOSS_Discriminator: 0.020681215450167656\n",
            "ITERATION_NO.: 90 LOSS_Generator: 7.148383140563965 LOSS_Discriminator: 0.05200953036546707\n",
            "ITERATION_NO.: 91 LOSS_Generator: 6.971802234649658 LOSS_Discriminator: 0.05706621706485748\n",
            "ITERATION_NO.: 92 LOSS_Generator: 7.267263889312744 LOSS_Discriminator: 0.030845986679196358\n",
            "ITERATION_NO.: 93 LOSS_Generator: 6.723495960235596 LOSS_Discriminator: 0.08326087146997452\n",
            "ITERATION_NO.: 94 LOSS_Generator: 6.405972480773926 LOSS_Discriminator: 0.05247972160577774\n",
            "ITERATION_NO.: 95 LOSS_Generator: 6.455639839172363 LOSS_Discriminator: 0.04612736403942108\n",
            "ITERATION_NO.: 96 LOSS_Generator: 7.126797676086426 LOSS_Discriminator: 0.052423518151044846\n",
            "ITERATION_NO.: 97 LOSS_Generator: 8.213008880615234 LOSS_Discriminator: 0.08039556443691254\n",
            "ITERATION_NO.: 98 LOSS_Generator: 7.902365684509277 LOSS_Discriminator: 0.07294086366891861\n",
            "ITERATION_NO.: 99 LOSS_Generator: 7.609053134918213 LOSS_Discriminator: 0.033478543162345886\n",
            "ITERATION_NO.: 100 LOSS_Generator: 7.299739837646484 LOSS_Discriminator: 0.1472638100385666\n",
            "ITERATION_NO.: 101 LOSS_Generator: 6.1966962814331055 LOSS_Discriminator: 0.10165457427501678\n",
            "ITERATION_NO.: 102 LOSS_Generator: 5.88414192199707 LOSS_Discriminator: 0.1124514788389206\n",
            "ITERATION_NO.: 103 LOSS_Generator: 6.522070407867432 LOSS_Discriminator: 0.12319162487983704\n",
            "ITERATION_NO.: 104 LOSS_Generator: 7.3932037353515625 LOSS_Discriminator: 0.050151675939559937\n",
            "ITERATION_NO.: 105 LOSS_Generator: 7.748655796051025 LOSS_Discriminator: 0.09429793804883957\n",
            "ITERATION_NO.: 106 LOSS_Generator: 7.882226943969727 LOSS_Discriminator: 0.06329566240310669\n",
            "ITERATION_NO.: 107 LOSS_Generator: 7.847131729125977 LOSS_Discriminator: 0.04206152260303497\n",
            "ITERATION_NO.: 108 LOSS_Generator: 7.2722978591918945 LOSS_Discriminator: 0.0766008049249649\n",
            "ITERATION_NO.: 109 LOSS_Generator: 6.127589225769043 LOSS_Discriminator: 0.0688994824886322\n",
            "ITERATION_NO.: 110 LOSS_Generator: 5.804363250732422 LOSS_Discriminator: 0.06060038506984711\n",
            "ITERATION_NO.: 111 LOSS_Generator: 6.474849700927734 LOSS_Discriminator: 0.08452407270669937\n",
            "ITERATION_NO.: 112 LOSS_Generator: 7.315622329711914 LOSS_Discriminator: 0.056180208921432495\n",
            "ITERATION_NO.: 113 LOSS_Generator: 8.398307800292969 LOSS_Discriminator: 0.052035290747880936\n",
            "ITERATION_NO.: 114 LOSS_Generator: 7.803071975708008 LOSS_Discriminator: 0.19402772188186646\n",
            "ITERATION_NO.: 115 LOSS_Generator: 6.805208683013916 LOSS_Discriminator: 0.10482999682426453\n",
            "ITERATION_NO.: 116 LOSS_Generator: 7.183131694793701 LOSS_Discriminator: 0.05150555819272995\n",
            "ITERATION_NO.: 117 LOSS_Generator: 6.3660888671875 LOSS_Discriminator: 0.05688824504613876\n",
            "ITERATION_NO.: 118 LOSS_Generator: 6.276899814605713 LOSS_Discriminator: 0.05332944914698601\n",
            "ITERATION_NO.: 119 LOSS_Generator: 7.0960540771484375 LOSS_Discriminator: 0.0979955792427063\n",
            "ITERATION_NO.: 120 LOSS_Generator: 7.483675479888916 LOSS_Discriminator: 0.04217710718512535\n",
            "ITERATION_NO.: 121 LOSS_Generator: 7.716009140014648 LOSS_Discriminator: 0.07141107320785522\n",
            "ITERATION_NO.: 122 LOSS_Generator: 7.324872016906738 LOSS_Discriminator: 0.0798909142613411\n",
            "ITERATION_NO.: 123 LOSS_Generator: 6.66806173324585 LOSS_Discriminator: 0.08582372963428497\n",
            "ITERATION_NO.: 124 LOSS_Generator: 7.088006496429443 LOSS_Discriminator: 0.07791664451360703\n",
            "ITERATION_NO.: 125 LOSS_Generator: 6.833277702331543 LOSS_Discriminator: 0.06360790133476257\n",
            "ITERATION_NO.: 126 LOSS_Generator: 7.189557075500488 LOSS_Discriminator: 0.03796444088220596\n",
            "ITERATION_NO.: 127 LOSS_Generator: 6.993650436401367 LOSS_Discriminator: 0.058592163026332855\n",
            "ITERATION_NO.: 128 LOSS_Generator: 7.521347522735596 LOSS_Discriminator: 0.09375854581594467\n",
            "ITERATION_NO.: 129 LOSS_Generator: 7.219532012939453 LOSS_Discriminator: 0.05345386639237404\n",
            "ITERATION_NO.: 130 LOSS_Generator: 6.575735092163086 LOSS_Discriminator: 0.08996269106864929\n",
            "ITERATION_NO.: 131 LOSS_Generator: 6.935606002807617 LOSS_Discriminator: 0.093511201441288\n",
            "ITERATION_NO.: 132 LOSS_Generator: 6.30649471282959 LOSS_Discriminator: 0.10510776937007904\n",
            "ITERATION_NO.: 133 LOSS_Generator: 6.254121780395508 LOSS_Discriminator: 0.06268837302923203\n",
            "ITERATION_NO.: 134 LOSS_Generator: 6.453647613525391 LOSS_Discriminator: 0.05150071904063225\n",
            "ITERATION_NO.: 135 LOSS_Generator: 7.028470039367676 LOSS_Discriminator: 0.06196112185716629\n",
            "ITERATION_NO.: 136 LOSS_Generator: 7.639837265014648 LOSS_Discriminator: 0.03919382393360138\n",
            "ITERATION_NO.: 137 LOSS_Generator: 7.366352081298828 LOSS_Discriminator: 0.07584814727306366\n",
            "ITERATION_NO.: 138 LOSS_Generator: 6.94525146484375 LOSS_Discriminator: 0.050745390355587006\n",
            "ITERATION_NO.: 139 LOSS_Generator: 6.459589958190918 LOSS_Discriminator: 0.03608066216111183\n",
            "ITERATION_NO.: 140 LOSS_Generator: 6.3502912521362305 LOSS_Discriminator: 0.07132019102573395\n",
            "ITERATION_NO.: 141 LOSS_Generator: 6.1385884284973145 LOSS_Discriminator: 0.09612855315208435\n",
            "ITERATION_NO.: 142 LOSS_Generator: 6.231055736541748 LOSS_Discriminator: 0.07307139039039612\n",
            "ITERATION_NO.: 143 LOSS_Generator: 6.229045391082764 LOSS_Discriminator: 0.039003193378448486\n",
            "ITERATION_NO.: 144 LOSS_Generator: 7.27388858795166 LOSS_Discriminator: 0.049308814108371735\n",
            "ITERATION_NO.: 145 LOSS_Generator: 8.187169075012207 LOSS_Discriminator: 0.10636425018310547\n",
            "ITERATION_NO.: 146 LOSS_Generator: 8.326590538024902 LOSS_Discriminator: 0.046776775270700455\n",
            "ITERATION_NO.: 147 LOSS_Generator: 7.7691121101379395 LOSS_Discriminator: 0.08459950238466263\n",
            "ITERATION_NO.: 148 LOSS_Generator: 7.409769058227539 LOSS_Discriminator: 0.08950400352478027\n",
            "ITERATION_NO.: 149 LOSS_Generator: 6.063665390014648 LOSS_Discriminator: 0.0881945937871933\n",
            "ITERATION_NO.: 150 LOSS_Generator: 5.418256759643555 LOSS_Discriminator: 0.15421240031719208\n",
            "ITERATION_NO.: 151 LOSS_Generator: 5.611881256103516 LOSS_Discriminator: 0.10313351452350616\n",
            "ITERATION_NO.: 152 LOSS_Generator: 7.353866100311279 LOSS_Discriminator: 0.12892097234725952\n",
            "ITERATION_NO.: 153 LOSS_Generator: 8.85313892364502 LOSS_Discriminator: 0.08631642162799835\n",
            "ITERATION_NO.: 154 LOSS_Generator: 9.446645736694336 LOSS_Discriminator: 0.07885204255580902\n",
            "ITERATION_NO.: 155 LOSS_Generator: 9.038475036621094 LOSS_Discriminator: 0.12349510192871094\n",
            "ITERATION_NO.: 156 LOSS_Generator: 8.10462760925293 LOSS_Discriminator: 0.09550781548023224\n",
            "ITERATION_NO.: 157 LOSS_Generator: 6.0739054679870605 LOSS_Discriminator: 0.08540818095207214\n",
            "ITERATION_NO.: 158 LOSS_Generator: 5.3112969398498535 LOSS_Discriminator: 0.05230773985385895\n",
            "ITERATION_NO.: 159 LOSS_Generator: 5.95889139175415 LOSS_Discriminator: 0.11807147413492203\n",
            "ITERATION_NO.: 160 LOSS_Generator: 7.608461856842041 LOSS_Discriminator: 0.04349595308303833\n",
            "ITERATION_NO.: 161 LOSS_Generator: 8.557721138000488 LOSS_Discriminator: 0.022280005738139153\n",
            "ITERATION_NO.: 162 LOSS_Generator: 9.489734649658203 LOSS_Discriminator: 0.09492719173431396\n",
            "ITERATION_NO.: 163 LOSS_Generator: 9.555068016052246 LOSS_Discriminator: 0.1063646525144577\n",
            "ITERATION_NO.: 164 LOSS_Generator: 9.18797492980957 LOSS_Discriminator: 0.06590066850185394\n",
            "ITERATION_NO.: 165 LOSS_Generator: 8.524432182312012 LOSS_Discriminator: 0.06997473537921906\n",
            "ITERATION_NO.: 166 LOSS_Generator: 7.428366184234619 LOSS_Discriminator: 0.05529492348432541\n",
            "ITERATION_NO.: 167 LOSS_Generator: 6.675828456878662 LOSS_Discriminator: 0.11484576016664505\n",
            "ITERATION_NO.: 168 LOSS_Generator: 5.287737846374512 LOSS_Discriminator: 0.07184739410877228\n",
            "ITERATION_NO.: 169 LOSS_Generator: 6.786190032958984 LOSS_Discriminator: 0.1520572155714035\n",
            "ITERATION_NO.: 170 LOSS_Generator: 7.118623733520508 LOSS_Discriminator: 0.06843706965446472\n",
            "ITERATION_NO.: 171 LOSS_Generator: 7.740654945373535 LOSS_Discriminator: 0.06468965113162994\n",
            "ITERATION_NO.: 172 LOSS_Generator: 8.652544975280762 LOSS_Discriminator: 0.031231865286827087\n",
            "ITERATION_NO.: 173 LOSS_Generator: 8.1389799118042 LOSS_Discriminator: 0.08134090900421143\n",
            "ITERATION_NO.: 174 LOSS_Generator: 8.051549911499023 LOSS_Discriminator: 0.04451587051153183\n",
            "ITERATION_NO.: 175 LOSS_Generator: 7.041656970977783 LOSS_Discriminator: 0.05788218975067139\n",
            "ITERATION_NO.: 176 LOSS_Generator: 6.72960090637207 LOSS_Discriminator: 0.07027928531169891\n",
            "ITERATION_NO.: 177 LOSS_Generator: 6.3433990478515625 LOSS_Discriminator: 0.038996901363134384\n",
            "ITERATION_NO.: 178 LOSS_Generator: 6.916087627410889 LOSS_Discriminator: 0.07467439025640488\n",
            "ITERATION_NO.: 179 LOSS_Generator: 7.601078510284424 LOSS_Discriminator: 0.04118504375219345\n",
            "ITERATION_NO.: 180 LOSS_Generator: 8.452342987060547 LOSS_Discriminator: 0.07746963202953339\n",
            "ITERATION_NO.: 181 LOSS_Generator: 8.407349586486816 LOSS_Discriminator: 0.16045048832893372\n",
            "ITERATION_NO.: 182 LOSS_Generator: 7.612691879272461 LOSS_Discriminator: 0.025708375498652458\n",
            "ITERATION_NO.: 183 LOSS_Generator: 7.046307563781738 LOSS_Discriminator: 0.03952854126691818\n",
            "ITERATION_NO.: 184 LOSS_Generator: 6.644814968109131 LOSS_Discriminator: 0.05547066032886505\n",
            "ITERATION_NO.: 185 LOSS_Generator: 7.117912769317627 LOSS_Discriminator: 0.052541233599185944\n",
            "ITERATION_NO.: 186 LOSS_Generator: 6.858222007751465 LOSS_Discriminator: 0.053588248789310455\n",
            "ITERATION_NO.: 187 LOSS_Generator: 6.064040660858154 LOSS_Discriminator: 0.08376892656087875\n",
            "ITERATION_NO.: 188 LOSS_Generator: 6.413402080535889 LOSS_Discriminator: 0.07187481224536896\n",
            "ITERATION_NO.: 189 LOSS_Generator: 6.857308387756348 LOSS_Discriminator: 0.030084621161222458\n",
            "ITERATION_NO.: 190 LOSS_Generator: 7.465242385864258 LOSS_Discriminator: 0.03192754089832306\n",
            "ITERATION_NO.: 191 LOSS_Generator: 7.9399566650390625 LOSS_Discriminator: 0.03728732466697693\n",
            "ITERATION_NO.: 192 LOSS_Generator: 7.995387077331543 LOSS_Discriminator: 0.07069149613380432\n",
            "ITERATION_NO.: 193 LOSS_Generator: 7.761843204498291 LOSS_Discriminator: 0.037248097360134125\n",
            "ITERATION_NO.: 194 LOSS_Generator: 7.4722723960876465 LOSS_Discriminator: 0.04520051181316376\n",
            "ITERATION_NO.: 195 LOSS_Generator: 6.899914741516113 LOSS_Discriminator: 0.08123702555894852\n",
            "ITERATION_NO.: 196 LOSS_Generator: 6.156962871551514 LOSS_Discriminator: 0.10808169096708298\n",
            "ITERATION_NO.: 197 LOSS_Generator: 6.6301116943359375 LOSS_Discriminator: 0.04142654687166214\n",
            "ITERATION_NO.: 198 LOSS_Generator: 6.76377010345459 LOSS_Discriminator: 0.10245408862829208\n",
            "ITERATION_NO.: 199 LOSS_Generator: 7.1294264793396 LOSS_Discriminator: 0.036487095057964325\n",
            "ITERATION_NO.: 200 LOSS_Generator: 7.482736587524414 LOSS_Discriminator: 0.03064248338341713\n",
            "ITERATION_NO.: 201 LOSS_Generator: 7.884866714477539 LOSS_Discriminator: 0.05151363089680672\n",
            "ITERATION_NO.: 202 LOSS_Generator: 7.579527378082275 LOSS_Discriminator: 0.02438507229089737\n",
            "ITERATION_NO.: 203 LOSS_Generator: 7.663264751434326 LOSS_Discriminator: 0.06792238354682922\n",
            "ITERATION_NO.: 204 LOSS_Generator: 6.838735580444336 LOSS_Discriminator: 0.07786254584789276\n",
            "ITERATION_NO.: 205 LOSS_Generator: 6.201409339904785 LOSS_Discriminator: 0.03787069022655487\n",
            "ITERATION_NO.: 206 LOSS_Generator: 5.925455093383789 LOSS_Discriminator: 0.055883392691612244\n",
            "ITERATION_NO.: 207 LOSS_Generator: 6.5108418464660645 LOSS_Discriminator: 0.08298736065626144\n",
            "ITERATION_NO.: 208 LOSS_Generator: 7.276615619659424 LOSS_Discriminator: 0.07242891937494278\n",
            "ITERATION_NO.: 209 LOSS_Generator: 7.71221923828125 LOSS_Discriminator: 0.035374514758586884\n",
            "ITERATION_NO.: 210 LOSS_Generator: 9.014545440673828 LOSS_Discriminator: 0.03624558448791504\n",
            "ITERATION_NO.: 211 LOSS_Generator: 9.23688793182373 LOSS_Discriminator: 0.039500847458839417\n",
            "ITERATION_NO.: 212 LOSS_Generator: 8.148726463317871 LOSS_Discriminator: 0.19611495733261108\n",
            "ITERATION_NO.: 213 LOSS_Generator: 6.939919948577881 LOSS_Discriminator: 0.14211347699165344\n",
            "ITERATION_NO.: 214 LOSS_Generator: 4.985881805419922 LOSS_Discriminator: 0.12444652616977692\n",
            "ITERATION_NO.: 215 LOSS_Generator: 6.516585826873779 LOSS_Discriminator: 0.23012509942054749\n",
            "ITERATION_NO.: 216 LOSS_Generator: 7.959695339202881 LOSS_Discriminator: 0.051439229398965836\n",
            "ITERATION_NO.: 217 LOSS_Generator: 8.96362018585205 LOSS_Discriminator: 0.22968558967113495\n",
            "ITERATION_NO.: 218 LOSS_Generator: 9.315099716186523 LOSS_Discriminator: 0.12308339029550552\n",
            "ITERATION_NO.: 219 LOSS_Generator: 9.009714126586914 LOSS_Discriminator: 0.04779138043522835\n",
            "ITERATION_NO.: 220 LOSS_Generator: 8.360862731933594 LOSS_Discriminator: 0.09645700454711914\n",
            "ITERATION_NO.: 221 LOSS_Generator: 8.057353019714355 LOSS_Discriminator: 0.07275523990392685\n",
            "ITERATION_NO.: 222 LOSS_Generator: 6.995326519012451 LOSS_Discriminator: 0.056173160672187805\n",
            "ITERATION_NO.: 223 LOSS_Generator: 5.813581943511963 LOSS_Discriminator: 0.05829270929098129\n",
            "ITERATION_NO.: 224 LOSS_Generator: 6.463359832763672 LOSS_Discriminator: 0.19643892347812653\n",
            "ITERATION_NO.: 225 LOSS_Generator: 6.927778244018555 LOSS_Discriminator: 0.0943157970905304\n",
            "ITERATION_NO.: 226 LOSS_Generator: 8.157761573791504 LOSS_Discriminator: 0.03227516636252403\n",
            "ITERATION_NO.: 227 LOSS_Generator: 8.968740463256836 LOSS_Discriminator: 0.06532062590122223\n",
            "ITERATION_NO.: 228 LOSS_Generator: 9.46153736114502 LOSS_Discriminator: 0.08307010680437088\n",
            "ITERATION_NO.: 229 LOSS_Generator: 8.655190467834473 LOSS_Discriminator: 0.09297578036785126\n",
            "ITERATION_NO.: 230 LOSS_Generator: 7.512587070465088 LOSS_Discriminator: 0.1053093820810318\n",
            "ITERATION_NO.: 231 LOSS_Generator: 6.812302112579346 LOSS_Discriminator: 0.08073035627603531\n",
            "ITERATION_NO.: 232 LOSS_Generator: 6.03870964050293 LOSS_Discriminator: 0.11788201332092285\n",
            "ITERATION_NO.: 233 LOSS_Generator: 7.206366539001465 LOSS_Discriminator: 0.09209215641021729\n",
            "ITERATION_NO.: 234 LOSS_Generator: 7.624115467071533 LOSS_Discriminator: 0.1255008429288864\n",
            "ITERATION_NO.: 235 LOSS_Generator: 8.33578109741211 LOSS_Discriminator: 0.03877805173397064\n",
            "ITERATION_NO.: 236 LOSS_Generator: 7.730103969573975 LOSS_Discriminator: 0.1547260582447052\n",
            "ITERATION_NO.: 237 LOSS_Generator: 7.327484607696533 LOSS_Discriminator: 0.050861917436122894\n",
            "ITERATION_NO.: 238 LOSS_Generator: 6.594301223754883 LOSS_Discriminator: 0.04108702391386032\n",
            "ITERATION_NO.: 239 LOSS_Generator: 6.522745132446289 LOSS_Discriminator: 0.10629761219024658\n",
            "ITERATION_NO.: 240 LOSS_Generator: 6.619304656982422 LOSS_Discriminator: 0.08077451586723328\n",
            "ITERATION_NO.: 241 LOSS_Generator: 6.485042095184326 LOSS_Discriminator: 0.05364394932985306\n",
            "ITERATION_NO.: 242 LOSS_Generator: 6.988523006439209 LOSS_Discriminator: 0.0452384352684021\n",
            "ITERATION_NO.: 243 LOSS_Generator: 7.504146575927734 LOSS_Discriminator: 0.09202645719051361\n",
            "ITERATION_NO.: 244 LOSS_Generator: 7.254492282867432 LOSS_Discriminator: 0.06923681497573853\n",
            "ITERATION_NO.: 245 LOSS_Generator: 7.603682041168213 LOSS_Discriminator: 0.06221579387784004\n",
            "ITERATION_NO.: 246 LOSS_Generator: 6.9523844718933105 LOSS_Discriminator: 0.0819890946149826\n",
            "ITERATION_NO.: 247 LOSS_Generator: 5.923563003540039 LOSS_Discriminator: 0.09284060448408127\n",
            "ITERATION_NO.: 248 LOSS_Generator: 6.145583629608154 LOSS_Discriminator: 0.10029434412717819\n",
            "ITERATION_NO.: 249 LOSS_Generator: 6.162795543670654 LOSS_Discriminator: 0.03814305365085602\n",
            "ITERATION_NO.: 250 LOSS_Generator: 7.335636615753174 LOSS_Discriminator: 0.12000232189893723\n",
            "ITERATION_NO.: 251 LOSS_Generator: 7.832803249359131 LOSS_Discriminator: 0.029145212844014168\n",
            "ITERATION_NO.: 252 LOSS_Generator: 7.391721248626709 LOSS_Discriminator: 0.041625142097473145\n",
            "ITERATION_NO.: 253 LOSS_Generator: 8.176612854003906 LOSS_Discriminator: 0.031884610652923584\n",
            "ITERATION_NO.: 254 LOSS_Generator: 7.483156681060791 LOSS_Discriminator: 0.10211515426635742\n",
            "ITERATION_NO.: 255 LOSS_Generator: 6.720095157623291 LOSS_Discriminator: 0.0492594838142395\n",
            "ITERATION_NO.: 256 LOSS_Generator: 7.154283046722412 LOSS_Discriminator: 0.09916052967309952\n",
            "ITERATION_NO.: 257 LOSS_Generator: 7.698587417602539 LOSS_Discriminator: 0.026471996679902077\n",
            "ITERATION_NO.: 258 LOSS_Generator: 7.778748035430908 LOSS_Discriminator: 0.032559093087911606\n",
            "ITERATION_NO.: 259 LOSS_Generator: 8.401235580444336 LOSS_Discriminator: 0.04235391318798065\n",
            "ITERATION_NO.: 260 LOSS_Generator: 8.438438415527344 LOSS_Discriminator: 0.03843000531196594\n",
            "ITERATION_NO.: 261 LOSS_Generator: 8.062153816223145 LOSS_Discriminator: 0.11119948327541351\n",
            "ITERATION_NO.: 262 LOSS_Generator: 7.032589912414551 LOSS_Discriminator: 0.14089754223823547\n",
            "ITERATION_NO.: 263 LOSS_Generator: 6.369668960571289 LOSS_Discriminator: 0.0709528997540474\n",
            "ITERATION_NO.: 264 LOSS_Generator: 5.8585357666015625 LOSS_Discriminator: 0.11783406138420105\n",
            "ITERATION_NO.: 265 LOSS_Generator: 6.735013484954834 LOSS_Discriminator: 0.07954789698123932\n",
            "ITERATION_NO.: 266 LOSS_Generator: 7.545568466186523 LOSS_Discriminator: 0.09020662307739258\n",
            "ITERATION_NO.: 267 LOSS_Generator: 7.975024223327637 LOSS_Discriminator: 0.08580808341503143\n",
            "ITERATION_NO.: 268 LOSS_Generator: 7.810125827789307 LOSS_Discriminator: 0.08669913560152054\n",
            "ITERATION_NO.: 269 LOSS_Generator: 8.10625171661377 LOSS_Discriminator: 0.05145424231886864\n",
            "ITERATION_NO.: 270 LOSS_Generator: 7.3848958015441895 LOSS_Discriminator: 0.057081736624240875\n",
            "ITERATION_NO.: 271 LOSS_Generator: 7.163653373718262 LOSS_Discriminator: 0.07950954884290695\n",
            "ITERATION_NO.: 272 LOSS_Generator: 7.020699977874756 LOSS_Discriminator: 0.09450609236955643\n",
            "ITERATION_NO.: 273 LOSS_Generator: 7.034884452819824 LOSS_Discriminator: 0.09689216315746307\n",
            "ITERATION_NO.: 274 LOSS_Generator: 6.6409687995910645 LOSS_Discriminator: 0.06968957930803299\n",
            "ITERATION_NO.: 275 LOSS_Generator: 7.874395847320557 LOSS_Discriminator: 0.07307499647140503\n",
            "ITERATION_NO.: 276 LOSS_Generator: 8.082328796386719 LOSS_Discriminator: 0.02429969049990177\n",
            "ITERATION_NO.: 277 LOSS_Generator: 8.330561637878418 LOSS_Discriminator: 0.09378999471664429\n",
            "ITERATION_NO.: 278 LOSS_Generator: 7.654918670654297 LOSS_Discriminator: 0.12130101025104523\n",
            "ITERATION_NO.: 279 LOSS_Generator: 6.677067756652832 LOSS_Discriminator: 0.04445737227797508\n",
            "ITERATION_NO.: 280 LOSS_Generator: 7.253685474395752 LOSS_Discriminator: 0.08016423881053925\n",
            "ITERATION_NO.: 281 LOSS_Generator: 6.924033164978027 LOSS_Discriminator: 0.06622011959552765\n",
            "ITERATION_NO.: 282 LOSS_Generator: 6.856624603271484 LOSS_Discriminator: 0.10407817363739014\n",
            "ITERATION_NO.: 283 LOSS_Generator: 6.964044570922852 LOSS_Discriminator: 0.07142151147127151\n",
            "ITERATION_NO.: 284 LOSS_Generator: 7.522336483001709 LOSS_Discriminator: 0.146896094083786\n",
            "ITERATION_NO.: 285 LOSS_Generator: 7.579428195953369 LOSS_Discriminator: 0.04461963474750519\n",
            "ITERATION_NO.: 286 LOSS_Generator: 7.638608455657959 LOSS_Discriminator: 0.12093447148799896\n",
            "ITERATION_NO.: 287 LOSS_Generator: 7.616639614105225 LOSS_Discriminator: 0.10947137326002121\n",
            "ITERATION_NO.: 288 LOSS_Generator: 7.4094438552856445 LOSS_Discriminator: 0.1056588739156723\n",
            "ITERATION_NO.: 289 LOSS_Generator: 7.201138973236084 LOSS_Discriminator: 0.08800307661294937\n",
            "ITERATION_NO.: 290 LOSS_Generator: 6.700211048126221 LOSS_Discriminator: 0.05461076647043228\n",
            "ITERATION_NO.: 291 LOSS_Generator: 7.9421491622924805 LOSS_Discriminator: 0.12065908312797546\n",
            "ITERATION_NO.: 292 LOSS_Generator: 8.438401222229004 LOSS_Discriminator: 0.0838463306427002\n",
            "ITERATION_NO.: 293 LOSS_Generator: 8.26737117767334 LOSS_Discriminator: 0.05748635530471802\n",
            "ITERATION_NO.: 294 LOSS_Generator: 7.46803092956543 LOSS_Discriminator: 0.10009859502315521\n",
            "ITERATION_NO.: 295 LOSS_Generator: 7.051098823547363 LOSS_Discriminator: 0.07774978131055832\n",
            "ITERATION_NO.: 296 LOSS_Generator: 6.987618923187256 LOSS_Discriminator: 0.08033570647239685\n",
            "ITERATION_NO.: 297 LOSS_Generator: 6.513929843902588 LOSS_Discriminator: 0.07364733517169952\n",
            "ITERATION_NO.: 298 LOSS_Generator: 6.504594802856445 LOSS_Discriminator: 0.09849952161312103\n",
            "ITERATION_NO.: 299 LOSS_Generator: 7.401234149932861 LOSS_Discriminator: 0.03468567132949829\n",
            "ITERATION_NO.: 300 LOSS_Generator: 8.546233177185059 LOSS_Discriminator: 0.051542047411203384\n",
            "ITERATION_NO.: 301 LOSS_Generator: 7.74003791809082 LOSS_Discriminator: 0.084917813539505\n",
            "ITERATION_NO.: 302 LOSS_Generator: 7.605152130126953 LOSS_Discriminator: 0.09760010242462158\n",
            "ITERATION_NO.: 303 LOSS_Generator: 6.3280029296875 LOSS_Discriminator: 0.05772285535931587\n",
            "ITERATION_NO.: 304 LOSS_Generator: 6.378320693969727 LOSS_Discriminator: 0.08344021439552307\n",
            "ITERATION_NO.: 305 LOSS_Generator: 6.840937614440918 LOSS_Discriminator: 0.0700339823961258\n",
            "ITERATION_NO.: 306 LOSS_Generator: 7.892480850219727 LOSS_Discriminator: 0.07103972136974335\n",
            "ITERATION_NO.: 307 LOSS_Generator: 8.416634559631348 LOSS_Discriminator: 0.05392708629369736\n",
            "ITERATION_NO.: 308 LOSS_Generator: 8.741799354553223 LOSS_Discriminator: 0.07577289640903473\n",
            "ITERATION_NO.: 309 LOSS_Generator: 7.840412616729736 LOSS_Discriminator: 0.08385622501373291\n",
            "ITERATION_NO.: 310 LOSS_Generator: 6.7463483810424805 LOSS_Discriminator: 0.06856607645750046\n",
            "ITERATION_NO.: 311 LOSS_Generator: 6.219850063323975 LOSS_Discriminator: 0.10993041098117828\n",
            "ITERATION_NO.: 312 LOSS_Generator: 6.482822418212891 LOSS_Discriminator: 0.09155905991792679\n",
            "ITERATION_NO.: 313 LOSS_Generator: 7.00114631652832 LOSS_Discriminator: 0.05145203322172165\n",
            "ITERATION_NO.: 314 LOSS_Generator: 8.211042404174805 LOSS_Discriminator: 0.03336567431688309\n",
            "ITERATION_NO.: 315 LOSS_Generator: 8.403168678283691 LOSS_Discriminator: 0.03679937869310379\n",
            "ITERATION_NO.: 316 LOSS_Generator: 8.658626556396484 LOSS_Discriminator: 0.07508021593093872\n",
            "ITERATION_NO.: 317 LOSS_Generator: 8.8827486038208 LOSS_Discriminator: 0.07343144714832306\n",
            "ITERATION_NO.: 318 LOSS_Generator: 7.822690486907959 LOSS_Discriminator: 0.023995185270905495\n",
            "ITERATION_NO.: 319 LOSS_Generator: 7.208700656890869 LOSS_Discriminator: 0.020106241106987\n",
            "ITERATION_NO.: 320 LOSS_Generator: 7.183864116668701 LOSS_Discriminator: 0.0890607163310051\n",
            "ITERATION_NO.: 321 LOSS_Generator: 6.558521270751953 LOSS_Discriminator: 0.10097664594650269\n",
            "ITERATION_NO.: 322 LOSS_Generator: 7.222574234008789 LOSS_Discriminator: 0.05647343769669533\n",
            "ITERATION_NO.: 323 LOSS_Generator: 7.275355339050293 LOSS_Discriminator: 0.04465635493397713\n",
            "ITERATION_NO.: 324 LOSS_Generator: 8.352563858032227 LOSS_Discriminator: 0.08024094998836517\n",
            "ITERATION_NO.: 325 LOSS_Generator: 7.833571910858154 LOSS_Discriminator: 0.1551334261894226\n",
            "ITERATION_NO.: 326 LOSS_Generator: 6.961460113525391 LOSS_Discriminator: 0.11960873752832413\n",
            "ITERATION_NO.: 327 LOSS_Generator: 6.903051376342773 LOSS_Discriminator: 0.05043887346982956\n",
            "ITERATION_NO.: 328 LOSS_Generator: 6.029150485992432 LOSS_Discriminator: 0.08852244168519974\n",
            "ITERATION_NO.: 329 LOSS_Generator: 6.333333969116211 LOSS_Discriminator: 0.0962575227022171\n",
            "ITERATION_NO.: 330 LOSS_Generator: 6.959384918212891 LOSS_Discriminator: 0.0767286866903305\n",
            "ITERATION_NO.: 331 LOSS_Generator: 7.522134304046631 LOSS_Discriminator: 0.10385763645172119\n",
            "ITERATION_NO.: 332 LOSS_Generator: 7.943053722381592 LOSS_Discriminator: 0.08978963643312454\n",
            "ITERATION_NO.: 333 LOSS_Generator: 7.530572891235352 LOSS_Discriminator: 0.09564998000860214\n",
            "ITERATION_NO.: 334 LOSS_Generator: 6.260148525238037 LOSS_Discriminator: 0.17752721905708313\n",
            "ITERATION_NO.: 335 LOSS_Generator: 5.64167594909668 LOSS_Discriminator: 0.07798078656196594\n",
            "ITERATION_NO.: 336 LOSS_Generator: 6.0837812423706055 LOSS_Discriminator: 0.10994505137205124\n",
            "ITERATION_NO.: 337 LOSS_Generator: 7.190052032470703 LOSS_Discriminator: 0.1428634524345398\n",
            "ITERATION_NO.: 338 LOSS_Generator: 8.739439964294434 LOSS_Discriminator: 0.0467345267534256\n",
            "ITERATION_NO.: 339 LOSS_Generator: 9.130462646484375 LOSS_Discriminator: 0.03702140599489212\n",
            "ITERATION_NO.: 340 LOSS_Generator: 9.955731391906738 LOSS_Discriminator: 0.023724086582660675\n",
            "ITERATION_NO.: 341 LOSS_Generator: 9.406357765197754 LOSS_Discriminator: 0.12248948216438293\n",
            "ITERATION_NO.: 342 LOSS_Generator: 8.387709617614746 LOSS_Discriminator: 0.04170965775847435\n",
            "ITERATION_NO.: 343 LOSS_Generator: 8.362104415893555 LOSS_Discriminator: 0.05343588814139366\n",
            "ITERATION_NO.: 344 LOSS_Generator: 6.288113594055176 LOSS_Discriminator: 0.06568151712417603\n",
            "ITERATION_NO.: 345 LOSS_Generator: 6.492832660675049 LOSS_Discriminator: 0.03975839167833328\n",
            "ITERATION_NO.: 346 LOSS_Generator: 6.269145488739014 LOSS_Discriminator: 0.10499688237905502\n",
            "ITERATION_NO.: 347 LOSS_Generator: 7.302350044250488 LOSS_Discriminator: 0.10280659794807434\n",
            "ITERATION_NO.: 348 LOSS_Generator: 8.247551918029785 LOSS_Discriminator: 0.04224943369626999\n",
            "ITERATION_NO.: 349 LOSS_Generator: 8.595484733581543 LOSS_Discriminator: 0.0571044385433197\n",
            "ITERATION_NO.: 350 LOSS_Generator: 8.509812355041504 LOSS_Discriminator: 0.07463173568248749\n",
            "ITERATION_NO.: 351 LOSS_Generator: 7.104272842407227 LOSS_Discriminator: 0.10488390177488327\n",
            "ITERATION_NO.: 352 LOSS_Generator: 6.290926933288574 LOSS_Discriminator: 0.056917477399110794\n",
            "ITERATION_NO.: 353 LOSS_Generator: 6.6447529792785645 LOSS_Discriminator: 0.08732111752033234\n",
            "ITERATION_NO.: 354 LOSS_Generator: 7.303985595703125 LOSS_Discriminator: 0.06115483492612839\n",
            "ITERATION_NO.: 355 LOSS_Generator: 8.337385177612305 LOSS_Discriminator: 0.039788857102394104\n",
            "ITERATION_NO.: 356 LOSS_Generator: 8.37723159790039 LOSS_Discriminator: 0.06330420076847076\n",
            "ITERATION_NO.: 357 LOSS_Generator: 8.206290245056152 LOSS_Discriminator: 0.12196798622608185\n",
            "ITERATION_NO.: 358 LOSS_Generator: 7.472789287567139 LOSS_Discriminator: 0.08595823496580124\n",
            "ITERATION_NO.: 359 LOSS_Generator: 7.116695404052734 LOSS_Discriminator: 0.06775709986686707\n",
            "ITERATION_NO.: 360 LOSS_Generator: 7.073077201843262 LOSS_Discriminator: 0.041441358625888824\n",
            "ITERATION_NO.: 361 LOSS_Generator: 7.4040632247924805 LOSS_Discriminator: 0.1098618134856224\n",
            "ITERATION_NO.: 362 LOSS_Generator: 7.319620132446289 LOSS_Discriminator: 0.027425337582826614\n",
            "ITERATION_NO.: 363 LOSS_Generator: 8.034911155700684 LOSS_Discriminator: 0.044460736215114594\n",
            "ITERATION_NO.: 364 LOSS_Generator: 7.664290904998779 LOSS_Discriminator: 0.07765807956457138\n",
            "ITERATION_NO.: 365 LOSS_Generator: 6.797020435333252 LOSS_Discriminator: 0.07416720688343048\n",
            "ITERATION_NO.: 366 LOSS_Generator: 6.815423488616943 LOSS_Discriminator: 0.0673670768737793\n",
            "ITERATION_NO.: 367 LOSS_Generator: 6.844078540802002 LOSS_Discriminator: 0.0561135858297348\n",
            "ITERATION_NO.: 368 LOSS_Generator: 6.569063186645508 LOSS_Discriminator: 0.06348668038845062\n",
            "ITERATION_NO.: 369 LOSS_Generator: 6.642218112945557 LOSS_Discriminator: 0.04914382845163345\n",
            "ITERATION_NO.: 370 LOSS_Generator: 6.810248851776123 LOSS_Discriminator: 0.0868690013885498\n",
            "ITERATION_NO.: 371 LOSS_Generator: 7.6554083824157715 LOSS_Discriminator: 0.11110487580299377\n",
            "ITERATION_NO.: 372 LOSS_Generator: 6.986144542694092 LOSS_Discriminator: 0.07204215228557587\n",
            "ITERATION_NO.: 373 LOSS_Generator: 6.511581897735596 LOSS_Discriminator: 0.07119205594062805\n",
            "ITERATION_NO.: 374 LOSS_Generator: 6.522190093994141 LOSS_Discriminator: 0.033015184104442596\n",
            "ITERATION_NO.: 375 LOSS_Generator: 6.898617744445801 LOSS_Discriminator: 0.05237485095858574\n",
            "ITERATION_NO.: 376 LOSS_Generator: 7.061005115509033 LOSS_Discriminator: 0.02859494835138321\n",
            "ITERATION_NO.: 377 LOSS_Generator: 6.974646091461182 LOSS_Discriminator: 0.12104597687721252\n",
            "ITERATION_NO.: 378 LOSS_Generator: 6.680596828460693 LOSS_Discriminator: 0.055783115327358246\n",
            "ITERATION_NO.: 379 LOSS_Generator: 6.298957347869873 LOSS_Discriminator: 0.03112119436264038\n",
            "ITERATION_NO.: 380 LOSS_Generator: 6.189333438873291 LOSS_Discriminator: 0.07105766236782074\n",
            "ITERATION_NO.: 381 LOSS_Generator: 6.4200873374938965 LOSS_Discriminator: 0.05724730342626572\n",
            "ITERATION_NO.: 382 LOSS_Generator: 6.4306840896606445 LOSS_Discriminator: 0.059333037585020065\n",
            "ITERATION_NO.: 383 LOSS_Generator: 7.2849040031433105 LOSS_Discriminator: 0.057447873055934906\n",
            "ITERATION_NO.: 384 LOSS_Generator: 7.565721035003662 LOSS_Discriminator: 0.08350600302219391\n",
            "ITERATION_NO.: 385 LOSS_Generator: 7.846280097961426 LOSS_Discriminator: 0.07545044273138046\n",
            "ITERATION_NO.: 386 LOSS_Generator: 7.382768154144287 LOSS_Discriminator: 0.05943648889660835\n",
            "ITERATION_NO.: 387 LOSS_Generator: 7.117741584777832 LOSS_Discriminator: 0.07268189638853073\n",
            "ITERATION_NO.: 388 LOSS_Generator: 7.053674221038818 LOSS_Discriminator: 0.051795072853565216\n",
            "ITERATION_NO.: 389 LOSS_Generator: 5.983532905578613 LOSS_Discriminator: 0.10272648185491562\n",
            "ITERATION_NO.: 390 LOSS_Generator: 5.5840840339660645 LOSS_Discriminator: 0.14737778902053833\n",
            "ITERATION_NO.: 391 LOSS_Generator: 6.302610397338867 LOSS_Discriminator: 0.053011149168014526\n",
            "ITERATION_NO.: 392 LOSS_Generator: 7.736074447631836 LOSS_Discriminator: 0.05640071630477905\n",
            "ITERATION_NO.: 393 LOSS_Generator: 8.645729064941406 LOSS_Discriminator: 0.04814089089632034\n",
            "ITERATION_NO.: 394 LOSS_Generator: 8.75894832611084 LOSS_Discriminator: 0.05455240607261658\n",
            "ITERATION_NO.: 395 LOSS_Generator: 8.427325248718262 LOSS_Discriminator: 0.14409148693084717\n",
            "ITERATION_NO.: 396 LOSS_Generator: 7.692281723022461 LOSS_Discriminator: 0.06599518656730652\n",
            "ITERATION_NO.: 397 LOSS_Generator: 6.180208206176758 LOSS_Discriminator: 0.11112158000469208\n",
            "ITERATION_NO.: 398 LOSS_Generator: 5.829984188079834 LOSS_Discriminator: 0.05014057084918022\n",
            "ITERATION_NO.: 399 LOSS_Generator: 5.920593738555908 LOSS_Discriminator: 0.09383440017700195\n",
            "ITERATION_NO.: 400 LOSS_Generator: 8.064851760864258 LOSS_Discriminator: 0.12565836310386658\n",
            "ITERATION_NO.: 401 LOSS_Generator: 9.279282569885254 LOSS_Discriminator: 0.04468698427081108\n",
            "ITERATION_NO.: 402 LOSS_Generator: 9.597214698791504 LOSS_Discriminator: 0.12292130291461945\n",
            "ITERATION_NO.: 403 LOSS_Generator: 9.329944610595703 LOSS_Discriminator: 0.09390553086996078\n",
            "ITERATION_NO.: 404 LOSS_Generator: 7.990499496459961 LOSS_Discriminator: 0.13105374574661255\n",
            "ITERATION_NO.: 405 LOSS_Generator: 6.778781890869141 LOSS_Discriminator: 0.057709839195013046\n",
            "ITERATION_NO.: 406 LOSS_Generator: 6.008974552154541 LOSS_Discriminator: 0.09382451325654984\n",
            "ITERATION_NO.: 407 LOSS_Generator: 6.945914268493652 LOSS_Discriminator: 0.12803500890731812\n",
            "ITERATION_NO.: 408 LOSS_Generator: 7.7186174392700195 LOSS_Discriminator: 0.06494428217411041\n",
            "ITERATION_NO.: 409 LOSS_Generator: 8.554974555969238 LOSS_Discriminator: 0.1157815009355545\n",
            "ITERATION_NO.: 410 LOSS_Generator: 8.862934112548828 LOSS_Discriminator: 0.1530323624610901\n",
            "ITERATION_NO.: 411 LOSS_Generator: 7.24492073059082 LOSS_Discriminator: 0.12556198239326477\n",
            "ITERATION_NO.: 412 LOSS_Generator: 6.107174873352051 LOSS_Discriminator: 0.11569037288427353\n",
            "ITERATION_NO.: 413 LOSS_Generator: 5.240942478179932 LOSS_Discriminator: 0.1266040802001953\n",
            "ITERATION_NO.: 414 LOSS_Generator: 6.7300286293029785 LOSS_Discriminator: 0.09507054090499878\n",
            "ITERATION_NO.: 415 LOSS_Generator: 7.780013561248779 LOSS_Discriminator: 0.08101679384708405\n",
            "ITERATION_NO.: 416 LOSS_Generator: 8.464799880981445 LOSS_Discriminator: 0.05418801307678223\n",
            "ITERATION_NO.: 417 LOSS_Generator: 8.62442684173584 LOSS_Discriminator: 0.09265129268169403\n",
            "ITERATION_NO.: 418 LOSS_Generator: 8.069674491882324 LOSS_Discriminator: 0.12186364829540253\n",
            "ITERATION_NO.: 419 LOSS_Generator: 7.3863654136657715 LOSS_Discriminator: 0.01607336476445198\n",
            "ITERATION_NO.: 420 LOSS_Generator: 7.033850193023682 LOSS_Discriminator: 0.040107205510139465\n",
            "ITERATION_NO.: 421 LOSS_Generator: 7.07321834564209 LOSS_Discriminator: 0.03399014472961426\n",
            "ITERATION_NO.: 422 LOSS_Generator: 7.337844371795654 LOSS_Discriminator: 0.06136975437402725\n",
            "ITERATION_NO.: 423 LOSS_Generator: 6.920404434204102 LOSS_Discriminator: 0.04158713296055794\n",
            "ITERATION_NO.: 424 LOSS_Generator: 7.460979461669922 LOSS_Discriminator: 0.08288617432117462\n",
            "ITERATION_NO.: 425 LOSS_Generator: 7.5464606285095215 LOSS_Discriminator: 0.021215103566646576\n",
            "ITERATION_NO.: 426 LOSS_Generator: 7.366696357727051 LOSS_Discriminator: 0.016404055058956146\n",
            "ITERATION_NO.: 427 LOSS_Generator: 7.681875228881836 LOSS_Discriminator: 0.021087780594825745\n",
            "ITERATION_NO.: 428 LOSS_Generator: 7.028300762176514 LOSS_Discriminator: 0.025162674486637115\n",
            "ITERATION_NO.: 429 LOSS_Generator: 7.200007915496826 LOSS_Discriminator: 0.05334899201989174\n",
            "ITERATION_NO.: 430 LOSS_Generator: 6.594478130340576 LOSS_Discriminator: 0.04266093671321869\n",
            "ITERATION_NO.: 431 LOSS_Generator: 6.2813334465026855 LOSS_Discriminator: 0.06361385434865952\n",
            "ITERATION_NO.: 432 LOSS_Generator: 7.330117702484131 LOSS_Discriminator: 0.09529593586921692\n",
            "ITERATION_NO.: 433 LOSS_Generator: 8.333846092224121 LOSS_Discriminator: 0.021945158019661903\n",
            "ITERATION_NO.: 434 LOSS_Generator: 8.612903594970703 LOSS_Discriminator: 0.037504032254219055\n",
            "ITERATION_NO.: 435 LOSS_Generator: 7.628897190093994 LOSS_Discriminator: 0.12018600851297379\n",
            "ITERATION_NO.: 436 LOSS_Generator: 6.883245944976807 LOSS_Discriminator: 0.15815125405788422\n",
            "ITERATION_NO.: 437 LOSS_Generator: 5.561690807342529 LOSS_Discriminator: 0.05745486542582512\n",
            "ITERATION_NO.: 438 LOSS_Generator: 6.124846935272217 LOSS_Discriminator: 0.0779522955417633\n",
            "ITERATION_NO.: 439 LOSS_Generator: 6.218072414398193 LOSS_Discriminator: 0.05479034036397934\n",
            "ITERATION_NO.: 440 LOSS_Generator: 7.289435863494873 LOSS_Discriminator: 0.06295639276504517\n",
            "ITERATION_NO.: 441 LOSS_Generator: 7.865114688873291 LOSS_Discriminator: 0.06336402893066406\n",
            "ITERATION_NO.: 442 LOSS_Generator: 8.181140899658203 LOSS_Discriminator: 0.06716737151145935\n",
            "ITERATION_NO.: 443 LOSS_Generator: 8.018537521362305 LOSS_Discriminator: 0.08923519402742386\n",
            "ITERATION_NO.: 444 LOSS_Generator: 7.515674591064453 LOSS_Discriminator: 0.02641931176185608\n",
            "ITERATION_NO.: 445 LOSS_Generator: 7.152217864990234 LOSS_Discriminator: 0.07511794567108154\n",
            "ITERATION_NO.: 446 LOSS_Generator: 6.283729076385498 LOSS_Discriminator: 0.05423463135957718\n",
            "ITERATION_NO.: 447 LOSS_Generator: 5.815324306488037 LOSS_Discriminator: 0.05396407097578049\n",
            "ITERATION_NO.: 448 LOSS_Generator: 6.531243324279785 LOSS_Discriminator: 0.07576905190944672\n",
            "ITERATION_NO.: 449 LOSS_Generator: 6.490182876586914 LOSS_Discriminator: 0.06478139758110046\n",
            "ITERATION_NO.: 450 LOSS_Generator: 7.138155937194824 LOSS_Discriminator: 0.05016461759805679\n",
            "ITERATION_NO.: 451 LOSS_Generator: 7.532544136047363 LOSS_Discriminator: 0.028125116601586342\n",
            "ITERATION_NO.: 452 LOSS_Generator: 7.976198673248291 LOSS_Discriminator: 0.049162618815898895\n",
            "ITERATION_NO.: 453 LOSS_Generator: 7.844338417053223 LOSS_Discriminator: 0.01985946297645569\n",
            "ITERATION_NO.: 454 LOSS_Generator: 7.777952671051025 LOSS_Discriminator: 0.0286534633487463\n",
            "ITERATION_NO.: 455 LOSS_Generator: 6.639559745788574 LOSS_Discriminator: 0.1088717132806778\n",
            "ITERATION_NO.: 456 LOSS_Generator: 6.809187412261963 LOSS_Discriminator: 0.07749477028846741\n",
            "ITERATION_NO.: 457 LOSS_Generator: 6.287957668304443 LOSS_Discriminator: 0.05890839919447899\n",
            "ITERATION_NO.: 458 LOSS_Generator: 6.861374378204346 LOSS_Discriminator: 0.03136761486530304\n",
            "ITERATION_NO.: 459 LOSS_Generator: 6.835076332092285 LOSS_Discriminator: 0.03461552783846855\n",
            "ITERATION_NO.: 460 LOSS_Generator: 7.428651332855225 LOSS_Discriminator: 0.06140434369444847\n",
            "ITERATION_NO.: 461 LOSS_Generator: 7.037542819976807 LOSS_Discriminator: 0.10831838846206665\n",
            "ITERATION_NO.: 462 LOSS_Generator: 7.628945350646973 LOSS_Discriminator: 0.0823018029332161\n",
            "ITERATION_NO.: 463 LOSS_Generator: 7.175745964050293 LOSS_Discriminator: 0.03377089276909828\n",
            "ITERATION_NO.: 464 LOSS_Generator: 6.842169284820557 LOSS_Discriminator: 0.09805498272180557\n",
            "ITERATION_NO.: 465 LOSS_Generator: 6.555526256561279 LOSS_Discriminator: 0.03431397303938866\n",
            "ITERATION_NO.: 466 LOSS_Generator: 6.125263690948486 LOSS_Discriminator: 0.10997277498245239\n",
            "ITERATION_NO.: 467 LOSS_Generator: 6.5576934814453125 LOSS_Discriminator: 0.0712922215461731\n",
            "ITERATION_NO.: 468 LOSS_Generator: 6.285462856292725 LOSS_Discriminator: 0.0924331396818161\n",
            "ITERATION_NO.: 469 LOSS_Generator: 6.445513725280762 LOSS_Discriminator: 0.05092242732644081\n",
            "ITERATION_NO.: 470 LOSS_Generator: 6.894692897796631 LOSS_Discriminator: 0.0538395531475544\n",
            "ITERATION_NO.: 471 LOSS_Generator: 6.9984211921691895 LOSS_Discriminator: 0.06188776716589928\n",
            "ITERATION_NO.: 472 LOSS_Generator: 7.515264511108398 LOSS_Discriminator: 0.024646682664752007\n",
            "ITERATION_NO.: 473 LOSS_Generator: 7.287448883056641 LOSS_Discriminator: 0.09646360576152802\n",
            "ITERATION_NO.: 474 LOSS_Generator: 6.579700469970703 LOSS_Discriminator: 0.053485266864299774\n",
            "ITERATION_NO.: 475 LOSS_Generator: 6.619845390319824 LOSS_Discriminator: 0.04733068495988846\n",
            "ITERATION_NO.: 476 LOSS_Generator: 6.549388408660889 LOSS_Discriminator: 0.09630467742681503\n",
            "ITERATION_NO.: 477 LOSS_Generator: 6.6258015632629395 LOSS_Discriminator: 0.08313184976577759\n",
            "ITERATION_NO.: 478 LOSS_Generator: 7.101301670074463 LOSS_Discriminator: 0.058954574167728424\n",
            "ITERATION_NO.: 479 LOSS_Generator: 6.849831581115723 LOSS_Discriminator: 0.020477496087551117\n",
            "ITERATION_NO.: 480 LOSS_Generator: 7.561874866485596 LOSS_Discriminator: 0.054925478994846344\n",
            "ITERATION_NO.: 481 LOSS_Generator: 6.962754726409912 LOSS_Discriminator: 0.05646149814128876\n",
            "ITERATION_NO.: 482 LOSS_Generator: 7.034909248352051 LOSS_Discriminator: 0.04991360008716583\n",
            "ITERATION_NO.: 483 LOSS_Generator: 6.879108905792236 LOSS_Discriminator: 0.03411459922790527\n",
            "ITERATION_NO.: 484 LOSS_Generator: 7.212978363037109 LOSS_Discriminator: 0.13112980127334595\n",
            "ITERATION_NO.: 485 LOSS_Generator: 6.510051727294922 LOSS_Discriminator: 0.12467566877603531\n",
            "ITERATION_NO.: 486 LOSS_Generator: 5.936778545379639 LOSS_Discriminator: 0.08289850503206253\n",
            "ITERATION_NO.: 487 LOSS_Generator: 6.216963291168213 LOSS_Discriminator: 0.044782854616642\n",
            "ITERATION_NO.: 488 LOSS_Generator: 6.880379676818848 LOSS_Discriminator: 0.051027216017246246\n",
            "ITERATION_NO.: 489 LOSS_Generator: 7.78983211517334 LOSS_Discriminator: 0.049312494695186615\n",
            "ITERATION_NO.: 490 LOSS_Generator: 7.834499359130859 LOSS_Discriminator: 0.05834062397480011\n",
            "ITERATION_NO.: 491 LOSS_Generator: 8.243720054626465 LOSS_Discriminator: 0.05455900728702545\n",
            "ITERATION_NO.: 492 LOSS_Generator: 7.662715911865234 LOSS_Discriminator: 0.04838394373655319\n",
            "ITERATION_NO.: 493 LOSS_Generator: 7.516827583312988 LOSS_Discriminator: 0.059279121458530426\n",
            "ITERATION_NO.: 494 LOSS_Generator: 6.8841118812561035 LOSS_Discriminator: 0.06754504144191742\n",
            "ITERATION_NO.: 495 LOSS_Generator: 6.568540573120117 LOSS_Discriminator: 0.02800813503563404\n",
            "ITERATION_NO.: 496 LOSS_Generator: 6.4763078689575195 LOSS_Discriminator: 0.08890412747859955\n",
            "ITERATION_NO.: 497 LOSS_Generator: 6.723567962646484 LOSS_Discriminator: 0.12291298806667328\n",
            "ITERATION_NO.: 498 LOSS_Generator: 6.548483848571777 LOSS_Discriminator: 0.07668901979923248\n",
            "ITERATION_NO.: 499 LOSS_Generator: 7.409173488616943 LOSS_Discriminator: 0.06284560263156891\n",
            "ITERATION_NO.: 500 LOSS_Generator: 7.905508995056152 LOSS_Discriminator: 0.09084294736385345\n",
            "ITERATION_NO.: 501 LOSS_Generator: 7.368930816650391 LOSS_Discriminator: 0.07070218771696091\n",
            "ITERATION_NO.: 502 LOSS_Generator: 7.5993781089782715 LOSS_Discriminator: 0.06301777064800262\n",
            "ITERATION_NO.: 503 LOSS_Generator: 7.0253214836120605 LOSS_Discriminator: 0.03646023944020271\n",
            "ITERATION_NO.: 504 LOSS_Generator: 6.957726001739502 LOSS_Discriminator: 0.07388188689947128\n",
            "ITERATION_NO.: 505 LOSS_Generator: 7.090720176696777 LOSS_Discriminator: 0.09075663983821869\n",
            "ITERATION_NO.: 506 LOSS_Generator: 6.863287925720215 LOSS_Discriminator: 0.09545016288757324\n",
            "ITERATION_NO.: 507 LOSS_Generator: 8.025293350219727 LOSS_Discriminator: 0.0523250475525856\n",
            "ITERATION_NO.: 508 LOSS_Generator: 8.249765396118164 LOSS_Discriminator: 0.08773830533027649\n",
            "ITERATION_NO.: 509 LOSS_Generator: 7.890415191650391 LOSS_Discriminator: 0.0715637356042862\n",
            "ITERATION_NO.: 510 LOSS_Generator: 7.5796122550964355 LOSS_Discriminator: 0.028922025114297867\n",
            "ITERATION_NO.: 511 LOSS_Generator: 7.680893421173096 LOSS_Discriminator: 0.0882587730884552\n",
            "ITERATION_NO.: 512 LOSS_Generator: 7.770861625671387 LOSS_Discriminator: 0.11314908415079117\n",
            "ITERATION_NO.: 513 LOSS_Generator: 7.200984001159668 LOSS_Discriminator: 0.10585835576057434\n",
            "ITERATION_NO.: 514 LOSS_Generator: 6.707233428955078 LOSS_Discriminator: 0.061176251620054245\n",
            "ITERATION_NO.: 515 LOSS_Generator: 6.0128254890441895 LOSS_Discriminator: 0.07080669701099396\n",
            "ITERATION_NO.: 516 LOSS_Generator: 6.851191520690918 LOSS_Discriminator: 0.04261462762951851\n",
            "ITERATION_NO.: 517 LOSS_Generator: 7.18708610534668 LOSS_Discriminator: 0.05252527445554733\n",
            "ITERATION_NO.: 518 LOSS_Generator: 7.963690757751465 LOSS_Discriminator: 0.02734559401869774\n",
            "ITERATION_NO.: 519 LOSS_Generator: 7.953863620758057 LOSS_Discriminator: 0.05289877951145172\n",
            "ITERATION_NO.: 520 LOSS_Generator: 8.38845157623291 LOSS_Discriminator: 0.07705555111169815\n",
            "ITERATION_NO.: 521 LOSS_Generator: 8.063674926757812 LOSS_Discriminator: 0.11645743995904922\n",
            "ITERATION_NO.: 522 LOSS_Generator: 7.266256332397461 LOSS_Discriminator: 0.06891492009162903\n",
            "ITERATION_NO.: 523 LOSS_Generator: 6.378433704376221 LOSS_Discriminator: 0.07811243087053299\n",
            "ITERATION_NO.: 524 LOSS_Generator: 6.0761590003967285 LOSS_Discriminator: 0.05684303864836693\n",
            "ITERATION_NO.: 525 LOSS_Generator: 6.179877758026123 LOSS_Discriminator: 0.06012439727783203\n",
            "ITERATION_NO.: 526 LOSS_Generator: 7.371640205383301 LOSS_Discriminator: 0.09427100419998169\n",
            "ITERATION_NO.: 527 LOSS_Generator: 8.551143646240234 LOSS_Discriminator: 0.05175694450736046\n",
            "ITERATION_NO.: 528 LOSS_Generator: 8.665782928466797 LOSS_Discriminator: 0.09875595569610596\n",
            "ITERATION_NO.: 529 LOSS_Generator: 7.5935893058776855 LOSS_Discriminator: 0.061308279633522034\n",
            "ITERATION_NO.: 530 LOSS_Generator: 7.137130260467529 LOSS_Discriminator: 0.05946122854948044\n",
            "ITERATION_NO.: 531 LOSS_Generator: 6.333125114440918 LOSS_Discriminator: 0.07406997680664062\n",
            "ITERATION_NO.: 532 LOSS_Generator: 5.774186611175537 LOSS_Discriminator: 0.07309912890195847\n",
            "ITERATION_NO.: 533 LOSS_Generator: 6.2896294593811035 LOSS_Discriminator: 0.06759779155254364\n",
            "ITERATION_NO.: 534 LOSS_Generator: 7.0005269050598145 LOSS_Discriminator: 0.0601191446185112\n",
            "ITERATION_NO.: 535 LOSS_Generator: 7.831341743469238 LOSS_Discriminator: 0.06571880728006363\n",
            "ITERATION_NO.: 536 LOSS_Generator: 8.099959373474121 LOSS_Discriminator: 0.08865805715322495\n",
            "ITERATION_NO.: 537 LOSS_Generator: 7.827780246734619 LOSS_Discriminator: 0.05704810470342636\n",
            "ITERATION_NO.: 538 LOSS_Generator: 7.101562976837158 LOSS_Discriminator: 0.04078049585223198\n",
            "ITERATION_NO.: 539 LOSS_Generator: 7.140141487121582 LOSS_Discriminator: 0.15214115381240845\n",
            "ITERATION_NO.: 540 LOSS_Generator: 6.4621686935424805 LOSS_Discriminator: 0.09640742838382721\n",
            "ITERATION_NO.: 541 LOSS_Generator: 6.215488433837891 LOSS_Discriminator: 0.08981640636920929\n",
            "ITERATION_NO.: 542 LOSS_Generator: 6.493875503540039 LOSS_Discriminator: 0.03432145342230797\n",
            "ITERATION_NO.: 543 LOSS_Generator: 6.870086193084717 LOSS_Discriminator: 0.02747613564133644\n",
            "ITERATION_NO.: 544 LOSS_Generator: 7.407688140869141 LOSS_Discriminator: 0.04209508001804352\n",
            "ITERATION_NO.: 545 LOSS_Generator: 7.579945087432861 LOSS_Discriminator: 0.08386018872261047\n",
            "ITERATION_NO.: 546 LOSS_Generator: 7.5876336097717285 LOSS_Discriminator: 0.07010696828365326\n",
            "ITERATION_NO.: 547 LOSS_Generator: 6.81719970703125 LOSS_Discriminator: 0.12020459026098251\n",
            "ITERATION_NO.: 548 LOSS_Generator: 6.264011859893799 LOSS_Discriminator: 0.041846103966236115\n",
            "ITERATION_NO.: 549 LOSS_Generator: 6.080877780914307 LOSS_Discriminator: 0.0968620628118515\n",
            "ITERATION_NO.: 550 LOSS_Generator: 7.124446392059326 LOSS_Discriminator: 0.0830988883972168\n",
            "ITERATION_NO.: 551 LOSS_Generator: 8.453651428222656 LOSS_Discriminator: 0.05936829373240471\n",
            "ITERATION_NO.: 552 LOSS_Generator: 8.350410461425781 LOSS_Discriminator: 0.11377209424972534\n",
            "ITERATION_NO.: 553 LOSS_Generator: 7.4382147789001465 LOSS_Discriminator: 0.07912985980510712\n",
            "ITERATION_NO.: 554 LOSS_Generator: 7.017941474914551 LOSS_Discriminator: 0.04825056716799736\n",
            "ITERATION_NO.: 555 LOSS_Generator: 6.408346176147461 LOSS_Discriminator: 0.07595481723546982\n",
            "ITERATION_NO.: 556 LOSS_Generator: 6.530491828918457 LOSS_Discriminator: 0.056685011833906174\n",
            "ITERATION_NO.: 557 LOSS_Generator: 6.829015731811523 LOSS_Discriminator: 0.1201990395784378\n",
            "ITERATION_NO.: 558 LOSS_Generator: 7.284632682800293 LOSS_Discriminator: 0.051581863313913345\n",
            "ITERATION_NO.: 559 LOSS_Generator: 7.757580757141113 LOSS_Discriminator: 0.040620945394039154\n",
            "ITERATION_NO.: 560 LOSS_Generator: 8.707321166992188 LOSS_Discriminator: 0.11058688163757324\n",
            "ITERATION_NO.: 561 LOSS_Generator: 8.164499282836914 LOSS_Discriminator: 0.0382111519575119\n",
            "ITERATION_NO.: 562 LOSS_Generator: 7.164600372314453 LOSS_Discriminator: 0.15125474333763123\n",
            "ITERATION_NO.: 563 LOSS_Generator: 6.372460842132568 LOSS_Discriminator: 0.07985468208789825\n",
            "ITERATION_NO.: 564 LOSS_Generator: 5.592283248901367 LOSS_Discriminator: 0.07849475741386414\n",
            "ITERATION_NO.: 565 LOSS_Generator: 5.917113780975342 LOSS_Discriminator: 0.07858766615390778\n",
            "ITERATION_NO.: 566 LOSS_Generator: 6.8142290115356445 LOSS_Discriminator: 0.06371955573558807\n",
            "ITERATION_NO.: 567 LOSS_Generator: 7.591475009918213 LOSS_Discriminator: 0.06256710737943649\n",
            "ITERATION_NO.: 568 LOSS_Generator: 9.053572654724121 LOSS_Discriminator: 0.029648276045918465\n",
            "ITERATION_NO.: 569 LOSS_Generator: 8.81936264038086 LOSS_Discriminator: 0.1198725625872612\n",
            "ITERATION_NO.: 570 LOSS_Generator: 8.770238876342773 LOSS_Discriminator: 0.12237128615379333\n",
            "ITERATION_NO.: 571 LOSS_Generator: 7.081367015838623 LOSS_Discriminator: 0.08105354011058807\n",
            "ITERATION_NO.: 572 LOSS_Generator: 5.408767223358154 LOSS_Discriminator: 0.047213394194841385\n",
            "ITERATION_NO.: 573 LOSS_Generator: 5.910168647766113 LOSS_Discriminator: 0.10712774842977524\n",
            "ITERATION_NO.: 574 LOSS_Generator: 7.389670372009277 LOSS_Discriminator: 0.09001842141151428\n",
            "ITERATION_NO.: 575 LOSS_Generator: 8.27872085571289 LOSS_Discriminator: 0.035410694777965546\n",
            "ITERATION_NO.: 576 LOSS_Generator: 8.87967300415039 LOSS_Discriminator: 0.0589795857667923\n",
            "ITERATION_NO.: 577 LOSS_Generator: 9.25745677947998 LOSS_Discriminator: 0.07755711674690247\n",
            "ITERATION_NO.: 578 LOSS_Generator: 8.440474510192871 LOSS_Discriminator: 0.17410647869110107\n",
            "ITERATION_NO.: 579 LOSS_Generator: 7.365057468414307 LOSS_Discriminator: 0.038356900215148926\n",
            "ITERATION_NO.: 580 LOSS_Generator: 6.659768104553223 LOSS_Discriminator: 0.06605231761932373\n",
            "ITERATION_NO.: 581 LOSS_Generator: 5.8535027503967285 LOSS_Discriminator: 0.09999831765890121\n",
            "ITERATION_NO.: 582 LOSS_Generator: 6.10072135925293 LOSS_Discriminator: 0.06598405539989471\n",
            "ITERATION_NO.: 583 LOSS_Generator: 7.121758460998535 LOSS_Discriminator: 0.062231775373220444\n",
            "ITERATION_NO.: 584 LOSS_Generator: 7.597836494445801 LOSS_Discriminator: 0.07368730753660202\n",
            "ITERATION_NO.: 585 LOSS_Generator: 8.293619155883789 LOSS_Discriminator: 0.06163721904158592\n",
            "ITERATION_NO.: 586 LOSS_Generator: 7.980523109436035 LOSS_Discriminator: 0.07042114436626434\n",
            "ITERATION_NO.: 587 LOSS_Generator: 7.6305766105651855 LOSS_Discriminator: 0.031687043607234955\n",
            "ITERATION_NO.: 588 LOSS_Generator: 7.724139213562012 LOSS_Discriminator: 0.04963262751698494\n",
            "ITERATION_NO.: 589 LOSS_Generator: 7.245928764343262 LOSS_Discriminator: 0.06339109688997269\n",
            "ITERATION_NO.: 590 LOSS_Generator: 6.933765888214111 LOSS_Discriminator: 0.141908198595047\n",
            "ITERATION_NO.: 591 LOSS_Generator: 7.230498790740967 LOSS_Discriminator: 0.052565742284059525\n",
            "ITERATION_NO.: 592 LOSS_Generator: 7.416166305541992 LOSS_Discriminator: 0.12296760082244873\n",
            "ITERATION_NO.: 593 LOSS_Generator: 7.582030773162842 LOSS_Discriminator: 0.07644335925579071\n",
            "ITERATION_NO.: 594 LOSS_Generator: 7.495716571807861 LOSS_Discriminator: 0.057920388877391815\n",
            "ITERATION_NO.: 595 LOSS_Generator: 7.240471363067627 LOSS_Discriminator: 0.13679298758506775\n",
            "ITERATION_NO.: 596 LOSS_Generator: 6.771205902099609 LOSS_Discriminator: 0.03918585181236267\n",
            "ITERATION_NO.: 597 LOSS_Generator: 7.7399983406066895 LOSS_Discriminator: 0.09324325621128082\n",
            "ITERATION_NO.: 598 LOSS_Generator: 7.45196533203125 LOSS_Discriminator: 0.028092162683606148\n",
            "ITERATION_NO.: 599 LOSS_Generator: 7.742453575134277 LOSS_Discriminator: 0.06902055442333221\n",
            "ITERATION_NO.: 600 LOSS_Generator: 8.0853853225708 LOSS_Discriminator: 0.07117399573326111\n",
            "EPOCH OVER: 49\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Image_no 1\n",
            "Image_no 2\n",
            "Image_no 3\n",
            "Image_no 4\n",
            "Image_no 5\n",
            "Image_no 6\n",
            "Image_no 7\n",
            "Image_no 8\n",
            "Image_no 9\n",
            "Image_no 10\n",
            "Image_no 11\n",
            "Image_no 12\n",
            "Image_no 13\n",
            "Image_no 14\n",
            "Image_no 15\n",
            "Image_no 16\n",
            "Image_no 17\n",
            "Image_no 18\n",
            "Image_no 19\n",
            "Image_no 20\n",
            "Image_no 21\n",
            "Image_no 22\n",
            "Image_no 23\n",
            "Image_no 24\n",
            "Image_no 25\n",
            "Image_no 26\n",
            "Image_no 27\n",
            "Image_no 28\n",
            "Image_no 29\n",
            "Image_no 30\n",
            "Image_no 31\n",
            "Image_no 32\n",
            "Image_no 33\n",
            "Image_no 34\n",
            "Image_no 35\n",
            "Image_no 36\n",
            "Image_no 37\n",
            "Image_no 38\n",
            "Image_no 39\n",
            "Image_no 40\n",
            "Image_no 41\n",
            "Image_no 42\n",
            "Image_no 43\n",
            "Image_no 44\n",
            "Image_no 45\n",
            "Image_no 46\n",
            "Image_no 47\n",
            "Image_no 48\n",
            "Image_no 49\n",
            "Image_no 50\n",
            "Image_no 51\n",
            "Image_no 52\n",
            "Image_no 53\n",
            "Image_no 54\n",
            "Image_no 55\n",
            "Image_no 56\n",
            "Image_no 57\n",
            "Image_no 58\n",
            "Image_no 59\n",
            "Image_no 60\n",
            "Image_no 61\n",
            "Image_no 62\n",
            "Image_no 63\n",
            "Image_no 64\n",
            "Image_no 65\n",
            "Image_no 66\n",
            "Image_no 67\n",
            "Image_no 68\n",
            "Image_no 69\n",
            "Image_no 70\n",
            "Image_no 71\n",
            "Image_no 72\n",
            "Image_no 73\n",
            "Image_no 74\n",
            "Image_no 75\n",
            "Image_no 76\n",
            "Image_no 77\n",
            "Image_no 78\n",
            "Image_no 79\n",
            "Image_no 80\n",
            "Image_no 81\n",
            "Image_no 82\n",
            "Image_no 83\n",
            "Image_no 84\n",
            "Image_no 85\n",
            "Image_no 86\n",
            "Image_no 87\n",
            "Image_no 88\n",
            "Image_no 89\n",
            "Image_no 90\n",
            "Image_no 91\n",
            "Image_no 92\n",
            "Image_no 93\n",
            "Image_no 94\n",
            "Image_no 95\n",
            "Image_no 96\n",
            "Image_no 97\n",
            "Image_no 98\n",
            "Image_no 99\n",
            "Image_no 100\n",
            "Image_no 101\n",
            "Image_no 102\n",
            "Image_no 103\n",
            "Image_no 104\n",
            "Image_no 105\n",
            "Image_no 106\n",
            "Image_no 107\n",
            "Image_no 108\n",
            "Image_no 109\n",
            "Image_no 110\n",
            "Image_no 111\n",
            "Image_no 112\n",
            "Image_no 113\n",
            "Image_no 114\n",
            "Image_no 115\n",
            "Image_no 116\n",
            "Image_no 117\n",
            "Image_no 118\n",
            "Image_no 119\n",
            "Image_no 120\n",
            "Image_no 121\n",
            "Image_no 122\n",
            "Image_no 123\n",
            "Image_no 124\n",
            "Image_no 125\n",
            "Image_no 126\n",
            "Image_no 127\n",
            "Image_no 128\n",
            "Image_no 129\n",
            "Image_no 130\n",
            "Image_no 131\n",
            "Image_no 132\n",
            "Image_no 133\n",
            "Image_no 134\n",
            "Image_no 135\n",
            "Image_no 136\n",
            "Image_no 137\n",
            "Image_no 138\n",
            "Image_no 139\n",
            "Image_no 140\n",
            "Image_no 141\n",
            "Image_no 142\n",
            "Image_no 143\n",
            "Image_no 144\n",
            "Image_no 145\n",
            "Image_no 146\n",
            "Image_no 147\n",
            "Image_no 148\n",
            "Image_no 149\n",
            "Image_no 150\n",
            "Image_no 151\n",
            "Image_no 152\n",
            "Image_no 153\n",
            "Image_no 154\n",
            "Image_no 155\n",
            "Image_no 156\n",
            "Image_no 157\n",
            "Image_no 158\n",
            "Image_no 159\n",
            "Image_no 160\n",
            "Image_no 161\n",
            "Image_no 162\n",
            "Image_no 163\n",
            "Image_no 164\n",
            "Image_no 165\n",
            "Image_no 166\n",
            "Image_no 167\n",
            "Image_no 168\n",
            "Image_no 169\n",
            "Image_no 170\n",
            "Image_no 171\n",
            "Image_no 172\n",
            "Image_no 173\n",
            "Image_no 174\n",
            "Image_no 175\n",
            "Image_no 176\n",
            "Image_no 177\n",
            "Image_no 178\n",
            "Image_no 179\n",
            "Image_no 180\n",
            "Image_no 181\n",
            "Image_no 182\n",
            "Image_no 183\n",
            "Image_no 184\n",
            "Image_no 185\n",
            "Image_no 186\n",
            "Image_no 187\n",
            "Image_no 188\n",
            "Image_no 189\n",
            "Image_no 190\n",
            "Image_no 191\n",
            "Image_no 192\n",
            "Image_no 193\n",
            "Image_no 194\n",
            "Image_no 195\n",
            "Image_no 196\n",
            "Image_no 197\n",
            "Image_no 198\n",
            "Image_no 199\n",
            "Image_no 200\n",
            "Image_no 201\n",
            "Image_no 202\n",
            "Image_no 203\n",
            "Image_no 204\n",
            "Image_no 205\n",
            "Image_no 206\n",
            "Image_no 207\n",
            "Image_no 208\n",
            "Image_no 209\n",
            "Image_no 210\n",
            "Image_no 211\n",
            "Image_no 212\n",
            "Image_no 213\n",
            "Image_no 214\n",
            "Image_no 215\n",
            "Image_no 216\n",
            "Image_no 217\n",
            "Image_no 218\n",
            "Image_no 219\n",
            "Image_no 220\n",
            "Image_no 221\n",
            "Image_no 222\n",
            "Image_no 223\n",
            "Image_no 224\n",
            "Image_no 225\n",
            "Image_no 226\n",
            "Image_no 227\n",
            "Image_no 228\n",
            "Image_no 229\n",
            "Image_no 230\n",
            "Image_no 231\n",
            "Image_no 232\n",
            "Image_no 233\n",
            "Image_no 234\n",
            "Image_no 235\n",
            "Image_no 236\n",
            "Image_no 237\n",
            "Image_no 238\n",
            "Image_no 239\n",
            "Image_no 240\n",
            "Image_no 241\n",
            "Image_no 242\n",
            "Image_no 243\n",
            "Image_no 244\n",
            "Image_no 245\n",
            "Image_no 246\n",
            "Image_no 247\n",
            "Image_no 248\n",
            "Image_no 249\n",
            "Image_no 250\n",
            "Image_no 251\n",
            "Image_no 252\n",
            "Image_no 253\n",
            "Image_no 254\n",
            "Image_no 255\n",
            "Image_no 256\n",
            "Image_no 257\n",
            "Image_no 258\n",
            "Image_no 259\n",
            "Image_no 260\n",
            "Image_no 261\n",
            "Image_no 262\n",
            "Image_no 263\n",
            "Image_no 264\n",
            "Image_no 265\n",
            "Image_no 266\n",
            "Image_no 267\n",
            "Image_no 268\n",
            "Image_no 269\n",
            "Image_no 270\n",
            "Image_no 271\n",
            "Image_no 272\n",
            "Image_no 273\n",
            "Image_no 274\n",
            "Image_no 275\n",
            "Image_no 276\n",
            "Image_no 277\n",
            "Image_no 278\n",
            "Image_no 279\n",
            "Image_no 280\n",
            "Image_no 281\n",
            "Image_no 282\n",
            "Image_no 283\n",
            "Image_no 284\n",
            "Image_no 285\n",
            "Image_no 286\n",
            "Image_no 287\n",
            "Image_no 288\n",
            "Image_no 289\n",
            "Image_no 290\n",
            "Image_no 291\n",
            "Image_no 292\n",
            "Image_no 293\n",
            "Image_no 294\n",
            "Image_no 295\n",
            "Image_no 296\n",
            "Image_no 297\n",
            "Image_no 298\n",
            "Image_no 299\n",
            "Image_no 300\n",
            "Image_no 301\n",
            "Image_no 302\n",
            "Image_no 303\n",
            "Image_no 304\n",
            "Image_no 305\n",
            "Image_no 306\n",
            "Image_no 307\n",
            "Image_no 308\n",
            "Image_no 309\n",
            "Image_no 310\n",
            "Image_no 311\n",
            "Image_no 312\n",
            "Image_no 313\n",
            "Image_no 314\n",
            "Image_no 315\n",
            "Image_no 316\n",
            "Image_no 317\n",
            "Image_no 318\n",
            "Image_no 319\n",
            "Image_no 320\n",
            "Image_no 321\n",
            "Image_no 322\n",
            "Image_no 323\n",
            "Image_no 324\n",
            "Image_no 325\n",
            "Image_no 326\n",
            "Image_no 327\n",
            "Image_no 328\n",
            "Image_no 329\n",
            "Image_no 330\n",
            "Image_no 331\n",
            "Image_no 332\n",
            "Image_no 333\n",
            "Image_no 334\n",
            "Image_no 335\n",
            "Image_no 336\n",
            "Image_no 337\n",
            "Image_no 338\n",
            "Image_no 339\n",
            "Image_no 340\n",
            "Image_no 341\n",
            "Image_no 342\n",
            "Image_no 343\n",
            "Image_no 344\n",
            "Image_no 345\n",
            "Image_no 346\n",
            "Image_no 347\n",
            "Image_no 348\n",
            "Image_no 349\n",
            "Image_no 350\n",
            "Image_no 351\n",
            "Image_no 352\n",
            "Image_no 353\n",
            "Image_no 354\n",
            "Image_no 355\n",
            "Image_no 356\n",
            "Image_no 357\n",
            "Image_no 358\n",
            "Image_no 359\n",
            "Image_no 360\n",
            "Image_no 361\n",
            "Image_no 362\n",
            "Image_no 363\n",
            "Image_no 364\n",
            "Image_no 365\n",
            "Image_no 366\n",
            "Image_no 367\n",
            "Image_no 368\n",
            "Image_no 369\n",
            "Image_no 370\n",
            "Image_no 371\n",
            "Image_no 372\n",
            "Image_no 373\n",
            "Image_no 374\n",
            "Image_no 375\n",
            "Image_no 376\n",
            "Image_no 377\n",
            "Image_no 378\n",
            "Image_no 379\n",
            "Image_no 380\n",
            "Image_no 381\n",
            "Image_no 382\n",
            "Image_no 383\n",
            "Image_no 384\n",
            "Image_no 385\n",
            "Image_no 386\n",
            "Image_no 387\n",
            "Image_no 388\n",
            "Image_no 389\n",
            "Image_no 390\n",
            "Image_no 391\n",
            "Image_no 392\n",
            "Image_no 393\n",
            "Image_no 394\n",
            "Image_no 395\n",
            "Image_no 396\n",
            "Image_no 397\n",
            "Image_no 398\n",
            "Image_no 399\n",
            "Image_no 400\n",
            "Image_no 401\n",
            "Image_no 402\n",
            "Image_no 403\n",
            "Image_no 404\n",
            "Image_no 405\n",
            "Image_no 406\n",
            "Image_no 407\n",
            "Image_no 408\n",
            "Image_no 409\n",
            "Image_no 410\n",
            "Image_no 411\n",
            "Image_no 412\n",
            "Image_no 413\n",
            "Image_no 414\n",
            "Image_no 415\n",
            "Image_no 416\n",
            "Image_no 417\n",
            "Image_no 418\n",
            "Image_no 419\n",
            "Image_no 420\n",
            "Image_no 421\n",
            "Image_no 422\n",
            "Image_no 423\n",
            "Image_no 424\n",
            "Image_no 425\n",
            "Image_no 426\n",
            "Image_no 427\n",
            "Image_no 428\n",
            "Image_no 429\n",
            "Image_no 430\n",
            "Image_no 431\n",
            "Image_no 432\n",
            "Image_no 433\n",
            "Image_no 434\n",
            "Image_no 435\n",
            "Image_no 436\n",
            "Image_no 437\n",
            "Image_no 438\n",
            "Image_no 439\n",
            "Image_no 440\n",
            "Image_no 441\n",
            "Image_no 442\n",
            "Image_no 443\n",
            "Image_no 444\n",
            "Image_no 445\n",
            "Image_no 446\n",
            "Image_no 447\n",
            "Image_no 448\n",
            "Image_no 449\n",
            "Image_no 450\n",
            "Image_no 451\n",
            "Image_no 452\n",
            "Image_no 453\n",
            "Image_no 454\n",
            "Image_no 455\n",
            "Image_no 456\n",
            "Image_no 457\n",
            "Image_no 458\n",
            "Image_no 459\n",
            "Image_no 460\n",
            "Image_no 461\n",
            "Image_no 462\n",
            "Image_no 463\n",
            "Image_no 464\n",
            "Image_no 465\n",
            "Image_no 466\n",
            "Image_no 467\n",
            "Image_no 468\n",
            "Image_no 469\n",
            "Image_no 470\n",
            "Image_no 471\n",
            "Image_no 472\n",
            "Image_no 473\n",
            "Image_no 474\n",
            "Image_no 475\n",
            "Image_no 476\n",
            "Image_no 477\n",
            "Image_no 478\n",
            "Image_no 479\n",
            "Image_no 480\n",
            "Image_no 481\n",
            "Image_no 482\n",
            "Image_no 483\n",
            "Image_no 484\n",
            "Image_no 485\n",
            "Image_no 486\n",
            "Image_no 487\n",
            "Image_no 488\n",
            "Image_no 489\n",
            "Image_no 490\n",
            "Image_no 491\n",
            "Image_no 492\n",
            "Image_no 493\n",
            "Image_no 494\n",
            "Image_no 495\n",
            "Image_no 496\n",
            "Image_no 497\n",
            "Image_no 498\n",
            "Image_no 499\n",
            "Image_no 500\n",
            "Image_no 501\n",
            "Image_no 502\n",
            "Image_no 503\n",
            "Image_no 504\n",
            "Image_no 505\n",
            "Image_no 506\n",
            "Image_no 507\n",
            "Image_no 508\n",
            "Image_no 509\n",
            "Image_no 510\n",
            "Image_no 511\n",
            "Image_no 512\n",
            "Image_no 513\n",
            "Image_no 514\n",
            "Image_no 515\n",
            "Image_no 516\n",
            "Image_no 517\n",
            "Image_no 518\n",
            "Image_no 519\n",
            "Image_no 520\n",
            "Image_no 521\n",
            "Image_no 522\n",
            "Image_no 523\n",
            "Image_no 524\n",
            "Image_no 525\n",
            "Image_no 526\n",
            "Image_no 527\n",
            "Image_no 528\n",
            "Image_no 529\n",
            "Image_no 530\n",
            "Image_no 531\n",
            "Image_no 532\n",
            "Image_no 533\n",
            "Image_no 534\n",
            "Image_no 535\n",
            "Image_no 536\n",
            "Image_no 537\n",
            "Image_no 538\n",
            "Image_no 539\n",
            "Image_no 540\n",
            "Image_no 541\n",
            "Image_no 542\n",
            "Image_no 543\n",
            "Image_no 544\n",
            "Image_no 545\n",
            "Image_no 546\n",
            "Image_no 547\n",
            "Image_no 548\n",
            "Image_no 549\n",
            "Image_no 550\n",
            "Image_no 551\n",
            "Image_no 552\n",
            "Image_no 553\n",
            "Image_no 554\n",
            "Image_no 555\n",
            "Image_no 556\n",
            "Image_no 557\n",
            "Image_no 558\n",
            "Image_no 559\n",
            "Image_no 560\n",
            "Image_no 561\n",
            "Image_no 562\n",
            "Image_no 563\n",
            "Image_no 564\n",
            "Image_no 565\n",
            "Image_no 566\n",
            "Image_no 567\n",
            "Image_no 568\n",
            "Image_no 569\n",
            "Image_no 570\n",
            "Image_no 571\n",
            "Image_no 572\n",
            "Image_no 573\n",
            "Image_no 574\n",
            "Image_no 575\n",
            "Image_no 576\n",
            "Image_no 577\n",
            "Image_no 578\n",
            "Image_no 579\n",
            "Image_no 580\n",
            "Image_no 581\n",
            "Image_no 582\n",
            "Image_no 583\n",
            "Image_no 584\n",
            "Image_no 585\n",
            "Image_no 586\n",
            "Image_no 587\n",
            "Image_no 588\n",
            "Image_no 589\n",
            "Image_no 590\n",
            "Image_no 591\n",
            "Image_no 592\n",
            "Image_no 593\n",
            "Image_no 594\n",
            "Image_no 595\n",
            "Image_no 596\n",
            "Image_no 597\n",
            "Image_no 598\n",
            "Image_no 599\n",
            "Image_no 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJa_mpGvAKH",
        "colab_type": "code",
        "outputId": "55665bfe-e615-4330-9548-c6ca77b4739b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "img=cv2.imread('testing100.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ffb214205f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAAD8CAYAAADuSp8SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydeXiU5bn/P+/MZGYyM5lM9j0BAiFA\n2MEACrLIKiBoERE3au2Gbd1O9ar2h7XHbtra1lqPWq1YtUVUBEXABRBBgkgwQAAhQEL2fZlJZjLb\n+/sj3g9vOFp7To/XodfxuS6uJMPMO+/7PPdzL9/7e9+Ppus6X42vxr/aMP1v38BX46vx3xlfCe5X\n419yfCW4X41/yfGV4H41/iXHV4L71fiXHF8J7lfjX3J8aYKrado8TdM+0TStQtO0e76s7/lq/N8c\n2peB42qaZgZOALOBGmA/sELX9aP/41/21fg/Ob4sjXsRUKHr+mld14PA34ArvqTv+mr8HxyWL+m6\nWUC14e8aoPjz3qxp2lfpu6/GZw5d17XPev3LEtwvHJqmfRP4pvztdruxWCx0dXURjUaxWCzouk5M\nTAw9PT2YTCYcDgdxcXG0tLTgcrkIBoNEIhHC4TCRSIT4+Hh8Ph8Wi4VwOIzFYiEYDBKNRtE0DU3T\ncDgcmM1mAoEAALquEwqFMJlMRCIRrFYrwWAQs9lMTEyM+j0ajRKJRDCbzTidToLBIJqmYTKZ6O7u\nxmq1EgqFsFgsmEwmwuEwNpuNSCRCTEwM3d3dmEwmNE0jHA6jaRoxMTF4PB56enro7u6WeSE/P5+W\nlhYikQiBQIDY2FhsNhs9PT2kpKTQ2tpKb28voVCIvLw8Ghsb8fv9JCQk0NHRoZ7XbDaj6zp2ux2/\n36+uD2A2mwGIRCLouo7VakXXdTWXMTExmM1m9Rwmkwmv14vVasVsNuP3+7Hb7ZhMJqLRqLpmOBwm\nGAyqtQuFQlitVhwOB16vFwCn04nX6yU2NlatUygUUu/XdR2z2Uxvb+/nys+X5SrUAjmGv7M/fU0N\nXdef1HV9gq7rEwB6e3v7PZimaSQnJwPgcDhwOp04HA78fj+aphGNRgmHwxh9dK/Xi6ZpSsB0XScx\nMRGr1YrdbicpKUkJIUBcXByAWmToW0iTyYSu62pR7XY7FouFmJgY5F7h3OLbbDZ1zUgkQiQSITY2\nlri4OCW0drsdh8OB1WpV14mJiaGjo4NgMIjNZgMgGo1is9nweDwkJiaqZ4mJiVHP1tPTg9PpZNCg\nQbS0tGA2m8nLy1ObA1ACpeu6mjMRULvdrja8CIl8twi2pmnqXqPRKL29vWqjulwu9b5oNKr+yZzJ\n/0UiEbU2oVBIrZtsIl3XMZlMBAIBdQ2LxYLNZiMUCv1dAfuyNO5+YIimaQPpE9hrgGv/3gd0XVda\nMRQKYbPZsNlsDBgwgGAwSEJCAk1NTWqRo9EoLpcLv9+vhMZqtRIOh9VCW61WPB4PsbGxuN1upb0a\nGhrw+/3qPTLhMkQz2u12QqEQXq+XtLQ0hg8fjqZpVFVV0djYqARbNKxRMHp6etQzaJpGd3c3uq6T\nl5eHrut0d3crYdB1nd7eXrUJjx49isfjwe12qwVNTU2lsLCQ6upq0tPT+drXvsbkyZMpKyvj+eef\nx+v1KiGTeRRBi0ajxMTEqLkJh8O43W56enrU84oVkLmUNTGbzcpCORwOpTBSUlJwOBy4XC6ampoI\nBAL4/f5+isTtdhMKhejt7SUajaoNA+DxeNT6yXy1tbVhNpuVNft740sRXF3Xw5qm3QpsA8zAM7qu\nl3/R52JiYrBY+m7p4osvZvny5cyfP5/Y2FgAqqqqePnll9mzZw+HDh2ivb1dmWZd14mNjSUUCikt\ntWjRIq6//noSExPx+XwkJCTQ09PDK6+8wnPPPUdHRwdWq1VNksViwWw243K58Pl8ymwBZGVl8dOf\n/pQhQ4Zw5MgR7r//fg4cOAD0aae4uDg6OjqUe6HrOl6vl9zcXKBP49x9993MnTsXt9tNe3s7Tz75\nJH/961/7CV00GsXhcNDV1YXJZCIpKUmZU5fLhdlsxmq1MnnyZBYvXsy8efOIRqM88cQT6v69Xq+a\nE9FcRvfE4XCwcOFCqqqqqKysJD4+ntraWtra2rBYLGr+RBOKRu7t7aWgoIA5c+Ywffp0Jk6cSDQa\npby8nIqKCp5//nn279+vTL/f7yc2NlZt8EgkgsvlYvny5cybN4+ioiI6OztpaWnho48+4qWXXuLk\nyZNKHozK5Pzxpfm4uq6/Cbz5j77f6XQSDocJBAK43W46Ojqora2ltbWVI0eOkJ+fz9ChQ1m5ciXv\nvfceNpuNhIQEtUtFa0SjUeLj43E4HAwbNgy/3099fT0xMTG0tbWRmZnJtGnT2Lt3L9XV1Zw9e7af\nZopGo0oArVarMmstLS3ouk51dTWvvfYaPp+PmJgY7HY7LS0tSkAikYgym7GxsTQ2NuLxeHjwwQdZ\nsWIF0CfE8fHxXHXVVWzevJnOzk4lHAB+vx+z2YzP5yMuLo4xY8YQDocpLi5m1KhRdHV1MWLECLZu\n3UpxcTE33ngjTz75JMOGDeOjjz4CzvmyNptN+a3p6enMmjWLmTNnMmrUKBobGzlz5gwej4f/+I//\n4NixY/T09CgtKRoyPj6e1tZWsrOzefjhh0lJSaGpqYkjR45w6tQpenp6WL58OS0tLRw6dEi5UjEx\nMfT29mIymbBarfT29nLLLbfw7W9/G7/fT0dHh3LhcnNzaWhoUJ/933IV/stDNCVAT08PJSUl1NfX\ns3fvXhwOB9/5zneIRCI0NTXR2NhIa2urEjhA+X/RaJTm5mY0TeO1117j6NGjHDhwALfbzYABA5gw\nYQLHjh3jyJEjKnATf1jMYjAYBPomPj09naamJu655x48Hg/vvPMO77//PoMHDyYcDnPq1CnsdjuB\nQEC5OWazmaSkJKXNf/WrX3HVVVfR09PDm2++iaZpzJgxA4vFQk5ODg0NDcTExKDrOj09PcTHxyst\nlZaWRm1tLQMGDKCtrY1Vq1Zx//3387vf/Y5Ro0YxatQo4uLiyM/Pp7y8XH0uHA4DKFfF4XBwzTXX\n8PWvf53Tp0/z6quv4vP5uPLKK2lra1OKIxKJKM0dDodV8OlwOBg9ejSFhYW89dZbPPfcc2RnZ9Pd\n3c3o0aNVLCFKRPxiQAW8ycnJzJw5kx07dvDCCy/Q09PD4sWLyc/P55FHHlHuVTAYJDY2VgWsnzUu\nGMEVP8zn86FpGhaLheTkZLKysvj+97/PgAEDiEQi7Nixg46ODiwWSz9TFAgECAaDWCwWNE3D7XZz\n9GhfviM3N5fe3l48Hg8Oh4P33nuPUCikAoa4uDj8fj+BQACr1Qr0mUWbzUZbWxujR49m3rx5VFZW\nsm7dOrq7uykqKuLgwYP9vl/cimAwSGdnJ5FIhOLiYqZNm0Y0GuUvf/kL+/bt48orryQcDvPxxx/T\n1NSk7kMWyu/343A41Ebt6emho6ODqqoq3n33XUaMGEFubi5XXHEFmZmZ7Nq1S92zruvqPmQj2Gw2\nYmJiyMrKoqOjgz/96U+0trZyxRVXUF9fz8MPP0x1dbUK2MTyiPBCn6tRUVFBU1MT+fn5XHbZZQwa\nNIjhw4dTWFjIrl27eOWVV5QCks2jaZpynaLRKDt27GDEiBFcdtll7NixA03TeOaZZ/jwww9JTk4m\nGo2quOPvjQtGcOPi4ujs7MThcBAIBEhLS2PVqlUsXbqU5ORkAoEA4XCYMWPGkJmZSXl5uTI/4szL\nJLndbiWQDQ0NLFmyhIEDBzJx4kRqampISEigqqqKaDSKyWQiFAopLSvBlslkoqenB03TVMR+5swZ\nRo8eTUZGBrW1tTQ3N9Pb26sifgnouru7FbpQU1NDbW0tCQkJJCcn893vfpe4uDhKSkp45pln+OST\nT7BYLAq1EDOtaRojR44kEAgwdOhQ9u/fz6pVq4iJiWHLli2MGTOGrKwsuru7+eCDD1RwaLRAZrOZ\nUChET08P0WiUkpIS5s2bx3e/+13MZjPHjx/n2Wef5eOPP1brcD5cBihYsampiT179rB8+XLy8/NJ\nS0sjEomwceNGfvSjH1FTU4PL5VJm3mQyKYQgGo3S2NjIk08+ybRp05g/fz6XXnopdrudY8eOYbVa\naWtrw2QyERsb+3ehMLiASDbd3d1omobNZlMBkpjMqqoqjh07xuHDhykuLuYnP/kJCQkJamEAtaPF\n3IvgdXR08Nhjj1FTU0N5eTkJCQlce+21pKWlERMTg81mw+/3K/MovrLZbCYSiTBw4ECmTZtGe3s7\nAPPnz8fhcLB7924ltIJ5ysbRNI34+Hjl4/7617+mubmZJUuWMHr0aBwOB2+88QaffPIJ0WiUYDBI\nd3d3P7NuMpk4fPgwGRkZOBwOiouLiY2N5aabbmLSpEmcOnWK3t5ePvzwQ3bt2kVDQwOZmZm4XC6F\nOQs2arfb0XWdN998k40bNzJx4kRGjRrFq6++yvbt29UaGDFZsQKRSITe3l4SEhKIRqMcP34ci8VC\nXl4ekUiE3/3ud9x5553U1dUp5EKsnqZpyopJUNna2sqxY8fIy8vjuuuuY+DAgWRmZpKamqqCx/NR\nns8aF4zG7e3txWw2097ejtlsprW1lXvuuYfCwkIFvXg8HvLz80lOTlZBkd1uV9hgbGwsgUCArq4u\nhb9mZWUB8PDDD+PxeLj77ruZMGEC8+fP5/nnn1fCbsQZRQihz81oaWnhwIED7N+/H6fTyV133UVS\nUhJnzpxRiyNaRSAxEZpwOMzJkydpbGwkNTWV06dPk5aWxrRp03jhhRfUPYv2N5r6Sy+9lJtvvpmx\nY8fi9/t56623qK6u5t/+7d8oKyvjzJkzbN68mX379jF69GhKS0uVptR1HYvFgtVqJRAIqGTIq6++\nSjgcZvny5YwZM4bdu3crATWZTAoVgT7hdblcBAIBOjs7mT9/PqtWrcLtdtPQ0IDX66WpqYn4+Hj8\nfr9SPoLGiCIyzqnFYiE/P5/W1lba2toYNmwYLpdL+fY9PT39FMfnjQtG4wrYnZCQQFxcnFqAqqoq\n3n77bd566y2mTp1KamoqjY2NtLe398P7NE1TGjA2NlZpvJ6eHnw+H3a7nbS0NEpLS+np6VG7XRZY\n4CiZcOgzl1lZWVgsFiU0AwYMIBwOU11drYTWKGySOZLvdDqdzJo1iyFDhnDq1CkOHTpENBpl1qxZ\nTJ48Wb3fmMQIBAJ4PB4GDhzIggULSEpKIjk5mYsuuojExETC4TBjx47loYce4sUXXyQ1NZX6+nqs\nVquCDsUKRKNRPB4Pw4cPZ/To0bS1tfG73/2O6upqxo0bh8Vi6YfdGrNuIsR2u53x48fzyCOPMGrU\nKJ599llWrFhBNBqlsLCQlpYW5cfKvIkS0HWdpKQkHA4HmZmZjBw5kpEjR/LUU0/xwgsv4HK5sFgs\nKrYJh8OEw+F/HVRBzEMoFGLgwIEsW7aMvLw8Dhw4QHV1NYsXL+ayyy6js7OTZ555hq6uLiwWi0rd\nCsIgExYMBhk6dChJSUlceeWVJCQk4PF42LZtG2PHjuXUqVPKpAUCAeWnOp1OAoGACjBiY2MZMmQI\nEyZMwGKxMH/+fD788EPa2tqUlpUFAhSwLy6IaOOYmBj8fj9Op1PhxG1tbQQCAfUcomFiY2OxWq3M\nnDkTu92Oz+ejoaGBIUOGEA6H+clPfsLBgwfx+Xx0d3erjOGkSZPYt28fgPJ1xZIlJCSQn5/Pli1b\nKCgowOVyEQ6HcTgcKqiTlLWsh8fjoaurC03TGD9+PMnJybz44ov84he/YOrUqcqHtVqtysqJ5QIU\nHtzU1KQCvaVLl1JcXMzGjRs5deqUcms0TVPYNaCCws8bF4zgQl80HggE8Pl8DBgwgNmzZzNr1izc\nbjcxMTE0NDTw05/+lG3btikuQzgcViZZriEg+9SpU5k7dy6RSITm5maOHTumcNCXXnqJqqqqfoC7\n+GXGnPmePXuYPn06K1eupLW1lYqKCjZv3qzMnwQfonV7e3uVBhe3ob6+Hk3TKCwspLCwEIvFwvvv\nv09dXR3x8fFqo4ngirbbv3+/8m3ffPNNPvjgA1pbWzl48CC6rjNw4ECFPkhKXCyGbCSHw6Hm5qKL\nLmLOnDlkZmZisVh49913sVgs2O12AAWHiYmXAEnTNCoqKqivryclJYV77rmHKVOmKHjPbDYTHx+v\nNpAxlSzzqWka7e3tBAIB7HY7BQUFLFy4UPn0brebzs5OhRYZ/e3PGl8KH/e/OjRN02NjYwkGg5hM\nJux2O4mJidx2222MHz+e5uZmKioq2LFjBx9//LFKLUoAJpMtQYhcKycnh8zMTKqrq7FarWRnZ5OX\nl8fJkyf5+OOP6erqwul0KtDdCMUYBbioqIiioiLa2tqorKykvr5eaRiZXOMkm81mPB4PXq8Xp9OJ\n2+1m9erVXHPNNXR3d/P666/z+OOPU1lZCfS5SSK04XBYmftJkyZhsVgYMWIEL7zwArm5uVRVVeH3\n+0lMTMRsNpOcnExFRYVKMIjvaDKZVFZRfMtx48YxZ84cvF4vZWVlfPTRR3R1dan5EysD/VPwuq6T\nkZHBD3/4Q2bOnElsbCxbtmzh6NGjbNiwga6uLkXk6e3tVWgPoNZE3CGTycS0adP45S9/SWJiIjt3\n7uSee+6hrq5OzZ+4bZ9ugM/M/V4wgiuCJ2YrJiZGQSsmk0kJtQRhTqcTn8+nSBqidXp6eoiLiyMa\njaq/jTir0W8TrSJJA/GRxYWQyRb/TaAuEQT5PzGxxny/BEbiw8rzSEoW+rSh8AUEEuvs7FQRfGpq\nqgo8e3t7FWZsMplUmla+WyArIeGIlpPNJRtanqOjowPogyElRSz3IViszIMgLtDnxghxSNwcm81G\nd3d3vwSG8fNwjv8hmlQgy66urn4MPGGjxcTE4PV6L3zBFewUzmkdIdsYo26ZELvdroTG5XLh9XoV\nKcZopgRQlyG72ZgKlYBENK5gsOffk6ZpxMbGKraVMU0sSILJZMJsNvdLkMhnJf0p/q+8V8y5zWaj\no6NDuQ+hUAin00lraysej4fu7u5+whcKhfD5fCQmJtLc3NyPOijfA6i0tVGzi6AD/cB+I61TUrVG\nF0KCaEn2dHV1KaGT9RFEQALO8wNfmV8h2Rj9YvmsIbi7sAX3c15XkypCIq8Z6YnnP4OYSVmY82Em\n4/XPh10kiJDvkajbiDYYr2dcGBF20b6ymEb2ldyvJFrEHTFuLvluERQxu3IvsvjyXvlpjOSNzyMb\nWHxXidyN82WcZ6Nwn3/fxt/lPfK7kV0m9yka/Pz1M96fcX5lPWQdLjgi+flDTJHFYiE2NlY56qIR\njZRFAJfLRWdnZ7+FMl7DCGQ7nU66u7vVpIgJkwkClIYQ3FNcDeEudHd3q2SDkEzgXHrW7/crIrqY\nVlk0+R3ORfsej4f29nbF9RV/u729XZlVl8ulNkcoFCIhIUF9j6ApNptNbSJxbwAFjcn8Se4/Pj4e\nXdcV2iHX8Pl8/fi4Mp+yecUlEwK/PJfD4QD6+CVGfkMwGMRqtap1kwBant+46Y0BpcPhUBpYrMdn\njQsGx5UJEh9NXhPfNzExEZvNhtPpJBKJKGhJfCwJpqBPcIUDKtc1m82K3G3UToDK6kiWR6Jz8bfF\n58rOzmbYsGFkZGQorDk1NVVpMIHWgsEgvb29pKWlKb/a4XAQExPDgAEDFAVS8GZBIj716VRlQVxc\nnOIPyLWFV+zxeEhNTVX8C9Ga58+BbDwjV1bTNDweDykpKbhcLtLS0vqtg/A1ZLMZs3ASEAreLgGy\ncW7lcxI3SDZU1kLeI8R6sUjCJgsEAkoGPm9cUK6CCIyYKpvNRnJyMitWrGDIkCGcPXuW1157jfr6\nekXEFoETTSraT3a32+0mMzMTn8/HxIkTVYCVlpZGdXU1ZWVldHR04PP5lAsi5t5ms6loecaMGfz8\n5z8nNTUVi8XC8ePH+fnPf87mzZvlGZQGk0WUe8jMzMTr9ZKfn09RURE+n48XX3xRYagSsDmdTrq6\nutRiiokVXzg+Ph6Px4Pf7yc3N5c777yTmJgYtm/fzhNPPKE4CULaTkpKoqWlRW3QjIwMkpKSaGxs\npK2tjYceeoiLLroITdN44403ePfddzl27JhKbxvNutvtVkJ4/fXXM2PGDLxeLz/+8Y9paGhQQixJ\noa6uLuLi4voF206nk9TUVAYOHEhcXBxOp5OGhgbeeecdtWF6e3uVm/WpJbmwXQVx+kWrdHV1kZOT\nw5w5c0hISODgwYPk5OSQlZVFTU0Nqampir4oplKAcDHNFouFSy65hNWrV5OTk6NSu1VVVWRlZWE2\nm7njjjsoLy8nGAwq8yaaTkpnLrnkEhwOBy+88AJ+v58rrrgCn89HeXm5yo5JWlTcDYnUCwoKmDFj\nBitWrCA7O5vExESCwSBTp05l7dq1lJeXK+EVv1NIJoIMCLrS1dVFKBTitttuY8aMGXR3dzNixAgy\nMjJYt26dYoNBHwzV2dmJruvEx8cTDAbJzc1l1apVTJ48mfLyco4cOcLmzZv51re+xa233kpFRQWH\nDx9W3y+aX1yk3t5ehg0bxty5c7nkkksUL+G+++4DoKamRrkHkoyQmrNoNMr48eO5/fbbmThxoopB\nQqEQf/3rX/nVr35Fb2+v4iLD/xKR/L86hPcpOzwzM5OpU6fi8/n493//dzo7O1Vmq6CggO7ubpUq\nNNZ7iYNvtVrJzc3llltuoaioiC1btvD+++8TCAQ4fPgwFouFZcuWUVNTQ2trq4qSdV2ns7NTBSkZ\nGRk0NjbS3NxMKBRix44dlJeXM3jwYIYMGYLValXEc/HbxB8W/3P8+PFkZ2f3I4fPmDGDrVu3EolE\nKC8v70cWknkQhls4HFYQ13333cfXvvY1Xn31VR588EG+9rWvqQSLPLtoXUkE2Gw2Fi1axLRp07jy\nyiupqKhg586d1NXVsWPHDl577TXGjx/P7t27VeWCBHFi2oU4v3z5coqKijh8+DA+n4+LLrqIhQsX\nsnHjRpWyF99UfGDB3BsaGjh48CCaprFhwwZ6e3tZunQp48aNY8qUKezbt08Jr9HX/qxxwQiu+FDi\n3wQCAaZMmcKAAQPYvn07wWCQ9PR0lixZQn19PevXrycuLk5pVtmp4r96vV48Hg+5ubls3ryZO++8\nU1XFnjp1ioyMDN544w2qq6uVNolEItjtdhXp+/1+GhoaqKysZMCAAZSWlqqNUlJSwrFjx0hISOhX\nASHuhtAtz5w5w5EjRzh58iQOh4Nrr70Wt9vN4cOHefvtt5XlkAJJ+V0qh7u7u8nNzUXTNO68806+\n8Y1v8Morr7Bp0ybsdjuDBg1SZHYRWHEzxB+2Wq184xvfIC4ujgMHDnDHHXdQUVHBiBEjMJvNVFZW\nUl5eTjQaJTk5WcFnYvoFvkpOTmbo0KEEAgH+/Oc/09TUxNSpU1mwYAEff/wxXq9X/YM+zoVURYuF\n2rx5M729vaxfvx6r1cqRI0f4wQ9+wM0330xVVRWtra3ExcUpOubnjQtGcAWXFZchLy+PcePGsXnz\nZoqLi0lNTWXs2LGkpaXx/PPP43A4lKmRaFgmWWh04jKUlpYya9YsCgsLlfkzmUysX79eBWHQB+FI\n/Vd+fj51dXWkpqZy9uxZzp49S2pqKgsWLGDEiBEcPHiQ0tJS4uLilF8tCyWaRgQgNTWVpKQkCgoK\nSElJIRwOc+LECQDlU3s8HoVBi19ot9uxWq00NDTgdDqZO3cuPT091NXVMXbsWK6//noikQj79u0j\nOTmZ6upqJagS7CUmJlJUVERubi5Hjhzh97//PT6fj9WrV3Pdddfx8ssv88c//pGMjAxqamr6lbGL\njy2YdTAYZO/evfztb3/j9ddfJyYmhuHDhzNixAj8fr/iLxtHOBxWmz0ajVJdXU1jYyOFhYU4HA5G\njhxJUlISJSUlnDx5Uq3nl0qy0TStEvACESCs6/oETdMSgXXAAKASuFrX9fYvupaQpyVCnzRpEnl5\necycOZOpU6cSCoVIS0vD6/XS0NCgfFrJqBlxSoGKjh8/zsGDB/n617+O2Wymrq6O2NhYWltbeeCB\nBzh58mS/BITAMYI4ZGdn09TUhK7rFBcXc/jwYZYuXUptbS1jxozh8OHDhMNh2tvbVZpZqpFtNhvx\n8fGkpaUxatQoLr744n5mXKyJ9EoIBoMqjSwbTiA8v9/PmDFjSE9PJxgMkp2dzdGjR+nt7aWyslJx\nZGUOQ6GQ0sBNTU0qALJYLBw5ckT1Zdi6dasisjc3NyvXC1AbQIj5EjBt3bqVyy+/nNtvv53hw4cz\nYcIEDhw4QENDQ79q62AwiMvlUoiJlM77/X7GjRvHfffdp/jSP/rRj/jwww/7lf1Ikubzxv+Exp2h\n63qL4e97gHd1Xf+F1tfs7h7g7i+6iARE8oDDhg3DbreTn5/P/v37qaur45NPPlE70m63KzaREX+U\nQM1ms+FyuThw4ABjx44lPj6e5ORk7rrrLvbs2aOCOKBfUaRcSwK4gQMHYrPZuPHGG5XPun79ehoa\nGtQiGwVF6IzhcBi/38/QoUMZMmQImqaxceNGmpubcbvdeL1evvnNb7J37142b95MMBjE4XCoNLZg\n1z6fj6KiIpYvX05dXR3JycmMHz+e3/zmN1RVVSmBEhhPonK5D03TOH36NHfddRcDBgzg/vvv5w9/\n+AM7d+7k1KlTAOTn51NdXY3f71f4uFgOeS6TyURGRgY5OTnMnTtXleq/8847/PGPf6SxsbFfUKbr\nupofub+MjAza29s5ffo0hw8fJicnh8TERGbOnMnevXsVfmu0Vp83vgxX4Qpg+qe/rwV28g8IrkT1\n0nBi6tSpNDQ0sH79eo4ePUpZWRm6rjN37lxqa2uVdgWUMy8BktvtJiEhgSlTppCRkcHf/vY3hg8f\nzvTp0xkyZAhbt24FUFzR1tbWfrsdYMCAAdx44400NTWRlZVFKBRi+PDhKinR2tqqaIGipSXhIMRo\nqSwOhUJUV1fzy1/+UuG3uq6zefNmWltbWbdu3X8qHZIsnMvlYvbs2aSkpPDwww9TUFDA8uXLufvu\nu3n++ecpLS2lpaVF+cUC6lg08qMAACAASURBVAtx3OFwcPHFF1NfX8+4ceN49913GTduHJWVlaSl\npakqaYvFQnZ2Nj6fT7HTpLGJ+P5jxozhtttuw2Qy8fDDD7N06VJsNhvNzc3KPTO6F/JPEATZGE88\n8QSnT5/m5MmTrFy5kqVLl7JhwwYqKyuVhfuiCoh/NgGhA29pmnZA62upBJCm63r9p783AGmf9UFN\n076padpHmqZ9BCgXIRKJMHjwYDIzM3nooYcU97SoqIhIJML69eupqKjA6/UqUyrp1kAgQG9vL52d\nnVx33XVcfvnlvPHGG7zyyivs37+fUChEenq6wjt1XVeNPQAF3wCkpaWRkJBAQkIC4XCY8vK+thDH\njh1j0KBBJCUlkZSURGdnp/LPhero8/lIS0vD6XSSkZFBQ0MDjY2N2Gw2Jk6cyJVXXsmAAQMUmUae\nQSyALHBvby/p6elMnTqVkpISdu3axa5du6iqqmLIkCE0NDTQ2dkJQH19vXIzpMxfOB933HEH3//+\n99mzZw8lJSVAH2oze/ZsBg0axMCBA0lJSaGlpYVAIEBcXJzCxqWAc8iQIdx6661YrVYeeOAB9u7d\ni9PpVGw142YTq2ckLMXFxSmsfPLkydTW1lJVVcXZs2fJyMhg8ODB/bKbwpD7vPHPatxLdF2v1TQt\nFXhb07Tjxv/UdV3/PB6CrutPAk9CXwLCbDaTmZmpzH9XVxd79uzB7/dz9OhR6uvryczMpLa2VmXY\nJGMmCy+az+VyMWbMGBISEhg7diwnT54kNzcXt9tNaWmpCoYk5WoM7FwulyLM7Nmzh0mTJjF8+HAS\nEhIwmUz85S9/YeLEicyfP58NGzawY8cOgsGgyoAJOUjM+okTJ/jggw+45ZZb+POf/8yRI0dISUlh\nwYIF+P1+jh8/rkyjQIFOp7Pfs5WVlbFw4UKGDRtGS0sLY8aMURZIEAwjZ0AyVDExMYwaNYqCggIK\nCwvJycmhoqKC9PR08vLy2LJlC8XFxezcuVOhGRIDSF8zv99PfHw811xzDcOGDWPz5s3MnDmTadOm\nceDAAdatW6c62YiwiuY1EpzEEuXk5JCbm0tJSQnf/OY3Fc9ZUA3R9F+qq6Dreu2nP5s0TdtAX3vR\nRk3TMnRdr9c0LQNo+keuJSwhi8VCfX0927ZtY+bMmVx22WXExcWxZ8+efkGQ+KISkEBfvlyA97fe\neovLLruM4uJirr32WrKystixYwcHDhzA4XDQ0dHRT0uImQ8Gg4TDYQ4ePEhLSwsOh4OtW7eSl5fH\nihUrmD59uiqzLi0tVfl/0TTCNbBaraqOSjZFamoqs2fPVuVEDz74IG+88YZykTweD01NTQpOEldo\n//79DBgwgGnTpqFpGu+9957qxCNVsRJMSX8HQBVcbt68mTlz5pCenk5+fj6dnZ1s2bKF3bt3U1VV\nRUtLi+KGOJ1OABUoS0ZS5mXKlCls2LCBn/zkJ5w+fVr5v4I3S/B5PhEoEokQFxfHsmXLuOKKKwgG\ng2RmZhIIBHjwwQc5duyYWo/zGX2fNf7bKV9N05yASdd176e/vw08AMwCWg3BWaKu6z/8gmvp4qNK\n8sBms5GUlMSgQYOora3lxIkTKsUL5zrGACofL7Q9SddmZmbS3d3NuHHjaG9vp7q6moaGBgWZSaBn\nMplU9gpQJl6Cl5SUFLKysqirq8Pn85GZmcnZs2dVft1ItIG+YCQuLo7MzEyqqqpU9a3ZbGbUqFEc\nPXpUQUPCFwZUYGUkgQvFUdM0heeePn1akVqMXFgjgUWqGtLT0+nq6iIrK4vOzk7V/kmQDzHjXq9X\nlRwJX0CyYACDBg0iNTW1nxKR+j6fz6eSFYLuhMNhRSKX5xNIb9GiRcTHx3Pq1CneeecdZbWE/Wbs\nBvQ/TmvUNG0QsOHTPy3Ai7quP6hpWhLwEpALVNEHh7V9wbV0qYE6nz4ogZexQ6CwtyR/Lv6YvEc0\nnLFyViZRFkT/tOxH/FOZ8IyMDDo6OkhLS6Ompob4+HiVlRIOgqRepYS+rq5OZaqE3ST34/F4iEQi\nKuqW55EN6vF4VLZOGsgJ50Japhp5EHFxcUqbGwMhIbEIti2dKMPhsEpkQF86WVAZ2SjCuhM+r1zX\n4/EoDnFPT4/iUBh9WSN7Tzax0RIKJ9dI0hGXQja6KBtJJBlhzX8JPq74N3JPQlgWjFW0gQDioonE\nFAmUIkGWROhwjukkm8PIlDISoI1dDY1AuGhBoQeKQMnkn8/rlfsTLfQZz6w+J/AYnOuZZQz4jEPu\nURIVgpmeP4zNOIxzajKZ1BzJcxqb+xmHWCU4h3YYKZTGezRyRiTzaLyPQCCg5kTWGehHS5VrGL/n\n8wT3gqE1yo6VJADQT2iN75OJk8hTenSJy2DkLIjgGoVQgprzSdFC3xMfSyZXJjIYDCoBk8jdqIWA\nftovPj5e3be4IcbKWCH0nN+FxthQWv72eDzKlEo6OTY2VgmQfLcMSZ+LADidTpxOp8KaRQFYrVZc\nLpeiQxqHzKPL5VKaUzS8bBzZ8BIMSrpZhlgi2dhGwZS/5T6M44u4CheMxhWh1fVzxY7ii0Kf/+d2\nu5XJMzKIRCulpqbS2dmpMmeCGITDYdUFW0D1QCDQr7QdzgmwCJ5R6I2aWaL286sDRNsIDCWUSCGt\nyKLL5pBuiFKpG41G6enpITExUQmouDSSSRK/Uvxy40YVzS5kFyGsiAY0Er2FIysBqdGvPH+Ivy3z\nLfxmo1CKKyACKWsnllIgM3Fd5Llko8M5K2W0AP8StEZj2x+ZSHlNfCKZQJkoEZxIJKIyNQ6Ho19X\nFcnbQ1+kbmQvSQ2ZLL78E20thZJi7sTlEE0nJtAYOYv5FN9a/Eih94mAi6YR7WVEJuQ7jJG2aD3x\nX2XDOp1OhefK3Eg3S7FCws/QdZ309HQSExPp7u7G5/PR0tJCSkqK0sTG9LmmaeooA/FR5Zp2u/0/\nuQuCKgD9iiBlvqFvI6SkpKheY+PGjaOzs5Oenh5OnTr1n4j+nzUuGMEV0yxmPzExkUsvvZRZs2ap\n7i2HDx9m3bp1CnMUU2iEw2SBhEyTmZmJzWajrq6OIUOGMH78eFpbWyktLaWmpqZfMCWCIvREQSdC\noRBDhw5lyZIlij3W0NDA9u3b+eSTT1SLI7/fr1yejo4O1Q8sJSVFdXi86qqrGDFiBJ2dnTz66KMc\nOHBAgfNirsWPFS0NfYudlpbG0qVLaW5uRtd1VVq+d+9e1V6/o6NDbXSZV4/HA/QJ9MyZM1m1ahVp\naWn4/X7eeOMNtmzZonp/mUwmlT0T8y7+vWQaFy5cyOzZs0lISADgzJkz/PrXv+bMmTMqaJU4QLSx\nXE82rtfrJSsrC7fbzRVXXEFBQQEffPABjz32mGrD9C8huGLOBAOdOHEiN998M+np6dTV1WE2m/nu\nd7/LiBEjuPfee1UDCekGA+eK9cTMy+KtXr2adevWsWrVKubMmUM4HObVV19ly5Yt7N27V/m7ohGM\nFauRSITp06fzi1/8gqysLJVrLykpobW1Vb23p6dH5eml/5nL5aK9vZ3k5GRuuukmfvCDHygNFRsb\ny8UXX8w111yjkiISZMXGxqo6NE3TVHnOmjVruPzyy2ltbeXs2bPcfffdnDp1Cq/XSzQapb29XQVr\nUn0g5UCpqamqYWB6ejrV1dVYLBYWLFjA1q1bVeM/0c7RaFRpUvHj09LS+H//7/+xdOlSenp6aG1t\nJTU1VaWQf/vb36r2U7Ie4lZJJbbFYiE+Pp7hw4dz/fXXc/jwYX7961+TlZXF0KFDlfX6onHBCK5k\nwuT3w4cP8/DDD3Ps2DE6OjoYNmwYK1aswOl0kpeXp4oVjR1gxPwKvjpz5kysViupqakkJiayd+9e\n6urqKC4u5rrrrlNnKlRVVfVDGgSI7+jooKioiLvvvpvDhw+zdu1aQqEQpaWl7N+/X7kL4veFw2FV\nQi4JjeTkZGbMmMFtt91GZ2cna9asYcqUKVx11VW43W5ycnKorKykvb1duTY9PT04HA5V6zZ79mxu\nvfVW1ZP3ySefVMJh5O0CahMLiUggw+985zssXbqUpKQkPvjgA1577TUGDRrE+PHjmTJlCl1dXcpF\nEbcGzp0PIUI+c+ZMTp48yeOPP05BQQHXX3+9StjIPBjdBdkAokHdbjcpKSksWbKEiy66iO7ubsaO\nHUtZWRmlpaUKrfmiBMQFI7gSRIim8Xq9qiX+kCFDcLlcPPXUU0yfPh2bzcaZM2dU9aq4GcYEQmFh\nId/+9rfZtGkTO3fuxGq1sm3bNgoLC7nqqqsIBoPs2LFDNVuDcyfIiDBKW6Ouri58Ph+vv/46mqap\n1vySoja2KpJsk8vloqurC4/Hw9SpU4lGo3z/+9+no6OD2267Dbvdzvvvv09jY6MSduMRVpK3z8jI\n4Nvf/jYTJ06kqqqKV199VdVoNTY2MnToUIqLi3nttdf6BXHSOM5kMjF69GgWL15MWloaW7du5Wc/\n+xllZWXExMSwYsUKvv3tb3P27Fl27typLJ8gOna7XWG8J06c4K233qKsrIyjR4+ydOlSYmJi+OUv\nf8l7772nNq/R3RJlIJtBeBgzZ87kxIkTlJSUUFlZybhx49i9ezd+v/8zYcDzxwUDhwnPQMjYEqFO\nmDABh8NBbm4umZmZvPHGG+qcA10/dxQSoCJ4TetrxpyTk0NaWhqnTp3i2LFjJCYmsnLlSpxOJ2vW\nrGHbtm3qABRAFSVKo41wOMzRo0fZs2cPCQkJ3HvvvSxbtoyMjAyi0Si1tbWqO7oIvQQ0EuScPXuW\n8vJy/H4/SUlJ/P73v2fw4MGcOHGC+++/n/379+Pz+XC73cq/F2aX2WxmzJgxDB06VLGs8vPzueGG\nG/j617/OqFGjmDt3riIlib9ujPotFgtFRUXqBB9J1owcOZLU1FTy8vLIyspixIgRJCcnK19WYgVJ\n8HR1dbF7926effZZXC4Xjz76KDNmzGDTpk08+eSTCimRYFMEV4K21NRU5bMPHjyYtWvX8vvf/x6T\nycSPfvQjVa1xPk3188YFo3GNmqqnpweXy0VCQgIzZsxQTdGuu+46Vq1aRUNDg6oOMILvgs9Kr9a3\n336bAQMGMHDgQE6fPk1TUxNVVVXs3buXTZs2qckVGElMovi3kgb+y1/+gtlsZsGCBQwaNIjOzk6e\nfvrpfsGLTLixbEf87PXr1zN48GAef/xxnE4nx44d49prr1VsNtGOorkFvnM6nao5dF1dHRs3buTs\n2bMsWrRIQW1Tp05l165dtLW19etDIIgJQGNjI5988olqlH3nnXfidrsJBoMMHjwYTdMUH9fYfMSI\nYwuEJgcBFhQUKCaetLyS89xkGLNhsiYOh4OcnBwyMjIYPnw4uq5z5MgR6uvrcblc6oBBIzb+WeOC\nEVwRAHEZHA4HH3/8MR0dHQpi+t73vsfs2bNVNalU0orgiaaQnlxPPPEENpuNo0ePkp2djdvt5r33\n3iM9PV21O5Ko3XgfRhhLfOhdu3ZRWlrKypUrWbJkCdu3b6e+vl4FcJJE6OrqAs4dcCcjOztbVQPL\neWsVFRXExcXR3t6uGsdJoaS4C16vl/fee4+nnnqKkSNHsmnTJkpKSoiPj2fUqFEUFxezdetWvF6v\nwoOFaCMJFZfLxalTpwiF+k77kXb2+fn5CrER4rucTmk8nkA69XR1damGf9XV1aSkpNDY2KiCw/Ph\nMBE84R9brVYmTZqE3W5n1qxZqjH12bNnOXjwYD9I74vGBecqiK8nvQ+E33r69GlKS0sZNGiQytIY\niwPhHAspPT2dUaNGUVtbS0tLC1arlZqaGmJiYpg0aRKJiYk4nU5MJhNOp1OB+kaCjyxaYmIiiYmJ\ntLe3c+zYMT744APy8/MVZ+D8RIRsHGmNWlxczE9/+lPmzp1LdXU1L774IhkZGdx0002qqZ3T6VR+\npQD4El2Hw2HefvttMjIy+PDDD5U5jUQiFBQUYLVaaWpqUs9g9CnFoghldPDgwaxZs4bS0lLC4TA5\nOTmqPejp06cVHCawoDDPjJt58ODB5OXl8fLLL9PR0cHs2bNVYxb5buP8ydy4XC5uuOEGVaW8fft2\nfvWrXxEOh5k7d67KIEoWTuKVzxsXjMYVLFTTNFJSUkhMTGTUqFHs2bMHr9ermoOkpaXhcDj+09lg\ncC4psHDhQm644QZOnDihtDP0JTkSExPJyclRkyzdHVtbW1VUbKRKSpl7ZWUlJpOJ8ePH09bWRmNj\nIz6fr19Zu3xGWsuLUA0fPpxwOMxDDz1EJBLh6quvVmU5on2lwZ1oTrmHQ4cOMW7cOMaMGaNall5+\n+eVkZWUxb948paEF0xb3ybiRamtrsdlstLe3M3nyZFwuFwsXLlRK4OjRo+qETmM5lASpJpOJ4cOH\nc/nll1NYWMjOnTtVgictLa3f5jEmcYy1Y3l5eSxatIjCwkJ2795NeXk5xcXFhEIhNm3aRGVlJdFo\nVCWZpBPm540LRnAl8yXCt2zZMkpLS9WOX7lyJddddx0bNmxQRzEZ+QRwLqqXKH3VqlU8//zznDlz\nhrFjx6JpGiNGjOCjjz5SBz9L1CxD8uyCXS5YsICBAweybt06pk2bxiWXXMLDDz9MW1tbP/aTkXwi\n/WkTEhLo7OxUKcyioiImTJhAMBikrKysX0Dn9/tpaekr3ZOeBgkJCTQ0NHDkyBGmTp3KJZdcogSr\nqKgIu93O2rVr2bNnj+JSAKokXFhihw4dYtOmTaxcuZI777yTSCSikhebNm3ikUceUWX6gPLPRdNK\nb+HvfOc72Gw2hg0bRkJCAi6Xi2effValtI3pbCMHQXz4xsZGxYK7+OKL2bdvH6+++iptbW1EIpF+\nzbC/qMr3guEqGFO+drud22+/nWXLlqmGdUlJSZw8eVJhqsbASoBuWayBAwcyevRoBg8erGh6AwYM\noLW1lebmZnbs2EFtba0K5iyWc63sRXuKvzxz5kzuvfdeUlJS6OzsZOvWrTzyyCMKvTCaQyOPQZAG\n6DupZ82aNQwdOhS/309JSQk//vGPOXr0aD9WmQRGQp6R7wyHw1x99dUsWbJEnQUm9XhSaCm+rM1m\nUzV0UkDp8/kYNGgQ3/rWtxg7dixWq5W6ujrWr1/P66+/rmA46GvGJ5ZEnk/X+xo7r169msLCQlXa\ns337dl544QXa2trUegD94g5p6hcbG0tRUZE6O66uro66ujp0XVflUeLXn0fCubBpjfJwwkF1u92M\nGzeOAZ+WcNfV1XHmzBnq6upU8Z5xdwufIBwOKzKOkHUkWyXmRxAMI1YoWlPcCmOXxaFDh5Kbm0t1\ndTXHjh3rR2gx8glEAGVDyCnwgUCAwsJCRo4cSVNTk2r5JC2buru7VZpUNo/UXRkZcx0dHQpp6O7u\nJhQKkZSURFtbm3I3pLOkLL40+JDrSNTf1dWlMFtj9YJsAmO3RdHmQoAym80qq2ckFxlpnDI/4isn\nJSWpYk7Beo28DeFqyGdAHTh4YQsunKMnyqTJRAH9Ivzze0sZ+biS6oRzwmjMBInwGonmQsyGcxCO\nVEdIOli0qjGTJFrBGCjKwgn7Kj4+XkXLckq4uA7CLxYqoMfjUYcUyuKGw2Hi4+Pp6OhQwiAbQoYI\npJEHYKw+MJlMCmoSE+zxePqRmMSnFTKTz+dTFSXyXMaGzpLcCIVCyl2SNTD+v7EmTuZbXpfNKVk+\nI34s8/8vIbif/t6PWiea4x9JA55/DSPl0Hjd82mMn/f5f2YYSdFGovVnffdnfZ/xfXKN8////M8Z\nNaDxGvK78RqfBfCfT9H8R5/xnxlfdI0LntYo2SJN01S3RjG50vHPbDarKFy0pDTSkKSFaAc5mVvc\nBjFfghpIy8uampp+ncsFHhIhk3N+hRgj7fFFcKTk2hjNi4shwYpoIeFQSCNlQLWSEm0mro24QBKp\nG4sgxXWIjY1Vmb/zewLLdSWbJiZatJ4EhcZyGSOiYkyoiPWTexDtaKy0Fi0pfr5YFNG0AuMZcXOx\nUhkZGTQ1NSmescz/3xsXjOAagywxRyIIIrSyMEb/0li7JOZJXI1QKERzc7PyFZOTk+nt7VX8A+NJ\nL3CuqbHwbKVfq/iPMpkC7Is7YAwSz3ct5LpwLuiS75BsmbE6A875ePK6cSMIZCY8Wcn0GecPUPCY\n+KuSSZMNJ21ExSWQjW20DCKA8qzGhIRcQ+bc6MKdP6ehUEhteklwmEwmUlJS0DSNzs5OtUZfVJYu\n4wsFV9O0Z4CFQJOu60WfvvaZ/cG0vpX6HbAA6AFu0nW99B+5EanhF+0g7Y6EOG1sIy8aUGCWT+9J\nZaZEgKQKVnxlQSiMFQQS5YqmamtrUxNuMp078l6OZ5owYQJz584lHO47/fydd97pV6kgvAfZgFIa\nEwr1tcKXJIv4nMLuEq0EKL/XKIgOh0NBdyJQxs4xYqXEtZKfgg+L1ZJNIRkzsU5yz1LBIfMpAi7z\nKjGBw+HAbO7rTu7z+aipqcFsNqvGd3IP4u+KHx4IBEhMTMThcJCamkoo1Nf3V5SABN5iTf/bggs8\nC/wBeM7w2uf1B5sPDPn0XzHw+Kc/v3DIxMqCi9AtW7aMlStXMnbsWEKhEKdOneLRRx9l27Zt/cqy\nI5EI7e3tqq5K2PlOp5PBgwdz1113kZiYyK5duygrK0PT+nq0SsZJIuqEhARCoZDq2ijR+OTJk7ny\nyiuZN2+egowee+wxtm3bpjZGV1dXP00rAU1aWhq33367agkVjUapqqrizJkz/PWvf6Wqqory8vJ+\nwZoIlZQaLViwgGuuuYbs7Gy6u7vp7Oxk7969qleEmG3R/mI9ZBOPHDmS5cuXM3r0aBISEhQTb8uW\nLfzpT39SJHDxg8U9gXP9KoTMvnr1aoYNG0ZVVRWa1scXfuSRR/j444/VfIXDYVXMKUFeTEwM06ZN\nY8aMGUyfPp38/Hxqamp49NFHqa2t5dChQ6qCWyzu540vTPnqur4LOL+8/Ar6+oLx6c8lhtef0/tG\nCeDR+pqCfOGIRCLKjxTNOG3aNH76059SUFCgML+ioiJuvvlmVS0q2kp8Y9HG3d3djB49mltuuYVn\nn32WKVOmkJCQwPLly0lLSyM7O5tJkyYpYonguHJ+A/SZ9kGDBhEXF8fy5ctZvHixKuHet28ff/zj\nH4lGo6ooUtM0ddaDsa3S5MmTWbJkCTExMezcuZOOjg4KCgqYM2cO//Zv/6aIREYXQVAB8T1nzJih\nTpkUrb1ixQpuueUWBg4cqATMSMYXH9tmszFlyhRFr3z33Xd59tlnOXjwIJMnT1aujbH+zAhxSfwx\ndepUXnzxRW644Qa8Xi+PPPIIt99+O21tbeo6sh5iKUUBRaNRbrvtNp555hluvfVWsrOzOXDgACaT\nidWrV9Pb20tXV5ciHn3RGRD/XR/38/qDZQHVhvfVfPpaPecNra/XmPQb6wdPiXmaP38+0WiUH//4\nx+zdu5dVq1axYsUKjh8/3k/D2Gw21VjD4XCQkJBAXl4eTz31FB6Ph4MHD/KHP/yBffv2MW7cOKZ/\n2vyupqZGBWrSw1X86sTERFVhcc8997B48WJaW1vZtm0bFRUV2O121cZJAjQ4d6aYmHThPlRXV/Pc\nc8/x5ptvctNNN3Httddy5swZfvGLX3D8+HHl68K5Kl8x916vlyNHjvDhhx+ye/duenp6yMnJ4de/\n/jVDhgxRWLAIjtGvFQ1YUlJCS0sLO3fupLa2lrFjxzJv3jwVKBpdHGNVrtHqLFq0iKFDh/Loo4/y\n+OOP09XVRWFhIZmZmaSmpvLmm29SW1vbD+oSxlpKSopK8X73u9+lq6uLkydPkpqayqJFi1Qhq5Rk\niU/8eeOfDs50/fP7g33B5/r1DpNoVkjQ8fHxNDY2AuD1eqmpqVHa5s033+xX8St5dvlptVqZNWsW\nGRkZVFdX85Of/IQPP/yQxMRE2traeOWVVxg/fjyvv/46vb29eDwehRNL5C3w0RVXXMG3vvUtotEo\nTzzxBHv27FFEnvnz5/PUU0/167QouKn42JqmcfDgQX7zm99QVFTEY489xsiRIzlz5gy//e1v2bVr\nl0q1ivAbs3fQt6m3bdtGXFyc4t4WFxfjcDjYuHEjlZWV/c6MECTAZDIpNtqhQ4dITExkwYIF5Obm\nMm/ePNxuN3/4wx/Uphcurzy7uEAiSNLadf/+/QAsX76cm266ifT0dH77299SW1urrqXrOs3Nzf34\nC+vWraO6upoDBw4wa9YsdZCJuGqCXAjx6csQ3M/rD1YL5Bjel/3pa184JGoVjdfd3c2TTz7JxRdf\nzMqVK5k8eTKLFy/m/fffp6ysTAmymCboI7cIPCRnNmRkZPDII4/w2GOPMWPGDFwuF7/61a944okn\nsFqtJCUlqfb9YpYlbVtcXMyaNWtISUnhyJEj+Hw+brvtNtxuN/X19VRUVKgzImQTCSwmhPj29nZ0\nXeeaa67hlltuQdM0RQ8ULSNBqKAlxgZyRuz3m9/8JrNnz1aB4lNPPcXTTz+tqo8lqJH3C8zW3d2N\nx+OhoKCAZcuWkZmZSU9PD+Xl5ezdu1edgWHM9Ml3y714vV6ef/55Ro8ezaxZs8jNzeXaa68lIyOD\nhx56iLVr1+Lz+RRZXKA4+XxnZydlZWXU1tbyne98h6lTp6JpGtnZ2dTV1bF582ZOnz7dz235e+O/\nS2vcBNz46e83AhsNr9+g9Y1JQKfBpfi7QxpsSNYoGo3S2NjIhg0bmD59OqtXr6auro577rlHaTjB\nCGXhBTEIhULs37+fRx99VJ2hcMcdd3DJJZdQWFhIfn6+ao3U3d2tzKz4ZtI7bNGiRWRkZLBhwwb2\n7t3LoEGDePnll1m7dq0qhTm/hbyRAhgXF0dKSgpTpkzh6quvVpajs7OTnTt30tDQ0K+ngBHjFI3r\ndruVv/zxxx9TUlKiqkCkb67Mh7gHovklSBMUIyUlBa/Xy2uvvcbVV1/N6dOnuf/++0lPT1ffaczY\nyfPIvJSWlnLfffcx+Owb+AAAIABJREFUefJkxb147rnnWLt2rUozG1EeQUHk+pLBtFgsbN68mXC4\nrwF1VlYWDzzwACNHjuxXevT3xj8Ch/2VvkbNyZqm1QBrgF8AL2madjOf9gf79O1v0geFVdAHh636\nouvLEJ9IzJV0sxk5ciRms5mGhgZycnIYNGgQ+/btw+Px9IOPotG+BhqS8z948CCnT5/G4/FQVFTE\nE088wZw5c0hOTuaaa64hNjaW5557TvVCaG1tVZMqKc/MzEwee+wxnn76aebMmcPu3bsJh8Pce++9\naJrGjh07FEyUmJioyC0Wi0WVyg8dOpSxY8fy29/+lpMnT3LXXXdRUFCAruu0traqs3oFhoJzbUJl\nM0YiESorK3n66ad56aWXsFgs3HvvvarmS3xD4WCIuyLaOhgMUldXxyuvvMJTTz2lzv2tra1l5MiR\n/dqaGoky4sIYuSHDhg1TwfKhQ4coLCwkOzubioqKfoIfjUZpaWnBbrfT2NiIxWKhrq5OYdAfffQR\nb731Fo2Njdxyyy3ccsstjBw5kvfff1+x6v4pwdV1fcXn/Nesz3ivDqz+omt+1hCtIrjn0KFDGT58\nOEuXLmX9+vUcPHiQNWvWcOWVV6oaLoGejGXVIkgWi0UFTw899BCHDx/G7/czf/58vva1rxEIBHj7\n7bcVS0wmS7RUMBjk7NmzfPTRR8THx/Pyyy+TmZnJ9773PZYsWUJJSQkffPBBP3MsSIcUWk6ePJmr\nrrqKPXv28MorrxAbG0tHRwcPPvgg119/PS+88EK/o6EkIDFm2WJjY1VZvAD1ixcvZs6cObS0tNDR\n0aEaoMC5s3ilqZ0QjQCV8LBYLEyYMIEVK1bg9Xrp6OhA13XV2A7oF6hKwLtw4ULuvvtumpub+fnP\nf87gwYNZvHgxo0aN4uzZs/2O+5LPSQYNIDc3l+PHjyu/u7GxkYqKCsrKylQPDCkylX4WnzcumMyZ\nBDRyXqzP52PlypXY7Xb2799PUlIS0JeoMDa9MEbPkt6UwGjmzJkMHDiQl156iVAoxKFDhxSkJcws\nyaKJthVtIQ2lf/jDH3L8+HEqKytZsGABBQUFNDc3K81ls9lU/VYgEMDn8xEfH69+Tp06lYkTJzJ0\n6FC2bdtGTk4OsbGxeDwe0tLSFEIhKIkxwLRYLKSnpzNjxgwSEhKorKwkPz+f5cuX4/V6efLJJ6mt\nrVXnRsA5XoYgMyKM+fn5amMsXryYK664ApfLxd/+9jfFLT4/4ydoi/RMmzVrFh0dHfzsZz8D+sqR\njhw5Qnt7u3IDZOPDOfKTruvExcWRnJzMsmXLcLvdak3Hjh3LHXfcgc/nY/v27cpifFYvM+O4oAQX\nIDk5mUgkQl1dHSdOnGDMmDF861vfUmB2eXm50gpiXkWjCFAuByVbLBYmT57M448/zokTJ9A0jUsv\nvZRPPvmEp59+mqNHjyqtBn0+mMvlUm3oA4EASUlJLFu2TJ0cU1paytq1a3n//feVZjRijuJqRKN9\nJ9RMmDCBa6+9llWrVnH99derrjIffPCB0ohGE69pmsr2+Xw+GhoaGDZsGIsXL1alOSUlJdx3332c\nPXtWaVDxLwXWAlT9m9Pp5NZbb6W9vR23280ll1xCb28vv/zlL9m4caPCz4VXIesh92d0Y8LhMFdd\ndRXhcJiysjJleYyZMok7xAokJSWhaRqDBw/mjjvuUEkUYzn9I488wsGDBxWC8UVchQuKHSb9uiS4\niUajrFmzhq9//ev4fD7eeustHnjgAcUxkIyMETD3eDyq6HLatGksWrSIGTNmEB8fT1dXF++99x5/\n+tOfKCsrU1wEo6mXMm7pz5Wdnc2sWbOIRqMcPXqUo0eP0tLSoibWSIcUAZRskvCAJ0yYwI033khG\nRgbNzc2cOHGCv/zlL5w9e1bBUJJOFRdBgjOn08lll13Gj370I7xeL08//TQfffRRv/b1xlKd81sm\nxcbGMnToUB588EFVdvTiiy+yfft2Dh06pBrjSXpX0sQyJJsn3YVGjRrF+PHjOXToEPv27aOsrEy9\n3xgYRqNRdY6FYN4WS98Rtfn5+YwYMYK8vDwOHz7MK6+8woEDB/5Tk79PCeoXNq3RyP2UB5YTbgRW\n8fl8imgt+W/JMgnhRNKkRrKI2+1Wfp8woQRCEg0pJeWiAVJSUhTZ2mw2093djdPpVJP6/6k78/As\nq2vt/55MZE7ezDOEIQECBBCQSUYHxAkRWqxatFrQ9rOiVqEeaz21tXqsVaGtVYutVlG0KKOKMiMQ\nQBAJU8KQAJnnhMzT+/0R7s1+U1FPz3d9F+e5Li7yTs+49tpr3ete91aVzdbMtcXh5IHkqcLCwsxx\nlUwKIxWbS1wNxbsS0PDyuiCybDO9zt87wxGQGiNgElz7HAUdSu7f7XYbgQ4ZnMWDNZ5cXlwEJcFd\nItbI2AQFKuewk7uIiAiz77q6OgIDAwkODjbH1D3oXsW85A1XFynDkVEp2dJDsFVj4EIGLvVGTTMa\n6ZouIyIiPEg8gEfJWPoGKrXabC9bgE2kEulzCYvtzuLXuXcvnYrx1d7ebs5VXlrHsYsXOl9l9Tpv\nVbfs/jAdV5/pmLbItbordEzb+PSvO8HHJu7bZenurDTlB917xlQQkayVBqcQIJt1JmKSRdG8tA33\n/P8AHp7DvoG6oTIiG7yXd7Wvxzbu7g9CD87+zP6O3eGghy1Glg0ZwYUHqgdvx4n6vPvx9Xf343Z/\n3V3dW1U9W/nQJprr/ugc7H3q7+6bfe7dnou5Zt1n23Dta7RDHfs4+q3OV/eqOzHeZrXZDbAXM9xL\nJjlTqVG8UXWhKu5ta2sz05Q8mS0VL/UXhQz2w1LSogdqa7oqprT7xXx8fEwflqAZ24gVi6lYoodk\nDzZbBkmVKMBckzyrzYoDzAyj+NSmLgqtkHey22p07op5pQgEGA8uD23HxXalywb97dKxQholv0It\n9GxknLo3cKHxU2QfeVGFY3bLkBhkKiBpZvym7ZIRBAEMDmpP535+fkRFRRkvrERCWa+mVOhKlBTL\n6cEGBAQYcN0WrdANE0Cv1+rp1yo0itf0YBVKqDggI1Z4IW+ueFQGYocxdieCBqOdFGkQw4WyreJR\n7cOWVpVBwoVijGYIhSBKwqR2I2PSQBLN0a7gaTCLgOQ4F8jrNrFc071dbdM9ER6s69V16RkpXxAM\nquWqvm27ZEIFu51EMZK/vz/R0dGG9ZWWlmZ4qPJ0dmPd+X2ZmFiBf0BAAElJSfj6+lJcXExTUxP+\n/v6cO3fOPLzGxkZcLpdJdOTF4+LiDJk8NjaWfv364eXlZbp1a2pqPJoBg4KCDOFdRQB50OjoaBIS\nEnC5XAQEBHD48GGznGhtba2JOf39/Q1rS95Oxu9yuUhJSaGgoMCUxXUcuKBgI48vD6fkSShBamoq\n06dPJyQkhAMHDvD555+bdiIZZ1hYmBFZDg4OBroGRkhICPHx8YSEhNC/f39qamrYtm2bQVuUrIri\n2V0o2tfXl/j4eIYOHUpYWBhtbW2cOHGCL774wsTTyjsu+VBB04ft0eLj45k6dSre3t5cccUVXH31\n1dTW1vLqq6/y/vvvU1xcbAzDx8eHyMhI2tsvrGbu5eVFTU0Nw4cP58YbbyQ+Pp729nY+/fRTNm7c\naNhoulnKZhWvdXZ2kpCQwKxZs0hPTycjI8Os69vU1MSePXt45JFHyMnJMbGZMFxN3/Jks2fPZt68\neURFRZlWpJycHP785z+zYcMGM+DkQZubm03HgEjlY8aM4cknnyQyMpKjR4+yZMkS48FUlBF2KgQF\nMAUSseYyMzMpLCxk//79jBw5kuuuu46srCyDtHh5eRnj04AR8SUoKIi7776b++67z4Q4fn5+PP/8\n8yxbtswMKN1PzWCaCSsqKpgyZQr/+Z//SXp6Oj4+PlRVVfH666/jOA67d+/2GIgX2y4Zw1Wsp2ZB\nZeBaFXHChAnGyObNm0dDQwPZ2dns2rXLTNeKixWDhYaGMm3aNB566CH69u1rGh+HDh0KwOrVqz0y\nYcXJmjpTUlKYN28el19+OQUFBZw6dYpNmzZRUlLCvHnzGDp0qIlDNa0KAbCbK9PT03nsscdoaGhg\n2bJl1NXV0bdvX6666iquu+469u/fj7e3twe5RA9fmXdcXBzz58+no6ODzZs38+WXX5KQkEBiYiJH\njhwhJCSEEydOmNBGRqj74uvrS3JyMosWLSIiIoItW7Zw9uxZ9u/fz4033sioUaPYsmWLEbDTYJbH\n1vX95Cc/4Uc/+pEhMpWUlHDLLbcYlp3QFQ0odTQILZo5cybPPPMMcXFxlJSU0NnZSUREBLm5uZSU\nlJjn8G0dEJeM4Qq3E+1Psj/f+973uOaaa2htbWXp0qX069ePwYMHs3v3brNCoy0XJMhHwiA///nP\nGTVqFC0tLWzevJlNmzYxe/ZsM43JUFQq1kj39vYmPDzclDV/8Ytf4OfnR//+/Zk/fz4hISHs3LmT\niooKg1EqFtR6DFrXITU1FV9fX1asWMEf//hHXC4Xc+bMMfGk5DpV09cDE6k6Li6OpKQkYmJiWLVq\nFUuXLmXIkCEMHTqUrKwsWltbyc/PNwmQsGT11LW3tzN06FCWLFlCREQES5cu5f333+f48eP88Ic/\npFevXiYkEoLS3NxsIEZRFBMTE5k1axYRERG8/PLL+Pv7M3bsWNavX8+6deuMh7V1FWw0w8vLi1Gj\nRhESEsKWLVsoKCjgpptuoq6ujpMnT3rwoW2m3Ndtl4zhqldMhuN2d60+M2rUKCIjI8nOzqawsJAx\nY8Zw9OhRiouLTeJiL1rS2NhoyMvDhw9nyJAhnD59miVLlrB69WrTTzZ37lxqamr4+OOPjccWvqms\n/8yZM2btiF//+tfs37+fu+66i9TUVD799FMef/xx0x0ryEfJpR5AS0sLBQUFuN1u0tPTyczMJD4+\nnpkzZ9LZ2cm7775LXV2dKYTABYhJzDDF3P7+/kRFRZGRkcFNN93E9u3bOX78OHBhAUJBUnZHtNvt\nZsqUKQwYMIC//vWvvPLKK/Tp04dFixZx1VVXkZWVxUcffWQwZsCjgqeignR609PTue222wgICGDb\ntm089dRTZvGTwMBAI8lkQ4e6T2fOnOH48ePU1dUxduxYfHx8+NOf/sSRI0eMExEf5Zu2S8Zw7RGq\nDDUhIcG00DiOw9y5c6mrq2Pp0qXmIas1REUHPbjw8HCGDBmCj48Pa9as4eOPP6a0tJSOjg4GDBjA\n4MGDzVoNypjVaWojBatWrSIyMpJFixZx9dVX4+3tzS9+8Qvee+89o0umqpeMywb0HcehrKyM/Px8\nxo8fb9ajCA4O5i9/+Qvbt283SpVKnISUKBuvr6+ntraW48ePk5SUxM9+9jMaGxvZu3evYXwJtdDA\nt7swWlpaKC4upri4mL59+7Jw4UL69u1LamoqjY2N/OUvf6GsrMzElprabXhP+cCbb77JuHHjiI+P\np6GhgU2bNpmcQAu4aAa0MV4lfCtXruT06dOMHj2aSZMmUV1dTWZmJoMHDzZJt43+XGy75OAwPz8/\nXC4XERER9OzZ03Sffv7556xdu5ZnnnmG7du3G8EQwV2AkRqKj48nMjISHx8fdu/eTWhoKIMHDyY5\nOdlAZA0NDeTn5xu5ImGZis8cxyEyMpK2tjZSUlIIDQ0lKiqKv//977z22mtmWlUiKQhLU6xibTV/\n/upXvyIrK4vx48cTHx/PH//4R/7xj38YGMhes03NgoIHvby8KCoqorq62nQKNzc306tXL1MVE1xo\nFwiEUvj4+LB3717mz5/P+vXrSU1NpaWlhePHj5v/lVjqmoSltrS04HK5TCx6yy234Ha7KSoqwsfH\nxyx8Is9qE8k19as7w3EcKioqjPaxEKTly5dz/PhxM9B07d+0XTKGKy/j6+tLdXU1cXFxhIeHs3r1\nanbs2MGxY8c4fPiw8QgimtslU2/vLhHo6upqKisr2bZtG2vXrmXx4sVs2bKFjo4Obr31VkaMGMGO\nHTsoLi42ng4uVG+kQ1tVVcW9997L97//fU6cOEFpaalZdy0sLIzQ0FDgAtHarjIJoxXfIDExkYkT\nJ7Jnzx5DOG9qajK9c7aOgJeXlyGmSMMgNTWVXr16cejQIfbt28fJkydJSkrCx8fHaCTYtEQZrZ+f\nH0OGDCEzM5OoqCg2btzIL3/5S3bv3k3Pnj3x9fU1bUbqVpbxe3t7m0JGZ2cnjz32GFdeeSW7d+/m\nd7/7HZWVlYwePZrExETCwsKMJKzuo907p/1HRUWRl5fHyJEj8fX1NQt1q9Ajz/6/xuPaJIuUlBR+\n/vOfM2bMGKqqqnjhhRdwu91cd911xpspfLDLn/YaXdHR0YSFhXH48GE6OzuZOHEiTz31FL/5zW84\ndeoUL730EkVFRbS2tpppXoiA2GJjxozh/vvv5/Tp08yZM4c//elPJCQkcMMNN1BdXU11dbXxGnbD\npCAw6PJ8U6dO5bnnnqOgoIDf//731NfX4+/vbzBMxaIaQIoVm5ubCQsLw8fHhxkzZlBbW8uf/vQn\no427fv1646HcbrdHo6LKwX5+fkyaNInvf//7RkXG29ubzMxM4uLizHIFUoGXx+zs7MTlchmcOyYm\nhmnTplFWVsbixYv5/PPPOXHiBB0dHVRWVlJTU2OWNlDIp+fhcrkMuaimpob58+cTGxtrwqzx48eT\nnJxs2usB45gutl0yMa4teRQaGsqECRPIzs5my5YtBAYGMnr0aCZMmMCWLVvIzc2loaHBJGeaUtVT\nJQLMjBldcg8JCQkEBAQQEBBAUVERTz31FGfPnjXHlFaXIB+Rr48cOWJat/v162ey5vT0dFN1E29X\n8ayMzxaIvv7666mvr2fDhg3MmzePjo4OTpw4YYScbSEUuLCakPQhlLhu2LCB7Oxs2traTAlWKjVK\nwnQewmM1EF0uFyEhIdxzzz2MHz+ekJAQtm7dyltvvWUqfiqjq+AhpCQyMtLkEgcPHmT//v1MmzaN\nuLg4Dh48yLlz58xMaC95pVKuhLiHDBnCggULGDt2rLmm8vJyDh48aGY/W6vtm7Z/V4LpSeDHQPn5\nrz3mdrs/Ov/ZL4C7gQ7gZ263e/23HQMw1EFl162trQwZMoRf/epXpKSkMHz4cPbt22eyT4l/KHtX\nCVZTVH19PQcPHmT27Nl4e3uTnZ1NQUEBubm5VFRUGFphR8eFNYAV59p6BM8++ywvvPACzz//vKkm\nic1fUFBgYkh5OJub2t7eTq9evcjMzCQgIIAZM2ZQVlbGsmXLWLdunSm66Pw1cARjScu2vLycd999\nl5/+9KcMHDiQxMREcnNzzT1QPKvav01Kqq6uZs2aNXR0dDBs2DD69OnD8ePHycrKYt26dRw4cMBU\nxUTuVqwtSSethLNu3Tq+973v8cknnxAfH09FRQUvv/yygRarq6vN8eUIFOf6+fkRFxdnBk1ubi5H\njhxh7969vP/++4a7YPNOvqnv7FtLvo7jTADq6VKosQ233u12/77bdwcC7wCjgARgA5Dmdrsvvu5P\n1+/cMhZNS3fffTdjx4412e327dtZvnw5hYWFHuiBgnjFUcIRleyMGDECx3GoqamhqKiItrY2M7Xa\nNESVWHUeKnP26NGDK664gjvvvNMsDCgiusIL/d5WPVSiOWLECH76058SGhpKXl4er7zyClVVVVRX\nV3vQGwXFCU9WnKgOieTkZB5++GHS09PZsmULb7/9NiUlJYZ0Y7Oq7PhSiSeAy+XCy6tLDdGGzORl\nZXS6Ji3JpZUue/XqxQMPPEBMTAxZWVmsX7/eLBBjt1GpAKQwzq5sjhkzhs7OTg4dOkRtbS319fVm\nBlFYIn71+Rnk36c1Oo7TC1j7HQz3F+dv3O/Ov14PPOl2u3d9m+FqmtWIE5vKz8+PkJAQqqqqPJQJ\nbe8oxpPAc017ymRVcRMR3CZ/aBqtqKgwU6YMUcs3iSsh7yym1rlz54zBqW9MTDRBW+olk/cUjCeD\nVWu7CPTqgpWCjIxL+1Tt3xa/7i4VKn6GCggaoCrMaH/2a5sDLKOxhQfDwsJMl4gMTAMvJCTEzBzi\nmWgA2qLXur/6TIPdTmbtosz5z75+pT7FRd/0jy5VxkPW6yfpUmk8CLwOuM6//0fgdut7S4FZ32H/\nbsDt7+/vDgoKcus14A4KCnIHBAS4/fz8pJhj/p03eI/3fHx8zGf6W/+8vb3dPj4+7vMe3u3l5eX2\n9vb2OK6Xl5d57ePjY/bh5eXl9vX1/Zd9dv+nfQcGBrq9vLzMOfn6+pr9dD9n/dP39ZuAgACzzx49\neri9vb3dTle3yEWPbf/ePlcvLy/zO+3D19fXXJf9PZ2H4zhuf39/j/336NHD3aNHD4/z0L3sfh66\nv/Zzsr/n7e3t8bnOWfeQ843jX/fv30UVXgb6AEPp0gV7/r+7A8dx5jmO84XjOF/AheY8G78TVKNR\nGRISYrgESrY0WrUPu3NB3k6f2dm2t7e3mUY1usFTkzY4OJj29nazILRCAG1qK7I7hLVp6oULohry\nrMJ8e/ToYdYI07HBk8QeG9sly6ZzEtNMgnKKuxWTa8YAPOJVfQ4XJFDhQsVSs5O8o8gxUrjRvVKI\npWJNcHCwKTFrZrGloGw4zO4OOW8DBuvW/dQ52k2fX7f9W4brdrtL3W53h9vt7gReoyumhf+GBJPb\n7X7V7XaPcLvdI6CrJ8vlchkVcfVB6QIElQUFBREdHY2Pj49hWQlP1QXrpory2L01RUamKpV9I1XE\n0PEAA+nABWK0AHRNs3rgtiSUpkoZnF5rMIkTYBubjgcYXrCm2+DgYGNgdkeu3ZKvDmgNNsF1ghtt\n45GhKiSQExCkaAtC6z2FEHISbW1tJrkDzPPT54IH1S2sc9O52i1GunZ7kF1s+7cM1/GUDr0ZOHT+\n79XAHMdxejiOk0qXTu6e77JPIQJSHoQLS2lKmxW6VP8U0Ovi7ERACZ5iKLiwiIkenOIwlUUV59mt\n2DIMdULYPWLx8fEmjgQ8NAAUxznnidr2A5eXtQekjNImxOt6NWvoWkSU10wiKE95AVyIoTUobd6y\nPKbL5SIpKcksNSv4MDAw8F+MUwaueyOPr7hXRQ67YqfY3ybc2G1GX5dQwoWZyU4QL7b9uxJMkxzH\nGUpXrJIPzAdwu92HHcd5DzgCtAM//TZEQZtNstGIDQkJMUnAokWLSElJ4auvvmLDhg0UFRVRU1Nj\nvIl9k+Q9ZFhhYWGkpKQwc+ZMQkNDWb9+PVu3bjW8WhmVtMv0WsWO/v37G4zx8OHD1NbWkpmZya23\n3orjOLzzzjvk5ORQUVFhDFAGq6VT3W43Y8eOZfz48bS2trJ+/Xp27NhhOMjdm0DtdSLs0vH48eOZ\nNm2awUk3btzIqlWrPMqu9j7Ed9CSsykpKcydO5drrrmGwsJCli1bxrZt2wyfV9rE6kpWJ0hTUxNh\nYWF4eXUpzowaNYoRI0YY9cbDhw8btUY5FCXEdXV1JhFOS0tj8uTJDBw4kOPHj/Pxxx+bhlYx5MrL\ny//n7DD310swLf2G7/8W+O237ffrNnmklJQUqqqqmDx5MiNGjCA0NJTJkyfj5+dHZmYm48eP5913\n3yUrK8so/GmqPn8OpqLk6+vLggULmDJlCjk5OURERLBw4UJ69+7N4sWLPfq1lMk3NjYa4kprayvl\n5eUkJCQwffp0fvOb3/Dll1+SkZFhjLC1tZUXX3zRDB5hl8roU1JSWLBgAddeey3Nzc3s37+fsrIy\njh49SlVVlUdsB11euLa21nQgQJcnf/DBB7nrrrsMZRLgxhtvZMyYMTz44IMekJa9P1XfRo4cycyZ\nMxk8eDANDQ0MHjyYcePGsX79egOJyWMLGnS73QQHBxt+x9VXX81zzz1HcnKyMezIyEhSU1NZvHix\nIb/reZ47d86oQDqOw8yZMxk/fjzvvPMOLS0t3HfffWRlZfHVV19RVFREY2OjkROwV4Pvvl0ylTN5\n27a2No4fP054eDiXX345s2bNMovzrVu3jpiYGO68805DVKmqqqKystIkM4qfROS4/fbb6ezs5LXX\nXmPt2rX06dOHN954g4kTJ7J06VKT6FRXVxu4S9Oi1h1LTExk/vz53HzzzXh7e5OSkgLAiRMnOHTo\nEGvWrKG8vNx4WoUjra2tREVF8bvf/Y7x48ezatUqnn/+eXr16kXPnj2ZNm0aBQUFHDhwwMPoVBHT\ndOvt7c348eOZOXOmIaMfPnyYmTNnMmXKFCIjIwkPD6esrMwsbmLHzxL0SE5OJiIigoceeoimpibu\nvvtupkyZwosvvmjOX7OXXVUMDQ2lvLyczMxMXnjhBVJSUmhoaKC+vp6oqCiuuuoqPvvsM5PEBQUF\nUVFRYcIcrWssSOzpp59m06ZN9O3bl6ioKHbu3ElERARJSUlUVFSYSuQ3bZeM4UrV232+3Knepqqq\nKj7++GPKysqIiopi2rRpBAQEkJCQwPjx49m9eze1tbVmShTDKTAwkEcffZSxY8cyffp004bS0NBg\nsN/g4GCKi4tNcqTjt7ZeWF2xZ8+ePPDAA9xwww2cPn2agwcPkpqaSk5ODq+99honT54kMDDQUBs1\nVSruTUxMZNCgQdTV1Rn1mqioKBISEujfvz+ffvqpwYrlSeX5IiIiaG5uJi0tjZ49exIaGkpubi4n\nT55k0qRJppr4ySef0NbWZtR6wLOM7OfnR2JiImPHjiUrK4sjR46YsrWvry+TJ09mxYoVZsDZLUzN\nzc2GFzJy5Eiio6NpbGzkH//4B3V1ddx7770UFRVx5swZo6BpV9B0X1W6Ligo4OzZs1x77bUMGTKE\nUaNGUVNTwyeffGI8rGbeb2rfuWQMV3FldHQ0LpeLxsZGNm7cyNixY7nhhhuM5lZoaCjNzc0cPnyY\nDz74gLy8PA9GlBItIRNRUVGGpZ+WlsYjjzxCc3MzS5cuNYtFKxlQTK0Eq6mpibq6OlwuFwUFBRw8\neJCBAwcCcPjfiEEwAAAgAElEQVTwYfLz8w0xvbq6GrjQtSC4qbCwkOzsbNLT07nuuusYNWoUMTEx\n9OrVi7feeoutW7caL2fLOTmOY/ZZUlLCpk2bmD9/Pmlpadx///24XC5yc3N54IEHyMnJ8dAfs1vR\nhZI88MADBAUFUVRUxO23307//v1JTEyktraWEydOeLTL6HcqdtTW1uLj48PBgwdpaGigqamJffv2\nccstt5h9SodNA8ZGb1TyDQwMZM+ePYSEhHD33XcTHR1Nfn4+27Zto7a21sBsSmC/abtkDFexpkad\nn58fKSkpxMfH4zgO2dnZREZGsm/fPj788ENOnDjB7t27PW64MtQePXpQW1vLqlWrqKioYNKkSbS0\ntDB37lx69erFG2+8weeff/4vyzKdO3eO0NBQWlpaiIqKMmsY6P3+/fuTnZ3N2rVrjSizWsYVA8pT\nqYpXWFjIz372MxYuXMgdd9xhwpg33niDf/7znx7YsK5Dg9huHC0sLOTAgQOkp6cTGBjI+vXreeml\nlygoKPBYw0Hwkl1OzsjIYMqUKfj7+7N7925OnjxJaWkpQ4YMISUlhbi4ODPwbXKLOnIV6585c4Zl\ny5Zx22238eCDD9K3b1+am5vZvHmzqQBKc00oj/ByVdNOnjxJZmYm7e3tnDp1isrKSlN+FnryXUKF\nS4bWKI/ao0cPevfuzauvvsrChQtpaWkhJyeHgoICfv3rX7Np0yZWr17N6dOnTUauB694UAa8bt06\nlixZQktLC4sWLaK2tpY//OEP/O1vfzMgu90YKA8jSE68idLSUoKCgujXrx+1tbV8/PHH7N27l7q6\nOoNiiPCtgWSTuqUN0dTURHZ2Nl5eXowfP57Bgwd7tNfoYQmOs0VLEhISPKCz5ORkcnNzqaqqMjTK\n7tAYdOUOSUlJpuu3qamJgwcP8sEHH/DSSy8RHBxMz549TalXOLBibSW6Ks+ePn2a1tZWBgwYQGdn\nJ9u2bWPlypWGJGQXSoTyiD3m5+dHWFgYY8aMoaioyCSukyZNMoUMuyT+TdslY7jq7g0PD2fEiBFM\nmjSJzs5Oli5dysmTJxk9ejTV1dVGWvPUqVPExsaaKU0eQ4B/e3u7ifkkhOGcb6MpLS31UGARbuo4\nDi6Xy7xXUlLC4MGDGTRoEDk5OVRVVdHY2Gji1qCgICOXLwzS/uft7U2fPn14/PHHGTduHM888wx3\n3303n3zyCf369eOuu+4y4Yndn6Xqk/S2Bg4cyM9//nMmTJjAihUr2LRpEwMGDOCaa64x1EUZujYN\nqObmZmpqaqivr2ft2rUmvAkMDDSLtpw9e9YklsKPxWWIiooyFUwNkKCgIJOLLF26lN69exMfH2+g\nNOG26twODg42sbMWLVmyZAlvvfUWn332Gfn5+eZ+iSZq47tft10yhisBiZKSEjIyMvDz86OoqIj2\n9nYSExMBuPfeezl9+rTJ/k+dOmUSOaEKYhqFhobiOI7Rtl26dCl1dXXccMMNZvEOlV+VsLW3txsy\nc0NDAz/72c9YsGAB+fn5vPLKKxw9epQrrrgC6Ipx9buCggIiIyON15UHlw5B//79+fDDD/noo49I\nTk5m7969+Pn5mYGnQSMiDnRpymrATZo0iVmzZlFVVcWf//xntmzZAsCAAQMMZ1eIjEqzqhx6eXmR\nl5fH6dOnqa+vZ//+/URHR/OTn/yEefPmsWvXLrKzs+ns7DQqMm73BUl8sdgiIiIYOHAg9913H4WF\nhRQUFJjqnISdRZC3r9/Pz4/Q0FAj7nLddddx7tw5mpubOXbsGPv27TNYsVbZVN7xTdslE+PGxMQY\nIviePXvMWgNz584lJyfHCEW0tbVRXV1tPISwTnUjyGs1NDQQHx/P1Vdfzfbt28nKyiIuLo5hw4aZ\nrFWjXKIbqq7V19eTlJREnz59SE9Pp1+/fowePdr0eg0aNIjS0lLTX6b4VhUnuyp39uxZduzYQWtr\nKzNmzCA8PByXy0VlZSVr1qwxS7CKu6pNxRD9LZjp5z//OZmZmSbmlA5BSEiIqSgqvpTXLC4uZvv2\n7YwcOZI5c+YQGBjIgAEDOHjwIK+++ir5+fnABS1cGbxk+MX+0mIuhw4dIj4+Hl/frlUzN27caAoi\nti6Yt7c3dXV1Rjo0PDycJUuWEB4ebvDhefPmcezYMfLy8jw6Uf7XGG5RURHFxcWEhYVx4sQJXnrp\nJaZOnYqfnx9Hjx4lISGBnTt3Ul5e7kHirq2tNUmMPK2y04qKCrKysrjjjjuYO3cuPj4+fPDBB5SV\nlZnYUaNb1aZz587h4+PDuXPn+PDDDwkNDSU5OdlwSL/44guOHj0KdEmXStRCN1x6thJKbmlpYd26\ndTz44IOMHj0at9tNTU0N999/v2m9ERqggoU00ry8vCgtLWXdunVMnjyZKVOmcMMNN1BTU8N7773H\n+vXr8fLyMnizNqEUMoCWlhbefvttwsLCuPHGG2lsbGTdunWsWLGC8vJyk1AK9hJltK2ta/3hhoYG\nQkND2b59O/3792f06NH4+fmRl5fHF198QV1dnfGyun92C5HOo6Ojg6+++gqXy8UPf/hDLr/8cqMN\nIY6wkAjRHy+2XTLaYWqU8/LyIjk5mba2NsrKyhgyZAhVVVWmbx+6su+ysjL91ng8wBDPlRxIdigs\nLIzTp09z4MABD7a97enEL1BCEhERYaSD2traqKys9NCXlScRccZWohG7SlxdeUyJUyuOE2tM8JXO\nXbOL+APx8fGMHDmSoKAgjhw5YpI8FS7s4wrOU+ihcCgsLIygoCAz/QOGoywesD2QRMhXISM0NNQs\n8tenTx/WrVtnlMRVvVTFUeGbUBOhN4ARSRk2bBgFBQXs27fPiMDYLfHnjf/S1seNjY01QLUuXE2L\nMoKQkBBTRnS7u9pkQkJCqK2tNQ9CJBSb2GLLiOqBaDpXjKlkQpJKNkTjdrtNXOzv709hYaHpTBDk\nFBgYSGVlpYeWAPAvOg0aFGJEqU1F1ys4UBCbCgrqi1PXrcjxNkNMiZUyeCVoChmUAwAm9qysrDTY\nuM7NlgBVKKYZwTZsxcKhoaFm9lKuIkKQyu4iL9nSSpIW1QxhN51aXRCXtuGCp0iwMm1NY/YDt34H\n8LXvfd11aT/d37O/L3Si+341FYoq+R2uyQyci91ju8RqV5n0mQaQfZ76+9vYU193jd33r9dflwzZ\nvFg7w7eP3f3aul+z/X/3Y9vhzDdtl7zh2likCB6aRm2RDk1t8lDC/pRwiVWmRehsvNaW5YeuES+h\nPG22d9Zv5W3a2to8lm61ubd2+5A98OQFFRrY6yQoARKaoWPq/HSNHR0dZmaxzykiIgLoKper7UdJ\nZlBQkLlehSuCqYQz+/r6mhYkeUtb5ESzlzabSH7+uRl4TG1Kug/6XM5HGLPui6pzYvHpOHreusaL\nGe4lA4eJoCHhOFtkWdO5DbIL7lGMqVFsKyXaPFwtoiyWfkdHh8f6A5p27eZCgfF2bCbStPahuFQx\nnJIRu3RZX19vSNPy1nq4ajO3ZYcU64eHhxuOsRoWNTiEpQo3tTUNABMfK07t6OgwnRLCvtvb26mp\nqTHxOlxQwNExROoWzdPulrAVeBSGdPew9rPSgLCJSDJa5S9CZr6tA+KS8bh6QBJcVlzk7e3NwIED\nmTJlCuHh4eTk5PDmm28a72tT8aKjow2lTiPWy6urqzUhIYGePXty6NAhY9yKBxVXC/pRotfR0WEG\nQEREBIGBgYSFhRmMuby83KzLJTRAD0KevbOz0+zPcRzi4uI8GiRFvLaSEQ9BkYiICCM3pa6E+Ph4\namtrzao1amIUqUbHlzHISHXdoaGhpkoIGJ6D0BF5P7szw8vLy3SF6G/xO6qqqjy8qthp6kYRQqJ7\nags7nzhxgtzcXBOSCNFQweJiHveSgcMiIyOpra2lqakJl8tlmFm33347s2bNIjExEcdxDOb45Zdf\nkpeXZ26glFL8/f0pLy8nPDyc1NRURo8eja+vLzNmzCA5OZkjR44AXVoFq1evZteuXcYgAA/DV9I2\nbdo07rnnHjIyMsyyTRs3buSJJ56grKzMY6qWh2tsbDQG6u3dJVl644038oMf/MCQiFavXs2KFSs8\nOnvb29tJSkqisrKSc+fOGQNta+taR7dPnz7MmDHDVK1OnDhhEJOwsDBaWlqM0rpWzbT5ArGxsTzx\nxBP07t2bhoYGNmzYwOrVq8191cwlTqxmiKioKBobG0lPT+fKK6+kb9++TJ06lfr6ev7zP/+T7Oxs\n6urqTBHCTh7ViaLkNCoqiltuuYXrr78ePz8/fv/737Nx40YPadRvyyMuGcNVNq24y9/fnyVLljB6\n9GiamprIz8+npqaG1NRU7rnnHn77299y+PBhEy9pdIvLGhkZycMPP8z1119vyr0+Pj707duXkpIS\nPvjgAxPnKSRRTKgmyKamJvr06cMjjzxCeno6lZWV5OfnExQUxIkTJ2hsbCQiIsJgnnBhzQZ5I029\nI0aM4OGHHyYkJISCggKqq6sZOXIkGzdu9Dh+Z2cnVVVVpkkxMTGRqqoq+vbty7PPPktcXBxFRUUU\nFBSwcOFC7r//fvLz800IUVVVZaZzTf/qBBk6dChz5sxhwoQJJgTw8fFhy5YtHkQYx+lq6Vc7vAZx\nfHw8CxYsYMaMGQb3jYqK4uWXX+bdd9/ld7/7nWnpF0SnmS02NtZ0QgQGBpKens6RI0fIzc0lLi7O\n5CE27vtN2yVjuMIaOzo6OHnypJHArKqqYsOGDezZs4fc3FzGjx9PXFwcV155Jb6+vmzZssWUb9XL\n5HK5GD58OFOmTOH06dNAFzWwX79+pKSk0NbWxt///ndyc3M5d+6ch+Kjki0NpIiICKKiotizZw9P\nPPGEwWrDw8NpbW01SZVd9bLVcBobGwkPD+fhhx+mpKSERx99lOrqampqapg4cSKxsbEmUVObi6Ak\nYdmBgYEMGTKE+Ph4/vGPf3D8+HEcx+Hhhx/m5ptvZtu2bWY20ixhJ4N1dXVERUUxZcoUJk2aREBA\nAMePHyc/P5/k5GQjfaVqouihcAHpKS8v5/bbb+faa6+lvr6eL7/80tA1p06dytSpU9m/fz+bNm0y\niZqSV5XoxTmePn06W7ZsYf369QwaNIgHHniA5uZmPv74Y4Mb/48lmP5/bVroTvFTcXExv/nNbxg7\ndiyxsbHExcUZmc3LL7+csrIyvvzyS48mvZaWFqNw2NzcTFlZGatXryY1NZUzZ86QmJhIfX09f/jD\nH0x7jre3NwkJCZSXl5tYWUash3706FHa29sZPny4aS/p2bMnAMeOHTMxJVxoCffz86OyspKIiAim\nTZvGiBEjWLRoEZs3byYqKoqoqChGjx5NYWGhwWiFT9vXo1khJCSEiooKs3TA/v37jdbt/Pnzefrp\npzl9+rQpFtjYtAokY8eOJSEhgd27d5v1I6688koGDBhAXl6eKZvbaxPLgBMTE5k6dSo+Pj688cYb\nbN68mcrKSkpKSvj444+NYqPNdNNvxTRTz9nll1/On/70J9xuN7169TJ4srgK3WHPr9u+S7NkMvAm\nEEtXc+Srbrf7JcdxIoDldImF5APfc7vd1U7XE3wJmA40Ane63e7933YcPayQkBDi4uJMjLV3716W\nLFlCVFSUac8uLy/nnXfeYdu2bUay08fHB5fLRUlJiSF8BAcHM2nSJAYMGEBoaCg+Pj4cPXqUv//9\n7/j5+RETE2MIH8rK5X10rNraWo4dO8Zdd91FZmamyYq3bt3Ktm3bjLKNXeywK2p1dXUMGDDANBkO\nGzaMpKQkZs2aRV1dHVu2bDHCznZHrMIWtahrQKanp/PWW2/R0dHB6NGjCQoKYuvWrezZs4fCwkIz\nkO11y/z9/Zk9ezaXXXYZZWVlrFmzhrq6OqZPn05CQgK33norWVlZ1NfXe0h8irSjpFmrxQ8ZMoT6\n+nry8vKYPn06iYmJHD9+nJycHENhBM9lo4KDg034UVVVhb+/P3379qV3794mP1Gi+V2M97t43Hbg\nYbfbvd9xnBBgn+M4nwF3AhvdbvczjuMsAhYBC4Fr6WpL7wdcTpd4yOXfdhDBO8JVKyoqmDBhAvff\nfz/p6eldJ9LezsGDB01SVF1dbRbJk8cKCgoydEM/Pz8mTJjgcZy0tDSee+45nn32WcrLy+ns7KS4\nuNhM14KhWltbzVJQyt4F/xw5coR9+/YZ+SZb8tSuMCnelcjez372MyZPnkx2drYhdCseVJwOF7oo\npJgeHR1thKXV6NnW1sbw4cPZuXMnv/3tb8nPzyc8PNy0QImAJF2EK6+8En9/f3bt2sXKlSuprq6m\ntbWV5557jujoaNOqDhfgOGX4bnfXsgZZWVkMGjSIjIwM3G43x44d48c//jFVVVW88sorZGVlebQQ\n2QUGLaCiGD8mJoa+ffuSmZlJaWkpR48eNTOg3RTwbxuu2+0upkutBrfbfc5xnKNAInATXW3rAG8A\nW+gy3JvoEshzA1mO44Q7jhN/fj/fuCkx8vLyIjMzk4ULFxITE0NOTg7x8fEUFxeze/du9u/fb0SR\ntQ6XsFTBPLt27eKJJ57ggQceIDo62pChg4ODufLKKzl27Bivv/66YVbZDDHduPDwcG677TaioqJ4\n6aWXyM3NJSEhgfr6eiZPnsyOHTs8oCgZjGAdNV36+/tz7Ngxdu7cSXNzM5MnTyY6Opp//vOfxksr\nUVJ8LCSgsbGR2NhY+vTpw2effUZlZSVBQUG0trby4YcfsnjxYk6dOmXkRuXlVHa1GXOlpaWsWLGC\nkpISWlpayM7Opr6+npiYGEJDQ00Pmgo6IoX7+PiQkZHBZZddRl5eHkePHiU0NJT77rsPgBUrVvDe\ne++ZYoqteaFYXzDenDlzCAoKorS0lOHDh1NdXc2GDRuorKw0v/8utMb/VgHC6RK/GwbsBmItYyyh\nK5SALqM+a/2s4Px737jZ1aQbb7yRV155hYyMDPbu3cvLL79MYWEhLpeLvLw8Q5dT/BcVFWUycxli\nU1MTy5cv584772T+/Pk8+eSTPPnkk5SXl5OSksLs2bPp37+/R21c06y/vz8ul4uHHnqIO+64g/ff\nf5+3336bXbt28ec//5ny8nKGDRtG7969TZeCKmo2HhoeHk5wcDBpaWkcOnSIJ554gpUrVxIcHIy3\nt7epECo8UMeBeLhqC8rNzaW1tZUtW7Zw7NgxJk6cSEREBAcOHODUqVMeM46INVp+VPtobm7mww8/\nZOPGjYSGhjJmzBgeeughBg8eTG5uLqdPnzYogE320bXNmjWLq666ioSEBM6ePUtcXByXX941kW7d\nupX6+nqPaV4VM+2nR48eJCQkEBMTw7p169i/fz+pqamUl5ezYsUKj0Tu/ymq4DhOMLACWOB2u+u6\n1b8lavadN8dx5gHz9Lqzs9PgmFOnTiU1NZXa2loiIyO59tpriY6O5vDhw5SVlZlVdsrKysz0qipM\ncHCwmepCQ0Opq6sjNDSUhIQE06Xq5dW1poJEi1WA0ENqaWkxSVVcXJxH1Ss0NJTU1FSzrpfOWwma\nl5cXUVFRlJaWUlVVRUREhIGcOjs76d27N6GhoXz66adUV1d7JJfyjILvWltbjdRTdXU1/fv3N+3i\ntbW17N69m5iYGEpKSgypXdOtig/e3t7U1tby6aefkpKSwv33309bWxuTJ08mIyOD3Nxcli1bRnV1\ntYHEBH8pXo+OjiYjI8Mw5W666Sb69+9Pc3MzWVlZZGdnm/47dZMInVDRIiAggLNnz3LgwAG+/PJL\nYmJiaG5uprCw0FQXbX7Kt/EYvpPhOo7je95o33a73R+cf7tUIYDTJclUdv7976Qf5na7XwVePb9/\nt6ZUQS2zZ88mODiYMWPG0N7eTmlpKR999BFfffWVwSlVkxfvVdoI0JUF/+hHP+L2228nPj7elG/b\n29vJycnh9ddfNyx+e5oVjFNVVcWmTZu47777ePHFFyksLOTIkSOEh4czfPhw/vnPf7Jnzx5j6CrZ\nKi5U9evcuXMcPHjQCHekpaXx1VdfsXLlSo8FmuWZBLVBV1yo9SiSk5NN8lZSUkJRUZFZYESlXTVN\ntrS0GEK8QpCSkhJ++MMfmi6H6upq9uzZw5IlS/j88889qoQagNJvKy8vJzc3l4yMDHr27ElgYCBn\nz55l+/bt7N6921Q6Fd/bg1ghVGdnl/r4ypUrmTBhAvHx8eZ3ERERpvPErjqqsve1NvltLvk8SvAG\nUOV2uxdY7z8HVFrJWYTb7X7UcZzrgP9DF6pwObDY7XaP+rp9W/tyy/O43W4yMjKYM2cO0FWQOHDg\nAIWFheTn5xt5HmWhqha1t7cb0ok8xZgxY7j11luNKF5ZWRknT55k69at7N+/3yAECjmk7t2jRw/C\nw8OJiori+uuvZ+jQoQwcOJD29nZOnjzJli1bWLVqlUEkZLjymopbRWYJDg5m4cKFjBkzhrVr17J5\n82ZOnjxpjte9wVGCcIKywsLCGDduHOPGjTNFgaVLl7J//34PppXia3s1SIUwwcHBLFq0iD59+lBX\nV0dWVhbbt283re3qBpHxCRXRQM7IyOCGG24gIyODjRs3cubMGYqKigwcKF6C0BFbY1cVTc0iffr0\nITo62sye6qTWvZPRn885/j12mOM444HtQDYg3/0YXXHue0AKcJouOKzqvKH/EZhGFxx2l9vt/uLb\nDFfTo4gs9fX1BoiXcIeIMjJUO4by9vY2o1iG3V2FUA81IiKC0tJSsziHDNBmhQlblVeMioqis7OT\niooKE/vZ06qMUGQbNTEKXtIU39zcbDJseVjRF7VCj+RNVe0TrhwQEECfPn0oKioiPz/fKFzW1dV5\nsNy0VoWYVxoMCoc0BWtKVju/iEw6Z4ULKh3LO9bW1uLr60tNTY1RDhJuK8irsbHRgx2nxFchl10p\nU3IqLoXO/3yse2nTGtWeLKa+at5iVtmNiLW1tR5Zq8qtukkyGAnHud1u05OleFW4q3QO7JhdZUs9\naPsBCx7SuchbwQVuq4xF6+kqqxbZOzg42MS38so6rlrEbc8looqSFhGQ7DhQcF1nZyeVlZUefF55\ncOkPO+e7RgTDCb6zY3w73LHpiEJdADOTydCUA6hSpmcij6xwymaRiWwuDF2lYktj4tI23IuRjrXZ\nSYxey5t+zf6Mh+1Oau6+z+6kaHvqtY/X/TzkPQQbXeS6/oU83f2zr0tC7M+6f6f7PvRe9/1/F5K2\nDNHmLH/TeVzsXl/MhuzPbL2Lrzt/+7X93iXPDhM6oKSro6PDQDmaRtSKLuaTWkrkiXSDg4KCPIjR\ncAG1UJXKJmTbJVt5HokZa51deQCbaB0WFmZoi3aJ04aRRO8TvikvruN195qa3m2jlTfTIFFMHhgY\n6DG1dq9UadZQAif+srBStULV1dWZ7F8eUZCW/TdcoEDKsMSp0DOzuzZkrPL4gvr0jO1KIWBYYbq/\n31SEuGSI5JoalSSpbKipzm7uszNnkcvhgvFrZR3AY3q0R7S+HxAQQGhoqPmOkhT1YOkmauqU4UGX\niLKQDIUMtrEpbtY5yLjUxKm/5bnlze1r1wCWIWvtMFWYZKQBAQGGCK9zk4HZA0jogRZjEVfYnnk0\nXctghevKgQCmoKPf6LXdB6dnI6hMA6a9vd0j7BKEZutCKNy62HbJeFwlJYoLdfLCPkeOHMm4ceNw\nuVxs376dVatWcfLkyX+Z6rVao01kjoiIoH///vTt25eamhqOHj3K0aNHzbHkcVUeVTzZ3NxsFqeL\njIykd+/eREZG0traSmlpKYcOHTIJlt01oSWW5GkUu0rnd8qUKXR2drJq1SoDRckD2U2dMsQePXqY\ntYlramqIiYkhJSWFc+fOkZOTQ3V1tUdCBhiD0ICQBsKAAQOMJsLRo0fJz883eLQSUiWYdnig0Ki5\nuZmkpCS+973vMXnyZPz9/Vm+fDlvv/22B8NMM4MtYGfPUqKYDh48mNTUVI4cOcLGjRvNqunfJsF0\nyRiuoBe7axS61ua69tprmTNnDq2trVRVVbFgwQJGjx7N/fffb2Q5NQ0LiVB2HBMTw7PPPsvYsWON\nDsJf//pXfH19CQ0N5cSJE0a82fa0SqhaW1vp168fTz31FKNHjzbIQGVlJU8//TQrV66kpqbGJFA2\nmqGwQ0IkvXv3Zty4cSQlJbFv3z4jDqJj21yB8PBws/JkbGws99xzD8OGDePs2bP07t2bwYMHU1JS\nwq9+9SvWrFljztdGWRRahIaG0qdPH3784x9z7bXXGl7I559/zlNPPWUW+tY0bqu0i6iv0GPAgAE8\n8sgjXHHFFdTW1hIXF4e/vz9btmwxItsaxApxVAxSmNKrVy9uvvlmpk+fTnx8vBmsb7/9Ni+99BJ1\ndXWmCnmx7ZIxXPEDNDVHR0ebdWhTU1NZu3Ytb731FqdPn2blypVGmM6Wm7cbG+V9H3zwQaZNm8aO\nHTuoqqoiJiaG4cOH06tXL2pra9m1axefffaZwRgVrgiuamho4Ac/+AEDBgxg48aN7Nixg6amJv7j\nP/6DmTNnsnXrVrMMK2BahTQlBgcH8+CDDxIZGcnBgwdZs2YNYWFhpKenk5iYaMIIXTd0eVtVsjR1\nHj9+nDVr1pCXl8cdd9xB7969jfKPCODycIChdkJXsaGsrIyGhga2b9/OypUrueKKK4iOjqazs9N4\nuY6ODsNK00yi2L2lpYXY2Fi+//3vExMTw1//+ldycnL46U9/Ss+ePUlISDBChNr0t43lBgUFMWPG\nDKZOncrBgwd5++23iYyMZP78+fTv3/9fco6LbZeM4UowTTX7+vp6Bg0axMSJE9m5cyfr1q1j2LBh\nPPTQQ/Tr148nnnjCxFVut9uDGqceraCgIGJjY6mvr+exxx7DcRx++ctfsmPHDrZu3YrjOBQWFnpA\nPXYVzHEckpKSGDhwIGfPnuWXv/wl4eHhzJ07l6CgIA4dOkRJSYkhvWhKVPwK0KdPH5KTk3nttddY\nt24dPucgSi0AACAASURBVD4+zJ49mwkTJlBdXU1tba0RCpGxyVNrCu/Zsyfbtm3jyJEjTJ48mXHj\nxhEQEEBhYaEplQpCUtlXnk7n09TUxNatW/nqq6+46qqrmDhxotGqkPhdTU2NgdlkPHZBALoUMDdu\n3EhBQYFZ9byyspKzZ8+a5FezpUr46lPTvQ0JCeHgwYM888wztLW18cADD1BTU8PKlSsNNKaBf7Ht\nkjFceYfAwEAjenH27FkqKiro168fjz/+uJnuFy9ezPbt241svAzNbgl3uVw0NTVx4MABBg0axBNP\nPGHiq7KyMo4fP26KGTarSnFic3OzibmLi4sZNmwYc+fONbJBa9euZfHixQCmZKy4UJUrx3EoLS1l\nzZo17Nmzh6ioKCZMmMBtt93GW2+9xZdffmmyeBUshJpo5oiMjOS6664zWf31119PcnIyNTU1PPbY\nY5w8edIgHomJiRQVFZlQQcYvxp202K644goCAwPNoiPiN+gYNq1QRiSq5c6dO4GuxGvo0KGEhISw\na9cuw4NW0USEHX1XtMmmpibOnTtHcnIy1113HZGRkQwdOpTVq1ezY8cOc5z/NT1nunE2SSQzM9MQ\nXRoaGsjNzWXx4sVs2bLF6H/ZU4uyZ8XC6hpwuVxcffXVnDt3jvfff99wRWtraw3wbSeFik/V7Pje\ne+9x0003sWDBAnx9fVm+fDlPPvmkUQy35ZTs2FJ1/lWrVjFx4kTmz59Pz549DZOquLjYTMcijCtk\nEGfB5XJx1VVX0a9fP0O0d7vd1NbWkpSUxJYtW4iOjqayspLKykpT5BC9U53Q4eHhxMfHc9lll1FS\nUkJ+fj4xMTE89NBD/PrXvzbfVVwPF+T4NQhs8kuvXr2YN28ebW1tphNFBitvqQGkpks5FuUtmZmZ\n5Ofnc/LkST755BNKS0sNOvO/JlRQIC8PNGrUKJ5++mmzplh7ezvvvPMOr776qjEOQVO6SXBhsbqW\nlhZ69uzJDTfcYCAZx3H45JNP2LZtmwc8ZXMMlPHCBUJ3WlqaWe9MRtfU1GQSSnkHTauKWbX2wsiR\nI5k+fbqJJR2nq01dZdb29nYPFW7F2kIztOqPy+WiurqawsJCmpqamDVrFl9++SWnT582g1H705Sr\nmeTUqVO8/vrrfPTRR6SkpODr68vEiRO58cYbWblyJbt27TJeUuehQSCUQNoX8fHxzJ8/n4CAAMrK\nysz56Lvy1joXGXB4eDgDBgygoqKCvXv30qtXLyoqKnj33Xc5cOCAgRy/S1HskjFcGY63tze9e/fm\n2WefpWfPnjzyyCPExsZy7733kpCQYOK4pqYmj8WP1aOlJeh9fX2NJOdf//pXRo4cyYgRI0yWrTXS\nbF6sPIq2lpYWZsyYwYIFCzhw4AAvvvgit956KwkJCURGRlJdXW3WF5OntPkKfn5+jBw5kn79+vHi\niy9SXFxM7969eeSRR0wrTlBQkGmJEeZrQ0r5+fk88MADZGZmMnz4cOMNp06dSmVlJXFxceTk5HiU\ndRWfy2g1mHJycjh69CjJycnEx8czatQoww22ixz6ZxtQeHg4nZ2dxMbGcv3115vj2KLYdlm3+0zY\ns2dP5s+fT2dnJ3/4wx9YtmwZa9asISUlxYiSaLDLHr5pu2QKEEouOjs7ueyyy+jbty+PPvoor776\nKmvXruXo0aNEREQYSXkvLy+jkyvuqYxF+xg7dizbt2/nmWeeYfny5QZsF3SmjFyJGWDUWwBzDtHR\n0bzwwgts2rSJ7du3m44BcWAVB9pxomaAgQMHsnPnTg4cOEB5eTl5eXlUVVURFxdnEsru0I+SzODg\nYMM/2LBhA3/5y19wHIe+ffuSnJxMU1MTsbGxxMbGGhRAqIK/vz+BgYFm/eMhQ4Zw1VVXma7etLQ0\nrrjiClpbW03Y1b3UbQ/G+Ph4Bg0axOjRo3Ech+LiYhobGxk4cKChXNq8CIUUmv0uu+wyJk6cyMcf\nf0xpaSkxMTFAF3WzqKjI3DNVIb/N614yHtfHx4fY2FgKCgoICgqisbGRsLAwpkyZQkFBAUVFRYan\nqsqS5JnkKVRKbGtro7i4mOPHjzNq1CimTZtmqmP9+/dn586dZkRrKpO3l4Kgr6+vUURva2tj6NCh\nZmWY6OhoM70rMbOnWPXBtba2kpeXx2WXXYbL5SI7O5vLLruM1NRUXnvtNRNn2+wqXZd4s52dnVx5\n5ZVUVFQwdOhQ7r77btrb29mzZ4+hFRYXF+NyuWhvb/dYFksFDZfLxV133UXPnj05duwYdXV1zJkz\nB5fLxZ///GfOnj1r4lg7k1cBCGDQoEHMnz/fSD5FRkZSVFTkoUNmFx40eynUUZfGrbfeyp133onj\nOERERLB582aPZaFsAtM3JWiXjOG2tLRQUVFBe3s7X3zxBTt37uTuu+8mNDTUqJC/8cYbJvFQAmBX\nvnTT/fz8KCsr4+mnn+bRRx/l0UcfJTExkerqapqamkhMTKS4uNh4J0n+SINLntPHx4fHHnuM//qv\n/+Luu+8mKCiISZMmUV1dbfQYbH6ANr3X2tpKVlYWs2bNYubMmTz88MP4+/uzdu1aPvroI+Pd5G3F\nlNJqP8KBU1JSePzxx/H396eyspJly5bx1ltvGYK4yriKMyW3pOy8urqaL774gokTJzJs2DB8fHxM\nAeVvf/uboVRqUT7dExmtSrdCcUpKSvDx8eHkyZMsX76czZs3e4jY2YQaedDVq1fj5eXFNddcw8CB\nAwkICDCLLqrzwiYsfZuuwiXDDlNiIc5tWFgYYWFhZGRkUFZWxuHDh6moqDCKiYJsNB3pwpVYBQUF\n4efnR0hIiFkNvaqqioKCAuNFFVd3Z2Bp/1pTOCEhgYULF5KSkkJBQQFvv/02GzZsMAYGmFq7PLcq\nTtLwjYqKIjIykqqqKvLPLyACF2J7m3Yo6MjPz4+IiAji4uJIS0ujtbWVo0ePUlhYSE1NjaFqdmfV\naSDburReXl6MGTOG3r17U1dXR15eHmfOnPFQIVc7vhyAOBUyxOTkZKqrq02DZU1NjelcsMlMCjFs\nDoPus5bdcrvd5OXlUVxcbL4nXQuVqL9JO+ySMVwZmqZpwDx4MaE0rSpzVdu4YKuoqCgaGho81grT\nKJYwnWrywk5tRpNibMDc5O6YpvBmdaRqelWjps2XVUHDzrRtg7K5veI3iFVmU/267wsuZOzycNL2\nstdaswejQg81NOq6taab3tcxJAGl+F2hh4xUTkP3SoRyoRm6v2KdqQSvHMCGDbvTS8Vo+x91QPz/\n2BzHcWs6kqHYdEAxhYTRds+Cbc6nSrUyMIUAnZ2d5oHZhmh7Xe1DD9CmHirxsNu3AY/f2fu1s3kb\n35RB2ZWt7hm0HqpNk7SZZd1buDVA1WajeyTvpwFpUwntTQPDHhTaR3eyTXfUxN6XnS/ovkrBUbmH\niizy0DpvHac7P+FihnvJoArynOoj040X+UQP2hZhkwCyphrdcD0kIQR6IIqbQkJCzBL2+r02JVsy\nDOG0Sp4k1AEXVlzXuWpfovfptb6nDN3t7urIUBJi0wPt/+XFZFg211XXIY+tWUrXqKWc1BKl/dqG\nJt1g+z3bgGWgdj8dYGYQlXd1D3QOtjO0QzrHcTwom0pq5VDUT/ddtksmObNbR1pbWz04nerktZMx\n2wPKgw0fPpzi4mJTpVHWD12q3SonS7NA8amtA9Cd5OFyuYz37tGjh1lkQ5UpeTGb3SUcVk2bdlih\nzwED3dnxoCpmEn4TThoWFkZVVZVpYlT3r+6bkiO4EGfa5VchBHaoJG9ooyqaTbQWhshHLpfLo7yu\n49kD3OZDA8YI7f4yIUEaRAoL7CqdEIlv4ip8q8d1HCfZcZzNjuMccRznsOM4D5x//0nHcQodxzlw\n/t906ze/cBznhOM4OY7jXPOtVgumumX35J/fl7lQGZnY/Bq9GtXZ2dmUlZXR1tZGVVWVqXRFRUUx\ncOBAkpKSPJTC09PTcblc5gF4eXmZKVIPTsmgigNSfFEnRXh4uIews2JenZNNnA4JCfForBSJRsfT\ngxdxRkbV1tZmlCjj4+NJSUkhICCAqqoqGhoaCA8PJyIiwjR2yth0f9T+rli6vb1r1U3NJpq67elf\nxG+71V3XJlw3OTmZ2NhY89z0vOyOXhk5XFhcW02VwcHB/7Kwi7ga/y9KvhfTDgN4we12/97+suM4\nA4E5QAaQAGxwHCfN7XZ/oxiUpl97lZmwsDADsk+ePJl+/fqRnJzMZ599xj/+8Q8PJEGer7W1lfr6\nesLCwown7NWrF3PnzqV3794UFxezYMEC0tLSOH36NN7e3qSkpHDq1CmTNMlIk5OTycjIYOrUqYSH\nhxsdgzfffJPc3FzDhei+2rdI3ZL09/f3p1evXsTGxjJ48GDi4uKMlNGOHTsAzOCwSd3iBwcGBpKU\nlMSNN97I1KlTiY6ONvKr27ZtY+/evTiOQ2xsLOXl5WawKfmKjY0lPT2dyy67jISEBHr06EFMTAz7\n9u1j6dKlZsoPCgryWDdO4ZGUdeQhb7nlFm677TbCw8M5deoUjz32mFllXYQceeSOjg5DUBfSEhgY\nyKBBg4iLi6OlpYW0tDSKiopYt24dcKGv73+E47ovrh12se0m4F23290C5DmOcwIYBez6puM0Nzeb\nBaA7O7uW57z55ptJS0tjzpw5REVFGe87efJk2tvbef311wEMDtmzZ086OjrIz88nIyODjo4OYmNj\nue222+jXrx+O4zBw4EDefPNNli1bxtmzZw2KoXYgJWU9evQgIyODX//61wwaNMgYYmdnl0L5ypUr\n2b17N5s3bzY0Sm1S0BEUlZqayqOPPsqYMWOMjpnYXfv27QMwpWPomlWCg4OprKw0yjzz58/njjvu\nMGhEeno6Q4cOpa2tjZycHBobGykoKPAo1+rv5ORkXnjhBdLS0ujs7DQIwbhx4wgPD+dvf/sbNTU1\nZpZSvBkYGEhDQ4PJFVJTU1m4cCHXXnst586d48yZMwwdOpTf/OY3PPvss1RWVhqDl0OR8UJXCONy\nubj33nu5+uqrcblc7Nmzh4SEBPLy8vj000+NFty3lXz/WzGu46kdNg74P47j/BD4gi6vXE2XUWdZ\nP/ta7TCnmwSTpl4tWHzPPfdw7733msWgV6xYQWBgILNnzzYjV55O2lgnT540y87n5eURERHBTTfd\nxJVXXsmOHTtYtWoVW7du5bHHHiMzM5Pp06ezfPlyI6cvry1Ipri4mFOnTpGamsrp06eNl5VgyahR\no4iIiGD16tVGbVtFBD308PBwbrnlFi6//HK++uorqqurSUxMJD09nbS0NNN1YMfVatJUwtqjRw/O\nnj3LqVOneOutt2hoaGDRokUkJyfjOI4h0+thq11Ix09NTSUwMJC8vDxWrlzJRx99xI9+9CNuuukm\nRo4cyZNPPmkaJ23UQcWDlpYW6uvrmT17NtOnT2fDhg089dRTJnYuLy9n/PjxpKWlsX//fk6cOGHa\ndBTiCOmJjY0lKSmJzz//nKNHj5pQbu/evdTU1ODj4+NBOLrY9p1RBaebdhhd8qF9gKF0eeTnv+u+\noEuCye12j3C73SP0sNra2sxaCo7jUFBQQEVFBc8//zz79+836zns2bOHU6dOmbhQkIrIM8p6r776\nan7yk5/g6+vL2rVryc7ONj1bc+bM4dprr2X8+PGGMqj4TJ4iJyeHl19+mVOnTpGTk8PBgwdNh4DL\n5SIkJISysjJCQ0NN46YSQy0sqISnoaGB/Px8tm7dypkzZygrK6OystKQV4R0wAXehrBkLy8vsrOz\nWb58OR0dHfzgBz8gISGBjo4O4uLiiImJISQkxCyuonBJ3ROHDh3it7/9LTt27GDfvn2MGjWKoUOH\n4uXlRUVFBbGxsQZDFvFHbUBKnkRoLykp4emnn6alpYXExERqa2v55S9/yXPPPcc999xD7969PRJi\neU9BYjU1NWRlZdHZ2WmKK2fOnKG4uJiYmBhzDkpgL7b929phbre71Pr8NWDt+ZffSTus+6a4pry8\nnMbGRhYvXsyRI0d4+umn+dWvfmWmq3nz5pGVlUVZWRnNzc1ERUWZdWy1rm5qaipjxoxhwYIF+Pj4\ncODAAeLj4ykqKuLEiRPs3buXKVOmUFtby759+9ixY4fBJ0W8FuJw8uRJzp49y9ChQxkxYgRRUVGc\nO3eODRs28O6775KdnU1xcbFJMqTKrVi1traW9957j0mTJjFnzhxmzpyJv78/ubm5xnjl6WwerP4/\nd+4cAwYMMNzkXr16kZCQgOM4lJeX8+677xodXx8fH9NOr5i5vLyckpIScnNzSU5O5vHHHycpKYnA\nwEBycnJ44403KCwsNMiKfe3ytuoK7uzspK6ujpiYGCIiIoiOjuY//uM/DMd39erVHD582AxuoQo2\nvtzU1MTYsWMZMGAAVVVV5ObmUldXR1vb/23vzeOrqq/1//fOPOckJxNJSAIhEggBwiSTiEBkUEAF\nAalDHau9Wq1eba1ca7XeomhbtVonrKAodcCiojIEZJAwhBBIGAIZCZnnOSHD/v6RrJUdKuC9/d0f\n8V4+r5cv4knOOXtYe33WetazntWmibVVGvVc64cokhvASuCYaZp/tLxu1by9Hsjo/vlz4APDMP5I\nV3IWA+y70PcI6C90RV9fX1JTU1m5ciXLli3DxcWFkydPkp2dTVVVlWbmzc3NeqEEXqqoqCAxMZGA\ngADa29tJSkrik08+wdHRkbCwMCZNmoSDgwMnTpwgJycHHx8fqqqqNOsGFGYDVKFRIK+0tDTee+89\n9u3bpyPvpVvCik1K7N3Z2cmxY8cIDAzk4MGDREZGMm7cOCZOnMhnn32mLDOB3YBeWXd8fDx33nkn\nwcHBtLa2kpycTGhoKGFhYVx++eUcP35cu6JlZpqjo6OqXTY3N2Oz2SgrK1N1oOzsbCIjI7nrrrtI\nSUmhtLRUVWjkXgj5R1CG1atXM27cOD766COqq6u15PzYY49RUFDAkSNHKC8vV8zaOltCcoja2lqK\ni4t1itK4ceMYM2YMXl5evPvuu4rO/H/BDpsE3AKkG4aR1v3ab4CbDMMYSZe8fh7wMwDTNI8YhvER\ncJQuROLfLoQoAOo1Bbdtbm4mNjaWGTNmUFxcTF1dHTabjbi4OI4fP46Xl5fW2WVrF8lNu93OoEGD\n9MLPmDGDgoICDh48iKurq06y2bVrF+np6dphIV5LeqQEeXj99ddJS0tj+PDhjBw5ksmTJ7N9+3a2\nb9+uxQXpgpCWm/b2Lp3eESNG8O///u/s3LmTadOmUV9fT1xcHJ988gkRERF0X7NemKVQLUWmdPDg\nwTg4OPDiiy9y+PBhWltbVfg6Li5OpZrk+62tSHV1ddp3d+DAAW6//Xaio6MJCAhg4cKFjB49mtjY\nWN2tJEEV7oJ8ppubG2PGjNFwKiAggMzMTPbt28fatWu1RCv4ekdHh5booUeHISIigvb2dp577jnN\nUxYsWMDPf/5zduzYwaFDhzQB/ZcM1zTNXcD3gWpfnec9zwLPXuizrUuqQ56entTU1ODu7s6NN97I\nwIED2b17N15eXvTv35/Y2FhVlpERofJ0CoAtA5yhS4X7ySefJC0tjenTp7N48WLc3Nx47bXX2LBh\nA6GhoQqAS4VMDD4nJ4dZs2ZRU1PDW2+9RXBwML/73e+YMGECkZGRisVaK0sSb4sx3nvvvURGRnLv\nvfcSHR1Nbm6uQkKpqam9uLyS0Fk9zoABA1iyZAlVVVXs3r2bhoYGoqKidEZYeXm5lsJ9fX1xd3en\noqJCcVV3d3eKi4sJDAykpKQER0dHKisrmT9/Pna7XT2zjEaV+FZK29biwcKFC7n88ss5efIk69ev\n19lqEydOZM+ePbi4uOiDL6GCtbxtml0zzqqrqzlw4ABVVVWYpkm/fv2YO3cuMTExHD58+IJGC32o\ncmYYhg7p6+joIDY2ltmzZ5OSksIrr7zC0qVLmTZtmoLikoFK23NjYyOlpaU4OjpSUlKiMw7q6+u5\n8sorSUxM5I477qClpYXf//73bNmyheLiYuWrSkIkF9vX15eQkBCuvvpqXFxcyMzM1LFLAo1ZiTnS\npSyVISGpxMTE0NbWRlBQEDk5OcyfP5/bb78dZ2dndu7cqaiCYK5yLSRkGTZsGKGhoXh4eDBz5kwc\nHR2ZM2cOkZGRlJaWsmfPHh3i5+rqqp26EmYIN1gKNkJ5DAsLIyoqitzcXAoLCxX4F26FdWKmaZpM\nnDiRKVOmkJOTw+OPP86ZM2e47bbbyM/P58orryQrK4tjx44pjiuGKuV5uV7l5eUkJydTV1dHTEwM\niYmJLF26lJqaGk6cOKEPieDp51p9xnBlWEhnZ6d2w4aFhXHw4EESExNZuHAhZWVlrF27thdbSooF\nHh4eTJgwgdLSUpqbmzl8+LDGpffeey+5ubm8//77hIaGkpKSQl5eHh0dHTrHTMQ3xHvV1tbi7e2t\nVEDZ8lxcXMjIyNCxrNK+YmVVybYoSjOzZs3iueee4x//+AeLFi1i2LBh/OlPf2Lnzp29tMDEY1ul\nPVNTU2ltbSUkJIQHHnhAcdKioiKeffZZtm3bpg+PPAR1dXXayNnY2EhYWBiPP/44KSkp+Pj4EBcX\nx4033khJSQlPP/10L2lQgcJsNpseg4ODA3FxcdrgKZM+09LScHNz49ixY9TV1ak8k5WfYN2FvLy8\nWLx4sd6r2NhYhg0bRkdHB0899RRHjhwB6AXJnWv1GcO1llzLy8vJycmhubmZefPmMW/ePKqqqnj+\n+efJzMzUJ9Kq5N3U1MSePXtwdOyaYpiSkkJLSwtDhw7lj3/8IykpKRQUFOhW6Ovrq9NwZHaCwHCA\ntqWvWrWKRx99VA3iwIEDPPPMM2RmZvZiZVkJKNJY6ezszNtvv82gQYMYNWoUI0aMoKGhgZdeeolX\nX31VCTDSwuPu7q48DaFilpSU8Prrr3PDDTco//fw4cO88sorHDp06J/4yYJFS+gj4iKBgYG89NJL\nahR5eXk888wzbNmyhfr6enx8fDQ8cXBwoKamRsu8bm5upKWlkZqaSlRUlFa5du/erRxna6FBJr0L\n90J2MQ8PD/r168fUqVNpamqivLycTZs28e6775Kbmwv0EIwk0TzX6jO0Rjc3N9zd3bUvzG63c/XV\nV2swv3fvXlJSUjSBsHYLCHvMz89PmyH79eun5VgXFxcKCwvVI0ZHR+v8AbnpHh4eGhtaKXYBAQHE\nx8erl5PSsPydJJXWVhNvb2/1MDU1NURHRzNp0iQ6Ojo4evQop0+f1iqVbKPSriTbuqenp46nKi4u\nJjw8HDc3N6qqqvDx8aG0tLRXyVsSS+HDurq6KqYrvWFXXnklzs7OOouhuLhYkQepdlnnlFlZeB0d\nHXh4eBAUFERHRwfFxcXU1NRoh4qEIMJxkJCj+/6qk5Fh4A4ODlRWVlJRUUFFRYUmg/Jeix5w3+bj\nAv8UG5ndXZ8CFQkRWW4U9BZ3Cw0N5cyZMzp42d/fn8rKSqqrq3FycmLEiBEKzotHkLp9Xl6e/h30\nCBLLZ4sxC3pgGF26WqKFIDGZ7AZnM7QExJf4ubW1laamJmVqAbrjWMMGIRhJ6CFZff/+/cnPz9ff\nmaapw6cFVhP2nDV2BTT7d3LqmhshU4os90MZaNXV1Tr2qrq6Wplx8jmAOhzRT5OH2Er1lKTZqiIp\nxygQHPSI/VkqaH3fcOGfBYAlw78QzU2WGIKVI2ttRZHtSwxRQhTgn8jj1hYU6/+f4xx6EbvFaOVf\n6BFgtm6r1vdZOwFknU1Kl9fk/+UcAGWanX39oLdBnA8ntR6rlWhvvW7yuvxr5UVYEZ7zXaMf+rs+\nTySXkqmPjw+urq46itN644RoLHRCuSlCTRQVQuEryE2w9jxZS4lWz2GlAEqcKJCaeCpnZ2ctM0sc\nKYRtK8fBbrcrLVK8nvwsxxEcHKzjl0Tz13pc4unlGKR8K1uqk5MT/v7+Sjb38PDQ3UmuiVxH2fKt\nxi4CHUJoFzadQHPiqWUqkbzu5uamxQXhAxuGocciny0GLgiHUBXl+KWLRZyJhCHSHHCh1WcMV1hJ\ndXV1OotMDE+I1HKTZVt2c3NTQTcHh675YhLzStnTxcUFm82mXglQ47fCTnJhJVaUeFGWELHFm1mz\nZughb5umqfRHm82m2LRpdgkwe3t7q4yqo6Oj1vXlO2UJechaDnZw6C3XKe3ywnG1etvm5maFoKQo\nIritGPTZPWTQm/Qt5yOGLFu87IJSsZSWKyEXWXct8dbyYIkT8vDw0GOQ6ydQqPVenWv1mVAhKChI\nEx3rdi84q3BQ6+vraWxs1PJkTU2NXgwhZzg7O2sRw8vLi7a2NiZOnKgx9KZNm2hsbFSPZ62anb2t\nizFb21OsIYtUmaTgIMZmdjcQys2TKlZUVJS2eFvJ487OzjQ2Nir/VgwuMDBQYUIJMWTyDfS0tku1\nUaiX1u1c+LXy8FtlUJ2dnamqqgJQNEQmnYt3lYfUx8dHv0NwYmm6FBhRdgW5Rt7e3tpEKc5Cjkvi\n4ODgYG2tr6mp0b8/X7Nkn4HDZBsXbYPBgwdr0WHAgAE4OTmRk5PDrl27+Prrr8nMzFTaHfSUFIWV\n5e3trRdlwYIFzJo1i8bGRjw8PBg9ejT5+fl88803FBUVqWeRm2CFuDw9PYmIiGD06NEsXryYlpYW\nUlNTcXFxIT8/n8zMTFJSUnoZtIQgzc3NTJkyhWuuuYb4+HjKysro168f7e3tvP3227S0tLB169Ze\nGrmAhj3V1dVUVFTg4uKCl5cXNpsNm83G8OHDSUhIYO3atZw8eRKAiooKfHx8eiECsrM4OjqSmJjI\nnXfeSWxsrJJlUlNT+eyzz9izZw9VVVVaLrY+bNYY1s/Pj4SEBIYPH87gwYMZOnQoaWlpbN++neTk\nZJqbm6msrNSHxtqGJfyNwMBABg8ezMCBA3XYX11dnYqVbNmyhX379imn4lyrzxiuZNlysefMmcMj\n3XsB4AAAIABJREFUjzyCq6sr6enpGIZBUFAQd955J1OmTGHJkiVaKZN6ektLi0IqkhU/+OCDXHHF\nFbz11lsEBgYyatQoYmNjVQr+0UcfpaCgQLcvgXbk58bGRoKDg5kwYQIDBgwgICCAwYMHk5GRwdix\nY3nvvfdIS0vr5fmEZebn58ctt9zCddddp7xZ4RyPGzeOffv2cerUKTIzM7V/TQorjY2N6ilFQvS+\n++5j7ty5SqMcPnw4y5cvZ//+/TrkBejVhNnW1qa9du7u7hw8eFBLv+7u7kydOpUdO3Zgt9uprKzU\nh0/CBdnFhJT/m9/8huPHj6vQHsDYsWO57rrr2LFjB++++26v7mS5t9K7N2vWLG677Tb8/f05ceIE\nf//731m/fj2PPvooCxYsoKCggOTk5F6Eo+9bfcZwhS/g6+tLU1MTI0eOxNnZmdTUVH71q1/R0NDA\n/fffz/z58+ns7FTWk2TScjMk5pS5CAUFBTQ1NeHv709dXR1//etfyczMJCkpiSuvvJKrrrqKjz/+\nWLdpQLdk8WDCOHvnnXew2WxMnjyZgIAAtmzZwjfffPNPHkoM1NfXl5aWFpKTk6mtrWXHjh0sXryY\nuLg4rcAVFRXpMUvMKVuxNHXabDYWLVrET3/6U9rb21VdfMSIEdx9991kZGTo+6TAInCSlFlFj7eh\noYGKigp8fX256667VPwPegoHVgRDHhw3NzfGjRuHg4MDb7/9tjLSJk2axPPPP09gYCAZGRn4+fnp\ncEWBLV1cXKirqyMhIUGrbjfddBPp6ekMHz6c4cOH68NdVFSkya7QK79v9RnDbW1t1XjUycmJL774\ngqlTpyrLKjIykqFDh+Li4sKOHTs4cuSIcjytJBdpTJSbsWHDBqKiokhISKC8vJzo6GiWL1+Oj48P\nSUlJHD16VGNUAfRF3EPiZilQODo6cuWVV1JZWclf/vIX9u7dqxl5dXW1bsESO1dXV/Pyyy/j7+/P\nxIkTmThxIn5+fhiGwalTp0hPT8fNzU1JMPKACA4qwHxwcLAmb5mZmfztb3+jsbGR3/72t7i6uuLn\n56cxr4wilaxdwh1Jpm666SaioqIUQ5YZGhImWBXeAb2mwgiTHQggLi6OpUuX4uXlxerVq/noo490\naItUBAWbBggMDGTIkCFKXV28eDFLliwhLi6OrKwsVq5cSXp6ei+I71yrzxiuzDOQTHjTpk188skn\n3HzzzSxbtgwHBwdiYmLYtm0ba9as0fkIcmElLpWKlSQT4j2jo6MZOXKkVpP+9re/8dZbb3Hy5Enl\nw1pbr03T1KTFx8eHCRMmEBMTQ2trK0888QT5+fmaUECPlJLNZqO1tVULExEREUyfPp3p06eTkJCg\nxlRQUMCePXu0WCJsNysOKglfTk4OmzdvZvbs2RrKhIeH09HRwfHjx8nLy9M+MtmBJN4XWE7OyzAM\nBgwYQFlZmX6Pr69vL8VEeYABbTs/c+aMyqq++OKLlJeXaxXvnXfe4c9//rMiG/IgyGhbgcRECT4u\nLo73339fw7xPP/2U3//+94qSCCH+fKvPwGENDQ20trbi6emp8eaqVat0OxkxYgSVlZU88sgjHD9+\nXOfvSswIPZBWeXk5ADU1NcTExDBmzBjCw8MJCAjgzJkzvP766zz77LPKivL19VW4x4o1uri4EBQU\nxKOPPsr48ePx9vampqaGsLAwDW0EahJDk5ss5J0pU6awdOlSRo4cqS09dXV1TJo0iWeeeUb1aa3x\npSR3MhnI29ubvLw81q1bh7OzM7feeiszZ86ksbGRvXv3qpq4xLbQo5Iu3luI6VVVVaxdu5Yvv/wS\nwzCYN28es2bNUlE76zEIbCZVSJvNpg2W1dXVrF+/njVr1vD6668rdChJtjx0EqfX19dz5MgRtm3b\nphBhU1MTmzZt4rnnnqOgoKDX+2THPNfqMx5XZEUlKXFxcaG0tFRLlZKg3HnnnaxYsQJAh51YJyJa\nk5T+/fszZcoU/P39tW1EPItwBaRcKdUka7XNzc2Nu+66CwcHB7788ku8vb0JDAwkPj6ejRs3qiFL\nt0BHRwdlZWV4eHjoCKhPP/2UiooKZsyYQXp6OnFxcWzevJnbb7+dGTNmMGXKFD744APV2AL0fETD\nQZow//GPf5CTk0NwcDC33HILu3fvZs+ePTrKSdhZYrAy7Ua6hp2cnLR7Qbo97rvvPu1GEExYroPs\nZiJSsmjRIjo6Onj66acZPHgwixYt4o033tCSsBDAre3lUgBpbm5m9OjR3H333Xh5efHhhx9y9OhR\n3NzcuP3223nnnXeorq5WERU59nOtPuNxpZ4u2e/o0aO54YYbmDBhAuXl5bzyyivk5eVx6623Eh8f\nrzqt0KMMIwZrt9uZP38+K1as4Oabb+bQoUPcddddOlBv8+bNqrVrs9l0CJ6Dg4N+v4uLC35+fowc\nOZL33nuPZ555htdee41jx44xevRo/P39tRtVQhVAHzrxVuLhfve737F161b+/Oc/4+bmpuIgUVFR\n6rmt0xSl0ibtQMXFxVRUVLBnzx5VoNy/f7+GCNLnJuiGJKpyPJMnT2bYsGG6MwwaNIjLL79c+RxW\n8RNZYoQeHh6MGDECPz8/3nnnHd566y3WrFlDU1OTxqymaeo9Ea8tSI+TU9cwmbvvvpuAgAC++OIL\nli9fTklJCYZhEBgYqFxieWjPx8WFPmS4gpt2dHSoJ+vXr59qwgoqYNXcEmkmudienp64uroSGhrK\nT3/6U6ZMmUJBQQEZGRmMGjUKb29vCgoKKCsrU49bWFjYS4zYGqdZobGCggK2bt3K0aNHsdlsSiix\n6mIJfize38XFhSFDhjBw4EBqa2ux2Wxcc801/Pa3v2XkyJFUVlaybds2Bf6tZWMHB4depJ+pU6dy\nyy234OjoyLBhw/D29mb79u1Kx5T5v1KpshZQgoODSUxMxDRN7RVbtGgR06dPVz0DYeXJli87jnT5\nZmdn89lnn+Hu7s6kSZPIz8/n8OHDXHXVVUpSl2O1Pjyenp7aui/aGW+++SY1NTVUVlZyxRVXaGs6\n9IgU/sutO/9/LblpIidaU1PDzp07uf/++4mOjtaBG7m5ufj5+an2bVNTE7W1tYrbdnR0UFhYSFZW\nFtHR0TQ2NrJgwQLsdjvbtm1j9erVVFVVaZuJ1VsKUC4PQ2VlJVlZWQwZMoTx48cTEhLCkiVLOHjw\nYK8RS9JEKdUxqV7V19czadIk7r//ftra2ggJCVE1yf379/Pyyy9z5MgRGhoatJQLvUk0Tk5ODBs2\njAcffBAvLy9iY2NJTEzkwIED1NfXa3XOGmNbNdBaW1ux2+1cdtlluLm5YbPZmDFjBiNGjGD37t38\nx3/8B1lZWWrkZ98PKT1nZGTw+OOPs2zZMubOnUtdXZ2q8kg3BqAld+uxODo64uvryyuvvEJ8fDwL\nFy4kOjqagQMHUlRUxN69e/V6SgHpbDG+s9cFS76GYbgBOwBXugz9E9M0f2sYxgBgLWAHDgC3mKZ5\nxjAMV2A1MBqoBBabppl3ge9QYWfZNuVGPvDAA9x+++24u7tz+vRpXnnlFdavX68KK9KOIuor0jc2\nduxYpkyZgqOjI3l5eVRVVSnpub6+XrmywrktKytTLyXGJc2Ov/3tb4mJiaGhoYHk5GSef/558vPz\ntedMSpySocuwPbvdjq+vL8899xzjx48HoLKyko0bN7Jq1SryuufoWrdpwVEFfquvr8fX15fbbruN\nu+++m7CwMBobG3n88cdZs2aNxqqCxki1SpI0CX/+8Ic/sHDhQpqbm9m7dy8bN25k3759ZGVlqRK5\n7BRWTobAcGFhYbi7u+Pp6cm8efO49tprsdlsvP/++7z66qsUFRVpbGst+QrXws/PD3d3d8aOHct9\n991HVVUVH3/8McePHyc7O1srfAIrWq7pf4/WaHQ9hp6maTYYXfoKu4AHgYeBdaZprjUM43XgkGma\nfzUM4+fAcNM07zUMYwlwvWmaiy9kuJJ9+vj4EBAQwOnTpzXhiYiI0FippKRExzh1dnZqZUi2VWFI\nCS9VKlH+/v7K6BcWlBDQBUqTMimg1TfTNAkPD1dvfvr0aVU0tJJOhAAtMXJNTY0mJjLhUur8tbW1\nevyBgYE6gV1ek4fYmmF7enoSFBREfHw8lZWVpKam6jZuDSuEeyHEFiGW+/n5ERoaSnNzs7b3Ozg4\n6EMsiIrwcq1cYCEySR7g6emJ3W6ntbWV8vJyVXuX6yFt6XI929ra8Pf3Vx6E/CxJscT2suOIp++O\nd/91Pq5hGB7dhnsfsAEIMU2z3TCMCcBTpmnONAxjY/fPyYZhOAElQKB5ni8yDMOULFIogQLhCEHZ\nqogiALd4KblZ4qVEpK2hoQGbzaasJUBvqmxDcuOtnF2RdJLXBJoRgxLCthyD1XhlyYMonbvyt4K1\nyndKZwCgDZNCZ3R2dqa2thYXFxf1qoKrilFLN4Z4W6ncifcTQxLYTrp35XwkNrVKQEmcbCXFy3dJ\nwnY2oV/Kw1YZKmHoWccGWEn4QpayDp2W77uQlL7GRuf7D3AE0oAG4DkgAMiy/L4/kNH9cwYQbvld\nNhDwPZ95D12aYyl0aTOYgNkdNvT6/7N/L/+e/Tt53dHRUT/j7M/7vs861/d83+dbf3e+vwFMR0fH\nH3S85/seJyen7z33s8/3fMdhvRbnO4+zj8XR0fGC1/B89+p89+/7fufg4PBPr53LJn9QcmZ2CXqM\nNAzDBnwGxP6Q913gM98E3oQeKX1p4ZD4TDylNZaUjlrxwtYxTbKVeXh4KD/U6OaLtre3q3e02WxK\n7RMvIFuaLIlzpaQsyYvV4wGaBIm36T43xU7lOCQZlFhUvJp4OUEmrPMqZIhfU1OTJmICFVn1bmWI\ntYRTcsyCiAhNUEIk+Tu51tYpmeJdAaWSClQpVETr5innKBKkcg6yk1nDBrl20BOKicd3cXHRY5Ow\n7XwdL/8lOMw0zRpgGzABsHWHAtBbH0y1w7p/70tXknbeJQ1+ZjcRW7YU2V7lRstNsdayrf1Kra2t\ntLS0aGwnMZpsW3LhgoKCesFPVh6r3CC5+FI6lmO0wj0+Pj6q+GIlpkg/l1x8MThJVr5vG7YmNFY1\nGHkgJO50cnLSYoOUigXGOztmBzRxFGOXB9Y6mEW2fkEohBsrsJ4YvpTTHR0ddfif9MlBT5wqD5eQ\nzaW6aQ13hDstuYq1L+9CbVo/RDssEGgzTbPGMAx3IJGucGEbsJAuZOE2YH33Wz7v/v/k7t9vPV98\nK0tOxto4KC0tHR0dOpc3JCSEL7/8ktTUVH3K5eKbZs9MBuExeHt7axLQ1tZGQEAA3t7enDp1Sltx\nBPQW/FSMysoPdnDoGjY3cOBA2tvbycjI0Jtk9fpCDpcavfADJNaU5FO0fEV/zJqYSOesKCgKMtDZ\n2SXdGRERQXR0NLGxseTn57NmzRpNGIWgIi0xQsiXYdSBgYGEhIToJPnS0lLy8/PVC0s3g9wTEVsJ\nDQ3Fx8eHhIQEXF1daWhooK6ujvz8fPz8/KisrNSijTR1WnclqaIJT9fd3Z3w8HAKCgrUORQXF/ci\nzJ/PbH5IqNAPWGUYhiNdHvoj0zS/NAzjKLDWMIzfAwfpEsaj+9/3jC5B5yq61MkvuMRrSktHTU2N\nMqmmTJnCgw8+SGhoqBrVkSNH1NjFu4ihSfu6dEgIghAUFERkZCTOzs7MmDEDDw8PCgsL2b17N+Xl\n5XrRxJilE6O9vZ3ExEQefvhh3N3dSUlJwcPDQ0kyEqLU19fT0dGhna7S0i3c3EcffZQrrrhCoTJ3\nd3d+//vf88Ybb6hhi+eR8qcYsru7O3PnzmXOnDkKrTk5dcnyFxQUsHnzZsV+rWGIlMBlgvqyZcuY\nNm0aYWFhKgj40EMP6fwy2fXk2oqXDwkJ4emnn2bq1Kl6nIZhkJ+fz7PPPsv69euVXGRNdCVkEUaY\naZrMmTOHBx54AJvNRlZWFg0NDRw+fJiVK1fq+V9o/RDtsMN0iTmf/XoOXUrjZ7/eAtx4wW8+a0kc\nJDV2aQWZPHkyDzzwAO3t7axbt45x48Zx2WWXqYaCFfYC1PAkc42IiCA0NJRRo0Yxb948IiMjaWho\nIDIyEsMwSEpKIiMjQ0fOi+EJh8HBwYEBAwbws5/9jP79+/PII49w8uRJQkNDFYO1ejarfJDAUZGR\nkTz22GPMmTOH7OxsPvzwQ1xdXbnjjjsoKytTUrd4buuAbRcXFyoqKli8eLEeQ0ZGBp9++ikjRozg\npptuIjY2lm3btmnMKHj02fzcsWPHEhERQW1tLdXV1VRWVqoSuuwGwoiTnUvQCmdnZ+rq6khOTiY/\nP5+kpCQSExNV7Hn9+vWK5FhREmuRp6GhgdGjR3P77bczYsQIamtriYiIoLGxkezs7F4DAi+kZtNn\nKmcSxAtpBrqSswEDBpCWlsbatWvJycnh9ddfx263K+Yo2Kf8vdy8wMBAHnvsMeLi4ggKCtJtNyUl\nheHDh9PY2MjBgwd56623KCkp0ZstS+JRm81GUFAQ/fr144UXXuC7777j2muvZc6cORQUFFBZWakJ\nlzw87e3tujW7ubkRHByMg4MD//mf/8nOnTux2Wzcfvvt7N27V/kTEuNKstfc3IzdbqexsZGQkBBi\nY7vy4ffff58vv/ySsLAwnS8s27zEi9YmSWuYExERQVRUFNXV1fz973+ns7OTkpISjS3lGljjXUmY\ncnNzefzxxxWik63ewcGBsLAwbDZbrwRPpgZZS8dBQUE89NBDWjJOTk6mpqaGr7/+mhMnTvTqs5Nd\n51yrzxiubD1SwZE4zcnJidWrV5OXl8eYMWNobW1l/fr1VFRU6FYuyYKPj48mZf369dP+qs8//1xj\nXBcXF2pra/nss8/Yv38/FRUVvRIXQTTkeNra2pg5cyamaXL06FEmTpzIwoULdd6CFb+UOFkSLmuz\nYlhYGKtWraK2tpaHH36Y0NBQnn76aSorKxXZkFhdsFLhDwie7enpSXFxMZdddhk333wzkZGRJCcn\ns2fPHlWREdklORe5lnV1dTQ2NtKvXz98fX2Jjo4mLy+Pyy+/nIMHD9Lc3KxlV3EegujIMUlVrra2\nlkWLFnHPPffQ2NhIVlaWPsCS/Jpmj6aabP+xsbFMmzaNzs5OCgsLCQwMxNvbm/r6empraxWtkYrm\n+VafMVyBm6SSIkB2ZGQkbm5uTJgwgZCQENLS0ti8eXMvZr7QGwUyk0rSpk2bSEhI4IMPPuD06dPc\neOONuLm5sWHDBtLT03sN/oMeEQz5f6kY+fr6kpuby913301sbCy1tbX/pJxtRSasWX1rayvV1dVk\nZ2dTX1/PtGnTCA0N5ZlnniElJUUNVtrUAaU4SrhRW1tLUlISP/nJT5g4caJCZidPnuSNN94gPT1d\nEyspYIjRykPh4uJCWloa1dXVBAQEMH/+fAoKCli1ahVlZWXU1dXpoBQJ1yRJ8vT0pLOzS/r+qquu\nws/Pj/nz59PW1kZWVhbFxcVERESQn5+vuYqELNbysTzkDg4OSoQ/deoUM2bMoLm5meLiYiW+C7J0\nrtVnDFe8E/SQoF1dXSkrK2PWrFmUl5ezb98+8vLyKCoq6iUOB2idXQzf3d2d9evXU1RUpGTx1NRU\nioqKNC6VkqSodcvnSHVOyNeFhYVERkYSGxuLYRh88sknHDt2TMkp1vFKEu5ItcrJyYmsrCxWrVrF\n8uXL6devH3/7299ISUnp1R5kKcyo0QrPVm50W1sbU6ZMoaWlhffff58333xTS+NnN0salrZ68X4L\nFixQxXBfX1927NjB6tWrFUqzQmXyAAg8ZrfbKSkpYdSoUZw5c4Zdu3axa9cu5s6dy+zZs/nuu+/I\ny8sD0J1Cvl+SuRMnTvD1118zffp0mpubKS0tJS8vj4EDB3L//ffz1FNPKfZ+odadPkNrFDBcFFyG\nDh3K5MmTKSoq4plnniErK4uIiAiKiop61dPlpsjJylZ99OhRSkpKKC4uZtSoUaxcuZJhw4YpzOTp\n6ak8XKt4nPAcpH4eHR1NdHQ048aNw9PTkxdffFEHqAgScLZSosSbclweHh4sWrSIyZMn09raSlJS\nEmPGjFEDkjBGwhQxZjmu+Ph4Hn30UQYPHqwGNXfuXFpbW7WxUjBs8XIyTFD6+ObOncuMGTPYuHEj\nq1evVqJ7dHS0zniQMrccj3jd1tZW6urqqK2t5bHHHuOxxx5j2bJlZGdns3//fjo6OnTwiiSs8M9z\nkfPy8nRU1wcffMDLL79Me3s7CQkJWuIH1NjPt/qM4UowLwnWzTffzIABA9i0aRO1tbWMGjUK0zTJ\nz89XQ5HsVeJTgY86O7sUv0U1ZtasWcpDHTp0KA0NDXqRJBaVeFkQCgk7KioqCAkJ0Q6C8vJyWlpa\nFAc9c+aMJiHQExtL2BMWFsZtt93Gtddey8mTJ6mqqiI6Opp+/fpppi7GJvGhGLFgn3feeSejRo2i\noqKCJ598kjVr1uDv78/PfvYzFcOT62adYSGZ+aBBg7jzzjvx9vbmm2++4dNPP6WlpYX8/HyOHTum\nGb+ESnIswrwLDg7WHr/q6mqd0j5z5kweeOABHB0dOXHihFb2BFKzFjmsg0zS0tJUi2LEiBGEh4dz\n6NChXrPefjTt6c7Ozvj7+1NeXk5kZCRTp04lNTWVxMREbr75Ztzc3HjiiSc4fPhwr4trJbpIh6mP\njw/h4eH84he/ICIigrKyMux2O56engwfPpyysjJycnKU4CJt4FKmFebUmTNnKCwspK6ujpaWFqqr\nqzl27BjJycm9yCZywyVeFvkmIeLMnz+fU6dOsXnzZq655hoefvhhnnzySfXwkhAJ8O/h4YGfnx+l\npaXYbDays7NxcHDQuDw2NhZXV1ftMxP4T0Is2Qmk/Ub6xiSG9ff3x8HBQZEWq6cVZMWaqFZXV7Nk\nyRKio6N57733CAoKYurUqdxxxx04ODjw3HPPkZ2d3Uv8Q4oeMrbqzJkzOhrL398fPz8/namxYcMG\nNm7cCKAw4IVqVn3GcDs7ezRRZUD09ddfT2JiIunp6fzlL38hNTVVY1CRKbLGddKAJ/FxeHg4cXFx\nREVFUVJSQnNzs877lfeLWLIkIqKvKxlyZGSkkspXrFhBSUkJ4eHhvUY0iYeW8+js7Ow1C7isrIzR\no0czcOBA3N3dlYcq8bCVlQVo97DEyRs2bOCmm25i8uTJjB8/Hnd3d7KystiyZYsOG5G/lQe6s7NT\nH+TKyko+/fRT7r33Xh555BECAwOVgxwUFKSMOWvIIjvXmTNn8PHxoX///txxxx1cd9112gFy+PBh\nXnvtNf7xj39owmhV5JFdQHDZmpoaIiMjlRTf0dHB7t27eemll3Tqj1xHq1by960+ox0GaM3f2dmZ\nIUOGMGnSJKqqqvjiiy/UuCSulQtrPX4hz0BXbCoM/erqatLS0nBxcSErK0sTOvFSVr6CNTEQQxo8\neDCmaXLy5EmFt6S1R44D6JUFi8dzdnYmJCSEhQsXMn78eLZu3cqXX36p2LEV1bCej7U8ahgG4eHh\nLFmyhEGDBnHkyBF27txJamqqIhni9a3GK9fK3d2dfv36cdNNN3HFFVcAkJSUxEcffURBQYE6A4nR\nBYWQ6+zp6UloaCg///nPiY+PJz09nW+//ZbDhw9TVlam75WdUErsEjI5OTmpUwkLC+Pf/u3fiIyM\nZMOGDSQlJVFTU9NLn1cS1e7P7dv6uFaVb0CJy0JAkRsiN0kYX8JhFQxVZhlIkiRcVqnM1dfXa/+a\n6FlJN4Rp9rRNWz2YGJa1v8za1yU7gGiZWfmxsiULU0pwVmvHgRV+E49jLfcKv1a4A9K/JWiGFVmR\n7Vm6Q+R1YawJO04INJK8WduP5FytnbuCWljRComD29raCAwMVAET2QmtfAXrrmLFv60iLAKdyTF1\na1P0bcMVOEaeVCuwL9ifxE8ST0JPN6tcKDFUIXrIf9I1IYYlF0xuplTKhKfwffRFQI2y+7h7USqt\nN17eK4YuhiPndj6M0so1EKOT91kzfblW33cPBaGRxkN5kGX7l3K0XB/r98jnWeN3MVKrCIt1hxDY\nTP6VB95a0JGf5fdnX09ru5E0B/R5w7UamXhI8aLQIwVkNURrjGi96FY4yapXYDUYMXCJo+R9Z9fI\nJXHrPk5NdmR7lPhNjB96uh8kVBAPLudgTeqsCdHZ90J2IHkP9A5HrFppcvzimYXkY92tZCuHnhG0\ngt0KYcfqIa1JsHjRsx9c6/eJQ7Eel/Cc5XjkO62/txoxoOFP93n1bUVyQNWvrWC6UButF1zCBDFs\n68W22+29FLAFj5Q41hqSWKEr6LqIwcHBWjmSJMTHx0ffI+VbT09PVT6X3UH+RlrFQ0NDcXBwwN/f\nHxcXFwICAnB1dcXDw4PAwECCgoJwcemaqeDn56ecVVlW3oAcN6CNhc7OzkoU9/Ly6mUQcm1E40Fw\naV9fX2RQjECI8gCJNxejthYvhLtgrarJMVl1gEWJSI5fro20Ill3JHFAcu1kl5HvON/qM4YrN0MI\n0laIB9AmRLk41vIh9HSTige2boFi5KGhoXqhxJP7+Pjo+8+cOaPTDtvb27W5srOzE5vNxg033MDM\nmTMJCQnB3d2d9vZ2AgMDFVUQ7yudurL9ClHa2tfV1tZGaWmplrWbmpp0ZxCStXhb0zQVK/b392fo\n0KEMHDhQ+QwCiwHq+STJkmqicJcFdREjlOsmx2Td5iWWFS6IQIRCG7V6SMk3pIQtYcPZIaAYuDxo\nVk6zNcm8ELWxz4QK8qRKo6PEUNDVrTBv3jwiIiI0lvziiy84dOiQissBqmkrXsL6lPfv35/hw4fT\n1NREYGAgXl5epKens23bNk0w5CZau4B9fHzo7OzUNnnBeVesWMFXX33FqVOn1HtL4UM8TUBAAFFR\nUXh5eTF16lSOHj1KcHAwAwcOpKGhgaNHj7JhwwZ9SESVXDyYwINyDkOGDGHZsmWMGTOG5ubW7OHe\nAAAaR0lEQVRm1q1bx9tvv01hYaFuyVaxO+uD7+vrS79+/Rg0aJBWAUtKSnTA3q5du6iqquqliSBx\nrVTAfH19GTt2LEOHDtUB1Nu3b6ewsJDKykqtfslDKwUJgQ3FkEV+YOLEiaqiuXHjRnJzc3uFIudr\nT+8zOK54D8la5WJdf/31/PrXvyYyMlKrQsXFxZw8eZLk5GQtIkjiIuVOiWednZ156qmnWLBggcJY\nO3fuJCoqCg8PD3bv3q24qRQDhKwSEhKCaZpERUXR2dnJvHnzdDB2aGioPhxCwBZiiKgUdnZ2cuON\nN7J48WLtDBDhkvb2dnbt2sW2bds0wxePI7GmJEKenp5MnDiRG2+8ER8fH77++msiIiK48cYbKS0t\nZfXq1ToBUip+gF5H6Iq7n3zySa6++moMw1C1xdbWVm644QbWrVvHCy+80GtXsOYNpmnyxBNPqDbD\nsWPHcHFxITExkX379rFq1SpVBYKenjK5NjabjfLychwcHIiIiGDZsmXMmzcPV1dXSktLCQgI4KWX\nXtL7dyGuQp8x3ObmZo3H5EINGjSI66+/Hi8vL44fP67aBocOHWLPnj10dHSoSIXMh3B1dVXi85w5\nc7j++usZOXIkpaWlbN++HZvNpjMf9u/fT0NDg+rbCqwlBigKObGxsZSXl9PU1MRXX33FwYMHCQ8P\nV9K1JJPWn2WKYnx8PF5eXiQnJ/PKK6/wyCOPMHbsWJycnDhx4gStra30799ft+W6ujqFsqTub5om\n8+fPJz8/nw8//JDq6moeeOABnXUh3F9JsGQ7lnygra2NcePGMX36dLZs2cKvf/1rXFxcmDlzJkuX\nLsXV1VVHr0qMCT3jp1xdXVUcZdOmTTz22GO0t7dz7bXXMnToUHJyclSrQRzJ2V5Xij3BwcEsX76c\ngQMH8tVXX1FfX09sbCxLly5lz549pKSkaBj2o2CHycl6eXlRWVlJSEgI/fv3x2azsXz5clpaWrj2\n2mvZvXs3u3bt0qmQfn5+SjiWLcbLy0vlQcPCwnj55ZdJSkpS9pKHhwe//OUv2b9/vyYl0h0rWlyA\njkotKSnpVekaOnQo8fHxuLm5kZ+fT1FRkXZsiLf28PDAZrPx7bff8u2331JZWUlMTAzh4eFAj6yq\nq6srubm5Kr4MPdoPooguf/Phhx9y+vRp5syZw9VXX82GDRtYt26dVukEKpTrYI3/hwwZQnV1NVu2\nbMHT05OZM2fyk5/8BEdHR7766iv27duHzWbTmNg6UFs6Mmprazly5AglJSX079+f9PR09u/fT15e\nnmpNAL06sCVGF6bd8OHDmThxIt988w1/+tOfSEhIICEhgYqKCnJzcxWh+Je5Csa5JZjeBa4Earv/\n9KemaaYZXfvMS8AcoKn79dQLfY/dbtd2kra2NoYOHcpTTz1FdnY20dHRSjj++uuvKSgo0JskJGsX\nFxc1OkdHR8aOHUtUt/L28ePHcXd3Z9iwYQQHB1NaWsqnn36qk2Csw5elEBEaGkp9fT1DhgzRMMJu\nt2sxICcnh1mzZvHBBx9oXCiIgjwEKSkpFBYWcsstt3DTTTcxatQo7Zh98skn+eqrr6irq9OdQ4gu\nAvpLGRWgqKhIdWlvu+02GhoaeOONN9RLdnZ26kMsYZYYYGdnJ5s3b6Z///7cfffdPPbYYwQEBJCd\nnc0zzzzD/v37FfeWUMNaIGlra6O8vJz09HTGjRvHuHHjyM7OJicnh5aWFt0ZrG3lwpeQAou0Y4mh\nDxs2jN/85jcMHjwYLy8vVq5cqbG6vPd8xvtDUIVWYJppmiOAkcAswzDGd//uUdM0R3b/l9b92mwg\npvu/e4C//oDv0KBeoJ9x48YpY37q1Kn4+PiwcuVKcnNz1SucOXNGb0xbW5tyV0WHoLS0FCcnJ2bO\nnElmZqYmVm1tbezfv1/xRTFW6CGXtLa2kpCQwLRp00hMTCQsLIz8/HwmTpxIfn4+3t7eREdHa7xq\nbb8uKytTTvCMGTO48847GT58OC4uLjQ0NJCbm0tCQgJRUVEaFjQ1NfVSShQPWldXR0VFBV9++SXH\njx8nKCgId3d3jSmFsyseX5ACqbxJuJCZmalsrMjISNrb28nNzWXbtm2Ulpb2KmhYPbbAbqZpkpyc\nTHh4OL/5zW+44ooruO666xg1apRyQyRxlqQaegoM4v1dXV2prq5myJAhzJkzBz8/Pz766CPlO8hu\ndz5tXPgBhmt2LZnb49z93/mgiPnA6u737aFLf6Hfhb7HesIdHR3s2bOHffv24eHhQWxsLAkJCdqh\nK1xaAfSt3E3xeidPnuSRRx7hww8/JDExkeXLlxMXF8f27dv57rvviIqKUuO38mClDdzBwYGlS5fy\ni1/8gmHDhqkB2O12ioqKFMu1atKapqlq6OXl5dTU1FBYWMiuXbs0/kxLS2P9+vXMnz+fadOmKfRl\nZYdZiwmA/m7+/Pl8+OGHxMXFcezYMQoLCzUJcuwWyBMaodBEoct4rrjiCubPn09xcTHr1q2jubmZ\nuLg4Zs+erVCktOeI15Rk0TS7xPxSUlLYunUrLS0tXHPNNSQmJjJ79mwCAgL0PRKTm91sPXlNYMw/\n/OEPXH755dTX15ORkcGWLVtISkrqNbbLSvE81/pBMa7R1Zp+ABgEvGqa5l7DMO4DnjUM40kgCfi1\naZqtQBhQYHn76e7Xis/3HWKI0IXhbt68mcLCQsaOHcutt96Kr68vTzzxBJWVlWzatEmDfmmOlGqZ\nVNQKCwvJz8/nyJEjZGRksGDBAgICAvD09OTzzz8nNTVVsU5JzgTL7ejoUPK4t7c3J0+eJDc3Fx8f\nH3JycqitrWXSpElK0BFdMsEsZch0Z2cnmzZtYvTo0ezfv5/AwEDc3d2ZNWsWHh4eFBcX640W3oCg\nA9CjtCjNkjfffLOGSEOGDOmFd0qsbq0mCsjf2trKwoUL6ezsZNmyZaSlpTF79mxuueUWZs+ezcaN\nG1VhEno0LiREMk1T5QJWrlyJj48PgYGB2nEtCZ1U6CQsECw9ICCA6upqbr31Vq655hqqqqrYtGkT\nLS0t1NTUMHDgQDIyMjTJvpCmAvzAAoRpmh2maY6kS7FmnGEYw4DH6ZJiGgv4A7/6IZ8lyzCMewzD\nSDEMIwVQAF62fC8vL3Jycvjqq6/48ssvSUpKIigoiMmTJys4LvCPlFxbW1upra3VsEMoiV988QUv\nvPACWVlZzJs3TzNpUVMUPoQUEiQmM02TU6dOMXDgQKZPn851111HU1MTS5cuxW63c+rUKY4cOaKG\nI9itxKxjx47lzJkzrFy5ktdee40vvviCuro6+vfvT0dHBw0NDZw5cwZvb29VgxHSjhCEJMYcNWoU\n+/btY+3atWRnZxMSEkJ8fLyGSVb4yMoKM82uUaxiHOvWreP06dPs27ePpqYmgoKCtFNDiPRCMBJM\nXCYiRUdHk5CQQG1tLUePHsU0u9Q1pRtZPKa1xG632zXvmD9/Pi0tLTzyyCP89a9/5bvvviM6OprC\nwkIaGhooLi7uFbL9y4Yry+yRYJplmmZxdzjQCvyNHo0FlWDqXlZ5JutnvWma5hjTNMfIxbYy6Ds7\nO7n77rtZunQpgwYNUgjpyJEjqjgj2lpWhpEgBJKw3XbbbUyaNInQ0FBCQkIICgpiypQpxMfH09DQ\noKQaAfrtdjuurq7k5eXx6KOP8vHHH9PZ2cldd92Fv7+/er6UlBS++uorDTVEpqmurk5jVbvdzqJF\niwgLC+Oqq67i3Xff5eGHHwbgD3/4A59++il1dXWqACPdEIKjCs2yf//+DBo0CG9vbyZPnoy7uzsB\nAQEEBwcTHR2t3R7idcVorDH84cOHiYqK4pe//CVLlizhV7/6Fb6+viQnJ6uUvrX8K9wLoShKDB4Q\nEMCAAQMIDg5WgRZ/f38dGigOQO6p4LLe3t5UVVXR1NTEqVOnOHr0qF7PmpoaDVckvv6Xu3yNc0gw\nGYbRzzTN4m4U4Tq6VBqhS4LpfsMw1gKXA7WmaZ43TJCTlO1FOkunTp3KjBkzND5MSkoiLS2tl6yR\nVHmElCP8gbq6Oqqrqxk2bBi33nqrDq9uamri888/5/jx45oMSFmzpaVF295rampISkoiKSkJHx8f\nJk2axOjRo6mvr+fhhx8mMzMTQMuf4mXsdjulpaVUVlbS0tLC8uXLiYiI0KTxwIEDrFmzhjVr1mCa\nJrW1tfj6+tLc3KyYq1wDCQVOnTrFhx9+iJ+fn7a5HDx4kGPHjmnbjlAnATU+IbQ7OTnxzjvvMG7c\nOB566CE6OzspKyvjL3/5Cx9//LESceS6SqhhDTlEjWbMmDHccccdKqqya9cuDZ+kemcttYuEaF1d\nHZ9//jmXX345K1asoLi4mEGDBiknWPrvRAnnQqHCDxF2Hg6soktqVCSYnjYMYysQCBh0SZDea3aJ\nPxvAX4BZdMFht5ummXKB7zDliQd0MMigQYO46qqr8PT05Pjx43zzzTeYpklDQ4NWqISbCj3KKb6+\nvtTU1ODr66tbVFhYGM7OzuzatYvc3FxOnz7dS1BP0Anx3CEhIXpBg4OD8fLyIjs7WxMHia1FR0t4\nsjK8rra2lrFjx3LVVVfh7e1NR0cHSUlJ5Ofna9+W7CxCL2xublaPIwYnTZDQVfqWEEOgQMnoJZGy\nIgIS8wvNs3///oSEhGAYBllZWTQ1NWmVTuAnCZ2svAKJXx0dHQkPD2fmzJl4enqqDH5BQUEv9Rl5\n8ABFcoRcNGbMGCZNmkRnZydff/21zpmz6mOIA+s+p75Na5SkxjrmVBIRiTkFFAeU3CztMUIel8+w\nStyLuJ3cQNFLkHKqCBHLxZcOWVHPLiwsVPJHUFAQDQ0NvfilYiwSNsiQPjkPa9wqO4TU9YULITFl\nS0uLQmwSPkljqKj2WDN3iUelYmflF1g7mMXwJAmUDg4h84ihWck00COZKtu4IC6CR1tL7SK4Ijg2\ndCXd0kIkotJC+LcKkAizzTpu6kfRAQE97S7Werl4QTkx6/EKadn6msAoVraS9bPlP+vfy2vynfLZ\n8tSf/feyjQP6s/U4rHL8YvDymfIe+WzrMVp/th6HXIfve+373mO5rgC9Yk7r7+QzJfu3cpvP/vuz\nP+dc2l5WaqX8/9n3yEootz4037f6vOFan1pHxy7tVesMB9myvLy8tENBtjS5iNIyLkNAhDMqnkFu\nkLe3t/6/Fe8Eegk0W3UB5OILTiwZtyQxcsM8PT3x8fHRIRwSVkh1Tgx/8ODBFBQUqBcVT3whXdj/\na6vPs8OsLclSoxeRCck6rSRwF5euIc8CZYkck3BPxUDr6+sJCAigsrJS3ytdDMK9lWV9+h0cHLS9\nRbyyeA9r28/ZjCyBlqCHcBMdHU1zczOFhYX4+/sTGBioDxWghRAPD48LzrC9tLpWn/G4YgjiFSVJ\nstvtzJ49m4SEBEpKSvjrX/9KXV2d0gQFsBcoyDAM+vXrpzV7mRLe3NyMzWajtrZWt1cPDw9tmxYv\nK60o3cel8dyQIUOYMGECTU1NbN68maKiol6iG1b+sMSWdrtdGwkHDx5MW1sbCxcupLy8nA8++IDG\nxkaCgoKoq6tT4bfzjbr/v7j6vMeVubWSKV922WW0t7cTExPDgw8+yPHjx3FxceGee+5h3bp1apjS\n1SqzEGpra6msrMRms2lGLtWdBQsWUFhYyGeffcagQYPIzs6mpqaml9CaVPAkZPHz8yMyMpKbb76Z\noKAgampqyMnJAbpmlgmUZu2U8PLyUu6v3W7nF7/4BdOnTycoKEg7WP38/Hj11VcxjC5x5MDAwPPq\nCFxavVefMVyrbkJwcDDl5eVUVFQwf/58HBy6pJDWrFlDa2srsbGxfPfdd3qjxYOKuotktz4+PqxY\nsUKLFwAFBQVUVVUxevRosrOz2b17t2K6Uh6FngSkpKSEkJAQKisrOXnypKqaFxYWqieXqpeV0C7l\nzwULFnD11VcTGRmpxYD29naWLl1Ka2srr732GqZpqtKhNd6+tM69+kzPmZBNTNPUoXGDBw8mJiaG\njIwMkpOTqa2t5dixY2zdupXa2loCAgJwc3NTYoxstXa7nYqKCn7+859z/fXX097ezmeffcbGjRv5\n9ttvcXJyYsCAAaxYsYLHH3+c0NBQTZwEsjIMg6amJvz8/BgyZAj+/v4UFxezY8cOvvnmG7Kzs/Xv\nRLgOujSvRFq0s7OTnTt3ar9ZQ0MDaWlpHDx4sBd2O3ToUIKCgnpl45fW+VefMlwBoUXKZ/DgwTrA\nuaSkhKFDh/KTn/yE4OBg7WQQ7qq1VFlXV0dAQAATJ06kvb2dhx56iBUrVlBeXo6bmxv33nsvM2fO\npKOjQ+c2QA9DzcreFx7B559/Tnp6Olu2bKG6uloBewkxJFEzDIP6+npqamrw9vZWGE6IKnv37uXj\njz+msrKSCRMm8OabbwJdFTd/f/+LeQt+VKvPhArilWT7ra+v5/nnn8ff35/9+/fj7e1NeXm5Kiee\nOHECPz+/XvMfBFcUic/CwkIGDhzIQw89RHFxMbNmzcLX11ebEjdu3Mhrr71GQUGBhhtW4TrZtqUg\nYLfbGT16NNXV1ZSWlmqoIu8RDywTLT09PRk/fjxhYWE6j2Hq1KnMnTtXKZpFRUWMHz+ed999V0nj\nl9aFV58xXCHOSFITFhZGUFAQQUFB+Pn54e3tzfz58zl+/LgSaerq6v6pIgNd8JmzszMvvPACQUFB\nXH311crdraysZPfu3bz66qtUVFRw4sQJbUsRAraVUF1aWsqhQ4eYMGECs2bNYvDgwaxevZqPPvpI\naX9SFbNqykJXZ+3o0aNpaWnhhRdeoLa2liVLllBeXq58Xnd3dzIzMxVrvrR+2OozhitGI/CWs7Mz\n1dXVui2XlZUxYMAARo0axaZNm5SIYx3mZ23lFqVFLy8vZTclJyezZs0aqqurOXTokHIQBAIT/FXK\nzWKUGRkZuLi4cOTIEQICApg2bRp5eXls3769l26CnEdUVBTl5eUYhkFMTAyenp7ExcXx4osvkpKS\nogIhV199tR5nSEiIElkurQuvPhPjikCys7OzclpramowDIOSkhICAgJ0CvqgQYPUUIVoI9ivbPMh\nISHceOONREVFkZubyxtvvEFgYCD33Xef9qcVFxfj7++Pu7s7drsd6Blx6unpqYyyIUOGYJpdao1b\nt25ly5YtXH755fj6+iqPQrxlS0sLp0+f1k5i4UnExMQQHx/PPffcw8cff6yqiZmZmezevRv4Zwmm\nS+vcq894XG9vb5qamvDy8tJqlRjWjBkzGDNmDDExMdTU1OhUSOkxkxsuZJDW1laGDRvG5MmTOXXq\nFCtXrlSvGxUVpTTJ+vp66uvr8fX17TXZUUgmbW1tjBgxgpiYGJycnGhpacHd3Z2JEyfi7e2tqodC\neoGu+bfSJi9EHYCxY8fy8ssv4+vr20sBJi0trdcMBitX4NI69+ozhis0xZaWFu0r++Mf/4jdbmfu\n3LlAF/f1xRdf5MCBA/j7+2u7jZRqQ0NDtWTq4+NDYWEhhtE1mbKgoIDq6mpSU1M5ffq0Vt78/PyU\nXSVYrvVhOHXqFDabjccff5zy8nJM0yQmJob33ntPSe/WMKOjowObzYajoyMpKSns2LGDIUOG4Ozs\nrGTq+vp6Dhw4wJ49e0hPTyc/P5/w8HDVzL20Lrz6TMnXKqgmwhLCSZg3bx6BgYHk5OSwf/9+iouL\n1cDE8wI6cunMmTMMGTKEX/3qVwwfPhxHR0e2bdtGZ2cn77zzDseOHcPT01MpjmJQsrXLEs/r5eXF\nmDFjiI+Px8fHh5MnT6oCjdAmhWshUJ6rq6tWye655x5GjhzJ+++/z4IFC3j33XdJS0vTBNPNzU2H\n0uXl5V0yXsvq8+ww4SqIRKd070q/k3WwiOhzCXNMiC5W9USJUUXUzsfHh6NHj9LU1ERwcDCdnZ1U\nVFQAPQQfK3lGEjRho1nVWYQJJsR1SeSE6yqhwqBBgzh16hSRkZE6clUE6mpqarjssstobW2lqqqK\nqqoqvLy8tEv40upafZ6rIBwBSbSswneiVCO8hKamJp3TIKVeafITRMHJyYmCggJ8fHyUviiG1tnZ\nqZoLQuaRWFRQCaEqSnHD2sYiuK2Qq618U8FyZUJNTEwMhYWFOsGyubmZ4OBgAgICSEtLw93dnebm\nZsLCwnBxcdFJk5fW+Vef8bgX+xgurb65+rrHbQAyL/ZB/A+uAKDiYh/E/9D6nzy3yHP9oq8Ybqa0\nqf9vXIZhpPxvPb+LdW59pgBxaV1a/5V1yXAvrR/l6iuG++bFPoD/4fW/+fwuyrn1CVTh0rq0/qur\nr3jcS+vS+i+ti264hmHMMgwj0zCMLMMwfn2xj+e/swzDeMcwjDLDMDIsr/kbhrHZMIyT3f/6db9u\nGIbxcvf5HjYMY9TFO/ILL8Mw+huGsc0wjKOGYRwxDOPB7tcv7vlJ1edi/EeXHlk2MBBwAQ4BQy/m\nMf03z2MKMArIsLz2PF2awQC/Bp7r/nkO8DVdmmvjgb0X+/gvcG79gFHdP3sDJ4ChF/v8LrbHHQdk\nmaaZY5rmGWAtXYrmP6plmuYOoOqsl+fTJRZI97/XWV7/Lyu2X6xldsnJpnb/XA8co0uo+6Ke38U2\n3HOpl/9vWMFmj7xqCRDc/fOP9pwNw4gCEoC9XOTzu9iG+39imV176I8avjEMwwv4FHjINM1eOlEX\n4/wutuH+IPXyH+kqlS2y+9+y7td/dOdsGIYzXUa7xjTNdd0vX9Tzu9iGux+IMQxjgGEYLsASuhTN\n/zesz4Hbun++DVhvef3W7ux7PD9Qsf1iLaOrtWMlcMw0zT9afnVxz68PZK1z6MpUs4EnLvbx/DfP\n4UO6pgq10RXT3QnY6ZpGdBLYAvh3/60BvNp9vunAmIt9/Bc4t8l0hQGH6VKeT+u+Zxf1/C5Vzi6t\nH+W62KHCpXVp/bfWJcO9tH6U65LhXlo/ynXJcC+tH+W6ZLiX1o9yXTLcS+tHuS4Z7qX1o1yXDPfS\n+lGu/wdAeAXL/MQV8wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RalhZHlCHPeG",
        "colab_type": "text"
      },
      "source": [
        "MODE COLLAPSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQh9debltAVr",
        "colab_type": "text"
      },
      "source": [
        "WE CAN SEE THE MIXING OCCURNG IN THE LATENT SPACE AND ESPECIALLY BECAUSE OF LOW DIMENSIONALITY 522 THOUGH THE POINTS IN LATENT SPACE ARE DISCRETE AND KIND OF ONE TO ONE MAPPING BUT THEY STILL SEEM TO BE QUITE CONTINUOUS. TAKING A HIGHER DIMENSIONAL DATASET MAY SHOW THE SPARSITY IN THE LATENT IN A BETTER WAY."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkKRbjiJ7AOX",
        "colab_type": "code",
        "outputId": "0a16bf82-ecd5-4612-ae76-cec20ec5bd9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "#layer3 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAHTCAYAAABiN8IeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydeZRdVZn2n12VhHnKHDJCCEOYAgZE\n0BZEURAN2K1Cd7toW40DtNDaS9FPl3zda9kup26X2rZRWOBqRFFAgUYGUSYZEwwQCCEDCWROGMNM\nUvv7owq+Os9+qu6uU/fuqpLntxaL7J33nrPPOe/ZO/e+z37fEGOEMcYYY1pP20APwBhjjHmj4EXX\nGGOMKYQXXWOMMaYQXnSNMcaYQnjRNcYYYwrhRdcYY4wpRL8W3RDCe0IIS0IIy0II5zZrUMb0hv3O\nlMY+Z5pFqLtPN4TQDuARAO8CsBrAPQBOjzE+1MtnWrYpeMSIEZV2e3t7YvPiiy/WOvawYcMq7ba2\n9N8qr7zySsPjhBCSvuHDhzc8Dp9PXdurr77a0Gbbtm1JH983df7tttuu0n755Zdr2QDYHGMco/4i\nl8Hmd/z81H1/6aWXah2b/U75Dz93hfocH1sdh/1O+f3WrVsb2nR0dCR9OX7XLJsYY3oD+sBg9zl1\nz3t4/xrCfqHgZ64Yqj7HY1TXyve/h3ewx7mu8R3umSMBLIsxrgCAEMIvAMwB0KMjNgs1se25556V\n9u67757YLFy4sNb5Ro4cWWnvvPPOic2jjz5aaat/zPBDB4Dx48dX2qtWrUpsdtppp4bn37BhQ6W9\n6667JjZPPfVU0sf3beXKlYnN1KlTK+1HHnkksZk0aVKlvXz58sQGQHpxfWfA/E693Pz81H1/8MEH\na51vjz32qLS33377xObxxx9veBz+BxEAjB49utJevXp1YsN+x20AWL9+faWtfPPZZ59N+nL8bsKE\nCZW2ejdybJpAU3yO/UctDDmMGVOdy3fZZZfEZsmSJbWOzfOmWoT5mSuUr7I/q+Ow/6jjbNy4sdLe\ncccdE5vnnnsu6cvxFX4v1Bj5/q9bty6xiTH26Ij9+Xl5IoDub/zqrr4KIYS5IYT5IYT5/TiXMa9h\nvzOlsc+ZptGfb7pZxBjnAZgHtPYnF2O6Y78zpbHPmRz68013DYDJ3dqTuvqMaSX2O1Ma+5xpGv0R\nUg1Dp7jgeHQ64D0A/jbG2GMAq+6//jgoz3E0AHjmmWcq7RdeeCGx4QA4kCdEYcaOHZv0bd68udJW\nsb2nn3466dthhx0qbRXcZwGUiptxDEPFQtSxc+JKu+22W6XN9zrnM12fWxBjnN3ww70wkH43atSo\nxIb9rJV+x7EkAHjyyScrbfYnQMe3MsUgFVRsmAU7yqYHcVPD89XxO45vPv/889i2bVt/hVTFfI7j\nvupd5/ng+eefT2xULDZHAMUon2NtiNKqqPeAtThK2MnkXIc6v7rWnLmO522lR2B6mI97nOtq/7wc\nY9waQjgLwHUA2gFc0JsTGtMM7HemNPY500z6FdONMV4D4JomjcWYLOx3pjT2OdMsnJHKGGOMKUTL\n1cuN4D2wHKMC0hiu2helYohMnTiagveJKXLjJ3USdqgYHaPi3mov5H777Vdpb9myJbHhPrUHmuM1\nTzzxRMMxDiQ58ULes7dp06bERu1dZZrld+r8HHfO3ftZZ0w5CRdU3Hvt2rVJ3957711pq9gZxyqV\n33EMVM0fg4WceCFfo7oepddg6sRvFcrnchJoKHJiuEzOdaiYqrpvkydPrrSVxoZj0crnGHWc3vA3\nXWOMMaYQXnSNMcaYQnjRNcYYYwrhRdcYY4wpRO3kGLVOFkLMqeLA8GemT5+e2HCC73333TexUYn6\neTO9Gg+LndSGcR7jPvvsk9jceuutSR8LT5QAiYP5KnCfk+xg//33T/oefvjhpI/ha1MFJzKrmvQ7\nOUYd6vodC1Y4YTqQFrpQvqmKP7D4QyWQ4D4WHQKpkGivvfZKbO65556kL0fAyGNUAr6c6lIsmgKA\nFStWJH0M+7Saq3KeY3+rDNWhra0t8bkc8Rrfc5Vkh4VpXJAEyCucop4V388cIRELlADggQceSPpy\nxIvsT+q94PdSiVHVmHKKg7DPKf/KXDN7nOv8TdcYY4wphBddY4wxphBedI0xxphCFI3ptre3R45Z\n8AZxFUPI2XzMsQ8VP1HxES7+fuCBByY2OUXIuUCzKhiv4OQGOc9DxYuXLVuWdb5G51cJ61966aWG\nx+EiED0kEBmQmG57e3vkQtccn1S+kZPsnD+nEgCoBBrsHyoWqzQIjc6fM+a6TJs2LelTCVfqkFNM\nQZGjiRiImG57e3vk4hOc7EMVX1eFAhjWoaiEKKrwBcc+VeEW1igo2J9VwYVmMWnSpKRv9erVTTm2\nKpSgYshMjsYGjukaY4wxA48XXWOMMaYQXnSNMcaYQvSr4EEIYSWALQC2Adg6EPE688bDfmdKY58z\nzaIZVYaOizFuzjHs6OhoGHRXgpYcIVVO5QtVMYPJEU1xZR4gFdCoRALvf//7k76rrrqq0v7Hf/zH\nxIYTFxxzzDENbVRChqVLlyZ9vIk9R8ihkmOUFOR1ke13McaGopy6QipOTqHuQ05VqhzRlBIysWBQ\njXnOnDlJ33XXXVdpn3rqqYkNV5eaPTtdZ1gQpq5DJSXg5Bg5fsf3GsivqtQk+uRzjZJhKNFozn1g\n8SO3AWDz5sbDzBFNTZw4Menj61Jz+l//9V8nfbfffnul/ba3vS2x4XlzxowZiQ3PdZwYCdDvHM//\nOQIwdW/rVEvqjn9eNsYYYwrR30U3Arg+hLAghDBXGYQQ5oYQ5ocQ5vfzXMa8Rp/8bgC+hZu/POxz\npin09+flt8YY14QQxgK4IYTwcIzxlu4GMcZ5AOYBnTlw+3k+Y4A++l1bW5v9zvQX+5xpCk1LjhFC\nOA/AczHGb/dks8MOO0ROAsBxRpVgmjcxcyIKIP29njfNAzomxMkxFLyJPSfukssFF1xQaaskF7zR\n/YADDkhsvve971XaKu6r4poc0/3973/f82C7ULEoLgKh4sdoQXKMHL/bfvvtI2+y56QOKk7DidVV\nkgtO2s7JXwCd+GGg/e673/1upa00APzeKb/74Q9/WGkfe+yxiY3SZLB/cLxPoe4t+52KUzY7OUaO\nz2233XZx/PjxlT6Obau5l5+58h2OeyoblRwjR9PCiTc4rp8LFw4AgG9961uVtnpWHKNXxTJ4zjz6\n6KMTG5VAY82aNZX2woULExtG3Uf2uccee0x9tPnJMUIIO4UQdnntzwBOALCo7vGMycF+Z0pjnzPN\npD8/L48DcEWXumsYgJ/HGK9tyqiM6Rn7nSmNfc40jdqLboxxBYBDmzgWYxpivzOlsc+ZZuItQ8YY\nY0whilYZCiFETmLBQel169Y1PI7aMM2CFhXI50B6LlOnTq20V61a1fAzo0ePTvpuvvnmhsfmSiAK\nJUDga1NJA5SQTCVFaEQ/Kn8MWJUhFkVxMoz169c3PM706dOTPhYJKSEg+2YuI0eOrLQ5KUAuN954\nY9J3yCGHVNpK7MT+8tBDDyU2/NyV2EzNMZ/4xCcq7ZzqLuqdykkCMRBVhoYNG5ZUVGPBU07SFFV9\nihOSqEQ8yg9zaFYFoUsuuSTpe/Ob31xpK2EX+9z999+f2Kxdu7bS5jUE0JXRvvrVr1baOSKxuklz\n4CpDxhhjzMDjRdcYY4wphBddY4wxphBedI0xxphCNKPKUP7Jhg1LshnlCKcYle2IM5dwFRNAi6u4\nYobKbsLVVebNm5fYcOD+jjvuSGyUKIIzISkBFIurlJCMxU3qvnL2KYWqJMPXxtmcAGDKlCmVNguX\ngLxKOq0ghJBklskRTjEqaxNft8pOoyqVsLho1qxZic1RRx1Vaf/0pz9NbFgwc9NNNyU2LGBRn1NZ\n3ligc9hhhyU2M2fOrLRZ5ALozD8snDrooIMSG3431L3lrE/8nOuKJ5sBi4JYOKWEjewrSjTJlX9y\nqrAp1D3n+e/CCy9MbPjZnX/++YnNCSeckPSxz6nKcCxuOvLIIxMbvkfqGS9YsKDhsffdd9/EhlHH\nZkGfygjWm9/5m64xxhhTCC+6xhhjTCG86BpjjDGFKBrT7ejoSOKT7e3tlbaq9sKoWEDOJm6O3ypU\nbIuTC7zvfe9LbDjOqzZVq3gtx81UAgK+R2rj+5IlSyptFVPg+BcAHH744ZX2/Plp2ePddtut0lab\n8blPXf9AEWNMnn0dv8up+NLT+RsxduzYpI+fhYqp/vrXv6601TNW18YJO3Iqrij/Ze2E8jt1bZxo\nZNGitH4AJwdR7zj3cUKKukkiWgFrSnLmI1XlJsfnchg3blzSx3oZlXji0ksvrbQPPPDAxEb5Cid3\nUXMN3xMV92a9ikrMo+ZxrjynNCbsqzk+pxLC9Ia/6RpjjDGF8KJrjDHGFMKLrjHGGFOIhotuCOGC\nEMLGEMKibn0jQwg3hBCWdv0//QHdmH5gvzMDgf3OtJqGVYZCCH8F4DkAP4sxHtTV900AT8YYvxFC\nOBfAHjHGLzY8WQgNFSUqKM2BaxUk58obSmylRBVc7eS3v/1tYvODH/yg0j7ppJMSmx133LHSVuIV\nlbDilltuqbTvueeexOYPf/hDpc0VYgDg4osvrrQ/+MEPJjZKLMNJNf74xz8mNixoURVheMN6DxU8\nsqsMlfY7JVhh0R/fByAVhyjhhxKVnHbaaZW2Smrx9a9/vdI++eSTExsWh6hno5J6cMWrW2+9NbG5\n/fbbK20lmPnd735Xab///e9PbP785z8nfZzg4c4770xsWBSl5iq+t6pKV1+qDDXL75TPsW+MGDEi\n+RwnosnxuVxYAKqS5XzhC1+otFWSi8mTJ1faStiljv2nP/2p0lZV13iOUomA+Djvec97Ehvlcywc\nUxWMWOzGgksgndt7EMTVrzIUY7wFAD/lOQAu6vrzRQBOaXQcY/qC/c4MBPY702rqbhkaF2N87Wvb\negCp9ryLEMJcAHNrnseY7tjvzECQ5Xf2OZNDv/fpxhhjbz/fxRjnAZgH5P3MZ0wO9jszEPTmd/Y5\nk0PdRXdDCGFCjHFdCGECgI0NP4HOOCvHKDgWqzYjc2xH/c7OqJjqtGnTkr5LLrmk0n755ZcTG46T\n5Zw/J5EAAFxzzTWV9s9//vOGx1bxku9+97uV9uLFixMbVYSAE9Sr2CbHwlUChmXLlunBNpdaftfe\n3p4k+ODkECoWyPoC9UwZZcNxcyBNaqGKIvCYVAyQz6d0CyrxxG9+85tK+4YbbkhsGFVw4Ctf+Uql\nfd999yU2Ks69efPmSlsVI2E4fq3GVCfpSQZ99rv29vZk3mKdA8dvgXSuU/NRDnvuuWfSd9VVV1Xa\nSj/zwgsvVNqsVQFSH1NjVAUH2OeUfoXhIhEAcOaZZ1baSqui4v+8tijdD/uPiqnznJlTSKc7dbcM\nXQngjK4/nwEgVR8Z03zsd2YgsN+ZppGzZegSAHcA2C+EsDqE8DEA3wDwrhDCUgDv7Gob0zTsd2Yg\nsN+ZVtPw5+UY4+k9/NXxTR6LMa9jvzMDgf3OtBpnpDLGGGMKUbTKEJCKKrjNghcgFb2ozdgsHBg9\nenRiozZDMxMmTEj6nn766Upbib1YCKMEAOeee27Sp5ICNOLss89O+p544olKWwXy1Zg2bNjQ8Hws\n7lDHGcyEEBKBBIso6lZz4aokSnjx8MMPNzyOEkmxyFCNh98fdS5OAAP0mLykV0499dSkb/369ZW2\n8jt+f4D0nVZwpSr2cUWThFNNgX0uRyyW43OcHEjNmStXrmx4HDWPscCNhVVAOtfddtttic3nP//5\nhufP4aijjkr6OMmQEnKpa8uZt/he8juoyKkW1R1/0zXGGGMK4UXXGGOMKYQXXWOMMaYQRWO6W7du\nTWIGvNFaberm38xVnIH7cuK3ChUT4oQVnDgcSK9DJRLgJO8AcNlll1Xa1113XWLDccLZs9M82lw4\n4cILL0xs6rJp06ZKW8U/GZXsoVFxjVaxdevWJEk8P2dOhg7kFXHg2FHdeLeKC11wwQWV9imnpCl/\nOQaoklOomN///b//t9JWBQ84ecIRRxyR2LAm4dprr01s6sIx3Dp+N1A+t23btiSWzYlMuHAAkMa6\nn3322cSG57qcuGMuP/7xjyvtOXPmJDasA7njjjuyjn3OOedU2irJDz+/WbNmJTYPPvhgpT1//vys\n8+fAPqe0FkxukZPX7fs+LGOMMcbUwYuuMcYYUwgvusYYY0whvOgaY4wxhQglhQYhhMhJCTiJhUrW\nkDNGDsDPnDkzseEAPJBWzFEbrflYSuzFIgGVZEMJWngD/fXXX5/YsFDgT3/6U2LDooxf/epXiY3a\nnK8qnTDTp0+vtJcvX57Y8PWq59jR0bEgxpiqwFpMCCGy2IHFaSzwq8vee++d9KnqUnx+tZl/6tSp\nlbaqVnTWWWdV2lOmTElsOIEHkPrCFVdckdjcddddvbaBVIB28803NzwXkJdQgH368ccfT2zGjBlT\nabMQpqOjAzHGVNXXYlRpP05q0SwBlJprOIEEkCa5UYk42C+VP33605+utNlPAS0SYz/45S9/mdiw\nKEsJYjlpykMPPZTY1BVy8nrAyV+A9Dmq5C8xxh7nOn/TNcYYYwrhRdcYY4wphBddY4wxphA59XQv\nCCFsDCEs6tZ3XghhTQhhYdd/J7V2mOaNhv3OlMY+Z0qQk5HqQgA/APAz6v+PGOO3+3pCztykAtUM\ni62U6ITFTb1lBOkOCzqUuIEFUEqk9KlPfarSfvHFFxMbJXjg8++///6JzYc+9KGkrw45FVjUGFk4\npbI3sXCDBXNA/jPp4kI00e/43DnCqd13373SznmmudfIPqwEfOz3f/jDHxIb9jt1fiXge+WVVypt\n5Xef+cxnkj4mp4JSjmhKib1YOKX8jgV7LEzsIxeiiT7HYp4c4RSLhJTQkcVjuT5Xp2rWwoULExs+\nn8raNGrUqKSP359p06YlNl/96lcbjjGHHNGUqkTH61GOz6m5jte57jT8phtjvAXAk43sjGkm9jtT\nGvucKUF/YrpnhRDu7/pJZo+ejEIIc0MI80MIzUuQad7I2O9MaexzpmnUXXR/BGA6gFkA1gH4Tk+G\nMcZ5McbZA7E/0/zFYb8zpbHPmaZSq8pQjPH1H7VDCD8BcHXTRiSoE39bsGBBYqNiWzmVP2666aZK\nW1Xe4E3kKkal4Oo3KpECx3C46k8zUZvqGVVVg+NXanN6fyntd2rTO7PjjjtW2mqjPiclANLY43bb\nbZfY3H777ZX2UUcdldjsu+++lbZK3KKeBfvQsmXLEhu+NlXdq1nkVGfK0SQ0m/74XJ3EQ6qqEMMx\n1EcffTSx4QpRQBp7VNoYjtGzfwHAQQcdVGmrBBoqzsnz+NKlSxMbfi9a+cxz1hXWPij6qFWp9003\nhNBdPXIqgEU92RrTLOx3pjT2OdNsGn7TDSFcAuBYAKNDCKsBfA3AsSGEWQAigJUAPtnCMZo3IPY7\nUxr7nClBw0U3xni66D6/BWMx5nXsd6Y09jlTAmekMsYYYwpRS0hVl7a2Nmy//faVvhxxBm8YVyjh\nFPPMM880tOHxAak45fDDD09sWBClquxwtR4gFavkJOdQG7ZZAPbHP/4xsWFBFpCKMtauXZvYsChC\nia34GeUIQkrR1taGHXbYodLHIhKVVIGfjRJ1sHBKiZZykhKoBAN8T5XfsQBp9erViY2quMVCG5X4\ng/1eCW/22muvSpurxAA64QoLx1QFIbZRwheuisPv4UCRM9cp8Rz7gUqywMIp5XPqWdVh1qxZSR8n\nkFDPjsVWQDq3Kp/nJC1qPmJ/UuJFrhakzr9y5crEJmc+ZpFsjgiwO/6ma4wxxhTCi64xxhhTCC+6\nxhhjTCGKxnQ7OjqS2BH/Pq9iqo899lilrZKz8+/6uRvT99ijmtXtIx/5SGKTU3Dhsssuq7QXL16c\n2HzrW99qeH4V/5s6dWqlrRKFX3XVVZW2SpSuYto5sVeOCXIBhNzjDBTK7/iZqvgax+X32WefxOaR\nRx5JzpUDx1RPO+20xIbHqGLKl156aaW9ZMmSxOab3/xm0sdxKZUIhM9/yCGHJDa/+93vKm313qm4\nmIohM+znq1atSmwGSwyX6ejoSN5BTuCjfI6vh2PmQJpAp7fk+t1hXcOJJ56Y2LBeRI2R5zqVWOW8\n885L+iZOnFhpq2fH78WRRx6Z2Fx33XVJH6OOnZPogn1+zZo1iU1fY7iMv+kaY4wxhfCia4wxxhTC\ni64xxhhTCC+6xhhjTCGKCqmAVGjx8ssvV9qqOgULSFQlHj7uoYcemtjcd999SR8H7t/5zncmNpx4\n4ze/+U1iw4kEjj766MRGBfJ5Y7sSKd14441JXx3qip3U5nOGRRqvvvpqYpMr+GgFLHDi8fH4gdSn\nVDUXPu4BBxyQ2ChRHfvd8ccfn9iwL8ybNy+x4aQoRxxxRGKjxHksmGFBGADceeedSV8dckRTCiXQ\nYTiZAc8nA1GZ6DXYN3gsw4cPTz7DFbxUAgd+j2bMmJHYqAo+fK/e8Y53JDYsQPrBD36Q2LAg6uCD\nD05sVHIgniPV87377rsr7brVynJEUwoW7Sr4Pqpz9TbX+ZuuMcYYUwgvusYYY0whvOgaY4wxhcip\npzsZwM8AjENnTcl5McbvhRBGAvglgGnorDP5oRhjugu+AZwMIyfulxMjWrQor9Y0bz5X8RH+Df9L\nX/pSYsNxMxVT5o3XQBoPuP/++3sebBcq7s3FDFRRgrpwMQCVqLxu3K4nWu13fA0qLsMxuJz4oIrB\nKbiIhUo8wckUVGJ5jsGp4gKq0AXH91XcmVGJa9gXc4o75MLHVkUpcgqm5NJqn+Pr4fhzT32NyE3W\nwPHRN73pTYkNxzTVsblPPZcPfOADSR8X41D6FZ4jlTYkp3BNXfhaVEy5vz6X8013K4DPxxhnAjgK\nwJkhhJkAzgVwY4xxBoAbu9rGNAv7nSmNfc60nIaLboxxXYzx3q4/bwGwGMBEAHMAXNRldhGAU1o1\nSPPGw35nSmOfMyXo05ahEMI0AIcBuAvAuBjja79hrkfnTzLqM3MBzK0/RPNGx35nSmOfM60iW0gV\nQtgZwGUAzokxVgJCsXNDo6wwEGOcF2OcHWOc3a+Rmjck9jtTGvucaSVZ33RDCMPR6YQXxxgv7+re\nEEKYEGNcF0KYACArms+Bag6U82Z/AHjiiScqbbWpnKthqCoTvKkbAM4888xKWwm5pkyZUmn/27/9\nW8Pz//3f/31iwwkRgDQBQY5IQI2xrnCKn4cSC3GfqiSzyy67VNpbtmypNZ7uNNPvGPY73rgPpNVx\nOHEBAOy8886V9ubNmxMb9dzPPvvsXscDpFV2/uEf/iGx4c/98z//c2Kz6667Jn1cHSgHVbmqLixQ\nUT7Ffq4EhHxvVQWwPo6rZT7H18MVxoDU55SQhwV2mzZtyjr/Zz/72UpbibZYWKqSBfF8fNZZZyU2\nLD4FgN/+9reVNl8HkFa7Uu9Ts1DvM891OeLBvr4XDb/phs6nfj6AxTHG73b7qysBnNH15zMA/JY/\na0xd7HemNPY5U4Kcb7rHAPgIgAdCCAu7+r4M4BsALg0hfAzAKgAfas0QzRsU+50pjX3OtJyGi26M\n8TYAPSXATBPGGtME7HemNPY5UwJnpDLGGGMKUbTK0LBhwzBy5MhKH2c3UaIADlwrsUSOgIJFCgDw\nwAMPVNpvectbEhse88c//vHEhjMaqQxHKmvTPffcU2mr6kQsXFDCHA7mq0wyLHYCUqHGmjVrEptZ\ns2ZV2gsXLkxsmimyaTbt7e2JaIPvKQs4gNTv1DPNySCkfJOrqbz97W9PbMaOHVtpK5EUC+/Wr1+f\n2Ci/5wovKoMQi5tU5iH2MyUEVMJHvrdq3Pvvv3+l/fDDDyc2g5X29vZEwMbPQYkNWdzDlYoALRLN\n4dZbb620jzvuuMRm/PjxlTaLr4A0a9Vtt92W2EydOjXp4/nvn/7pnxKb//mf/6m099lnn8SGK2Kp\nuUfdNxa7qjmShWSqspgSYPUFf9M1xhhjCuFF1xhjjCmEF11jjDGmEEVjulu3bq212ZljSaqSiqrA\nwqjqEBxXUfGBtWvXNrTheAFX1ADSRBgAMGrUqEr7sssuS2w4NqQqEfFx1KZ6vg4gjZspVAy3Eer8\nKgFCCbZt25bEcHNgv+N7DAAbNmyoNSaOIas4HfurSlzCvqFiWerdOOaYYyptjqUB6fXPnz8/sRk3\nrpoRUY1RxXk58YeiTgw3JyZagm3btslYeiM4/s3JV4BUj5ALz72qIhZXiVI6kCOPPLLSPuKIIxIb\npX844YQTKm1OlgGkvsJxaCCNO6v5WOln1LrBqBguwz7V17nO33SNMcaYQnjRNcYYYwrhRdcYY4wp\nhBddY4wxphChpLglhJCcjMUpSiRQp5KIEgCMGTMm6Tv44IMr7f322y+xOeOMMyptlRCBRQnLly9P\nbL7zne8kfVxphDd+A6k4RW0Y52QHihEjRiR9SvDQiJzqKKNHj05sNm/evGAgyp4pv2P/UD7FFUaU\nEI9R91jdr5kzZ1baqrrW3LnV0qxKsPHss5XKc1Ko+OMf/zjpY/HJihUrEht+plxtCwBWrVqV9DEs\nMgTykoowu+22W9LHwhtOZPPMM89g69atPaV2bBnK59iflOiMhVQ5PqdQ92rfffettFX1qc985jOV\nNt9PIL0OVSFLCUKXLFlSaau5jhNvqDk7p6Jas3xOrSMsvu3BL3uc6/xN1xhjjCmEF11jjDGmEDn1\ndCeHEP4YQngohPBgCOHsrv7zQghrQggLu/47qfXDNW8U7HemNPY5U4KGMd0QwgQAE2KM94YQdgGw\nAMAp6Kwp+VyM8dvZJxNxDkYl8+c4h0pOz7EHFf9SMRTeDK6SWR9wwAGV9n333ZfYcHzksMMOS2xU\n3IyTbufEHVRMl+McasO4un5m+vTpSR8nV1Bx4B122KHS5g3sAPDoo49mx3QHg99xcgiOnwJICimo\ne6zesZwCEfwsli5dmthwfI01CoBOjsGx35zYvoo7c9xXxSBzElRMmjQp6eMiCFu3bk1s+PpZI7Jx\n40a88sorWTHdweBzPG/x/Hacr7AAACAASURBVASkMUT17JTP8f1Tvspxe1UAhd8LlWBHxV05AYx6\nnoyaR/ieqHuUg4oX8xjVPWKfU3HfTZs29TjX5dTTXQdgXdeft4QQFgOY2OhzxvQH+50pjX3OlKBP\nMd0QwjQAhwG4q6vrrBDC/SGEC0IIqUSz8zNzQwjzQwhpDjljMrDfmdLY50yryF50Qwg7A7gMwDkx\nxmcB/AjAdACz0Pmvw3Q/DIAY47wY4+yB2Cpihj72O1Ma+5xpJVmLbghhODqd8OIY4+UAEGPcEGPc\nFmPsAPATAEf2dgxj+or9zpTGPmdaTY6QKgC4CMCTMcZzuvVP6IqBIITwzwDeHGM8rcGxkpOxEEUl\nKeDN181K8qBQwXUOpquKMCxuUsH9TZs2JX2cOKFOFSYgFbmoyi5KCMTjVkk2WDihxFYsuOghgUlf\nhFRN9TsWqPDmeSX2YZ9q1oZ7xdixY5M+Flup57f33ntX2uq+s1gPSN+7OhVxgFR4o/xO9bFwSlXl\nYvhagVRspYRcMcZcIVVLfY7bLBAFUn8q7XM5Vdf4mauKPsrnWHDE58pl6tSplbbyXfWu8Nyu5mNG\niQd5jlbXj17mupzSfscA+AiAB0IIr9V4+zKA00MIswBEACsBfDLjWMbkYr8zpbHPmZaTo16+DYD6\nl+I1zR+OMZ3Y70xp7HOmBM5IZYwxxhRiwAseMCrOwPGBiRPTrXNqEzejYigcL+bN9UAaJ1IJvjk5\nhoopqM3gOTHVOnDMDtBx5pwN6jnwc1MxHfQhpttMcvxOJXbnJCzKNzmmqBg2LP1Bie+78juOFSnd\nwo477lhpq2es4tUcF8spXJCDSv6uxpSTqCUH9nOVOCc3pttMcnxO3Suea1SxjB7erQqswwDSe66O\nzXObih+zP/cQ00zgeTtnzs5BFW7IfQ8YTo6kPrPzzjs3PBd6mev8TdcYY4wphBddY4wxphBedI0x\nxphCeNE1xhhjClFaSLUJwCoAowHUywIxsHjc/WNqjDHNPtJihrjfDcUxA4Nn3Pa5enjc/aNHvyu6\n6L5+0hDmD8X8pB730GYo3oehOGZg6I672QzV++Bxtw7/vGyMMcYUwouuMcYYU4iBWnTnDdB5+4vH\nPbQZivdhKI4ZGLrjbjZD9T543C1iQGK6xhhjzBsR/7xsjDHGFMKLrjHGGFOI4otuCOE9IYQlIYRl\nIYRzS58/lxDCBSGEjSGERd36RoYQbgghLO36f5oxfAAJIUwOIfwxhPBQCOHBEMLZXf2Detytxj7X\nWux3Gvtd6xjKPld00Q0htAP4IYATAcxEZ3HomSXH0AcuBPAe6jsXwI0xxhkAbuxqDya2Avh8jHEm\ngKMAnNl1fwf7uFuGfa4I9jvCftdyhqzPlf6meySAZTHGFTHGVwD8AsCcwmPIIsZ4C4AnqXsOgIu6\n/nwRgFOKDqoBMcZ1McZ7u/68BcBiABMxyMfdYuxzLcZ+J7HftZCh7HOlF92JAB7v1l7d1TdUGBdj\nXNf15/UAxg3kYHojhDANwGEA7sIQGncLsM8VxH73Ova7Qgw1n7OQqiaxc6/VoNxvFULYGcBlAM6J\nMT7b/e8G87hN7wz2Z2e/+8tkMD+7oehzpRfdNQAmd2tP6uobKmwIIUwAgK7/bxzg8SSEEIaj0wkv\njjFe3tU96MfdQuxzBbDfJdjvWsxQ9bnSi+49AGaEEPYKIYwAcBqAKwuPoT9cCeCMrj+fAeC3AziW\nhBBCAHA+gMUxxu92+6tBPe4WY59rMfY7if2uhQxpn4sxFv0PwEkAHgGwHMD/KX3+PozzEgDrALyK\nznjMxwCMQqcibimA3wMYOdDjpDG/FZ0/p9wPYGHXfycN9nEXuC/2udaO236n74v9rnVjHrI+5zSQ\nxhhjTCEspDLGGGMK4UXXGGOMKYQXXWOMMaYQXnSNMcaYQnjRNcYYYwrhRdcYY4wphBddY4wxphBe\ndI0xxphCDOvPh0MI7wHwPQDtAH4aY/xGA/skE0dnNq//T91kHcOGVS+lvb09sXn55Zebcmw1xm3b\ntjU8Tltb+m8c7tu6dWtDG3Uc/hzfV0CPe8SIEZX2K6+8ktjwvVTXyvdI2cQYN8cYxyR/0Ufq+B3f\nD253dHTUGgtft3o26p7WObYaY864c/xOPS/1OYY/l+t3w4cPr7RfffXVhudX18q+yefq6OhAjDEd\nVB8ZzD6n7rm6n3WO3UqfU8dR18LkzLUK9jk11+Y8I74ONeZt27b1ONfVXnS7FWl+FzpTh90TQrgy\nxvhQb5/jl4Qfct2FcfTo0ZX2TjvtlNgsX7681rF33333SltNIk888UTD4+ywww5J3y677FJpb968\nObHZcccdK21eKNXntt9++8TmxRdfTPrGjx9faT/22GOJzW677VZpP/kkl94E9thjj0r72WefTWxe\nfvnlVUlnH6njdyGExM/4BXzhhRdqjWfkyJGVNj8rAFi5cmWtY/M9Ve+Gus+MGhO/H08//XRiwz7E\n9xBI/Z7vK6D/0cF+t2ZNWguAx/j8888nNuybvNioz/SVuj7H92K77bartLds2VJrPOxzfFwAePzx\nx5M+Ri2M7HPq/uW8K2qu4+f53HPPJTZ8z9QY2VfVFyw1R7PPbdiwIbFhH1e+y++TOv9TTz3V41zX\nn5+Xh0yRZvMXhf3OlMY+Z5pGfxbdrCLNIYS5IYT5IYT5/TiXMa/RZ79zfnHTT+xzpmn0K6abQ4xx\nHoB5gI7pGtMKuvtdW1ub/c60HPucyaE/i26tIs0cBM8JijeKxwFpfGT9+vUNjwPoYDrD8dJRo0Y1\nPLY6l4qPcJxVBe45bpcjNsgRZAE6hsvkiDI2bdpUaauYcpPos9/FGJNryHnuKnbOcHxr48a0ZraK\n+eT4Pd9T1hYAeQJCFTtTfUwdfUWO8ATIiznmHJv1BTlCoBrU8jmOB+b4HMdnlZ/ws1M+lzPXqXvD\nPscxcyAv7qrmumbE1xW5zzjH5/idV88sR0fRG/35eXmoF2k2QxP7nSmNfc40jdrfdGOMW0MIZwG4\nDp0y+gtijA82bWTGCOx3pjT2OdNM+hXTjTFeA+CaJo3FmCzsd6Y09jnTLFoupGrEzjvvXGmrWBPb\nqD2FOfG3nJhKDmpPbs75FXViTkoZyXEVFfdWsdkJEyZU2moPLj8TFVvkMT3zzDOJzWCC9xGqvYds\no64pJzZbdzM/o/xe7dGsg/LfnKQeKobMKB8fO3Zspa3ikjl+x8fub7ytleQkBOHnqa4nZ85o1lyn\nzl93rmM4RwGQt3eZ49W518q5HFROBPY5FdPm8/U1Vu00kMYYY0whvOgaY4wxhfCia4wxxhTCi64x\nxhhTiKJCKpV4PmeTPjNmTFq8gTd1T5o0KbFZvXp10sfJq1966aXEhoULu+66a2LDwXUWKAG64AIH\n6pVYp1ElFSAV/ajgPif8BoB169YlfQwLQJSgZzCjks/nJH5gX1VCHr4X6h6rRC38vJTf8XNmQaH6\n3Lhx4xIbVUyAE9s/9dRTiQ2jkrLw+6OEMCyaAlLhlErmwO/dUPK7EELDRBdKEMVJZZQIj99tFggB\neYVTlHiQBZmqWEbO+Xk+BtJCDUq02Wg8QJ6wNsfnVFEGTlbUCkGov+kaY4wxhfCia4wxxhTCi64x\nxhhTiFCyBFV7e3vkGAHHdFWifBXvYlS8i1GJBDiuwoWWAR0LZnjcOWOui4rbqYLMdchNqsHkxKYB\nLIgxzq45tNq0t7dH9g/e9K98Iyfuy3EhFYNSyQTUfWZUwohGx85JaFGXnJh2aXKS68QYG1cIaTLD\nhg2LnPyBfU75QI7P8TNXCUpUH2tRVEw353mWnOvqrgetJPP6e5zr/E3XGGOMKYQXXWOMMaYQXnSN\nMcaYQnjRNcYYYwrRr+QYIYSVALYA2AZgayORTIyxodBDiTVUcoFGKIGYqg7E5CQJUJvB+XwquP7B\nD34w6Vu8eHGlfeCBByY2nHBAJQdhIdUjjzyS2ChxEz+PnCofKklCKwU84vwr0Ue/ayS+UH6XI05j\n4ZTyuxyfykElZWHUczj99NOTPk6KonyahT9K1MKJN5YtW5Y1JhYD5SQhUCI1FvmxTZ0qXoq++lxH\nR0ficzyWZvmcukZO8gDUS0SkEkhwIhP1bn30ox9N+jiphkr8we+Kujb2uaVLlyY2yldY2FbX5zgR\nUl/Fp83ISHVcjDFNf2JMa7HfmdLY50y/8c/LxhhjTCH6u+hGANeHEBaEEOYqgxDC3BDC/BDC/JJ7\ngs1fNPY7Uxr7nGkK/UqOEUKYGGNcE0IYC+AGAP8UY7ylJ/vhw4dHjh3lxGs5YYX6DZ3jFcpGxSdy\nkm5zLI1jXUCazJvbgI73felLX6q0H3vsscSGr00lx/j5z39eaR9yyCGJDcePgfT6V6xYkdgw6t5y\nnHnt2rXqo01JjtFXvxs2bFjk+FlOfJ9jQCq+w3EqlZRAJd7g2LnyDT6/iskddNBBlfaUKVMSGxU7\nmzu3um6o95DjaypxzC9/+ctKe+LEiYnNvffem/TxteQkAlFaAn4X1HU0IzlGHZ/j55eTeILnmhyf\nU/dF+SHHGTk2CaTJRpTvvP3tb6+02QcBPe5TTz210lZzL7+X6r249NJLK21VLOOOO+5I+lhbUDex\nCxc16WENa01yjBjjmq7/bwRwBYAj+3M8Y3Kw35nS2OdMs6i96IYQdgoh7PLanwGcAGBRswZmjMJ+\nZ0pjnzPNpD/q5XEAruj6aWMYgJ/HGK9tyqiM6Rn7nSmNfc40jdqLboxxBYBDmzgWYxpivzOlsc+Z\nZtKMfbrZxBiTYP7YsWMrbSWoYOHA1KlTExsWBSjRidownkNO4gcWBSiRwA9/+MOkb/bsaqx9zz33\nTGxYlLBoUfrL1rve9a6Gx5k2bVrS9/3vfz/pa4QSLtRJYFKKEEIy5hy/Y7GTEimxP6vKLTmVYxSc\ndEBV0nrggQcqbZVc4Ytf/GLSN2rUqEpbiWpYHLd8+fLEhv13woQJDY8DpAKsHJSAL0cQNxC0tbWB\nK6pxQhDlcyzSVMI09qe685qCfW7mzJmJDYuUchMB8fukklNwAhYlyOT3UIlWlXjx97//fdLXCDXX\n9beylvfpGmOMMYXwomuMMcYUwouuMcYYUwgvusYYY0whigqpgLRqBAshVACcWbVqVdLHwqE6FTUA\nnV3l2GOPrbR/8pOfJDYsbuBMUwBwwgknJH18P5RYh69l1qxZiQ1nsuFKHIAW2bAoQImtOOONOg5n\n32HRCABs2rQp6StBjDF5PixYYdELkF63yhbGQhclpMpBVZc68cQTK+2f/vSnDY/DgjpA+wtfG2fZ\nAVKBjBJJHX300ZW2Er6o+8ZCQ3VsFhCqak18HZw1K6dqVivYtm1b8t6yDyphHF+zeo8nTZpUaa9e\nvTqxUVmqOLvTjBkzEpsPf/jDlfbFF1+c2LAASlW/UhmxWDimqlYx6n065ZRTKm3lc48++mjSxz6n\nKmvxfKxEYnwf1XPsbf3xN11jjDGmEF50jTHGmEJ40TXGGGMKUTymyzELjv3lJBJQ8TdV+acOKra1\nbt26SluNcd68eZX2cccdl9ioOAcnlViwYEFiw8lBVCyEY1ePP/54YpMTQ1q5cmViw/G2nLilekYD\nhUqOwYkW6l6TijPWQSVB4DGpTflcLWjOnDmJjYqvcxxMJVzZvLlar13FwDhhhorf5sDvGFAvXs6V\nxDhGV4q2traGc1uO7kT5XE5CkJzqcXvttVfSx++Jio1yQhSVQENVJ1qyZEmlrWLRrPvYZ599Epub\nb7650lY+l7OOsH8Dqc/lVKHLiU13x990jTHGmEJ40TXGGGMK4UXXGGOMKUTDRTeEcEEIYWMIYVG3\nvpEhhBtCCEu7/r9Ha4dp3mjY78xAYL8zrSY0CriHEP4KwHMAfhZjPKir75sAnowxfiOEcC6APWKM\naSmT9FjJyVjIo8RGXMlFVZXICXgrOGGFSvzw8Y9/vNJmIQEAHHDAAZW2qiAyf/78hn233nprYrNi\nxYpKWwkg+Dh/9Vd/ldg8+OCDSd8ee1Tnj2XLliU2jBLm8DPqwa8WxBjTmydotd8xyu9YDMKVeYD6\nVW74+fB7AKSJCpTIj5NqsK8AwL333pv0Pfzww5X2XXfdldiwSGzvvfdObLjK0f7775/YqDFxEgsl\nqmFynpEixphmiuiBZvldjs/lJLDgaliAnltyOPLIIyttJd47+eSTK20lRBs3blyl/ec//zmxYf8C\nUuES+w6QPmMlpGJxF4vnAD2P87Fz3t2cZ9QDPc51Db/pxhhvAcAr2hwAF3X9+SIAp8CYJmK/MwOB\n/c60mrpbhsbFGF/T+K8HMK4nwxDCXABze/p7Y/qA/c4MBFl+Z58zOfR7n26MMfb2U0qMcR6AeUDe\nTy7G5GC/MwNBb35nnzM51F10N4QQJsQY14UQJgDICjK0tbUlm7058QPHBoE0qUPdpPIqJnb99ddX\n2lw4AUgTfHMcFABeeeWVSpuvC9Bxs+uuu67SVrEQRsUiTjvttEp74cKFiY26txyz4c3xQBrXUDF1\nTvKhClfkbFhvQC2/CyEkG9hffPHFSlvFBrmIQ93k+RwDA4Bbbrml0laFNjgGdvDBByc2fE/VGO+4\n446kj+NpKikKo2KJ7373uyttpVtQY+J3SMVrOc6tYursd/ycm+BzQE2/43dp69atlbaKDbLP1dWq\nKJ+7++67K20u5AKkySlUUQS+pzw/AjrJD2sEcgqgqIIPRx11VKV95513NjwOkBZzUYlt+JmphDCc\nWEYlx1CFEl6j7pahKwGc0fXnMwD8tuZxjOkL9jszENjvTNPI2TJ0CYA7AOwXQlgdQvgYgG8AeFcI\nYSmAd3a1jWka9jszENjvTKtp+PNyjPH0Hv7q+CaPxZjXsd+ZgcB+Z1qNM1IZY4wxhRh0VYaU2Cen\nGgeLrVRFHRZdKDhIDqQBfxZEAKnggQVaAPC9732v4flzUJvauUoLC7sALSpRiQsYFk7lVNZpkoCl\nKYQQEr9joZcaLwuAlMiMj6NEFWqjPqOq/HACDSVOYxHLjTfemNhcddVVDc+fg0rgwX6XmTggy+92\n2223SjsnmUFvApaSqCpDLBbL8TmV+IHFjypZTY7P3XTTTUkfC/re+ta3JjYsblJJdx555JGG568L\nn1+J8FRSixyxIK8bOWKvvlay8jddY4wxphBedI0xxphCeNE1xhhjClE0ptvR0ZHELDgGpOKV/Jln\nn302seFYTk4cOJf/+q//qrS5SAKQxlBvu+22rGNzUguV+P3pp5+utFVSeY5z5MTMcuF4tYptMnWT\n07eCjo4OmaykOxMmTEj62O+UT/E1NTOWzX735je/ObFhv+MECD3BPqyubenSpZX2pEmTEhu+3rrJ\nHBR8LBVTZtjvBtLnGiXxUQkseG7jJC6KZsaxf/CDH1TaU6dOTWz4nuYk9AGA44+vCsBV/P/mm2+u\ntDmZEpDGUJv5jOv4nNL49Ia/6RpjjDGF8KJrjDHGFMKLrjHGGFMIL7rGGGNMIULuZvamnEyUu9p1\n110rbSWSqsPuu++e9LEgCchLksBMmTIl6fvUpz5Vae+zzz5Zn2NR0i9+8YvE5k9/+lOlvWTJksSG\nN8jnJALJhSttcPUbILsiz4IY4+ymDSwT5Xc77bRTpa2EVsOHD6+01SZ4FnHkVlfiY6ukMIyqbsV+\np0R2SgDFyQN+/etfJza33357pa0EM3z9OdeRCydlUSItTh6hhEcxxjRTQotRPseJU5QAioVDan7m\na1SJINTnGlU9UijR5DnnnFNpqwpZSiTGvnLFFVckNlwR66GHHmo4xmbCCVlUBaXMd7fHuc7fdI0x\nxphCeNE1xhhjCuFF1xhjjClETj3dC0IIG0MIi7r1nRdCWBNCWNj130mtHaZ5o2G/M6Wxz5kS5GSk\nuhDADwD8jPr/I8b47b6ekDPG5AinuPKDEqawyCQ3S0mOcIpFAY899ljDz6hqMyxIAlJRxJgxYxIb\nFhe0EhavAGlVofHjxyc2LNxSlU9U5aNeuBBN9DsWhDTKUAWk15Djd0rAoj6XIzhiUYeq7sTXpTL4\nKAEWj0n563333Vdpt1J0qcbIGZ1y/I6FbAPlcyGE5NnUyRyl5jGeD1XWJJVhLEc4lSMsZRslHlR+\nyD6vRG/NzKTXCCW25XukBGFcwSlXPPkaDb/pxhhvAdC83G7GZGC/M6Wxz5kS9Ceme1YI4f6un2TS\nf6Z2EUKYG0KYH0KY349zGfMa9jtTmj77XMmtmGZoUXfR/RGA6QBmAVgH4Ds9GcYY58UYZw/E/kzz\nF4f9zpSmls+pvbPGADWrDMUYX/9RO4TwEwBX5362TkWInIpB7ORPPPFEYqPiVtynEmjwb/ijRo1K\nbA499NBKe9q0aVnnX7t2baWtqgzVSaRQl5wqMTmVT/oYS8uiP37X10ogQBpTVN9e2J+VRkHFtzku\np+47b8znBCRAmphAJcLgRCDqfCqZCY+xh4QnTUHFq5lGVXuA5lZ5Aur7XIyx1nvK702O36p3ra7P\n8f3j5EUAsN9++1XaSquifJWT6qj5kD/XzApKjHpXOQGOqpbG9NXnan3TDSF0r4N2KoBFPdka0yzs\nd6Y09jnTbBp+0w0hXALgWACjQwirAXwNwLEhhFkAIoCVAD7ZwjGaNyD2O1Ma+5wpQcNFN8Z4uug+\nvwVjMeZ17HemNPY5UwJnpDLGGGMKUUtIVZe2trYkeM7iCCX64GC2Eg6w2EltGFdB+TqB+oMPPjjp\nW7duXaWtEmgceOCBSR9fm9qwPWvWrEr7wQcfTGxY3PX4448nNmPHjk36uErLqlWrGtqoyhsTJkyo\ntPl+DCQ5fqeEH5zcQAnIcpK7KH+tU01LifPWrFlTaT/66KOJzb777pv08f2YOHFiYnPkkUdW2jfd\ndFNiw+Ic5Rsq4Qp/Tr0v7Hfqnu25556VNgsTB4oQQpI0geca9a7nJHGpK2SsI2JVz47niMWLFyc2\ne++9d9LHCVD22muvxOatb31rpa0qEeVUS1ICMD5/3bmuvz7nb7rGGGNMIbzoGmOMMYXwomuMMcYU\nomhMt6OjI4lHcCyNE0EAaXKMyZMnJzYcw8xNhsDnP+ywwxIbjqWpWMxVV11VaaskF5/97GeTPr4W\n3kAOpIk/Dj/88MTmtttuS/oYlTAkJ87D16s2gw+mGC7T0dGRxNM4dqPguC/HrYH0upXfqXvMsTKV\nYGD//fevtFXii+uvv77SVv7zt3/7t0kf+zRrIoA0dvimN70psbn77ruTPkbFxXKSsHDcV8UyB0sM\nl1HJMRrFeIE0OYPyOX5WyudUH7/HXFADSPUjU6dOTWxuuOGGSlvFnU8++eSkj3UErEcA0iQps2en\nCeXmz2+c2bWu/oKfUSt8zt90jTHGmEJ40TXGGGMK4UXXGGOMKYQXXWOMMaYQRYVUQFqphUUmalMz\nCw7UpmYWDuyzzz6JzbJly5I+3mh93HHHJTacwOKHP/xhYjN+/PhKWyUkUOcfN25cQxsWq4wZMyax\nyaHO5nggTyTFSU3U5vxWVkdqBAtU2A9ZtAOkVXVUAgc+7vTp0xOb5cuXJ338LN7xjnckNpzM5Cc/\n+Uliw++LSkqwYMGChsd+6KGHEpslS5b0+plc6vrdpk2bGtrwc+N5oNlVh/oCXzfPNUrIxOI9leSG\nfW7GjBmJzdKlSxsee86cOYkNH2vevHmJDV+HErbecsstSR+LBZUgiuf60j6XI7Zin+PnAfReEcvf\ndI0xxphCeNE1xhhjCuFF1xhjjClETj3dyQB+BmAcOmtKzosxfi+EMBLALwFMQ2edyQ/FGJ/q6Tg9\nwfFStRm5t9/He0JtyFdw4g1O8g6km9FVIgHu403egE4ucO+991baKobDn1OxxVbCyTm4DegN8v08\nZ0v9jgtiKL9j38gh9z5wohLld+zDyjcYZfM3f/M3Sd/ll19eaa9cuTKx4eTzqtDGQFPnGfVEq32O\nUT7HOoIccu8Bx8hVkh2OqSo9ArNixYqkT2ljvv/971faGzduTGw+/OEPV9p1fU7FWfmdVzaqj+mv\nz+V8090K4PMxxpkAjgJwZghhJoBzAdwYY5wB4MautjHNwn5nSmOfMy2n4aIbY1wXY7y3689bACwG\nMBHAHAAXdZldBOCUVg3SvPGw35nS2OdMCfq0ZSiEMA3AYQDuAjAuxvjaXpL16PxJRn1mLoC59Ydo\n3ujY70xp7HOmVWQLqUIIOwO4DMA5McbKZqbYuekxqs/FGOfFGGfHGNPM1cY0wH5nSmOfM60k65tu\nCGE4Op3w4hjjawqMDSGECTHGdSGECQDSqHgGvIlZbRjPEUWNGjWq0lZBesWnP/3pSlslcOCEA6ry\nBScSePe7353YKHEVbyLn6i9AKm7ISRqgxE6cEAJIxQXqc/yMuBKHsmlGIoxW+h3fC3VNOcIL3ry/\nfv36rPN/5CMf6XU8QFpl6IADDkhsFi9eXGmrqjS33npr0scCmXe9612JzZQpUyrtnIpCfM8Afd9Y\nQKk+xz7EVWqANPmFuo99oZU+x++WqqiW43OcHCe3wpdKhsEcfPDBlXZO4g31rnP1K8Xb3va2pO/Q\nQw/t9VwKTtYB6CpLfG/V5zipj6pGpgRwfaHhN93Q6SnnA1gcY/xut7+6EsAZXX8+A8Bv+zUSY7ph\nvzOlsc+ZEuR80z0GwEcAPBBCWNjV92UA3wBwaQjhYwBWAfhQa4Zo3qDY70xp7HOm5TRcdGOMtwFI\nf3Ps5PjmDseYTux3pjT2OVMCZ6QyxhhjClG0ylB7e3tSoYFFUir7FAsQlFiCM/zkwkKmE044IbFh\nccpHP/rRhsdRFWEOOeSQpI/FMZ/73OcSm5/97GeVthL9cGYrVWVDVVxhocDmzZsTm4kTJ1baa9as\nSWxYGDOYaGtrS6ogceafHHGEusYcUZvipptuqrRPOumkxIYrUH3gAx9IbM4///xKmwV9PfGe97yn\n0lbVZK655ppK+8knjSgwzQAAIABJREFUn2xoo4R4KoMPzwPq2CwYUveaxTH9FVI1i7a2tuTd4mxl\nLJBUsN8C9X3uhhtuqLRPPvnkxIYFqe9973sTm//8z/+sdf5jjjmm0r766qsTm4ULF1baSkjFc50S\nm6nMXuxzTz/9dGLDQt7czIZ9wd90jTHGmEJ40TXGGGMK4UXXGGOMKUTRmO62bdtq/UY+YsSISltt\nKq9b5YbjkxwvANL4ACctAIDDDjus0v7yl7+c2KiN1hyfve222xKbVatWVdrXXXddYjN+/PhKW91n\n3vjd05gYFcNlOIacm5yjBB0dHbWqt6jYOcPXreJL6rpXr15dad94442JDfv0iSeemNi8//3vr7Q5\nJgcAu+66a9LH75SqFHPfffdV2r/4xS8SG04Ooipw5d4TJid2ycfO0X+UoKOjo9acxD6ntAY5fqlg\nvcwVV1yR2LDu44wzzkhsPvGJT1Tae+yxR2Kzyy67JH3scypZECdgYc0CkMa5VXIQNf/kVBCqsz71\nda7zN11jjDGmEF50jTHGmEJ40TXGGGMK4UXXGGOMKURRIZUcAFV6UAkIuE9tts9BbTTnDfhKLPLJ\nT36y0h49enRiw2ICdR233357wz7eHA4AixYtqrRHjhyZ2ORUt1ECDJUMoxHqPrJoRNnUfW4lUBVs\nOPGCStxS99h77rlnpX3llVcmNuxTU6dOTWy4Tz1jrigEpIK9O++8M7HhhC877rhjYpNT4UZ9Tolo\nGpFT8YWvXyWEGSzk+Jwaf841qfln0qRJlbYS7+2+++6VtqpsxZWAcucVFqkq0ehvfvObSltVMMrx\nOTX/PPvss8Kyd9S1sSBVVchSyYlet+/zKIwxxhhTCy+6xhhjTCG86BpjjDGFCI02j4cQJgP4GYBx\nACKAeTHG74UQzgPwCQCv7WD/cozxGn2U14/VcKe6iv/wGNWGcU4AoOIevf3O/hpbt25N+jgBgNpA\nzRuv99prr8RGFWXgpNs5Y1QJEPh668ZPOeG3OpYaI8c+VLzqmWeeWRBjnJ0zjtJ+p+KF/EyVT3ES\nAOWbyqc4DqQ27vOzUPed48xTpkxJbJQvcEw1x+843qc+VycJCaDfe77f6h6xJoTbL730Ejo6Onoq\n11dhMPgcxwvVc+F4pdIaqHk9J3EIj0m9x+w7XBAG0Il4eN5U7wXDRQoUdec6Fa/lGHLOfVTx81df\nfbXHuS5HSLUVwOdjjPeGEHYBsCCE8Fq5iv+IMX474xjG9BX7nSmNfc60nJwi9usArOv685YQwmIA\nE3v/lDH9w35nSmOfMyXoU0w3hDANwGEA7urqOiuEcH8I4YIQQpqAs/Mzc0MI80MI8/s1UvOGxX5n\nSmOfM60ie9ENIewM4DIA58QYnwXwIwDTAcxC578Ov6M+F2OcF2OcnRvLM6Y79jtTGvucaSVZyTFC\nCMPR6YQXxxgvB4AY44Zuf/8TAFdnHqvS5kC1Eku89NJLlbYKgOdsfM6pBqESXzz55JOVthI38MZz\nFkgBWkjFwgUlxMk5DidJUNeqRC4sjlHjZpRwgu9RnWodTDP9jsUOOVWRWMjDVVKAPOEQi3uAVESi\nEp7wPVWCDRb5KVGNSlTAlbpyhFTKN9jv1Purqu2wQCZHDMOJbNSY+Jn1tcpQM32Onzs/c+UX/P4r\nm5zqRUoAxfOoqg7EIiklHuTPqWeu5ij1jjVC+QWLBVUCDTVuFuvlJLtR72V/57aG33RD5506H8Di\nGON3u/V3n3lPBbCIP2tMXex3pjT2OVOCnG+6xwD4CIAHQgiv5Sj8MoDTQwiz0CmtXwngk/rjxtTC\nfmdKY58zLSdHvXwbAPW7QK/71IzpD/Y7Uxr7nClBw+QYTT1ZxoZxtRmaYyEqmbWKITAqJsaxLE6y\nAaRxIY6NAGlcIydGBgDjxo2rtDds2NCDZd9Q9yh3Ez2Tk0SeY0jqHgHITo7RTHL8Lif5vLLhuKsi\nx+/U8+Lzq3uq4lkMx2+BNJa/adOmxKYOObHEXPieqFgmX5u6HzHGvgcT+0mOz6nnwu+aiunm6C7U\nsXkezTm/mjNy5jblzxxTbZbP5VwrkDfXcbIbpdloFKvvose5zmkgjTHGmEJ40TXGGGMK4UXXGGOM\nKYQXXWOMMaYQpYVUmwCsAjAaQLpjf/DjcfePqTHGNMNBixnifjcUxwwMnnHb5+rhcfePHv2u6KL7\n+klDmD8UU6V53EOboXgfhuKYgaE77mYzVO+Dx906/POyMcYYUwgvusYYY0whBmrRnTdA5+0vHvfQ\nZijeh6E4ZmDojrvZDNX74HG3iAGJ6RpjjDFvRPzzsjHGGFMIL7rGGGNMIYovuiGE94QQloQQloUQ\nzi19/lxCCBeEEDaGEBZ16xsZQrghhLC06/9pFegBJIQwOYTwxxDCQyGEB0MIZ3f1D+pxtxr7XGux\n32nsd61jKPtc0UU3hNAO4IcATgQwE511KmeWHEMfuBDAe6jvXAA3xhhnALixqz2Y2Arg8zHGmQCO\nAnBm1/0d7ONuGfa5ItjvCPtdyxmyPlf6m+6RAJbFGFfEGF8B8AsAcwqPIYsY4y0AuG7bHAAXdf35\nIgCnFB1UA2KM62KM93b9eQuAxQAmYpCPu8XY51qM/U5iv2shQ9nnSi+6EwE83q29uqtvqDAuxriu\n68/rAYzrzXggCSFMA3AYgLswhMbdAuxzBbHfvY79rhBDzecspKpJ7NxrNSj3W4UQdgZwGYBzYozP\ndv+7wTxu0zuD/dnZ7/4yGczPbij6XOlFdw2Ayd3ak7r6hgobQggTAKDr/xsHeDwJIYTh6HTCi2OM\nl3d1D/pxtxD7XAHsdwn2uxYzVH2u9KJ7D4AZIYS9QggjAJwG4MrCY+gPVwI4o+vPZwD47QCOJSGE\nEACcD2BxjPG73f5qUI+7xdjnWoz9TmK/ayFD2udijEX/A3ASgEcALAfwf0qfvw/jvATAOgCvojMe\n8zEAo9CpiFsK4PcARg70OGnMb0Xnzyn3A1jY9d9Jg33cBe6Lfa6147bf6ftiv2vdmIeszzkNpDHG\nGFMIC6mMMcaYQnjRNcYYYwrhRdcYY4wphBddY4wxphBedI0xxphCeNE1xhhjCuFF1xhjjCmEF11j\njDGmEP1adIdKkWbzl4X9zpTGPmeaRe2MVF1Fmh8B8C50pg67B8DpMcaHevlM7EyZ+f9pa6uu+9u2\nbas1nmHDhvG5EptXX3214XHU59rb2yttNcac+5hz7I6OjlrHyblv6nN835p1bfxcAWDr1q2bY4xj\nGh6s9/MMKr/j56fu8datWxseR90vPlbdMaoxqfMx/Nzr+p06V47f5bwLjHpXOzo60oH3gWb5HI8t\nxy8Uf8k+x89cfYZt1PzE90j1tcrngN7numGqM5PXizQDQAjhtSLNvTkihg8fXunbYYcdKu1nnnmm\n1mB23333SnvEiBGJzdq1axseh8cHALvuumulrcaYs6Bvv/32Sd9OO+1Uab/00kuJTc6CtmXLll4/\nA+hrGzOm6hdPPPFEYsOOqByz0XMFgM2bN69KOvtOU/xu5513rrSffJJreOex2267VdrK79avX9/w\nOMo3+B6qZ5ODGtOOO+5YaatnyhM3L5QA8Pzzz1faagJkHwfS91W9Uy+++GKlnTNp8/Ooe8+IWj7H\n94uvedOmTbUGw9eo3usNGzY0PA77gDrWU0891cfRdaJ8hc+nFit+5sp32UYtnnyvgdQP2XcB4Nln\nK9UB5RjZx/fYY4/EZv369T3Odf35eTmrSHMIYW4IYX4IYb7zPJsmYL8zpbHPmabRn2+6WcQY5wGY\nBwBtbW32RFME+50pjX3O5NCfb7pDvUizGZrY70xp7HOmafRHSDUMneKC49HpgPcA+NsY44O9fCY5\nGcdpVGyJ413qN3yOIfDv/oCOfeTEYhmO8QLAyy+/3PBzOTbNQsV0FTnPn+Ofzz33XK0xAVgQY5xd\n98PA4PM7jvmomHwr/Y7jrmqMJf2umTTL72KM/RVSFfO57bbbrqFNyblOxUZ5TMq/hqrPcby8rsYI\nvcx1tX9ejjFuDSGcBeA6AO0ALujNCY1pBvY7Uxr7nGkm/YrpxhivAXBNk8ZiTBb2O1Ma+5xpFs5I\nZYwxxhSi5erlvqL2d3F8QsXNOJbFsRGgeXEG3svV0/nqMGrUqKQvZ69hzsZvBcdsnn766cSGY2kc\nawPSeNFgj+lwXErtB8zxO46JqxiYuqd1aKXfTZyY7IDBmjWNtUL8vuYmfOD4tLo29ju1r5TjmYN5\nqw77nHp27Ie8/x5IY8Pjxo1LbHL26eagfJffi5z90+pzU6ZMSWyWL1/e8Dh1fW6XXXaptNW95Riu\n8rkXXngh63w94W+6xhhjTCG86BpjjDGF8KJrjDHGFMKLrjHGGFOIokKqtra2JOk0C25eeeWV5HP8\nGbXJm0UuKuG+EvdwoFwFyVkAoDae8+eUoEadf+TIkZV2jmhKJb5gkYASQIwfPz7p42T8Kjk9Jwbv\nR3KMAaG9vT0R7vA1KZ9iUVuO36lkBup5ceINleCAfTgn8QX7AaDfKfa7HNGUgs+nEuTniLTUuFno\n0l8BS0mGDRuW3GN+J5UAaPTo0ZV2js+p+6J8jkVaaj7i90S96zwmNWcoeE7MEU0puMCAKhwxefLk\npO/xxx+vtDkRBpAKqVrhc/6ma4wxxhTCi64xxhhTCC+6xhhjTCFqFzyow7BhwyL/rs+b4tVG65xE\nC7xhWiXZUIwdO7bSVjFVVeyYyYkNNwt1j1QssSQc5+4huXq/Cx7UYfjw4ZHjaxwvVPG1nATx/CxU\nkg0Vi+U459q1axMbFYtlcgpzlyTXNznmWHceykkK09+CB3UYMWJE5Pgsx0dVvDTnPvA9VvoVlchl\n2rRplfaKFStqnb+uz/G7kePfCn7mqtC8OnbmHNWQTN/tca7zN11jjDGmEF50jTHGmEJ40TXGGGMK\n0a99uiGElQC2ANgGYOtAxOvMGw/7nSmNfc40i2Ykxzguxrg5xzDGmAT4WcDCwiagXsUMJYxRfY89\n9lifj62qg6hgPnPmmWcmfSwuUBu9WWSjBBjr1q2rtHkjeE/wRnPeHA6kQhi18T6nak9d4UQP9Mnv\nWIzHfljX7/i61TUqIdHKlSsrbXW/GE6oAeRVePnKV76S9I0ZM6bSXrVqVWLz8MMPV9qcSAUANm7c\nWGmvXr264XiANPGIqjKU4y+NKvcMlM91dHQkPsfivZxkNT0duztKtKnEPZyMQlUL47lFJS1hkaoS\nUv3oRz9K+ljI9eijjyY2d9xxR6W9dOnSxIbnw9w5nH1OVRnKEYXxvVXvpRKyvYZ/XjbGGGMK0d9F\nNwK4PoSwIIQwVxmEEOaGEOaHEOYP5lqXZkjRJ78b6O1U5i8Cz3WmKfT35+W3xhjXhBDGArghhPBw\njPGW7gYxxnkA5gFAe3u7PdE0gz753bBhw+x3pr/Y50xTaFpyjBDCeQCeizF+uyeb9vb2yEkkcpLn\nc1xBxbH4t3gVd1RwfEIlnuck4CrO8f73v7/SfvOb35zYqLjdMcccU2mrOAP3KZtLLrmk0lZJPm6+\n+eakj5MJ1C1mMGHChEqbY8xdND05Ro7ftbW1xZwCAwzHvFTcnu+XslFaAvYhFUvnGNT06dMTm9NO\nO63Sfstb3pLYqJgTH0sloOG4lCpmcPHFF1faHAcGgN///vdJH887dZPJ5Phds5Nj5PocJ2Ook+xE\nJfnh+Heuz7FugePxQFoo4Pjjj09sPvzhD1faBx98cGKj5jrWj6hEJjxu9V786le/qrRvv/32xOb6\n669P+vh8ah7NYc8996y0VWIbtCI5RghhpxDCLq/9GcAJABbVPZ4xOdjvTGnsc6aZ9Ofn5XEAruj6\nRjkMwM9jjNc2ZVTG9Iz9zpTGPmeaRu1FN8a4AsChTRyLMQ2x35nS2OdMM/GWIWOMMaYQzUiOkX+y\nYcPA1V5YWKWC+zmbylmkkCOUAfIqTbBQ4W1ve1tiw4kU7r///sTmqKOOSvp487cSlPA9Ugk0WCyj\nRDdKJHXvvfcmfY1Qwg2V3GCwMHz48MRn+F6oe8o2nFACSJMx5AqClECE4TGfeuqpiQ0/i0ceeSSx\n2WuvvZK+RYsahyR32223SlvdI37P1D3aZ599kj5O1JBDjpBtsDBixIhElMTPXN1PFoSy+AhI54Pc\nKj9qbmVmz65qfz7wgQ8kNjzXrFmzJrHheR5IhXgqyRCLF5XYiedaJdpSfphz/Yw6dn8ryPmbrjHG\nGFMIL7rGGGNMIbzoGmOMMYXwomuMMcYUoqiQauvWrUmmJM5MoypfcGYXVYljypQplbaqPKGEGJyl\nZN99901sPvnJT1bav/71rxMbFoaojDAqkxZnu+JsJ0B6T+65557E5r3vfW+lre6REq88/fTTlTZn\nQQLyRGl8vSz2APovQKjLq6++mmQqYl9QWcZYMKKELyxSUpVTVFYhvl+zZs1KbD7/+c9X2ldeeWVi\nwwJCJU5RfseZj970pjclNpMmTaq0r7023ZrKfqcyQim/u++++yrt0aNHJzY8V6hr46xZO+ywQ69/\nX4qXX345qSTF41c+x+NXWcB4rlP3V90rzjr2zne+M7H59Kc/XWn/7ne/S2x23333SlvNNarKGc8J\nJ598cmIzY8aMSvs3v/lNYsNZstT5VXUi7mP/BtIqWbz2AKm4K+ded8ffdI0xxphCeNE1xhhjCuFF\n1xhjjClE0ZhuW1tbErPg38dz4n4q7puz8TmnotL+++/f8Hx33HFHYsMxMo4NADpeumTJkkpbVd7g\nxBNqjDwmFdPOSQSiqhNNnDix0lab4bmqk4pzDBRtbW3J8+GEAirBAPumSlSg7oU6fyNUTJdjZ1df\nfXVikxNvP+yww5I+vrbFixcnNvxMld6B9QUqvphTz3jz5s1JX47fcbx8sNRObm9vT/QaHF/OqSim\nkjyod1udn+GY6hFHHJHYcEKUyy+/PLFZtWpVw/Ord4UTmSj9A1dC4gQjQOpzK1asSGzU9TNqjs7x\nuf7Obf6ma4wxxhTCi64xxhhTCC+6xhhjTCEaLrohhAtCCBtDCIu69Y0MIdwQQlja9f/0B3xj+oH9\nzgwE9jvTakIjcVEI4a8APAfgZzHGg7r6vgngyRjjN0II5wLYI8b4xYYnC6GxkikDVWVIbZDOgatq\nKNHJO97xjkpbJQB48sknK20ltuJEFEAqVlm2bFliw2IKVUFo7dq1lbaqYqMqAfEm7hYnsFgQY5zd\n2Owv3++OPvroSvuEE05IbDhhxYMPPpjYXH/99ZX2nXfemdiwoAdIfUqJY7g6kPI7rvjCCSEAnSiA\nhVNKQNgsYoyhsVUnzfK7HJ9TAjsWgrGwB8gT7yl4Hvu7v/u7xGa//fartO++++7E5l//9V8rbTWv\nKSETJ77gORNIBYVTp05NbPidUyJAlZxI+WYL6XGua/hNN8Z4CwC+O3MAXNT154sAnNKv4RlD2O/M\nQGC/M62m7pahcTHG177urQcwrifDEMJcAHNrnseY7tjvzECQ5Xf2OZNDv/fpxhhjbz+lxBjnAZgH\nNO9nPmPsd2Yg6M3v7HMmh7qL7oYQwoQY47oQwgQAjTNT9APesJ2TCEOhignMnz+/0t57770TGz4f\nJ/gA0viMSkShknBz7EPFR7hPHYc3uudsYAfSzeecQF+NUSWn5zhTX5OAZ1LU70aOHFk9OcUvc+EE\n9QBw++23V9pHHXVUYsMFFlTckxNoqEITatz8nFW8n/1MxcQ4YUZO4gYgTTav3mn2O34eQKplYL9r\ngs8BLfI7pafhBDp147fTpk1L+v7whz9U2nPmzElsOF6qkuXwsRcuXJjYKF/lY6m5hrUot912W2LD\n75PSwah4OX9O+RwXOMhJiKOKIvRWaKPulqErAZzR9eczAPy25nGM6Qv2OzMQ2O9M08jZMnQJgDsA\n7BdCWB1C+BiAbwB4VwhhKYB3drWNaRr2OzMQ2O9Mq2n483KM8fQe/ur4HvqN6Tf2OzMQ2O9Mq3FG\nKmOMMaYQDZNjNJO2trbIQgcOuOdUwuGKMUBaJUZtzs7ZgK8+97nPfa7Sft/73pfY8Abtn/70p4kN\nV8dQjBgxIunjcavr4EpIXNEDSCuyAHoTOcPiDiW64ee2yy67JDZbtmzJTo7RTNra2uL2229f6WMR\nR45vqOpW6j7XgcWCAPCFL3yh0j722GMTmwULFlTayu/uv//+huc/5JBDkj4WmqhEIPy+qPuofFqJ\naJg6fsciv6effhqvvvpqdnKMZtHe3p74HFeEypl7m+lznIhHiUa/+MVqzg9OHgSkCVj++7//O7FR\nPscJWU488cTEhkVRLDgEUuGSWjOUoDAn8Q+L9VT1MRbnKYHu2rVr6yfHMMYYY0xz8KJrjDHGFMKL\nrjHGGFOIfmek6gsxxuT3d44BjR07NvkcJ4dQv7MzdROoq89961vfqrRVnIVj1bmJFI4/viqKVLG9\nyy+/vOFxcuJDOfFbBW9qV7FhZsuWLbXO1Sr42jmxvEr4wUn568bScmLpqkDF17/+9Upbxa44BsXX\n1RNcYEHF91R8mMl5z3Litwr2O5VwheFnNlDEGJN3khM2cGITIL3muj6nNBUcU16+fHli8+///u+V\nNutZAGDcuHG9tnuCi3zMnDkzsbnlllsaHidH91O3cAsXYVCxYYYTtDTC33SNMcaYQnjRNcYYYwrh\nRdcYY4wphBddY4wxphBFk2Ooclc5VUFYXMSbvAFdnScHFjfkCFFUco6vfOUrlfb++++f2KigPF/v\n//7v/yY2LC7gRBzNRFXM4KpKKklBTpIEAAOSHKOu33FyBiWIqlt5iH045z3kij4A8LWvfa3Snjp1\namKjjs1CEyVgufbaayttTsTRTDiRBJDebyUqyvG7GGPx5BghhMjPmJOEKJ9jUZISj+VWcmLqVGA6\n7bTTkr6zzz670lbVn1i0BaQC2CVLliQ2V1xxRaV91VVXNRxjXZQgllE+l7lmODmGMcYYM9B40TXG\nGGMK4UXXGGOMKUROPd0LQggbQwiLuvWdF0JYE0JY2PXfSa0dpnmjYb8zpbHPmRLkZKS6EMAPAPyM\n+v8jxvjtvpwshJCII3KC+ZzRRglDWCSgBAgqM85LL73U8Pw8ZlWJiMeUm/2JxRWPP/54YlNXOFGH\nnAwsLDAC0kw6dSvLdONCNNHvWCCW43c5latYVKHEGUr4pzJQMezDqpoJC1bUcdW4uU9Vc1mxYkXD\nMTJ8P4A8caISqfH1q+vg6+Xn3McsbBeiiT6XI5xiGl2PQmW2Uu9xTuYkPtb06dMTGxatqnusskbx\n87z00ksTG5UlqxF77LFH0qcy4vE4lehu1113rbTVWsPzgnpGvWXNavhNN8Z4C4AnG9kZ00zsd6Y0\n9jlTgv7EdM8KIdzf9ZNM+k+NLkIIc0MI80MI80tuTzJ/sdjvTGnsc6Zp1F10fwRgOoBZANYB+E5P\nhjHGeTHG2THG2epnNmP6gP3OlMY+Z5pKrSpDMcbXMwKEEH4C4OrMz2VViGD4t/ec2KCKn6g4L2/s\n5ioTQBoLUBU8JkyY0PBc6nO8+Xry5MmJDcdZcqosKXLibWozON9/VQmJqVtZpjf643eNquGouAxP\nnDn3XSUO4eQiQJ7fsQ+ryik8bhXLGj9+fNLH7+GsWbMSm1WrVlXaTz31VGLDqPitinOznyltBduo\n2CVTZ37pjbo+BzSOZSu/4Hc0p1pXbmIg1mKwDkMda/369YkN6wiUX/B8CKRzwlve8pbEZtOmTUlf\nI9T5lc/ztag5gZPdKP0KzwN99bla33RDCN3v6KkAFvVka0yzsN+Z0tjnTLNp+E03hHAJgGMBjA4h\nrAbwNQDHhhBmAYgAVgL4ZAvHaN6A2O9MaexzpgQNF90Y4+mi+/wWjMWY17HfmdLY50wJnJHKGGOM\nKUTRKkNtbW2x0YZxVbGCN84rQYkSAOXAVT1yqsaoIP0ZZ5xRaStxw7Rp05I+Fk6tXr06sXn00Ucr\n7R//+MeJTU7lCyXcGD16dKWtknPkVCfhxA09bMQfkCpDbW1tkQVHLOrg+wCkyRhU5RT2xdxkDOwL\nK1eubPiZMWPGJH1cBUbddyXOO/DAAyttJRLjRA3nnXdeYtPovgLa71gUtW7dusSG33t1bydNmlRp\nq/dnIKoMtbW1RR4/C27Gjh2bfC7H5/g+5M59XPns4YcfbviZGTNmJH2nnnpqpb1oURrmVvM4C6dU\nZSm+/n/5l39pOMbNmzcnfaoSHCcMqSPaAvJ8Dq4yZIwxxgw8XnSNMcaYQnjRNcYYYwpRKzlGM+EY\nr0rYzvFKjsMC6Ubn3CTcHHtQifoPP/zwSpt/0weAa6+9ttLOjany+VV8gGMPRx99dGKjEtYzKkuO\niuEyOcUCcpKpDxSq4AGj4kIck1N+x36mkvKr+8UxZDW+Qw89tNJWMcCrr67malD+q+Jb/J6tWbMm\nseFrO+GEExKb6667Lulj1LWpGC6TE9PtIZ424IQQkvedfWPjxo3J59hG+RwnzFDziooFs/+ouOtB\nBx1UaatEOL/85S8rbeVz/6+984uxqrrC+Lck9olBsSiMMJmpgMRRCMS/EUI0fRH/QRM1xYTwgNIE\nm6gxMca+1PhSk3aSPhQIRYUa0pZEjMTMgy2BNDwIHciIwAQGsJNihn8WAzy107v6cO+YOWuvy9lz\n554995TvlxBn79n3nH33+e7Z3lnfWWvhwoVBn/2MeZ8L61t4+umngzHbtm0L+izemsTEcFNojt90\nCSGEkERw0yWEEEISwU2XEEIISQQ3XUIIISQRSY1UqhoEz23iBa+qg33g3jPtWOPSvHnzgjGnTp0K\n+mxSgGeffTYYs2DBgkx706ZNwRhbJcUmiwCAzz//POiziTa++OKLYIw9lvfeYoxUMZV/PCOQNaB5\nhixr1vGSrjRaHWmiVCqV4NxWd17iCaspr+KKNfB5yQQGBweDPvs5eP7554MxNoHG5s2bgzF2nT3T\nVG9vb9DX2dlbIkGUAAAM2klEQVSZaXu6e+yxxzLt5cuXB2Os7ryqOF4FIYtntrJ9nqnFXker30aT\n5kyUSqUSVJyyJp2YhCye+cd+Hu39CQBOnDjhzmksL774YjDGzsnTnF1zz7S0a9euoM9+Nvr7+4Mx\na9asybS9+7HV3MmTJ4MxMdfdqwRnE2h49yx7Hb0kH17VsFH4TZcQQghJBDddQgghJBHcdAkhhJBE\nxNTT7QDwBwAzUa0puUVVfysitwH4M4AuVOtMvqCql8c7ARtnaFYxg9j44bfffptp33PPPcEYG0sa\nGBgIxtj4gJfA3ibZAIB33nkn0/ZioU899VTusWPwYmI2HuM9VG/jlh7Njp0VrTsbF/N010gMOvY1\ndr28ogR23c+cOROMsYkJhoaGgjF33XVX0Pfuu+9e9zgA8NJLL2XaXgIRL4Zr8bwEbW1tucfxktlY\nYgtMxFC05uxcPc3ZvrykLvWO42HjxV5yDIvnY7DJirzCCV68+r333su0vcQbb775ZqbtJUuyMVxv\njTwfgX2/XtzVS9hhsXqOXf9RYr7pjgB4Q1W7ATwC4BUR6QbwFoA9qjofwJ5am5BmQd2R1FBzpHBy\nN11VHVbVw7WfrwIYADAbwEoA22vDtgNYVdQkyY0HdUdSQ82RFIzrkSER6QKwBMABADNVdTSB6jlU\n/yTjvWY9gPWNT5Hc6FB3JDXUHCmKaCOViEwF8DGA11Q18wCaVgORYTCy+rstqvrAZBQvJ+WHuiOp\noeZIkUR90xWRm1EV4Q5VHX3q+byItKvqsIi0AwhLZvjHyrRtUN4+eByLDZLHVDEBgMcffzzT9qoD\nWQPUww8/HIw5cOBA7rkOHTqUO+b+++8P+pYuXZppx5h1vAe/PZOANet4hhpraPEeBvcMWBOlmbqz\nurJtq8NYrBkkttrS4sWLM23PbGSvu1flZ+fOnbnnOn36dO6Y7u7u3PN7+p06dWqm7X1+PKOJfb+e\nGcaaCj1tjtfEkkezNCci7ufEjsnDM5PZNY+pFAaEySm8z+yyZcsy7ZUrVwZjrCHKI8aQZKtoAcCi\nRYsybc88aI2BniHqu+++C/rs+/U0Z81m3n1hoprLvdNIVRnvAxhQ1Z4xv9oNYG3t57UAPp3QTAgZ\nA3VHUkPNkRTEfK1cCmANgK9EZDRv19sAfgVgp4isAzAE4IVipkhuUKg7khpqjhRO7qarqvsB1Ps7\nyI+bOx1CqlB3JDXUHEkBM1IRQgghiUhaZcgzF9jgtheAt8FsrxKRV40jhs8++yzTXrFiRTDGGhes\n2QCIM1J5LFmyJNP2qgVZU9iXX34ZjLFGspjKLkBYVcMzINjKNZNVLahRRCQwSNj18XRnzVZz5swJ\nxjSaHeyTTz7JtK2hDwiNQ/fdd18wJsZI5bFw4cJM2zNJ2et8+PDhYIzNaOZlEPKw9wHvdXZMs01T\nRaKqgQnKZqTyjFTW3ONVrTp+/HhDc9q6dWumbY1yQHivvfvuuxs6l4etUrVv375gjF0j715jP7ve\nPcvDrq1nJLPvvwjN8ZsuIYQQkghuuoQQQkgiuOkSQgghiRCvqk1hJxNp6GQ2tuPFK20yCBvDA/yH\noW0syUt8sXr16kzbVv3x5mjjwEAYPwXCuKH33np7ezPt119/PRhj40fnz58PxngPetuYXLPitd65\nKpXKocnI1tOo7mIqMMXozvMg2IQGNlkGAKxbty7Tfu6554IxVndeUhQvqYS9Pl4lqb6+vkx7/fow\nw6FNBmKrdnnn8uYU60HIw37GR0ZGUKlU8rNQNJlGNWfvEV5M0V5jb33nzp0b9B09ejTTfvTRR4Mx\nGzZsyLSfeeaZ3PN7mvf6bAzb23tsMoyXX345GGO9BZ4fwFsTe6/1EtI0Qp1ERHXvdfymSwghhCSC\nmy4hhBCSCG66hBBCSCK46RJCCCGJSJocIwavMocXlLfEVLXwmDVrVqbtJbloa2vLtG3VISBMcuGZ\nVzyT0uDgYKa9f//+YMxHH32UaXsmKa8aicUzd125csUZeX28SlDWJBEzn1bCM0M0S3eeGaa9vT3T\n7u/vD8Z8+OGHmfaDDz4YjLEGLO/aeNfCGqAOHjwYjNm4cWOmfezYsWCMV1XIMm3atKAvNqHBWDxz\njL1uRVS7KgrvWjVLc9Y0BQCdnZ2ZtpeIx14XLyHLvffem2l718Uz5tljHzlyJBjT09OTae/duzcY\nE8Ott94a9DVyr/OYaLIgftMlhBBCEsFNlxBCCElETD3dDhHZKyLHReSYiLxa6/+liHwjIv21f08W\nP11yo0DdkdRQcyQFuckxRKQdQLuqHhaRNgCHAKxCtabkNVX9dfTJIh4YtwkJgLg4jY3txMZ4Yx7Y\ntnGW22+/PRhz7ty5TNsWIAD8OId9bzEPbHtrZOd47dq13ON4eAlE7Ly9NbJ9XrxqZGQkOjlGWXRn\n1927xs2iq6sr6LMFF6z/APDnZPs83dlr6vkUrF4aTa4SE8uMWVsbX6xUKlDVqOQYZdGc9x4bwVtz\nu8bd3d3BGFtwwXsftnABEN4TPK+BfZ13H7H3ei+xildMIqbgREzSmJhzqWrde11MPd1hAMO1n6+K\nyACA2bkzIWQCUHckNdQcScG4Yroi0gVgCYBRi+/PReSIiHwgItPrvGa9iPSJSJ/3e0LyoO5Iaqg5\nUhTRm66ITAXwMYDXVPUKgE0A5gJYjOr/Hf7Ge52qblHVByYj5y4pP9QdSQ01R4okatMVkZtRFeEO\nVd0FAKp6XlX/q6oVAL8H8FBx0yQ3ItQdSQ01R4omN6Yr1Sjx+wAGVLVnTH97LQYCAD8BED6NHR4r\nMGNYw5Nn5LHmAs8AEGOcsg81A6Hx45ZbbgnG2CoWFy5cCMZYM4GXbGF4eDjoawTPbNHR0ZE7xjMF\nxFQ1sXgPnts1qmOkyj32KM3U3U033RS8T2s080xCdg1jkwBYvOQQ9kH9GN0NDQ0FY6yJwzvO2bNn\nc+cYg2e2uuOOOzLtWCNVTAUni6c7m3BhIokLmqm5KVOmBNf98uXLmbZ3j4hZhxjjlFfZylaA8kx3\ndj2tacrDJhgCgK+//jroayRhjnfPsIllbMWuetjPuKdn+3mO+Vx6a33p0qW684jJSLUUwBoAX4nI\naNqctwGsFpHFABTAPwD8LOJYhMRC3ZHUUHOkcGLcy/sBeJb7XqePkKZA3ZHUUHMkBcxIRQghhCQi\nNzlGU08W8cC4FzfzYg8W+3d2D6+Ygv0bvhd3sLEZL/FEo3EW+34vXryYe5wYvHX0HuKOiUna9b96\n9WowxsZw68Rvo5NjNBMR0UYSCkyfnn0yxHtNjO685AEWL5bXLN3deeedua+zyV0axdNYbBEGS8z7\nt/4O77ixyTGaiYhoI4lTZsyYkWl719fGhr17uHfPtOO89bSv88bE7BleIhd7T2iW18C713keDZtE\nw/MGWU9ATIy9jpbr3uv4TZcQQghJBDddQgghJBHcdAkhhJBEcNMlhBBCEpHaSHURwBCAGQDqPz3c\nunDeE6NTVcMSTQVTct2Vcc5A68ybmmsMznti1NVd0k33+5OK9JUxPynnXW7KuA5lnDNQ3nk3m7Ku\nA+ddHPzzMiGEEJIIbrqEEEJIIiZr090ySeedKJx3uSnjOpRxzkB5591syroOnHdBTEpMlxBCCLkR\n4Z+XCSGEkERw0yWEEEISkXzTFZEnROSEiJwSkbdSnz8WEflARC6IyNExfbeJyF9EZLD23+nXO0Zq\nRKRDRPaKyHEROSYir9b6W3reRUPNFQt150PdFUeZNZd00xWRKQB+B2AFgG5Ui0N3p5zDONgG4AnT\n9xaAPao6H8CeWruVGAHwhqp2A3gEwCu19W31eRcGNZcE6s5A3RVOaTWX+pvuQwBOqeoZVf03gD8B\nWJl4DlGo6t8A/Mt0rwSwvfbzdgCrkk4qB1UdVtXDtZ+vAhgAMBstPu+CoeYKhrpzoe4KpMyaS73p\nzgbwzzHts7W+sjBTVYdrP58DMHMyJ3M9RKQLwBIAB1CieRcANZcQ6u57qLtElE1zNFI1iFaftWrJ\n561EZCqAjwG8pqpXxv6uledNrk+rXzvq7v+TVr52ZdRc6k33GwAdY9pzan1l4byItANA7b8XJnk+\nASJyM6oi3KGqu2rdLT/vAqHmEkDdBVB3BVNWzaXedP8OYL6I/EhEfgDgpwB2J57DRNgNYG3t57UA\nPp3EuQSIiAB4H8CAqvaM+VVLz7tgqLmCoe5cqLsCKbXmVDXpPwBPAjgJ4DSAX6Q+/zjm+UcAwwD+\ng2o8Zh2AH6LqiBsE8FcAt032PM2cl6H655QjAPpr/55s9XknWBdqrth5U3f+ulB3xc25tJpjGkhC\nCCEkETRSEUIIIYngpksIIYQkgpsuIYQQkghuuoQQQkgiuOkSQgghieCmSwghhCSCmy4hhBCSiP8B\nWo0p4EUpgXcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emKAhl5VTgyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iq1TfmdTg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer2 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer2_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1vZmGsyTg3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#layer1 visualization\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "columns = 3\n",
        "rows = 3\n",
        "for i in range(1, columns*rows +1):\n",
        "    img=cv2.imread('latent_layer1_'+str(i)+'.png')\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HJvYuG-CYE",
        "colab_type": "text"
      },
      "source": [
        "SO IT SEEMS THAT THE LATENT SPACE IS VERY CONTINUOUS BUT IN REALITY IT IS DISCRETE IN CASE OF A NORMAL AUTOENCODER TO VISUALIZE IT WE NEED A LARGER DIMENSION DATASET TO PROOVE IT WE NEED TO PLOT THE LATENT SPACE BY CONVERTING IT INTO TWO DIMENSIONS USING PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvIql91TgwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WZqSH9vTgtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here is all it begins and ends\n",
        "obj=training(plot=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axzG2TQeOhNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}